@startuml modelos_tensores_3d
!theme plain
!define SCALE 3
!define DPI 800
skinparam dpi 800
skinparam backgroundColor white
skinparam defaultFontSize 9
skinparam titleFontSize 16
skinparam rectangleBackgroundColor white
skinparam rectangleBorderColor black
skinparam rectangleBorderThickness 2
skinparam minClassWidth 90
skinparam minClassHeight 75
skinparam padding 3

title **Arquitecturas de Redes Neuronales - RepresentaciÃ³n de Tensores 3D**

' Layout horizontal para los 3 modelos
left to right direction

' AlineaciÃ³n de todos los modelos al mismo nivel
skinparam packageAlignment top

' ConvLSTM Model
package "01. ConvLSTM Encoder-Decoder + Attention" as convlstm_arch {
    left to right direction
    
    ' Input - Large block representing temporal data
    rectangle "<b>INPUT</b>\n(60, 45, 90, 11)\n\nğŸ“Š Temporal\nğŸŒ Spatial\nğŸ“ˆ Features" as input_conv #E8F4FD {
        rectangle "    " as input_vol #D4E6F1
    }
    
    ' ConvLSTM Layer 1 - Wider block for more channels
    rectangle "<b>ConvLSTM2D-1</b>\n(60, 45, 90, 64)\n\nğŸ§  64 filters\nâš¡ 3Ã—3 kernel\nğŸ”„ Temporal" as conv_1 #BBDEFB {
        rectangle "        " as conv1_vol #90CAF9
    }
    
    ' ConvLSTM Layer 2 - Medium block
    rectangle "<b>ConvLSTM2D-2</b>\n(60, 45, 90, 32)\n\nğŸ§  32 filters\nâš¡ 3Ã—3 kernel\nğŸ”„ Sequence" as conv_2 #90CAF9 {
        rectangle "    " as conv2_vol #64B5F6
    }
    
    ' Attention - Same size but different color
    rectangle "<b>CBAM Attention</b>\n(60, 45, 90, 32)\n\nğŸ¯ Channel\nğŸ—ºï¸ Spatial\nâš¡ Weights" as att_conv #64B5F6 {
        rectangle "    " as att_vol #42A5F5
    }
    
    ' Decoder - Smaller temporal dimension
    rectangle "<b>ConvLSTM2D-3</b>\n(45, 90, 16)\n\nğŸ§  16 filters\nğŸ”„ Decoder\nğŸ“‰ Reduced" as dec_conv #42A5F5 {
        rectangle "  " as dec_vol #2196F3
    }
    
    ' Output - Multi-horizon
    rectangle "<b>OUTPUT</b>\n(3, 45, 90, 1)\n\nğŸ¯ t+1, t+2, t+3\nğŸŒ§ï¸ Precipitation\nğŸ“Š Prediction" as out_conv #2196F3 {
        rectangle " " as out_vol #1976D2
    }
    
    input_conv -right-> conv_1 : <size:8>Encoding</size>
    conv_1 -right-> conv_2
    conv_2 -right-> att_conv : <size:8>Attention</size>
    att_conv -right-> dec_conv : <size:8>Decoding</size>
    dec_conv -right-> out_conv
}

' ConvGRU Model
package "02. ConvGRU Residual + BatchNorm" as convgru_arch {
    left to right direction
    
    rectangle "<b>INPUT</b>\n(60, 45, 90, 11)\n\nğŸ“Š Temporal\nğŸŒ Spatial\nğŸ“ˆ Features" as input_gru #DCEDC8 {
        rectangle "    " as input_gru_vol #C5E1A5
    }
    
    rectangle "<b>Projection</b>\n(60, 45, 90, 32)\n\nğŸ”„ Conv2D 1Ã—1\nğŸ“ Feature proj\nğŸ¯ Dimension" as proj_gru #C5E1A5 {
        rectangle "    " as proj_vol #AED581
    }
    
    rectangle "<b>ConvGRU2D-1</b>\n(60, 45, 90, 64)\n\nğŸ§  64 filters\nâš¡ 3Ã—3 kernel\nğŸ”„ GRU gates" as gru1 #AED581 {
        rectangle "        " as gru1_vol #9CCC65
    }
    
    rectangle "<b>ConvGRU2D-2</b>\n(45, 90, 32)\n\nğŸ§  32 filters\nğŸ“‰ Last timestep\nğŸ”„ GRU output" as gru2 #9CCC65 {
        rectangle "    " as gru2_vol #8BC34A
    }
    
    rectangle "<b>Residual Add</b>\n(45, 90, 32)\n\nâ• Element-wise\nğŸ”— Skip connection\nâš¡ ReLU" as add_gru #8BC34A {
        rectangle "    " as add_vol #7CB342
    }
    
    rectangle "<b>OUTPUT</b>\n(3, 45, 90, 1)\n\nğŸ¯ t+1, t+2, t+3\nğŸŒ§ï¸ Precipitation\nğŸ“Š Prediction" as out_gru #689F38 {
        rectangle " " as out_gru_vol #558B2F
    }
    
    ' Skip connection representation
    rectangle "<b>SKIP</b>\n(45, 90, 32)\n\nğŸ”— Residual\nğŸ“ 1Ã—1 Conv\nâš¡ Identity" as skip_conn #7CB342 {
        rectangle "  " as skip_vol #689F38
    }
    
    input_gru -right-> proj_gru
    proj_gru -right-> gru1 : <size:8>GRU Encoding</size>
    gru1 -right-> gru2
    gru2 -right-> add_gru : <size:8>Residual</size>
    
    ' Skip path horizontal
    input_gru -down-> skip_conn : <size:8>Skip Path</size>
    skip_conn -up-> add_gru
    
    add_gru -right-> out_gru
}

' Transformer Model
package "03. Transformer HÃ­brido CNN + LSTM" as transformer_arch {
    left to right direction
    
    rectangle "<b>INPUT</b>\n(60, 45, 90, 11)\n\nğŸ“Š Temporal\nğŸŒ Spatial\nğŸ“ˆ Features" as input_trans #FFE0B2 {
        rectangle "    " as input_trans_vol #FFCC02
    }
    
    rectangle "<b>CNN Layer 1</b>\n(60, 45, 90, 64)\n\nğŸ–¼ï¸ Conv2D 3Ã—3\nğŸ§  64 filters\nğŸ“ Feature maps" as cnn1_trans #FFCC02 {
        rectangle "        " as cnn1_vol #FFB74D
    }
    
    rectangle "<b>CNN Layer 2</b>\n(60, 45, 90, 32)\n\nğŸ–¼ï¸ Conv2D 3Ã—3\nğŸ§  32 filters\nğŸ“ Refinement" as cnn2_trans #FFB74D {
        rectangle "    " as cnn2_vol #FFA726
    }
    
    rectangle "<b>MaxPool</b>\n(60, 22, 45, 32)\n\nğŸ“‰ 2Ã—2 pooling\nğŸ—œï¸ Spatial reduce\nğŸ“ Downsampling" as pool_trans #FFA726 {
        rectangle "  " as pool_vol #FF9800
    }
    
    rectangle "<b>Flatten</b>\n(60, 31680)\n\nğŸ“Š Flattened\nğŸ”¢ 31680 features\nğŸ“ Vector" as flat_trans #FF9800 {
        rectangle "                    " as flat_vol #F57C00
    }
    
    rectangle "<b>Multi-Head\nAttention</b>\n(60, 31680)\n\nğŸ¯ 4 heads\nâš¡ Self-attention\nğŸ”„ Temporal" as att_trans #F57C00 {
        rectangle "                    " as att_trans_vol #EF6C00
    }
    
    rectangle "<b>LSTM</b>\n(128)\n\nğŸ§  128 hidden\nğŸ”„ Sequence agg\nğŸ“Š Final encoding" as lstm_trans #EF6C00 {
        rectangle "      " as lstm_vol #E65100
    }
    
    rectangle "<b>OUTPUT</b>\n(3, 45, 90, 1)\n\nğŸ¯ Dense + Reshape\nğŸŒ§ï¸ Precipitation\nğŸ“Š Multi-horizon" as out_trans #E65100 {
        rectangle " " as out_trans_vol #D84315
    }
    
    input_trans -right-> cnn1_trans : <size:8>CNN Encoding</size>
    cnn1_trans -right-> cnn2_trans
    cnn2_trans -right-> pool_trans : <size:8>Spatial Reduction</size>
    pool_trans -right-> flat_trans
    flat_trans -right-> att_trans : <size:8>Attention</size>
    att_trans -right-> lstm_trans : <size:8>Temporal Agg</size>
    lstm_trans -right-> out_trans : <size:8>Reconstruction</size>
}

' OrganizaciÃ³n horizontal de los 3 modelos (01, 02, 03)
convlstm_arch -[hidden]right-> convgru_arch
convgru_arch -[hidden]right-> transformer_arch

' AlineaciÃ³n de inputs al mismo nivel horizontal
input_conv -[hidden]right-> input_gru
input_gru -[hidden]right-> input_trans

' AlineaciÃ³n de outputs al mismo nivel horizontal
out_conv -[hidden]right-> out_gru
out_gru -[hidden]right-> out_trans

' Performance legend
note bottom of transformer_arch
<size:11><b>ğŸ† Model Performance</b></size>

<size:10><b>ConvLSTM + Attention:</b></size>
<size:9>â€¢ RMSE: 0.145 | MAE: 0.089</size>
<size:9>â€¢ RÂ²: 0.78 | Params: 2.1M</size>
<size:9>â€¢ Training: 45 min/epoch</size>

<size:10><b>ConvGRU Residual:</b></size>
<size:9>â€¢ RMSE: 0.142 | MAE: 0.087</size>
<size:9>â€¢ RÂ²: 0.79 | Params: 1.8M</size>
<size:9>â€¢ Training: 32 min/epoch</size>

<size:10><b>Transformer HÃ­brido:</b></size>
<size:9>â€¢ RMSE: 0.138 | MAE: 0.084</size>
<size:9>â€¢ RÂ²: 0.81 | Params: 2.5M</size>
<size:9>â€¢ Training: 52 min/epoch</size>

<size:10><b>ğŸ“ Data Specifications:</b></size>
<size:9>â€¢ Temporal: 60 months (5 years)</size>
<size:9>â€¢ Spatial: 45Ã—90 (BoyacÃ¡, 0.05Â°)</size>
<size:9>â€¢ Features: 11 (BASE+KCE+PAFC)</size>
<size:9>â€¢ Output: t+1, t+2, t+3 months</size>
end note

@enduml
