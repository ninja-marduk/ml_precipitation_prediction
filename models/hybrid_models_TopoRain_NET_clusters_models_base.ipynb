{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f64a97",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/hybrid_models_TopoRain_NET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "itarOiHGzTAU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "itarOiHGzTAU",
    "outputId": "92741f25-a5ad-49cd-ed9e-e851ec87059e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-29 19:28:55,478 [INFO] Configuraci√≥n de threading de TensorFlow aplicada\n",
      "Entorno configurado. Usando ruta base: ..\n",
      "Entorno configurado. Usando ruta base: ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/dask/dataframe/_pyarrow_compat.py:15: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 13.0.0. Please consider upgrading.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-29 19:28:57,939 [INFO] Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\n",
      "2025-05-29 19:28:58,929 [INFO] Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 19:28:58,929 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 19:28:58,930 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 19:28:58,930 [INFO] Definiendo m√°scaras para los niveles de elevaci√≥n...\n",
      "2025-05-29 19:28:58,931 [INFO] Distribuci√≥n de celdas por nivel de elevaci√≥n:\n",
      "2025-05-29 19:28:58,931 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 19:28:58,932 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 19:28:58,932 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 19:28:58,929 [INFO] Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 19:28:58,929 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 19:28:58,930 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 19:28:58,930 [INFO] Definiendo m√°scaras para los niveles de elevaci√≥n...\n",
      "2025-05-29 19:28:58,931 [INFO] Distribuci√≥n de celdas por nivel de elevaci√≥n:\n",
      "2025-05-29 19:28:58,931 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 19:28:58,932 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 19:28:58,932 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 19:28:58,940 [INFO] Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\n",
      "2025-05-29 19:28:58,940 [INFO] Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\n",
      "2025-05-29 19:28:59,058 [INFO] Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 19:28:59,059 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 19:28:59,060 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 19:28:59,060 [INFO] Definiendo m√°scaras para los niveles de elevaci√≥n...\n",
      "2025-05-29 19:28:59,061 [INFO] Distribuci√≥n de celdas por nivel de elevaci√≥n:\n",
      "2025-05-29 19:28:59,062 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 19:28:59,062 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 19:28:59,063 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 19:28:59,065 [INFO] Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\n",
      "2025-05-29 19:28:59,058 [INFO] Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 19:28:59,059 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 19:28:59,060 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 19:28:59,060 [INFO] Definiendo m√°scaras para los niveles de elevaci√≥n...\n",
      "2025-05-29 19:28:59,061 [INFO] Distribuci√≥n de celdas por nivel de elevaci√≥n:\n",
      "2025-05-29 19:28:59,062 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 19:28:59,062 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 19:28:59,063 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 19:28:59,065 [INFO] Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\n",
      "2025-05-29 19:28:59,183 [INFO] Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 19:28:59,184 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 19:28:59,184 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 19:28:59,185 [INFO] Definiendo m√°scaras para los niveles de elevaci√≥n...\n",
      "2025-05-29 19:28:59,186 [INFO] Distribuci√≥n de celdas por nivel de elevaci√≥n:\n",
      "2025-05-29 19:28:59,186 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 19:28:59,187 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 19:28:59,187 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 19:28:59,183 [INFO] Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 19:28:59,184 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 19:28:59,184 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 19:28:59,185 [INFO] Definiendo m√°scaras para los niveles de elevaci√≥n...\n",
      "2025-05-29 19:28:59,186 [INFO] Distribuci√≥n de celdas por nivel de elevaci√≥n:\n",
      "2025-05-29 19:28:59,186 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 19:28:59,187 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 19:28:59,187 [INFO]   Nivel 3 (>2264m): 996 celdas\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TopoRain-Net: entrenamiento y evaluaci√≥n de modelos espec√≠ficos por nivel de elevaci√≥n.\n",
    "Implementa modelos BiGRU autoencoder-decoder para cada nivel de elevaci√≥n,\n",
    "con fusi√≥n optimizada de caracter√≠sticas CEEMDAN y TFV-EMD usando XGBoost.\n",
    "Un meta-modelo integra las predicciones de los tres modelos de elevaci√≥n.\n",
    "Genera m√©tricas, scatter, mapas y tablas (global, por elevaci√≥n, por percentiles).\n",
    "\"\"\"\n",
    "\n",
    "import warnings, logging\n",
    "from pathlib import Path\n",
    "# Configuraci√≥n del entorno (compatible con Colab y local)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuraci√≥n de logging y trazabilidad mejorada\n",
    "# -----------------------------------------------------------------------------\n",
    "# Crear directorio para logs\n",
    "LOG_DIR = Path(\"logs\")\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configurar formato de timestamp\n",
    "timestamp_format = \"%Y-%m-%d_%H-%M-%S\"\n",
    "run_timestamp = datetime.datetime.now().strftime(timestamp_format)\n",
    "log_filename = f\"toporain_net_run_{run_timestamp}.log\"\n",
    "\n",
    "# Configurar logging con formato detallado y salida a archivo\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_DIR / log_filename),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Clase para trazabilidad del proceso\n",
    "class ProcessTracker:\n",
    "    def __init__(self, name=\"TopoRain-NET\"):\n",
    "        self.name = name\n",
    "        self.start_time = time.time()\n",
    "        self.section_times = {}\n",
    "        self.current_section = None\n",
    "        self.section_start = None\n",
    "        self.metrics = defaultdict(dict)\n",
    "        self.resources = []\n",
    "        self.checkpoints = []\n",
    "        \n",
    "    def start_section(self, section_name):\n",
    "        \"\"\"Inicia el cron√≥metro para una secci√≥n del proceso\"\"\"\n",
    "        if self.current_section:\n",
    "            self.end_section()\n",
    "            \n",
    "        self.current_section = section_name\n",
    "        self.section_start = time.time()\n",
    "        logger.info(f\"‚ñ∂Ô∏è INICIANDO: {section_name}\")\n",
    "        # Registrar recursos al inicio\n",
    "        self._log_resources()\n",
    "        \n",
    "    def end_section(self):\n",
    "        \"\"\"Finaliza la secci√≥n actual y registra el tiempo transcurrido\"\"\"\n",
    "        if not self.current_section:\n",
    "            return\n",
    "            \n",
    "        elapsed = time.time() - self.section_start\n",
    "        self.section_times[self.current_section] = elapsed\n",
    "        logger.info(f\"‚úì COMPLETADO: {self.current_section} en {elapsed:.2f} segundos\")\n",
    "        # Registrar recursos al final\n",
    "        self._log_resources()\n",
    "        self.current_section = None\n",
    "        \n",
    "    def log_metric(self, section, metric_name, value):\n",
    "        \"\"\"Registra una m√©trica\"\"\"\n",
    "        self.metrics[section][metric_name] = value\n",
    "        logger.info(f\"üìä M√âTRICA: {section} - {metric_name}: {value}\")\n",
    "        \n",
    "    def add_checkpoint(self, description, data=None):\n",
    "        \"\"\"A√±ade un punto de control con datos opcionales\"\"\"\n",
    "        checkpoint = {\n",
    "            'timestamp': time.time(),\n",
    "            'description': description,\n",
    "            'elapsed_total': time.time() - self.start_time,\n",
    "            'data': data\n",
    "        }\n",
    "        self.checkpoints.append(checkpoint)\n",
    "        logger.info(f\"üîñ CHECKPOINT: {description}\")\n",
    "        \n",
    "    def _log_resources(self):\n",
    "        \"\"\"Registra el uso de recursos actual\"\"\"\n",
    "        mem_info = get_memory_info()\n",
    "        cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "        \n",
    "        # Obtener informaci√≥n de GPU si est√° disponible\n",
    "        gpu_info = get_gpu_memory_info()\n",
    "        gpu_usage = None\n",
    "        if gpu_info and gpu_info[0]['memory_used_mb'] > 0:\n",
    "            gpu_usage = {\n",
    "                'used_mb': gpu_info[0]['memory_used_mb'],\n",
    "                'total_mb': gpu_info[0]['memory_total_mb'],\n",
    "                'percent': gpu_info[0]['memory_used_percent']\n",
    "            }\n",
    "        \n",
    "        resources = {\n",
    "            'timestamp': time.time(),\n",
    "            'memory_used_gb': mem_info['total_gb'] - mem_info['free_gb'],\n",
    "            'memory_total_gb': mem_info['total_gb'],\n",
    "            'memory_percent': mem_info['used_percent'],\n",
    "            'cpu_percent': cpu_percent,\n",
    "            'gpu': gpu_usage\n",
    "        }\n",
    "        self.resources.append(resources)\n",
    "        \n",
    "    def _convert_numpy_types(self, obj):\n",
    "        \"\"\"\n",
    "        Convierte recursivamente tipos de numpy a tipos nativos de Python\n",
    "        para hacer el objeto JSON serializable\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (np.bool_)):\n",
    "            return bool(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self._convert_numpy_types(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_numpy_types(item) for item in obj]\n",
    "        elif isinstance(obj, tuple):\n",
    "            return tuple(self._convert_numpy_types(item) for item in obj)\n",
    "        else:\n",
    "            return obj\n",
    "        \n",
    "    def summary(self):\n",
    "        \"\"\"Genera un resumen del proceso\"\"\"\n",
    "        total_time = time.time() - self.start_time\n",
    "        \n",
    "        # Calcular estad√≠sticas de recursos\n",
    "        if self.resources:\n",
    "            avg_mem = sum(r['memory_percent'] for r in self.resources) / len(self.resources)\n",
    "            max_mem = max(r['memory_percent'] for r in self.resources)\n",
    "            avg_cpu = sum(r['cpu_percent'] for r in self.resources) / len(self.resources)\n",
    "            max_cpu = max(r['cpu_percent'] for r in self.resources)\n",
    "        else:\n",
    "            avg_mem = max_mem = avg_cpu = max_cpu = 0\n",
    "        \n",
    "        summary_dict = {\n",
    "            'name': self.name,\n",
    "            'total_time': total_time,\n",
    "            'start_time': self.start_time,\n",
    "            'end_time': time.time(),\n",
    "            'section_times': self.section_times,\n",
    "            'metrics': dict(self.metrics),\n",
    "            'resources': {\n",
    "                'avg_memory_percent': avg_mem,\n",
    "                'max_memory_percent': max_mem,\n",
    "                'avg_cpu_percent': avg_cpu,\n",
    "                'max_cpu_percent': max_cpu\n",
    "            },\n",
    "            'num_checkpoints': len(self.checkpoints)\n",
    "        }\n",
    "        \n",
    "        # Convertir tipos numpy a tipos nativos de Python para JSON\n",
    "        summary_dict = self._convert_numpy_types(summary_dict)\n",
    "        \n",
    "        # Guardar resumen en formato JSON\n",
    "        summary_path = LOG_DIR / f\"summary_{run_timestamp}.json\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary_dict, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"üìë RESUMEN DEL PROCESO GUARDADO: {summary_path}\")\n",
    "        \n",
    "        # Imprimir resumen\n",
    "        logger.info(f\"üìã RESUMEN DE EJECUCI√ìN - {self.name}\")\n",
    "        logger.info(f\"  Tiempo total: {total_time:.2f} segundos\")\n",
    "        logger.info(f\"  Secciones completadas: {len(self.section_times)}\")\n",
    "        for section, time_taken in sorted(self.section_times.items(), key=lambda x: x[1], reverse=True):\n",
    "            logger.info(f\"    - {section}: {time_taken:.2f} segundos\")\n",
    "        logger.info(f\"  Checkpoints registrados: {len(self.checkpoints)}\")\n",
    "        logger.info(f\"  Uso de recursos:\")\n",
    "        logger.info(f\"    - Memoria promedio: {avg_mem:.1f}%\")\n",
    "        logger.info(f\"    - Memoria m√°xima: {max_mem:.1f}%\")\n",
    "        logger.info(f\"    - CPU promedio: {avg_cpu:.1f}%\")\n",
    "        logger.info(f\"    - CPU m√°xima: {max_cpu:.1f}%\")\n",
    "        \n",
    "        return summary_dict\n",
    "\n",
    "# Inicializar el rastreador de procesos\n",
    "tracker = ProcessTracker()\n",
    "\n",
    "# Funci√≥n para decorar funciones con trazabilidad\n",
    "def trace(section_name=None):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            func_name = section_name or func.__name__\n",
    "            tracker.start_section(func_name)\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                tracker.end_section()\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå ERROR en {func_name}: {str(e)}\")\n",
    "                tracker.end_section()\n",
    "                raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Intentar configurar el paralelismo antes de cualquier operaci√≥n que inicialice el contexto\n",
    "try:\n",
    "    # Configurar threading para TensorFlow\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "    logger.info(\"Configuraci√≥n de threading de TensorFlow aplicada\")\n",
    "except RuntimeError as e:\n",
    "    # Si ya se inicializ√≥ el contexto, informar pero seguir adelante\n",
    "    logger.warning(f\"No se pudo configurar threading de TensorFlow: {str(e)}. Continuando con valores por defecto.\")\n",
    "\n",
    "# Detectar si estamos en Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Si estamos en Colab, clonar el repositorio\n",
    "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
    "    %cd ml_precipitation_prediction\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy\n",
    "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
    "else:\n",
    "    # Si estamos en local, usar la ruta actual\n",
    "    if '/models' in os.getcwd():\n",
    "        BASE_PATH = Path('..')\n",
    "    else:\n",
    "        BASE_PATH = Path('.')\n",
    "\n",
    "BASE = Path(BASE_PATH)\n",
    "print(f\"Entorno configurado. Usando ruta base: {BASE}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FULL_NC      = BASE/\"data\"/\"output\"/\"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
    "FUSION_NC    = BASE/\"models\"/\"output\"/\"features_fusion_branches.nc\"\n",
    "TRAINED_DIR  = BASE/\"models\"/\"output\"/\"trained_models\"\n",
    "TRAINED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PRED_DIR = BASE/\"models\"/\"output\"/\"predictions\"\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HISTORY_DIR = BASE/\"models\"/\"output\"/\"histories\"\n",
    "HISTORY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INPUT_WINDOW   = 60\n",
    "OUTPUT_HORIZON = 3\n",
    "\n",
    "import numpy            as np\n",
    "import pandas           as pd\n",
    "import xarray           as xr\n",
    "import geopandas        as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs      as ccrs\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics        import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from xgboost                import XGBRegressor\n",
    "# A√±adir importaci√≥n de LightGBM y reducci√≥n de dimensionalidad\n",
    "from lightgbm               import LGBMRegressor\n",
    "from sklearn.decomposition  import PCA\n",
    "from sklearn.pipeline       import Pipeline\n",
    "\n",
    "from tensorflow.keras.models    import Sequential, Model\n",
    "from tensorflow.keras.layers    import Input, Dense, LSTM, GRU, Flatten, Reshape, Dropout, Concatenate, BatchNormalization, TimeDistributed, RepeatVector, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Importar TensorFlow aqu√≠ y configurarlo antes de cualquier operaci√≥n\n",
    "\n",
    "# Actualizar importaci√≥n de mixed_precision para compatibilidad con versiones recientes de TF\n",
    "try:\n",
    "    # Para TensorFlow 2.4+\n",
    "    from tensorflow.keras import mixed_precision\n",
    "except ImportError:\n",
    "    # Fallback para versiones m√°s antiguas de TF\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "import ace_tools_open as tools\n",
    "\n",
    "# Configurar crecimiento de memoria GPU din√°mico para evitar ResourceExhaustedError\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logger.info(f\"GPU configurada para crecimiento din√°mico de memoria: {len(gpus)} GPUs disponibles\")\n",
    "    except RuntimeError as e:\n",
    "        logger.error(f\"Error configurando GPU: {str(e)}\")\n",
    "\n",
    "# Tambi√©n limitar la memoria de TensorFlow para operaciones CPU\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "\n",
    "# Funciones auxiliares para gesti√≥n eficiente de memoria\n",
    "def get_memory_info():\n",
    "    \"\"\"Obtiene informaci√≥n de memoria del sistema\"\"\"\n",
    "    mem_info = psutil.virtual_memory()\n",
    "    return {\n",
    "        'total_gb': mem_info.total / (1024**3),\n",
    "        'available_gb': mem_info.available / (1024**3),\n",
    "        'used_percent': mem_info.percent,\n",
    "        'free_gb': mem_info.free / (1024**3)\n",
    "    }\n",
    "\n",
    "# Funciones auxiliares para persistencia de modelos\n",
    "def get_model_path(model_type, level_name, component_idx=None):\n",
    "    \"\"\"\n",
    "    Genera la ruta para guardar o cargar un modelo espec√≠fico\n",
    "    \n",
    "    Args:\n",
    "        model_type: Tipo de modelo ('fusion', 'bigru', 'meta')\n",
    "        level_name: Nombre del nivel de elevaci√≥n\n",
    "        component_idx: √çndice de componente (para modelos de fusi√≥n)\n",
    "        \n",
    "    Returns:\n",
    "        Path: Ruta completa del archivo del modelo\n",
    "    \"\"\"\n",
    "    if model_type == 'fusion':\n",
    "        return TRAINED_DIR / f\"fusion_xgb_{level_name}_comp{component_idx}.pkl\"\n",
    "    elif model_type == 'bigru':\n",
    "        return TRAINED_DIR / f\"BiGRU_{level_name}_model.keras\"\n",
    "    elif model_type == 'meta':\n",
    "        return TRAINED_DIR / \"meta_fusion_model.pkl\"\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo de modelo no reconocido: {model_type}\")\n",
    "\n",
    "def save_model(model, model_type, level_name, component_idx=None, extra_info=None):\n",
    "    \"\"\"\n",
    "    Guarda un modelo con su informaci√≥n asociada\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a guardar\n",
    "        model_type: Tipo de modelo ('fusion', 'bigru', 'meta')\n",
    "        level_name: Nombre del nivel de elevaci√≥n\n",
    "        component_idx: √çndice de componente (para modelos de fusi√≥n)\n",
    "        extra_info: Informaci√≥n adicional a guardar (pesos, m√©tricas, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True si se guard√≥ correctamente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_path = get_model_path(model_type, level_name, component_idx)\n",
    "        \n",
    "        # Para modelos XGBoost y otros que requieren pickle\n",
    "        if model_type in ['fusion', 'meta']:\n",
    "            with open(model_path, 'wb') as f:\n",
    "                import pickle\n",
    "                data_to_save = {'model': model}\n",
    "                if extra_info:\n",
    "                    data_to_save['info'] = extra_info\n",
    "                pickle.dump(data_to_save, f)\n",
    "        \n",
    "        # Para modelos Keras\n",
    "        elif model_type == 'bigru':\n",
    "            model.save(model_path)\n",
    "            \n",
    "            # Si hay info adicional, guardarla por separado\n",
    "            if extra_info:\n",
    "                info_path = model_path.parent / f\"{model_path.stem}_info.pkl\"\n",
    "                with open(info_path, 'wb') as f:\n",
    "                    import pickle\n",
    "                    pickle.dump(extra_info, f)\n",
    "        \n",
    "        logger.info(f\"Modelo {model_type} para {level_name} guardado en: {model_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al guardar modelo {model_type} para {level_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def load_model(model_type, level_name, component_idx=None):\n",
    "    \"\"\"\n",
    "    Carga un modelo previamente guardado\n",
    "    \n",
    "    Args:\n",
    "        model_type: Tipo de modelo ('fusion', 'bigru', 'meta')\n",
    "        level_name: Nombre del nivel de elevaci√≥n\n",
    "        component_idx: √çndice de componente (para modelos de fusi√≥n)\n",
    "        \n",
    "    Returns:\n",
    "        model: Modelo cargado o None si no existe\n",
    "        extra_info: Informaci√≥n adicional o None si no existe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_path = get_model_path(model_type, level_name, component_idx)\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            return None, None\n",
    "            \n",
    "        # Para modelos XGBoost y otros almacenados con pickle\n",
    "        if model_type in ['fusion', 'meta']:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                import pickle\n",
    "                data = pickle.load(f)\n",
    "                if isinstance(data, dict) and 'model' in data:\n",
    "                    model = data['model']\n",
    "                    extra_info = data.get('info')\n",
    "                else:\n",
    "                    # Compatibilidad con formato antiguo\n",
    "                    model = data\n",
    "                    extra_info = None\n",
    "        \n",
    "        # Para modelos Keras\n",
    "        elif model_type == 'bigru':\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "            \n",
    "            # Intentar cargar info adicional si existe\n",
    "            extra_info = None\n",
    "            info_path = model_path.parent / f\"{model_path.stem}_info.pkl\"\n",
    "            if info_path.exists():\n",
    "                with open(info_path, 'rb') as f:\n",
    "                    import pickle\n",
    "                    extra_info = pickle.load(f)\n",
    "        \n",
    "        logger.info(f\"Modelo {model_type} para {level_name} cargado desde: {model_path}\")\n",
    "        return model, extra_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al cargar modelo {model_type} para {level_name}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def model_exists(model_type, level_name, component_idx=None):\n",
    "    \"\"\"\n",
    "    Verifica si existe un modelo previamente guardado\n",
    "    \n",
    "    Args:\n",
    "        model_type: Tipo de modelo ('fusion', 'bigru', 'meta')\n",
    "        level_name: Nombre del nivel de elevaci√≥n\n",
    "        component_idx: √çndice de componente (para modelos de fusi√≥n)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True si el modelo existe\n",
    "    \"\"\"\n",
    "    model_path = get_model_path(model_type, level_name, component_idx)\n",
    "    return model_path.exists()\n",
    "\n",
    "# Funciones auxiliares para monitorear la memoria de la GPU\n",
    "def get_gpu_memory_info():\n",
    "    \"\"\"Obtiene la informaci√≥n de memoria de la GPU disponible\"\"\"\n",
    "    if not gpus:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Intentar usar NVIDIA-SMI a trav√©s de subprocess if est√° disponible\n",
    "        import subprocess\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used,memory.free,memory.total', '--format=csv,noheader,nounits'],\n",
    "            encoding='utf-8')\n",
    "        gpu_info = []\n",
    "        for line in result.strip().split('\\n'):\n",
    "            values = [float(x) for x in line.split(',')]\n",
    "            gpu_info.append({\n",
    "                'memory_used_mb': values[0],\n",
    "                'memory_free_mb': values[1],\n",
    "                'memory_total_mb': values[2],\n",
    "                'memory_used_percent': values[0] / values[2] * 100\n",
    "            })\n",
    "        return gpu_info\n",
    "    except (ImportError, subprocess.SubprocessError, FileNotFoundError):\n",
    "        # Si nvidia-smi no est√° disponible, usar tensorflow para obtener informaci√≥n limitada\n",
    "        try:\n",
    "            memory_info = []\n",
    "            for i, gpu in enumerate(gpus):\n",
    "                # En versiones nuevas de TF podemos obtener informaci√≥n de memoria usando experimental.VirtualDeviceConfiguration\n",
    "                try:\n",
    "                    mem_info = tf.config.experimental.get_memory_info(f'GPU:{i}')\n",
    "                    total_memory = mem_info['current'] + mem_info['peak']  # Aproximaci√≥n\n",
    "                    memory_info.append({\n",
    "                        'memory_used_mb': mem_info['current'] / (1024 * 1024),\n",
    "                        'memory_free_mb': (total_memory - mem_info['current']) / (1024 * 1024),\n",
    "                        'memory_total_mb': total_memory / (1024 * 1024),\n",
    "                        'memory_used_percent': mem_info['current'] / total_memory * 100 if total_memory else 0\n",
    "                    })\n",
    "                except (KeyError, AttributeError, ValueError):\n",
    "                    # Si no podemos obtener informaci√≥n espec√≠fica, proveer una estimaci√≥n\n",
    "                    memory_info.append({\n",
    "                        'memory_used_mb': -1,  # No conocido\n",
    "                        'memory_free_mb': -1,  # No conocido\n",
    "                        'memory_total_mb': -1,  # No conocido\n",
    "                        'memory_used_percent': -1  # No conocido\n",
    "                    })\n",
    "            return memory_info\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Funci√≥n mejorada para limpiar la memoria\n",
    "def clear_memory(force_garbage_collection=True):\n",
    "    \"\"\"\n",
    "    Limpia la memoria de manera m√°s agresiva, liberando recursos de TensorFlow y Python\n",
    "\n",
    "    Args:\n",
    "        force_garbage_collection: Si es True, fuerza la recolecci√≥n de basura\n",
    "    \"\"\"\n",
    "    # 1. Limpiar sesi√≥n de TensorFlow para liberar variables y tensores\n",
    "    try:\n",
    "        tf.keras.backend.clear_session()\n",
    "        logger.debug(\"Sesi√≥n de Keras limpiada\")\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Error al limpiar sesi√≥n de Keras: {str(e)}\")\n",
    "\n",
    "    # 2. Reiniciar gr√°fico de operaciones de TF si est√° disponible\n",
    "    try:\n",
    "        # Para versiones antiguas de TF que tienen reset_default_graph\n",
    "        if hasattr(tf, 'reset_default_graph'):\n",
    "            tf.reset_default_graph()\n",
    "            logger.debug(\"Gr√°fico de TF reiniciado\")\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Error al reiniciar gr√°fico de TF: {str(e)}\")\n",
    "\n",
    "    # 3. Forzar recolecci√≥n de basura de Python\n",
    "    if force_garbage_collection:\n",
    "        import gc\n",
    "        # Realizar m√∫ltiples pasadas para asegurar la limpieza completa\n",
    "        collected = gc.collect()\n",
    "        logger.debug(f\"GC recolect√≥ {collected} objetos\")\n",
    "\n",
    "        # Segunda pasada para objetos que posiblemente se liberaron en la primera\n",
    "        collected = gc.collect()\n",
    "        logger.debug(f\"GC recolect√≥ {collected} objetos adicionales\")\n",
    "\n",
    "    # 4. Intentar liberar memoria al sistema operativo\n",
    "    if 'psutil' in sys.modules:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        try:\n",
    "            # En Linux\n",
    "            if hasattr(process, 'memory_full_info'):\n",
    "                mi = process.memory_full_info()\n",
    "                logger.debug(f\"Memoria usada: RSS={mi.rss/1e6:.1f}MB, VMS={mi.vms/1e6:.1f}MB\")\n",
    "\n",
    "            # En sistemas POSIX, sincronizar filesystem para liberar buffers\n",
    "            if hasattr(os, 'sync'):\n",
    "                os.sync()\n",
    "                logger.debug(\"Sincronizado el sistema de archivos\")\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error en operaciones de memoria del proceso: {str(e)}\")\n",
    "\n",
    "    # 5. Intentar liberar la memoria GPU espec√≠ficamente\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Ejecutar operaciones vac√≠as para forzar sincronizaci√≥n de GPU\n",
    "            dummy = tf.random.normal([1, 1])\n",
    "            _ = dummy.numpy()  # Forzar ejecuci√≥n\n",
    "            logger.debug(\"Operaciones GPU sincronizadas\")\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error al sincronizar GPU: {str(e)}\")\n",
    "\n",
    "# Funci√≥n para crear un conjunto de datos de TensorFlow con mejor manejo de errores de memoria\n",
    "def create_tf_dataset(X, Y, batch_size=32, force_cpu=False, max_retries=3):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow Dataset from numpy arrays with batching.\n",
    "    Includes robust error handling with automatic CPU fallback and batch size adjustment.\n",
    "\n",
    "    Args:\n",
    "        X: Input features array\n",
    "        Y: Target labels array\n",
    "        batch_size: Size of batches for training\n",
    "        force_cpu: If True, forces operations to run on CPU\n",
    "        max_retries: Maximum number of retry attempts with smaller batch size\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset object configured for training\n",
    "    \"\"\"\n",
    "    # Verificar la memoria disponible y ajustar par√°metros autom√°ticamente\n",
    "    gpu_info = get_gpu_memory_info()\n",
    "    mem_info = get_memory_info()\n",
    "\n",
    "    # Estimar si la GPU est√° cerca del l√≠mite (>80% usada) para decidir si forzar CPU\n",
    "    auto_force_cpu = False\n",
    "    if gpu_info and not force_cpu:\n",
    "        for gpu in gpu_info:\n",
    "            if gpu['memory_used_percent'] > 80:\n",
    "                logger.warning(f\"GPU usage high ({gpu['memory_used_percent']:.1f}%), forcing CPU execution\")\n",
    "                auto_force_cpu = True\n",
    "                break\n",
    "\n",
    "    # Si hay m√°s de un intento, reducir el batch size\n",
    "    actual_force_cpu = force_cpu or auto_force_cpu\n",
    "    actual_batch_size = batch_size\n",
    "\n",
    "    # Bucle de reintento con tama√±os de batch m√°s peque√±os\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Verificar si hay NaNs antes de convertir a tensores\n",
    "            if np.isnan(X).any() or np.isnan(Y).any():\n",
    "                logger.warning(\"Se detectaron NaNs en los datos. Reemplazando con ceros.\")\n",
    "                X = np.nan_to_num(X, nan=0.0)\n",
    "                Y = np.nan_to_num(Y, nan=0.0)\n",
    "\n",
    "            # Estrategia espec√≠fica para CPU o GPU\n",
    "            if actual_force_cpu:\n",
    "                with tf.device('/CPU:0'):\n",
    "                    # Convertir a tensores expl√≠citamente para mejor control\n",
    "                    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "                    Y_tensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "\n",
    "                    # Crear dataset usando los tensores convertidos\n",
    "                    dataset = tf.data.Dataset.from_tensor_slices((X_tensor, Y_tensor))\n",
    "                    logger.info(f\"Dataset creado en CPU con batch_size={actual_batch_size}\")\n",
    "            else:\n",
    "                # Intentar crear el dataset con GPU\n",
    "                dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "                logger.info(f\"Dataset creado en GPU con batch_size={actual_batch_size}\")\n",
    "\n",
    "            # Configurar el dataset para entrenamiento con un buffer size adaptativo\n",
    "            # Usar buffer size m√°s peque√±o para reducir uso de memoria\n",
    "            samples = len(X)\n",
    "            buffer_size = min(samples, 1000)  # M√°ximo 1000 elementos en memoria\n",
    "\n",
    "            # Ajustar buffer size si la memoria est√° baja\n",
    "            if mem_info['available_gb'] < 2.0:  # Menos de 2GB disponibles\n",
    "                buffer_size = min(buffer_size, 100)  # Reducir a m√°ximo 100 elementos\n",
    "                logger.warning(f\"Memoria disponible baja ({mem_info['available_gb']:.1f}GB), buffer reducido a {buffer_size}\")\n",
    "\n",
    "            dataset = dataset.shuffle(buffer_size=buffer_size, seed=42)\n",
    "            dataset = dataset.batch(actual_batch_size)\n",
    "            dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "            # Probar que el dataset funciona extrayendo un batch\n",
    "            try:\n",
    "                for _ in dataset.take(1):\n",
    "                    pass  # Solo verificar que podemos iterar\n",
    "                logger.info(\"Dataset verificado correctamente\")\n",
    "            except tf.errors.ResourceExhaustedError as e:\n",
    "                raise e  # Relanzar para manejar en el bloque catch\n",
    "\n",
    "            return dataset\n",
    "\n",
    "        except (tf.errors.ResourceExhaustedError, tf.errors.InternalError, tf.errors.FailedPreconditionError,\n",
    "                tf.errors.AbortedError, tf.errors.OOM) as e:\n",
    "            # Si estamos en el √∫ltimo intento, reducir dr√°sticamente\n",
    "            if attempt == max_retries - 1:\n",
    "                logger.error(f\"Error cr√≠tico al crear dataset: {str(e)}\")\n",
    "\n",
    "                # √öltimo intento desesperado: m√≠nimo batch size y forzar CPU\n",
    "                logger.warning(\"Intento final con configuraci√≥n m√≠nima (batch=1, CPU)\")\n",
    "                with tf.device('/CPU:0'):\n",
    "                    logger.info(\"Creando dataset final con configuraci√≥n m√≠nima\")\n",
    "                    # Crear con el menor batch posible\n",
    "                    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "                    Y_tensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "                    dataset = tf.data.Dataset.from_tensor_slices((X_tensor, Y_tensor))\n",
    "                    dataset = dataset.batch(1)  # M√≠nimo batch size\n",
    "                    return dataset\n",
    "            else:\n",
    "                # Reducir batch size y forzar CPU en pr√≥ximo intento\n",
    "                prev_batch = actual_batch_size\n",
    "                actual_batch_size = max(1, actual_batch_size // 2)\n",
    "                actual_force_cpu = True\n",
    "\n",
    "                logger.warning(f\"Intento {attempt+1}/{max_retries}: Reduciendo batch size de {prev_batch} a {actual_batch_size} y forzando CPU\")\n",
    "\n",
    "                # Limpiar memoria antes del pr√≥ximo intento\n",
    "                clear_memory()\n",
    "                time.sleep(1)  # Peque√±a pausa para permitir que el sistema se estabilice\n",
    "\n",
    "# Funci√≥n gen√©rica para predecir en lotes\n",
    "def predict_in_batches(model, X, batch_size=32, verbose=0):\n",
    "    \"\"\"\n",
    "    Genera predicciones de cualquier modelo en lotes para evitar problemas de memoria\n",
    "\n",
    "    Args:\n",
    "        model: Modelo entrenado (Keras, TensorFlow, etc.)\n",
    "        X: Datos de entrada (numpy array)\n",
    "        batch_size: Tama√±o del lote para procesamiento\n",
    "        verbose: Nivel de verbosidad para las predicciones\n",
    "\n",
    "    Returns:\n",
    "        Array con predicciones\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "\n",
    "    # Si X es muy peque√±o, predecir directamente\n",
    "    if n_samples <= batch_size:\n",
    "        return model.predict(X, verbose=verbose)\n",
    "\n",
    "    # Ajustar batch_size seg√∫n memoria disponible\n",
    "    try:\n",
    "        mem_info = get_memory_info()\n",
    "        adaptive_batch = min(batch_size, max(8, int(mem_info['available_gb'] * 10)))\n",
    "        logger.info(f\"Generando predicciones en lotes de {adaptive_batch} muestras\")\n",
    "        batch_size = adaptive_batch\n",
    "    except:\n",
    "        # Si falla la adaptaci√≥n, usar el batch_size proporcionado\n",
    "        logger.info(f\"Generando predicciones en lotes de {batch_size} muestras\")\n",
    "\n",
    "    # Inferir la forma de salida del modelo haciendo una predicci√≥n en un √∫nico ejemplo\n",
    "    try:\n",
    "        sample_pred = model.predict(X[:1], verbose=0)\n",
    "        output_shape = sample_pred.shape[1:]  # Excluye la dimensi√≥n del batch\n",
    "    except:\n",
    "        # Si falla, asumir forma desconocida y manejarla despu√©s\n",
    "        output_shape = None\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # Procesar por lotes\n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        batch_X = X[start_idx:end_idx]\n",
    "\n",
    "        # Para mayor seguridad, comprobar si hay NaNs\n",
    "        has_nans = np.isnan(batch_X).any()\n",
    "        if has_nans:\n",
    "            logger.warning(f\"Detectados NaN en lote {start_idx}-{end_idx}, realizando imputaci√≥n\")\n",
    "            # Reemplazar NaNs con 0 para evitar errores\n",
    "            batch_X = np.nan_to_num(batch_X, nan=0.0)\n",
    "\n",
    "        # Predecir lote\n",
    "        try:\n",
    "            batch_preds = model.predict(batch_X, verbose=0 if start_idx > 0 else verbose)\n",
    "            predictions.append(batch_preds)\n",
    "\n",
    "            # Liberar memoria cada 5 lotes\n",
    "            if (start_idx // batch_size) % 5 == 0 and start_idx > 0:\n",
    "                # Liberar memoria expl√≠citamente\n",
    "                if 'gc' in sys.modules:\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al predecir lote {start_idx}-{end_idx}: {str(e)}\")\n",
    "            # Intentar con un batch m√°s peque√±o como √∫ltimo recurso\n",
    "            try:\n",
    "                smaller_batch = max(1, batch_size // 4)\n",
    "                logger.warning(f\"Reintentando con batch m√°s peque√±o: {smaller_batch}\")\n",
    "                mini_batch_preds = []\n",
    "                for mini_start in range(start_idx, end_idx, smaller_batch):\n",
    "                    mini_end = min(mini_start + smaller_batch, end_idx)\n",
    "                    mini_X = X[mini_start:mini_end]\n",
    "                    mini_pred = model.predict(mini_X, verbose=0)\n",
    "                    mini_batch_preds.append(mini_pred)\n",
    "                batch_preds = np.vstack(mini_batch_preds)\n",
    "                predictions.append(batch_preds)\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"Error en segundo intento de lote: {str(e2)}\")\n",
    "                # Si tambi√©n falla, rellenar con ceros\n",
    "                if output_shape:\n",
    "                    batch_size_curr = end_idx - start_idx\n",
    "                    zeros_shape = (batch_size_curr,) + output_shape\n",
    "                    logger.warning(f\"Rellenando con ceros de forma {zeros_shape}\")\n",
    "                    predictions.append(np.zeros(zeros_shape))\n",
    "                else:\n",
    "                    raise e2\n",
    "\n",
    "    # Concatenar resultados\n",
    "    try:\n",
    "        return np.vstack(predictions)\n",
    "    except:\n",
    "        # Si vstack falla (por ejemplo, formas inconsistentes), devolver una lista\n",
    "        logger.warning(\"No se pudo concatenar predicciones, devolviendo lista de arrays\")\n",
    "        return predictions\n",
    "\n",
    "# Funciones espec√≠ficas para XGBoost con optimizaci√≥n de memoria\n",
    "def predict_xgb_in_batches(model, X, batch_size=100):\n",
    "    \"\"\"\n",
    "    Genera predicciones XGBoost en lotes para evitar problemas de memoria\n",
    "\n",
    "    Args:\n",
    "        model: Modelo XGBoost entrenado\n",
    "        X: Datos de entrada (numpy array)\n",
    "        batch_size: Tama√±o del lote para procesamiento\n",
    "\n",
    "    Returns:\n",
    "        Array con predicciones\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    predictions = np.zeros(n_samples)\n",
    "\n",
    "    # Ajustar tama√±o de lote seg√∫n memoria disponible\n",
    "    mem_info = get_memory_info()\n",
    "    adaptive_batch = min(batch_size, max(10, int(mem_info['available_gb'] * 10)))\n",
    "\n",
    "    logger.info(f\"Generando predicciones XGBoost en lotes de {adaptive_batch} muestras\")\n",
    "\n",
    "    # Procesar por lotes\n",
    "    for start_idx in range(0, n_samples, adaptive_batch):\n",
    "        end_idx = min(start_idx + adaptive_batch, n_samples)\n",
    "        batch_X = X[start_idx:end_idx]\n",
    "\n",
    "        # Para mayor seguridad, comprobar si hay NaNs\n",
    "        has_nans = np.isnan(batch_X).any()\n",
    "        if has_nans:\n",
    "            logger.warning(f\"Detectados NaN en lote {start_idx}-{end_idx}, realizando imputaci√≥n\")\n",
    "            # Reemplazar NaNs con 0 para evitar errores\n",
    "            batch_X = np.nan_to_num(batch_X, nan=0.0)\n",
    "\n",
    "        # Predecir lote\n",
    "        batch_preds = model.predict(batch_X)\n",
    "        predictions[start_idx:end_idx] = batch_preds\n",
    "\n",
    "        # Liberar memoria cada 5 lotes\n",
    "        if (start_idx // adaptive_batch) % 5 == 0 and start_idx > 0:\n",
    "            if 'gc' in sys.modules:\n",
    "                import gc\n",
    "                gc.collect()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def train_xgb_with_memory_optimization(X_train, y_train, X_val=None, y_val=None, params=None):\n",
    "    \"\"\"\n",
    "    Entrena un modelo XGBoost con optimizaciones de memoria y velocidad\n",
    "\n",
    "    Args:\n",
    "        X_train: Datos de entrenamiento\n",
    "        y_train: Etiquetas de entrenamiento\n",
    "        X_val: Datos de validaci√≥n (opcional)\n",
    "        y_val: Etiquetas de validaci√≥n (opcional)\n",
    "        params: Par√°metros de XGBoost (diccionario)\n",
    "\n",
    "    Returns:\n",
    "        Modelo XGBoost entrenado\n",
    "    \"\"\"\n",
    "    # Par√°metros por defecto optimizados para velocidad y memoria\n",
    "    default_params = {\n",
    "        'n_estimators': 60,  # Reducido para mayor velocidad\n",
    "        'max_depth': 4,      # Reducido para mayor velocidad\n",
    "        'learning_rate': 0.2, # Aumentado para convergencia m√°s r√°pida\n",
    "        'subsample': 0.7,     # Reducido para mayor velocidad\n",
    "        'colsample_bytree': 0.7, # Reducido para mayor velocidad\n",
    "        'tree_method': 'hist',  # M√©todo m√°s eficiente en memoria\n",
    "        'predictor': 'cpu_predictor',  # Evitar problemas de GPU\n",
    "        'n_jobs': 1  # Un hilo por modelo para permitir paralelismo entre modelos\n",
    "    }\n",
    "\n",
    "    # Actualizar con par√°metros personalizados si se proporcionan\n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "\n",
    "    # Crear y entrenar modelo con early stopping si hay datos de validaci√≥n\n",
    "    if X_val is not None and y_val is not None:\n",
    "        model = XGBRegressor(**default_params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "    else:\n",
    "        # Sin early stopping si no hay datos de validaci√≥n\n",
    "        model = XGBRegressor(**default_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_xgb_horizon_predictions(meta_models, base_model_preds, cells, horizons=3):\n",
    "    \"\"\"\n",
    "    Genera predicciones por horizonte usando modelos XGBoost en metamodelado\n",
    "\n",
    "    Args:\n",
    "        meta_models: Lista de modelos XGBoost (uno por horizonte)\n",
    "        base_model_preds: Diccionario de predicciones de modelos base {modelo: predicciones}\n",
    "        cells: N√∫mero de celdas espaciales\n",
    "        horizons: N√∫mero de horizontes temporales\n",
    "\n",
    "    Returns:\n",
    "        Array de predicciones (muestras, horizontes, celdas)\n",
    "    \"\"\"\n",
    "    # Determinar n√∫mero de muestras del primer modelo base\n",
    "    first_model = list(base_model_preds.keys())[0]\n",
    "    n_samples = base_model_preds[first_model].shape[0]\n",
    "\n",
    "    # Inicializar array para predicciones\n",
    "    Y_pred = np.zeros((n_samples, horizons, cells))\n",
    "\n",
    "    # Procesar cada horizonte\n",
    "    for h in range(horizons):\n",
    "        logger.info(f\"Generando predicciones para horizonte {h+1}/{horizontes}\")\n",
    "\n",
    "        # Si no hay modelo para este horizonte, continuar al siguiente\n",
    "        if h >= len(meta_models):\n",
    "            logger.warning(f\"No hay modelo meta-XGB para horizonte {h+1}\")\n",
    "            continue\n",
    "\n",
    "        # Preparar caracter√≠sticas para este horizonte\n",
    "        X_meta_batches = []\n",
    "        batch_size = 100\n",
    "\n",
    "        # Procesar por lotes para evitar problemas de memoria\n",
    "        for start_idx in range(0, n_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "\n",
    "            # Preparar entradas para metamodelo\n",
    "            X_meta_batch_parts = []\n",
    "            for model_name in base_model_preds:\n",
    "                if h < base_model_preds[model_name].shape[1]:\n",
    "                    # Extraer predicciones del modelo base para este horizonte\n",
    "                    model_preds = base_model_preds[model_name][start_idx:end_idx, h, :]\n",
    "                    X_meta_batch_parts.append(model_preds.reshape(end_idx - start_idx, -1))\n",
    "\n",
    "            # Concatenar caracter√≠sticas de todos los modelos base\n",
    "            if X_meta_batch_parts:\n",
    "                X_meta_batch = np.hstack(X_meta_batch_parts)\n",
    "\n",
    "                # Predecir con el meta-modelo XGB para este lote\n",
    "                Y_pred[start_idx:end_idx, h, :] = meta_models[h].predict(X_meta_batch).reshape(-1, cells)\n",
    "\n",
    "            # Liberar memoria cada 5 lotes\n",
    "            if (start_idx // batch_size) % 5 == 0 and start_idx > 0:\n",
    "                # Liberar memoria expl√≠citamente\n",
    "                if 'gc' in sys.modules:\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "\n",
    "    return Y_pred\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Carga de datos con separaci√≥n expl√≠cita de caracter√≠sticas CEEMDAN y TFV-EMD\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\")\n",
    "ds_full = xr.open_dataset(FULL_NC)\n",
    "ds_fuse = xr.open_dataset(FUSION_NC)\n",
    "\n",
    "# precipitacion y variables\n",
    "prec = ds_full[\"total_precipitation\"].values  # (T, ny, nx)\n",
    "lags = sorted([v for v in ds_full.data_vars if \"_lag\" in v])\n",
    "da_lags = np.stack([ds_full[lag].values for lag in lags], axis=-1)  # (T, ny, nx, n_lags)\n",
    "\n",
    "# Separar caracter√≠sticas CEEMDAN y TFV-EMD para optimizar su fusi√≥n\n",
    "ceemdan_branches = [\"CEEMDAN_high\", \"CEEMDAN_medium\", \"CEEMDAN_low\"]\n",
    "tvfemd_branches = [\"TVFEMD_high\", \"TVFEMD_medium\", \"TVFEMD_low\"]\n",
    "fusion_branches = [\"FUSION_high\", \"FUSION_medium\", \"FUSION_low\"]\n",
    "\n",
    "# Cargar datos CEEMDAN\n",
    "da_ceemdan = np.stack([ds_fuse[branch].values for branch in ceemdan_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar datos TFV-EMD\n",
    "da_tvfemd = np.stack([ds_fuse[branch].values for branch in tvfemd_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar fusi√≥n predefinida (para referencia)\n",
    "da_fusion = np.stack([ds_fuse[branch].values for branch in fusion_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "\n",
    "# topograf√≠a y cluster\n",
    "elev = ds_full[\"elevation\"].values.ravel()  # (cells,)\n",
    "slope = ds_full[\"slope\"].values.ravel()\n",
    "\n",
    "# Manejar correctamente los valores de cluster (pueden ser texto)\n",
    "cluster_values = ds_full[\"cluster_elevation\"].values.ravel()\n",
    "# Verificar si los valores son strings o num√©ricos\n",
    "if isinstance(cluster_values[0], (str, np.str_)):\n",
    "    # Usar un LabelEncoder para convertir strings a enteros\n",
    "    le = LabelEncoder()\n",
    "    cluster = le.fit_transform(cluster_values)\n",
    "    logger.info(f\"Clusters codificados de texto a n√∫meros: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    # Si ya son num√©ricos, convertir a enteros\n",
    "    cluster = cluster_values.astype(int)\n",
    "\n",
    "# dimensiones\n",
    "lat = ds_full.latitude.values\n",
    "lon = ds_full.longitude.values\n",
    "ny, nx = len(lat), len(lon)\n",
    "cells = ny*nx\n",
    "T = prec.shape[0]\n",
    "\n",
    "logger.info(f\"Dimensiones: T={T}, ny={ny}, nx={nx}, cells={cells}\")\n",
    "logger.info(f\"Shapes: prec={prec.shape}, da_ceemdan={da_ceemdan.shape}, da_tvfemd={da_tvfemd.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Definir m√°scaras para los niveles de elevaci√≥n\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Definiendo m√°scaras para los niveles de elevaci√≥n...\")\n",
    "mask_nivel1 = elev < 957  # nivel_1: 58-956m\n",
    "mask_nivel2 = (elev >= 957) & (elev <= 2264)  # nivel_2: 957-2264m\n",
    "mask_nivel3 = elev > 2264  # nivel_3: 2264-4728m\n",
    "\n",
    "logger.info(f\"Distribuci√≥n de celdas por nivel de elevaci√≥n:\")\n",
    "logger.info(f\"  Nivel 1 (<957m): {np.sum(mask_nivel1)} celdas\")\n",
    "logger.info(f\"  Nivel 2 (957-2264m): {np.sum(mask_nivel2)} celdas\")\n",
    "logger.info(f\"  Nivel 3 (>2264m): {np.sum(mask_nivel3)} celdas\")\n",
    "\n",
    "# Crear diccionario de m√°scaras para facilitar el procesamiento\n",
    "elevation_masks = {\n",
    "    \"nivel_1\": mask_nivel1,\n",
    "    \"nivel_2\": mask_nivel2,\n",
    "    \"nivel_3\": mask_nivel3\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Implementar funci√≥n para optimizar fusi√≥n de CEEMDAN y TFV-EMD con XGBoost\n",
    "# -----------------------------------------------------------------------------\n",
    "import concurrent.futures\n",
    "import tqdm\n",
    "from functools import partial\n",
    "\n",
    "@trace(\"Optimizaci√≥n de fusi√≥n\")\n",
    "def optimize_fusion_with_xgboost(ceemdan_data, tvfemd_data, target_data, masks, test_size=0.2, force_retrain=False):\n",
    "    \"\"\"\n",
    "    Optimiza la fusi√≥n de CEEMDAN y TFV-EMD usando XGBoost para cada nivel de elevaci√≥n.\n",
    "    Implementa paralelismo adaptativo basado en CPU/GPU y memoria disponible.\n",
    "    \n",
    "    Args:\n",
    "        ceemdan_data: Array de caracter√≠sticas CEEMDAN (T, ny, nx, 3)\n",
    "        tvfemd_data: Array de caracter√≠sticas TFV-EMD (T, ny, nx, 3)\n",
    "        target_data: Array de valores objetivo (precipitaci√≥n) (T, ny, nx)\n",
    "        masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        test_size: Proporci√≥n del conjunto de prueba\n",
    "        force_retrain: Si es True, fuerza el reentrenamiento aunque existan modelos guardados\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary con modelos XGBoost para fusi√≥n por nivel y componente\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    # Comprobar si todos los modelos ya existen\n",
    "    all_models_exist = True\n",
    "    if not force_retrain:\n",
    "        for level_name in masks:\n",
    "            for component_idx in range(3):\n",
    "                if not model_exists('fusion', level_name, component_idx):\n",
    "                    all_models_exist = False\n",
    "                    break\n",
    "            if not all_models_exist:\n",
    "                break\n",
    "                \n",
    "        if all_models_exist:\n",
    "            logger.info(\"Todos los modelos de fusi√≥n existen. Cargando...\")\n",
    "            return load_all_fusion_models(masks)\n",
    "    \n",
    "    # Determinar recursos computacionales disponibles\n",
    "    mem_info = get_memory_info()\n",
    "    cpu_count = os.cpu_count()\n",
    "    \n",
    "    print(f\"\\nüñ•Ô∏è  Recursos detectados: {cpu_count} CPUs, {mem_info['total_gb']:.1f}GB RAM ({mem_info['available_gb']:.1f}GB disponible)\")\n",
    "    \n",
    "    # SOLUCI√ìN: Aumentar agresivamente el n√∫mero de trabajadores para forzar paralelismo\n",
    "    # y aprovechar mejor los recursos subutilizados\n",
    "    optimal_workers = max(3, min(cpu_count - 1, 8))  # M√≠nimo 3 workers, m√°ximo CPU-1 o 8\n",
    "    \n",
    "    # Verificar disponibilidad de GPU para tree_method\n",
    "    gpu_available = len(gpus) > 0\n",
    "    tree_method = 'gpu_hist' if gpu_available else 'hist'\n",
    "    \n",
    "    print(f\"üîß Configuraci√≥n optimizada: {optimal_workers} workers en paralelo FORZADOS, tree_method={tree_method}\")\n",
    "    print(f\"üß† Memoria disponible: {mem_info['available_gb']:.2f}GB ({mem_info['used_percent']:.1f}% usado)\")\n",
    "    \n",
    "    # Total de componentes a procesar\n",
    "    total_levels = len(masks)\n",
    "    total_components = total_levels * 3  # 3 componentes por nivel\n",
    "    \n",
    "    # Inicializar estructuras de datos para resultados\n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]  # Placeholder para los 3 componentes\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "    \n",
    "    # Barra de progreso global\n",
    "    print(f\"\\nüìä Iniciando entrenamiento acelerado de {total_components} componentes ({total_levels} niveles √ó 3 componentes)\")\n",
    "    \n",
    "    # Funci√≥n para procesar un componente espec√≠fico\n",
    "    def process_component(level_name, mask, component_idx):\n",
    "        # Verificar si el modelo ya existe (a menos que se fuerce reentrenamiento)\n",
    "        if not force_retrain and model_exists('fusion', level_name, component_idx):\n",
    "            print(f\"üîÑ Nivel {level_name}, componente {component_idx}: Cargando modelo existente...\")\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            if model and info:\n",
    "                weights = info.get('weights')\n",
    "                rmse = info.get('rmse', 0.0)\n",
    "                fit_time = info.get('fit_time', 0.0)\n",
    "                total_time = 0.1  # Tiempo m√≠nimo para evitar divisiones por cero\n",
    "                print(f\"‚úÖ {level_name}, comp{component_idx} (cargado): RMSE={rmse:.4f}, pesos=[CEEMDAN={weights[0]:.2f}, TFV-EMD={weights[1]:.2f}]\")\n",
    "                return {\n",
    "                    'level': level_name,\n",
    "                    'component': component_idx,\n",
    "                    'model': model,\n",
    "                    'weights': weights,\n",
    "                    'rmse': rmse,\n",
    "                    'fit_time': fit_time,\n",
    "                    'total_time': total_time,\n",
    "                    'loaded': True\n",
    "                }\n",
    "        \n",
    "        # Si llegamos aqu√≠, necesitamos entrenar el modelo\n",
    "        print(f\"‚ñ∂Ô∏è  Nivel {level_name}, componente {component_idx}: Iniciando entrenamiento r√°pido...\")\n",
    "        comp_start = time.time()\n",
    "        cells_in_level = np.sum(mask)\n",
    "        \n",
    "        # Reformatear los datos para el entrenamiento\n",
    "        X_ceemdan = ceemdan_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        X_tvfemd = tvfemd_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        y_target = target_data.reshape(T, -1)[:, mask]\n",
    "        \n",
    "        print(f\"   Datos: {X_ceemdan.shape[0]} muestras, {cells_in_level} celdas\")\n",
    "        \n",
    "        # Concatenar caracter√≠sticas\n",
    "        X_combined = np.column_stack([X_ceemdan, X_tvfemd])\n",
    "        \n",
    "        # Divisi√≥n simple para mayor velocidad (sin estratificaci√≥n que consume tiempo)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_target, test_size=test_size, random_state=42\n",
    "        )\n",
    "        split_method = \"simple (optimizado para velocidad)\"\n",
    "        \n",
    "        print(f\"   Split: {X_train.shape[0]} train, {X_test.shape[0]} test ({split_method})\")\n",
    "        \n",
    "        # SOLUCI√ìN: Optimizar hiperpar√°metros para mayor velocidad\n",
    "        n_samples, n_features = X_train.shape\n",
    "        # Reducir profundidad y n√∫mero de √°rboles para entrenamientos m√°s r√°pidos\n",
    "        max_depth = min(4, max(3, int(np.log2(n_features/2))))  # Profundidad reducida\n",
    "        n_estimators = min(60, max(30, int(30 + 5 * np.log(n_samples))))  # Menos √°rboles\n",
    "        learning_rate = min(0.3, max(0.08, 0.2))  # Learning rate m√°s alto para convergencia r√°pida\n",
    "        subsample = 0.7  # Usar menos datos por √°rbol\n",
    "        colsample = 0.7  # Usar menos columnas por √°rbol\n",
    "        \n",
    "        # Configurar modelo XGBoost con paralelismo m√°s eficiente\n",
    "        model = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample,\n",
    "            tree_method=tree_method,\n",
    "            n_jobs=1,  # Un hilo por modelo para maximizar paralelismo entre modelos\n",
    "            enable_categorical=False,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        # Entrenar modelo con mensaje de progreso\n",
    "        print(f\"   Entrenamiento ultra-r√°pido: {n_estimators} estimators, depth={max_depth}, lr={learning_rate:.3f}\")\n",
    "        fit_start = time.time()\n",
    "        \n",
    "        # Entrenamiento simplificado para mayor velocidad\n",
    "        model.fit(\n",
    "            X_train, y_train.ravel(),\n",
    "            eval_set=[(X_test, y_test.ravel())],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        fit_time = time.time() - fit_start\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test.ravel(), y_pred))\n",
    "        \n",
    "        # Extraer pesos de importancia para CEEMDAN vs TFV-EMD\n",
    "        importance = model.feature_importances_\n",
    "        cells_per_feature = cells_in_level\n",
    "        \n",
    "        # Promedio de importancia para cada fuente\n",
    "        ceemdan_importance = np.mean(importance[:cells_per_feature])\n",
    "        tvfemd_importance = np.mean(importance[cells_per_feature:])\n",
    "        \n",
    "        # Normalizar para que sumen 1\n",
    "        total_importance = ceemdan_importance + tvfemd_importance\n",
    "        ceemdan_weight = ceemdan_importance / total_importance\n",
    "        tvfemd_weight = tvfemd_importance / total_importance\n",
    "        \n",
    "        comp_time = time.time() - comp_start\n",
    "        \n",
    "        print(f\"‚úÖ {level_name}, comp{component_idx}: RMSE={rmse:.4f}, tiempo={comp_time:.1f}s, \"\n",
    "              f\"pesos=[CEEMDAN={ceemdan_weight:.2f}, TFV-EMD={tvfemd_weight:.2f}]\")\n",
    "        \n",
    "        weights = (ceemdan_weight, tvfemd_weight)\n",
    "        \n",
    "        # Guardar modelo para uso futuro con informaci√≥n adicional\n",
    "        info = {\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'training_date': datetime.datetime.now().strftime(timestamp_format),\n",
    "            'hyper_params': {\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'learning_rate': learning_rate,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        save_model(model, 'fusion', level_name, component_idx, info)\n",
    "        \n",
    "        # Devolver resultados\n",
    "        return {\n",
    "            'level': level_name,\n",
    "            'component': component_idx,\n",
    "            'model': model,\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'loaded': False\n",
    "        }\n",
    "    \n",
    "    # Procesar niveles y componentes usando paralelismo adaptativo\n",
    "    all_tasks = []\n",
    "    for level_name, mask in masks.items():\n",
    "        # Crear tareas para todos los componentes\n",
    "        for component_idx in range(3):\n",
    "            all_tasks.append((level_name, mask, component_idx))\n",
    "    \n",
    "    # SOLUCI√ìN: FORZAR paralelismo siempre\n",
    "    all_results = []\n",
    "    \n",
    "    # Mostrar mensaje claro sobre el modo paralelo\n",
    "    print(f\"\\n‚ö° Activando procesamiento paralelo forzado con {optimal_workers} workers para acelerar el entrenamiento\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=optimal_workers) as executor:\n",
    "        # Crear lista de futuros\n",
    "        futures = []\n",
    "        for level_name, mask, component_idx in all_tasks:\n",
    "            futures.append(executor.submit(\n",
    "                process_component, level_name, mask, component_idx\n",
    "            ))\n",
    "        \n",
    "        # Mostrar progreso mientras se completan las tareas\n",
    "        completed = 0\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            completed += 1\n",
    "            progress = completed / len(futures)\n",
    "            print(f\"‚è≥ Progreso global: {completed}/{len(futures)} componentes ({progress:.1%})\")\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en componente: {str(e)}\")\n",
    "    \n",
    "    # Organizar resultados por nivel y componente\n",
    "    for result in all_results:\n",
    "        level = result['level']\n",
    "        component = result['component']\n",
    "        \n",
    "        # Guardar modelo y pesos\n",
    "        fusion_models[level][component] = result['model']\n",
    "        fusion_weights[level][component] = result['weights']\n",
    "        \n",
    "        # Registrar m√©tricas para trazabilidad\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"rmse\", result['rmse'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"ceemdan_weight\", result['weights'][0])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"tvfemd_weight\", result['weights'][1])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"train_time\", result['fit_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"total_time\", result['total_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"loaded\", result.get('loaded', False))\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\nüèÅ Optimizaci√≥n de fusi√≥n completada:\")\n",
    "    for level_name, components in fusion_models.items():\n",
    "        valid_components = sum(1 for model in components if model is not None)\n",
    "        print(f\"  - {level_name}: {valid_components}/3 componentes entrenados\")\n",
    "    \n",
    "    tracker.add_checkpoint(\"Optimizaci√≥n de fusi√≥n completada\", \n",
    "                          {\"num_models\": sum(len(models) for models in fusion_models.values())})\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "def load_all_fusion_models(masks):\n",
    "    \"\"\"\n",
    "    Carga todos los modelos de fusi√≥n existentes\n",
    "    \n",
    "    Args:\n",
    "        masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fusion_models, fusion_weights)\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "        \n",
    "        # Cargar los tres modelos de componentes\n",
    "        for component_idx in range(3):\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            \n",
    "            if model is not None and info is not None:\n",
    "                fusion_models[level_name][component_idx] = model\n",
    "                fusion_weights[level_name][component_idx] = info['weights']\n",
    "                logger.info(f\"Modelo fusi√≥n {level_name}, componente {component_idx} cargado\")\n",
    "                \n",
    "                # Registrar m√©tricas para trazabilidad\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"rmse\", info.get('rmse', 0))\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"ceemdan_weight\", info['weights'][0])\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"tvfemd_weight\", info['weights'][1])\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"loaded\", True)\n",
    "            else:\n",
    "                logger.warning(f\"No se pudo cargar el modelo fusi√≥n {level_name}, componente {component_idx}\")\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Carga de datos con separaci√≥n expl√≠cita de caracter√≠sticas CEEMDAN y TFV-EMD\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\")\n",
    "ds_full = xr.open_dataset(FULL_NC)\n",
    "ds_fuse = xr.open_dataset(FUSION_NC)\n",
    "\n",
    "# precipitacion y variables\n",
    "prec = ds_full[\"total_precipitation\"].values  # (T, ny, nx)\n",
    "lags = sorted([v for v in ds_full.data_vars if \"_lag\" in v])\n",
    "da_lags = np.stack([ds_full[lag].values for lag in lags], axis=-1)  # (T, ny, nx, n_lags)\n",
    "\n",
    "# Separar caracter√≠sticas CEEMDAN y TFV-EMD para optimizar su fusi√≥n\n",
    "ceemdan_branches = [\"CEEMDAN_high\", \"CEEMDAN_medium\", \"CEEMDAN_low\"]\n",
    "tvfemd_branches = [\"TVFEMD_high\", \"TVFEMD_medium\", \"TVFEMD_low\"]\n",
    "fusion_branches = [\"FUSION_high\", \"FUSION_medium\", \"FUSION_low\"]\n",
    "\n",
    "# Cargar datos CEEMDAN\n",
    "da_ceemdan = np.stack([ds_fuse[branch].values for branch in ceemdan_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar datos TFV-EMD\n",
    "da_tvfemd = np.stack([ds_fuse[branch].values for branch in tvfemd_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar fusi√≥n predefinida (para referencia)\n",
    "da_fusion = np.stack([ds_fuse[branch].values for branch in fusion_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "\n",
    "# topograf√≠a y cluster\n",
    "elev = ds_full[\"elevation\"].values.ravel()  # (cells,)\n",
    "slope = ds_full[\"slope\"].values.ravel()\n",
    "\n",
    "# Manejar correctamente los valores de cluster (pueden ser texto)\n",
    "cluster_values = ds_full[\"cluster_elevation\"].values.ravel()\n",
    "# Verificar si los valores son strings o num√©ricos\n",
    "if isinstance(cluster_values[0], (str, np.str_)):\n",
    "    # Usar un LabelEncoder para convertir strings a enteros\n",
    "    le = LabelEncoder()\n",
    "    cluster = le.fit_transform(cluster_values)\n",
    "    logger.info(f\"Clusters codificados de texto a n√∫meros: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    # Si ya son num√©ricos, convertir a enteros\n",
    "    cluster = cluster_values.astype(int)\n",
    "\n",
    "# dimensiones\n",
    "lat = ds_full.latitude.values\n",
    "lon = ds_full.longitude.values\n",
    "ny, nx = len(lat), len(lon)\n",
    "cells = ny*nx\n",
    "T = prec.shape[0]\n",
    "\n",
    "logger.info(f\"Dimensiones: T={T}, ny={ny}, nx={nx}, cells={cells}\")\n",
    "logger.info(f\"Shapes: prec={prec.shape}, da_ceemdan={da_ceemdan.shape}, da_tvfemd={da_tvfemd.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Definir m√°scaras para los niveles de elevaci√≥n\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Definiendo m√°scaras para los niveles de elevaci√≥n...\")\n",
    "mask_nivel1 = elev < 957  # nivel_1: 58-956m\n",
    "mask_nivel2 = (elev >= 957) & (elev <= 2264)  # nivel_2: 957-2264m\n",
    "mask_nivel3 = elev > 2264  # nivel_3: 2264-4728m\n",
    "\n",
    "logger.info(f\"Distribuci√≥n de celdas por nivel de elevaci√≥n:\")\n",
    "logger.info(f\"  Nivel 1 (<957m): {np.sum(mask_nivel1)} celdas\")\n",
    "logger.info(f\"  Nivel 2 (957-2264m): {np.sum(mask_nivel2)} celdas\")\n",
    "logger.info(f\"  Nivel 3 (>2264m): {np.sum(mask_nivel3)} celdas\")\n",
    "\n",
    "# Crear diccionario de m√°scaras para facilitar el procesamiento\n",
    "elevation_masks = {\n",
    "    \"nivel_1\": mask_nivel1,\n",
    "    \"nivel_2\": mask_nivel2,\n",
    "    \"nivel_3\": mask_nivel3\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Implementar funci√≥n para optimizar fusi√≥n de CEEMDAN y TFV-EMD con XGBoost\n",
    "# -----------------------------------------------------------------------------\n",
    "import concurrent.futures\n",
    "import tqdm\n",
    "from functools import partial\n",
    "\n",
    "@trace(\"Optimizaci√≥n de fusi√≥n\")\n",
    "def optimize_fusion_with_xgboost(ceemdan_data, tvfemd_data, target_data, masks, test_size=0.2, force_retrain=False):\n",
    "    \"\"\"\n",
    "    Optimiza la fusi√≥n de CEEMDAN y TFV-EMD usando XGBoost para cada nivel de elevaci√≥n.\n",
    "    Implementa paralelismo adaptativo basado en CPU/GPU y memoria disponible.\n",
    "    \n",
    "    Args:\n",
    "        ceemdan_data: Array de caracter√≠sticas CEEMDAN (T, ny, nx, 3)\n",
    "        tvfemd_data: Array de caracter√≠sticas TFV-EMD (T, ny, nx, 3)\n",
    "        target_data: Array de valores objetivo (precipitaci√≥n) (T, ny, nx)\n",
    "        masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        test_size: Proporci√≥n del conjunto de prueba\n",
    "        force_retrain: Si es True, fuerza el reentrenamiento aunque existan modelos guardados\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary con modelos XGBoost para fusi√≥n por nivel y componente\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    # Comprobar si todos los modelos ya existen\n",
    "    all_models_exist = True\n",
    "    if not force_retrain:\n",
    "        for level_name in masks:\n",
    "            for component_idx in range(3):\n",
    "                if not model_exists('fusion', level_name, component_idx):\n",
    "                    all_models_exist = False\n",
    "                    break\n",
    "            if not all_models_exist:\n",
    "                break\n",
    "                \n",
    "        if all_models_exist:\n",
    "            logger.info(\"Todos los modelos de fusi√≥n existen. Cargando...\")\n",
    "            return load_all_fusion_models(masks)\n",
    "    \n",
    "    # Determinar recursos computacionales disponibles\n",
    "    mem_info = get_memory_info()\n",
    "    cpu_count = os.cpu_count()\n",
    "    \n",
    "    print(f\"\\nüñ•Ô∏è  Recursos detectados: {cpu_count} CPUs, {mem_info['total_gb']:.1f}GB RAM ({mem_info['available_gb']:.1f}GB disponible)\")\n",
    "    \n",
    "    # SOLUCI√ìN: Aumentar agresivamente el n√∫mero de trabajadores para forzar paralelismo\n",
    "    # y aprovechar mejor los recursos subutilizados\n",
    "    optimal_workers = max(3, min(cpu_count - 1, 8))  # M√≠nimo 3 workers, m√°ximo CPU-1 o 8\n",
    "    \n",
    "    # Verificar disponibilidad de GPU para tree_method\n",
    "    gpu_available = len(gpus) > 0\n",
    "    tree_method = 'gpu_hist' if gpu_available else 'hist'\n",
    "    \n",
    "    print(f\"üîß Configuraci√≥n optimizada: {optimal_workers} workers en paralelo FORZADOS, tree_method={tree_method}\")\n",
    "    print(f\"üß† Memoria disponible: {mem_info['available_gb']:.2f}GB ({mem_info['used_percent']:.1f}% usado)\")\n",
    "    \n",
    "    # Total de componentes a procesar\n",
    "    total_levels = len(masks)\n",
    "    total_components = total_levels * 3  # 3 componentes por nivel\n",
    "    \n",
    "    # Inicializar estructuras de datos para resultados\n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]  # Placeholder para los 3 componentes\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "    \n",
    "    # Barra de progreso global\n",
    "    print(f\"\\nüìä Iniciando entrenamiento acelerado de {total_components} componentes ({total_levels} niveles √ó 3 componentes)\")\n",
    "    \n",
    "    # Funci√≥n para procesar un componente espec√≠fico\n",
    "    def process_component(level_name, mask, component_idx):\n",
    "        # Verificar si el modelo ya existe (a menos que se fuerce reentrenamiento)\n",
    "        if not force_retrain and model_exists('fusion', level_name, component_idx):\n",
    "            print(f\"üîÑ Nivel {level_name}, componente {component_idx}: Cargando modelo existente...\")\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            if model and info:\n",
    "                weights = info.get('weights')\n",
    "                rmse = info.get('rmse', 0.0)\n",
    "                fit_time = info.get('fit_time', 0.0)\n",
    "                total_time = 0.1  # Tiempo m√≠nimo para evitar divisiones por cero\n",
    "                print(f\"‚úÖ {level_name}, comp{component_idx} (cargado): RMSE={rmse:.4f}, pesos=[CEEMDAN={weights[0]:.2f}, TFV-EMD={weights[1]:.2f}]\")\n",
    "                return {\n",
    "                    'level': level_name,\n",
    "                    'component': component_idx,\n",
    "                    'model': model,\n",
    "                    'weights': weights,\n",
    "                    'rmse': rmse,\n",
    "                    'fit_time': fit_time,\n",
    "                    'total_time': total_time,\n",
    "                    'loaded': True\n",
    "                }\n",
    "        \n",
    "        # Si llegamos aqu√≠, necesitamos entrenar el modelo\n",
    "        print(f\"‚ñ∂Ô∏è  Nivel {level_name}, componente {component_idx}: Iniciando entrenamiento r√°pido...\")\n",
    "        comp_start = time.time()\n",
    "        cells_in_level = np.sum(mask)\n",
    "        \n",
    "        # Reformatear los datos para el entrenamiento\n",
    "        X_ceemdan = ceemdan_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        X_tvfemd = tvfemd_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        y_target = target_data.reshape(T, -1)[:, mask]\n",
    "        \n",
    "        print(f\"   Datos: {X_ceemdan.shape[0]} muestras, {cells_in_level} celdas\")\n",
    "        \n",
    "        # Concatenar caracter√≠sticas\n",
    "        X_combined = np.column_stack([X_ceemdan, X_tvfemd])\n",
    "        \n",
    "        # Divisi√≥n simple para mayor velocidad (sin estratificaci√≥n que consume tiempo)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_target, test_size=test_size, random_state=42\n",
    "        )\n",
    "        split_method = \"simple (optimizado para velocidad)\"\n",
    "        \n",
    "        print(f\"   Split: {X_train.shape[0]} train, {X_test.shape[0]} test ({split_method})\")\n",
    "        \n",
    "        # SOLUCI√ìN: Optimizar hiperpar√°metros para mayor velocidad\n",
    "        n_samples, n_features = X_train.shape\n",
    "        # Reducir profundidad y n√∫mero de √°rboles para entrenamientos m√°s r√°pidos\n",
    "        max_depth = min(4, max(3, int(np.log2(n_features/2))))  # Profundidad reducida\n",
    "        n_estimators = min(60, max(30, int(30 + 5 * np.log(n_samples))))  # Menos √°rboles\n",
    "        learning_rate = min(0.3, max(0.08, 0.2))  # Learning rate m√°s alto para convergencia r√°pida\n",
    "        subsample = 0.7  # Usar menos datos por √°rbol\n",
    "        colsample = 0.7  # Usar menos columnas por √°rbol\n",
    "        \n",
    "        # Configurar modelo XGBoost con paralelismo m√°s eficiente\n",
    "        model = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample,\n",
    "            tree_method=tree_method,\n",
    "            n_jobs=1,  # Un hilo por modelo para maximizar paralelismo entre modelos\n",
    "            enable_categorical=False,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        # Entrenar modelo con mensaje de progreso\n",
    "        print(f\"   Entrenamiento ultra-r√°pido: {n_estimators} estimators, depth={max_depth}, lr={learning_rate:.3f}\")\n",
    "        fit_start = time.time()\n",
    "        \n",
    "        # Entrenamiento simplificado para mayor velocidad\n",
    "        model.fit(\n",
    "            X_train, y_train.ravel(),\n",
    "            eval_set=[(X_test, y_test.ravel())],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        fit_time = time.time() - fit_start\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test.ravel(), y_pred))\n",
    "        \n",
    "        # Extraer pesos de importancia para CEEMDAN vs TFV-EMD\n",
    "        importance = model.feature_importances_\n",
    "        cells_per_feature = cells_in_level\n",
    "        \n",
    "        # Promedio de importancia para cada fuente\n",
    "        ceemdan_importance = np.mean(importance[:cells_per_feature])\n",
    "        tvfemd_importance = np.mean(importance[cells_per_feature:])\n",
    "        \n",
    "        # Normalizar para que sumen 1\n",
    "        total_importance = ceemdan_importance + tvfemd_importance\n",
    "        ceemdan_weight = ceemdan_importance / total_importance\n",
    "        tvfemd_weight = tvfemd_importance / total_importance\n",
    "        \n",
    "        comp_time = time.time() - comp_start\n",
    "        \n",
    "        print(f\"‚úÖ {level_name}, comp{component_idx}: RMSE={rmse:.4f}, tiempo={comp_time:.1f}s, \"\n",
    "              f\"pesos=[CEEMDAN={ceemdan_weight:.2f}, TFV-EMD={tvfemd_weight:.2f}]\")\n",
    "        \n",
    "        weights = (ceemdan_weight, tvfemd_weight)\n",
    "        \n",
    "        # Guardar modelo para uso futuro con informaci√≥n adicional\n",
    "        info = {\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'training_date': datetime.datetime.now().strftime(timestamp_format),\n",
    "            'hyper_params': {\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'learning_rate': learning_rate,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        save_model(model, 'fusion', level_name, component_idx, info)\n",
    "        \n",
    "        # Devolver resultados\n",
    "        return {\n",
    "            'level': level_name,\n",
    "            'component': component_idx,\n",
    "            'model': model,\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'loaded': False\n",
    "        }\n",
    "    \n",
    "    # Procesar niveles y componentes usando paralelismo adaptativo\n",
    "    all_tasks = []\n",
    "    for level_name, mask in masks.items():\n",
    "        # Crear tareas para todos los componentes\n",
    "        for component_idx in range(3):\n",
    "            all_tasks.append((level_name, mask, component_idx))\n",
    "    \n",
    "    # SOLUCI√ìN: FORZAR paralelismo siempre\n",
    "    all_results = []\n",
    "    \n",
    "    # Mostrar mensaje claro sobre el modo paralelo\n",
    "    print(f\"\\n‚ö° Activando procesamiento paralelo forzado con {optimal_workers} workers para acelerar el entrenamiento\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=optimal_workers) as executor:\n",
    "        # Crear lista de futuros\n",
    "        futures = []\n",
    "        for level_name, mask, component_idx in all_tasks:\n",
    "            futures.append(executor.submit(\n",
    "                process_component, level_name, mask, component_idx\n",
    "            ))\n",
    "        \n",
    "        # Mostrar progreso mientras se completan las tareas\n",
    "        completed = 0\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            completed += 1\n",
    "            progress = completed / len(futures)\n",
    "            print(f\"‚è≥ Progreso global: {completed}/{len(futures)} componentes ({progress:.1%})\")\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en componente: {str(e)}\")\n",
    "    \n",
    "    # Organizar resultados por nivel y componente\n",
    "    for result in all_results:\n",
    "        level = result['level']\n",
    "        component = result['component']\n",
    "        \n",
    "        # Guardar modelo y pesos\n",
    "        fusion_models[level][component] = result['model']\n",
    "        fusion_weights[level][component] = result['weights']\n",
    "        \n",
    "        # Registrar m√©tricas para trazabilidad\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"rmse\", result['rmse'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"ceemdan_weight\", result['weights'][0])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"tvfemd_weight\", result['weights'][1])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"train_time\", result['fit_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"total_time\", result['total_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"loaded\", result.get('loaded', False))\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\nüèÅ Optimizaci√≥n de fusi√≥n completada:\")\n",
    "    for level_name, components in fusion_models.items():\n",
    "        valid_components = sum(1 for model in components if model is not None)\n",
    "        print(f\"  - {level_name}: {valid_components}/3 componentes entrenados\")\n",
    "    \n",
    "    tracker.add_checkpoint(\"Optimizaci√≥n de fusi√≥n completada\", \n",
    "                          {\"num_models\": sum(len(models) for models in fusion_models.values())})\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "def load_all_fusion_models(masks):\n",
    "    \"\"\"\n",
    "    Carga todos los modelos de fusi√≥n existentes\n",
    "    \n",
    "    Args:\n",
    "        masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fusion_models, fusion_weights)\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "        \n",
    "        # Cargar los tres modelos de componentes\n",
    "        for component_idx in range(3):\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            \n",
    "            if model is not None and info is not None:\n",
    "                fusion_models[level_name][component_idx] = model\n",
    "                fusion_weights[level_name][component_idx] = info['weights']\n",
    "                logger.info(f\"Modelo fusi√≥n {level_name}, componente {component_idx} cargado\")\n",
    "                \n",
    "                # Registrar m√©tricas para trazabilidad\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"rmse\", info.get('rmse', 0))\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"ceemdan_weight\", info['weights'][0])\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"tvfemd_weight\", info['weights'][1])\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"loaded\", True)\n",
    "            else:\n",
    "                logger.warning(f\"No se pudo cargar el modelo fusi√≥n {level_name}, componente {component_idx}\")\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Carga de datos con separaci√≥n expl√≠cita de caracter√≠sticas CEEMDAN y TFV-EMD\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\")\n",
    "ds_full = xr.open_dataset(FULL_NC)\n",
    "ds_fuse = xr.open_dataset(FUSION_NC)\n",
    "\n",
    "# precipitacion y variables\n",
    "prec = ds_full[\"total_precipitation\"].values  # (T, ny, nx)\n",
    "lags = sorted([v for v in ds_full.data_vars if \"_lag\" in v])\n",
    "da_lags = np.stack([ds_full[lag].values for lag in lags], axis=-1)  # (T, ny, nx, n_lags)\n",
    "\n",
    "# Separar caracter√≠sticas CEEMDAN y TFV-EMD para optimizar su fusi√≥n\n",
    "ceemdan_branches = [\"CEEMDAN_high\", \"CEEMDAN_medium\", \"CEEMDAN_low\"]\n",
    "tvfemd_branches = [\"TVFEMD_high\", \"TVFEMD_medium\", \"TVFEMD_low\"]\n",
    "fusion_branches = [\"FUSION_high\", \"FUSION_medium\", \"FUSION_low\"]\n",
    "\n",
    "# Cargar datos CEEMDAN\n",
    "da_ceemdan = np.stack([ds_fuse[branch].values for branch in ceemdan_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar datos TFV-EMD\n",
    "da_tvfemd = np.stack([ds_fuse[branch].values for branch in tvfemd_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar fusi√≥n predefinida (para referencia)\n",
    "da_fusion = np.stack([ds_fuse[branch].values for branch in fusion_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "\n",
    "# topograf√≠a y cluster\n",
    "elev = ds_full[\"elevation\"].values.ravel()  # (cells,)\n",
    "slope = ds_full[\"slope\"].values.ravel()\n",
    "\n",
    "# Manejar correctamente los valores de cluster (pueden ser texto)\n",
    "cluster_values = ds_full[\"cluster_elevation\"].values.ravel()\n",
    "# Verificar si los valores son strings o num√©ricos\n",
    "if isinstance(cluster_values[0], (str, np.str_)):\n",
    "    # Usar un LabelEncoder para convertir strings a enteros\n",
    "    le = LabelEncoder()\n",
    "    cluster = le.fit_transform(cluster_values)\n",
    "    logger.info(f\"Clusters codificados de texto a n√∫meros: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    # Si ya son num√©ricos, convertir a enteros\n",
    "    cluster = cluster_values.astype(int)\n",
    "\n",
    "# dimensiones\n",
    "lat = ds_full.latitude.values\n",
    "lon = ds_full.longitude.values\n",
    "ny, nx = len(lat), len(lon)\n",
    "cells = ny*nx\n",
    "T = prec.shape[0]\n",
    "\n",
    "logger.info(f\"Dimensiones: T={T}, ny={ny}, nx={nx}, cells={cells}\")\n",
    "logger.info(f\"Shapes: prec={prec.shape}, da_ceemdan={da_ceemdan.shape}, da_tvfemd={da_tvfemd.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Definir m√°scaras para los niveles de elevaci√≥n\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Definiendo m√°scaras para los niveles de elevaci√≥n...\")\n",
    "mask_nivel1 = elev < 957  # nivel_1: 58-956m\n",
    "mask_nivel2 = (elev >= 957) & (elev <= 2264)  # nivel_2: 957-2264m\n",
    "mask_nivel3 = elev > 2264  # nivel_3: 2264-4728m\n",
    "\n",
    "logger.info(f\"Distribuci√≥n de celdas por nivel de elevaci√≥n:\")\n",
    "logger.info(f\"  Nivel 1 (<957m): {np.sum(mask_nivel1)} celdas\")\n",
    "logger.info(f\"  Nivel 2 (957-2264m): {np.sum(mask_nivel2)} celdas\")\n",
    "logger.info(f\"  Nivel 3 (>2264m): {np.sum(mask_nivel3)} celdas\")\n",
    "\n",
    "# Crear diccionario de m√°scaras para facilitar el procesamiento\n",
    "elevation_masks = {\n",
    "    \"nivel_1\": mask_nivel1,\n",
    "    \"nivel_2\": mask_nivel2,\n",
    "    \"nivel_3\": mask_nivel3\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Implementar funci√≥n para optimizar fusi√≥n de CEEMDAN y TFV-EMD con XGBoost\n",
    "# -----------------------------------------------------------------------------\n",
    "import concurrent.futures\n",
    "import tqdm\n",
    "from functools import partial\n",
    "\n",
    "@trace(\"Optimizaci√≥n de fusi√≥n\")\n",
    "def optimize_fusion_with_xgboost(ceemdan_data, tvfemd_data, target_data, masks, test_size=0.2, force_retrain=False):\n",
    "    \"\"\"\n",
    "    Optimiza la fusi√≥n de CEEMDAN y TFV-EMD usando XGBoost para cada nivel de elevaci√≥n.\n",
    "    Implementa paralelismo adaptativo basado en CPU/GPU y memoria disponible.\n",
    "    \n",
    "    Args:\n",
    "        ceemdan_data: Array de caracter√≠sticas CEEMDAN (T, ny, nx, 3)\n",
    "        tvfemd_data: Array de caracter√≠sticas TFV-EMD (T, ny, nx, 3)\n",
    "        target_data: Array de valores objetivo (precipitaci√≥n) (T, ny, nx)\n",
    "        masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        test_size: Proporci√≥n del conjunto de prueba\n",
    "        force_retrain: Si es True, fuerza el reentrenamiento aunque existan modelos guardados\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary con modelos XGBoost para fusi√≥n por nivel y componente\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    # Comprobar si todos los modelos ya existen\n",
    "    all_models_exist = True\n",
    "    if not force_retrain:\n",
    "        for level_name in masks:\n",
    "            for component_idx in range(3):\n",
    "                if not model_exists('fusion', level_name, component_idx):\n",
    "                    all_models_exist = False\n",
    "                    break\n",
    "            if not all_models_exist:\n",
    "                break\n",
    "                \n",
    "        if all_models_exist:\n",
    "            logger.info(\"Todos los modelos de fusi√≥n existen. Cargando...\")\n",
    "            return load_all_fusion_models(masks)\n",
    "    \n",
    "    # Determinar recursos computacionales disponibles\n",
    "    mem_info = get_memory_info()\n",
    "    cpu_count = os.cpu_count()\n",
    "    \n",
    "    print(f\"\\nüñ•Ô∏è  Recursos detectados: {cpu_count} CPUs, {mem_info['total_gb']:.1f}GB RAM ({mem_info['available_gb']:.1f}GB disponible)\")\n",
    "    \n",
    "    # SOLUCI√ìN: Aumentar agresivamente el n√∫mero de trabajadores para forzar paralelismo\n",
    "    # y aprovechar mejor los recursos subutilizados\n",
    "    optimal_workers = max(3, min(cpu_count - 1, 8))  # M√≠nimo 3 workers, m√°ximo CPU-1 o 8\n",
    "    \n",
    "    # Verificar disponibilidad de GPU para tree_method\n",
    "    gpu_available = len(gpus) > 0\n",
    "    tree_method = 'gpu_hist' if gpu_available else 'hist'\n",
    "    \n",
    "    print(f\"üîß Configuraci√≥n optimizada: {optimal_workers} workers en paralelo FORZADOS, tree_method={tree_method}\")\n",
    "    print(f\"üß† Memoria disponible: {mem_info['available_gb']:.2f}GB ({mem_info['used_percent']:.1f}% usado)\")\n",
    "    \n",
    "    # Total de componentes a procesar\n",
    "    total_levels = len(masks)\n",
    "    total_components = total_levels * 3  # 3 componentes por nivel\n",
    "    \n",
    "    # Inicializar estructuras de datos para resultados\n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]  # Placeholder para los 3 componentes\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "    \n",
    "    # Barra de progreso global\n",
    "    print(f\"\\nüìä Iniciando entrenamiento acelerado de {total_components} componentes ({total_levels} niveles √ó 3 componentes)\")\n",
    "    \n",
    "    # Funci√≥n para procesar un componente espec√≠fico\n",
    "    def process_component(level_name, mask, component_idx):\n",
    "        # Verificar si el modelo ya existe (a menos que se fuerce reentrenamiento)\n",
    "        if not force_retrain and model_exists('fusion', level_name, component_idx):\n",
    "            print(f\"üîÑ Nivel {level_name}, componente {component_idx}: Cargando modelo existente...\")\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            if model and info:\n",
    "                weights = info.get('weights')\n",
    "                rmse = info.get('rmse', 0.0)\n",
    "                fit_time = info.get('fit_time', 0.0)\n",
    "                total_time = 0.1  # Tiempo m√≠nimo para evitar divisiones por cero\n",
    "                print(f\"‚úÖ {level_name}, comp{component_idx} (cargado): RMSE={rmse:.4f}, pesos=[CEEMDAN={weights[0]:.2f}, TFV-EMD={weights[1]:.2f}]\")\n",
    "                return {\n",
    "                    'level': level_name,\n",
    "                    'component': component_idx,\n",
    "                    'model': model,\n",
    "                    'weights': weights,\n",
    "                    'rmse': rmse,\n",
    "                    'fit_time': fit_time,\n",
    "                    'total_time': total_time,\n",
    "                    'loaded': True\n",
    "                }\n",
    "        \n",
    "        # Si llegamos aqu√≠, necesitamos entrenar el modelo\n",
    "        print(f\"‚ñ∂Ô∏è  Nivel {level_name}, componente {component_idx}: Iniciando entrenamiento r√°pido...\")\n",
    "        comp_start = time.time()\n",
    "        cells_in_level = np.sum(mask)\n",
    "        \n",
    "        # Reformatear los datos para el entrenamiento\n",
    "        X_ceemdan = ceemdan_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        X_tvfemd = tvfemd_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        y_target = target_data.reshape(T, -1)[:, mask]\n",
    "        \n",
    "        print(f\"   Datos: {X_ceemdan.shape[0]} muestras, {cells_in_level} celdas\")\n",
    "        \n",
    "        # Concatenar caracter√≠sticas\n",
    "        X_combined = np.column_stack([X_ceemdan, X_tvfemd])\n",
    "        \n",
    "        # Divisi√≥n simple para mayor velocidad (sin estratificaci√≥n que consume tiempo)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_target, test_size=test_size, random_state=42\n",
    "        )\n",
    "        split_method = \"simple (optimizado para velocidad)\"\n",
    "        \n",
    "        print(f\"   Split: {X_train.shape[0]} train, {X_test.shape[0]} test ({split_method})\")\n",
    "        \n",
    "        # SOLUCI√ìN: Optimizar hiperpar√°metros para mayor velocidad\n",
    "        n_samples, n_features = X_train.shape\n",
    "        # Reducir profundidad y n√∫mero de √°rboles para entrenamientos m√°s r√°pidos\n",
    "        max_depth = min(4, max(3, int(np.log2(n_features/2))))  # Profundidad reducida\n",
    "        n_estimators = min(60, max(30, int(30 + 5 * np.log(n_samples))))  # Menos √°rboles\n",
    "        learning_rate = min(0.3, max(0.08, 0.2))  # Learning rate m√°s alto para convergencia r√°pida\n",
    "        subsample = 0.7  # Usar menos datos por √°rbol\n",
    "        colsample = 0.7  # Usar menos columnas por √°rbol\n",
    "        \n",
    "        # Configurar modelo XGBoost con paralelismo m√°s eficiente\n",
    "        model = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample,\n",
    "            tree_method=tree_method,\n",
    "            n_jobs=1,  # Un hilo por modelo para maximizar paralelismo entre modelos\n",
    "            enable_categorical=False,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        # Entrenar modelo con mensaje de progreso\n",
    "        print(f\"   Entrenamiento ultra-r√°pido: {n_estimators} estimators, depth={max_depth}, lr={learning_rate:.3f}\")\n",
    "        fit_start = time.time()\n",
    "        \n",
    "        # Entrenamiento simplificado para mayor velocidad\n",
    "        model.fit(\n",
    "            X_train, y_train.ravel(),\n",
    "            eval_set=[(X_test, y_test.ravel())],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        fit_time = time.time() - fit_start\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test.ravel(), y_pred))\n",
    "        \n",
    "        # Extraer pesos de importancia para CEEMDAN vs TFV-EMD\n",
    "        importance = model.feature_importances_\n",
    "        cells_per_feature = cells_in_level\n",
    "        \n",
    "        # Promedio de importancia para cada fuente\n",
    "        ceemdan_importance = np.mean(importance[:cells_per_feature])\n",
    "        tvfemd_importance = np.mean(importance[cells_per_feature:])\n",
    "        \n",
    "        # Normalizar para que sumen 1\n",
    "        total_importance = ceemdan_importance + tvfemd_importance\n",
    "        ceemdan_weight = ceemdan_importance / total_importance\n",
    "        tvfemd_weight = tvfemd_importance / total_importance\n",
    "        \n",
    "        comp_time = time.time() - comp_start\n",
    "        \n",
    "        print(f\"‚úÖ {level_name}, comp{component_idx}: RMSE={rmse:.4f}, tiempo={comp_time:.1f}s, \"\n",
    "              f\"pesos=[CEEMDAN={ceemdan_weight:.2f}, TFV-EMD={tvfemd_weight:.2f}]\")\n",
    "        \n",
    "        weights = (ceemdan_weight, tvfemd_weight)\n",
    "        \n",
    "        # Guardar modelo para uso futuro con informaci√≥n adicional\n",
    "        info = {\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'training_date': datetime.datetime.now().strftime(timestamp_format),\n",
    "            'hyper_params': {\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'learning_rate': learning_rate,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        save_model(model, 'fusion', level_name, component_idx, info)\n",
    "        \n",
    "        # Devolver resultados\n",
    "        return {\n",
    "            'level': level_name,\n",
    "            'component': component_idx,\n",
    "            'model': model,\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'loaded': False\n",
    "        }\n",
    "    \n",
    "    # Procesar niveles y componentes usando paralelismo adaptativo\n",
    "    all_tasks = []\n",
    "    for level_name, mask in masks.items():\n",
    "        # Crear tareas para todos los componentes\n",
    "        for component_idx in range(3):\n",
    "            all_tasks.append((level_name, mask, component_idx))\n",
    "    \n",
    "    # SOLUCI√ìN: FORZAR paralelismo siempre\n",
    "    all_results = []\n",
    "    \n",
    "    # Mostrar mensaje claro sobre el modo paralelo\n",
    "    print(f\"\\n‚ö° Activando procesamiento paralelo forzado con {optimal_workers} workers para acelerar el entrenamiento\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=optimal_workers) as executor:\n",
    "        # Crear lista de futuros\n",
    "        futures = []\n",
    "        for level_name, mask, component_idx in all_tasks:\n",
    "            futures.append(executor.submit(\n",
    "                process_component, level_name, mask, component_idx\n",
    "            ))\n",
    "        \n",
    "        # Mostrar progreso mientras se completan las tareas\n",
    "        completed = 0\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            completed += 1\n",
    "            progress = completed / len(futures)\n",
    "            print(f\"‚è≥ Progreso global: {completed}/{len(futures)} componentes ({progress:.1%})\")\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en componente: {str(e)}\")\n",
    "    \n",
    "    # Organizar resultados por nivel y componente\n",
    "    for result in all_results:\n",
    "        level = result['level']\n",
    "        component = result['component']\n",
    "        \n",
    "        # Guardar modelo y pesos\n",
    "        fusion_models[level][component] = result['model']\n",
    "        fusion_weights[level][component] = result['weights']\n",
    "        \n",
    "        # Registrar m√©tricas para trazabilidad\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"rmse\", result['rmse'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"ceemdan_weight\", result['weights'][0])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"tvfemd_weight\", result['weights'][1])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"train_time\", result['fit_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"total_time\", result['total_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"loaded\", result.get('loaded', False))\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\nüèÅ Optimizaci√≥n de fusi√≥n completada:\")\n",
    "    for level_name, components in fusion_models.items():\n",
    "        valid_components = sum(1 for model in components if model is not None)\n",
    "        print(f\"  - {level_name}: {valid_components}/3 componentes entrenados\")\n",
    "    \n",
    "    tracker.add_checkpoint(\"Optimizaci√≥n de fusi√≥n completada\", \n",
    "                          {\"num_models\": sum(len(models) for models in fusion_models.values())})\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "# Funciones para evaluaci√≥n de modelos\n",
    "@trace(\"Evaluaci√≥n global de modelos\")\n",
    "def calculate_global_metrics(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas globales para todos los modelos.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Diccionario de predicciones por modelo\n",
    "        ground_truth: Valores reales de precipitaci√≥n\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con m√©tricas para cada modelo\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    metrics_list = []\n",
    "    \n",
    "    logger.info(\"Calculando m√©tricas globales para todos los modelos...\")\n",
    "    \n",
    "    for model_name, preds in predictions.items():\n",
    "        # Aplanar arrays para c√°lculo de m√©tricas globales\n",
    "        y_true = ground_truth.reshape(-1)\n",
    "        y_pred = preds.reshape(-1)\n",
    "        \n",
    "        # Filtrar NaN si existen\n",
    "        mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "        if np.sum(mask) < len(mask):\n",
    "            logger.warning(f\"Modelo {model_name}: {len(mask) - np.sum(mask)} valores NaN detectados y excluidos\")\n",
    "            y_true = y_true[mask]\n",
    "            y_pred = y_pred[mask]\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # MAPE con manejo de divisiones por cero\n",
    "        mask_nonzero = y_true != 0\n",
    "        if np.sum(mask_nonzero) > 0:\n",
    "            mape = 100 * np.mean(np.abs((y_true[mask_nonzero] - y_pred[mask_nonzero]) / y_true[mask_nonzero]))\n",
    "        else:\n",
    "            mape = np.nan\n",
    "        \n",
    "        metrics_list.append({\n",
    "            'Model': model_name,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'R¬≤': r2\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Modelo {model_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, MAPE={mape:.2f}%, R¬≤={r2:.4f}\")\n",
    "    \n",
    "    # Crear DataFrame con m√©tricas\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df\n",
    "\n",
    "@trace(\"Evaluaci√≥n por niveles de elevaci√≥n\")\n",
    "def calculate_metrics_by_elevation(predictions, ground_truth, elevation_masks):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas separadas por nivel de elevaci√≥n para todos los modelos.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Diccionario de predicciones por modelo\n",
    "        ground_truth: Valores reales de precipitaci√≥n\n",
    "        elevation_masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con m√©tricas para cada modelo y nivel de elevaci√≥n\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    metrics_list = []\n",
    "    \n",
    "    logger.info(\"Calculando m√©tricas por nivel de elevaci√≥n...\")\n",
    "    \n",
    "    # Para cada nivel de elevaci√≥n\n",
    "    for level_name, mask in elevation_masks.items():\n",
    "        # M√°scara a √≠ndices\n",
    "        level_indices = np.where(mask)[0]\n",
    "        \n",
    "        # Para cada modelo\n",
    "        for model_name, preds in predictions.items():\n",
    "            # Preparar datos para este nivel\n",
    "            y_true_level = []\n",
    "            y_pred_level = []\n",
    "            \n",
    "            # Recopilar predicciones para todos los timesteps y horizontes\n",
    "            for t in range(ground_truth.shape[0]):\n",
    "                for h in range(ground_truth.shape[1]):\n",
    "                    y_true_level.append(ground_truth[t, h, level_indices])\n",
    "                    y_pred_level.append(preds[t, h, level_indices])\n",
    "            \n",
    "            # Convertir a arrays y aplanar\n",
    "            y_true_level = np.concatenate(y_true_level)\n",
    "            y_pred_level = np.concatenate(y_pred_level)\n",
    "            \n",
    "            # Filtrar NaN si existen\n",
    "            mask_valid = ~np.isnan(y_true_level) & ~np.isnan(y_pred_level)\n",
    "            if np.sum(mask_valid) < len(mask_valid):\n",
    "                logger.warning(f\"Modelo {model_name}, nivel {level_name}: {len(mask_valid) - np.sum(mask_valid)} valores NaN detectados y excluidos\")\n",
    "                y_true_level = y_true_level[mask_valid]\n",
    "                y_pred_level = y_pred_level[mask_valid]\n",
    "            \n",
    "            # Calcular m√©tricas para este nivel\n",
    "            mae = mean_absolute_error(y_true_level, y_pred_level)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true_level, y_pred_level))\n",
    "            r2 = r2_score(y_true_level, y_pred_level)\n",
    "            \n",
    "            # MAPE con manejo de divisiones por cero\n",
    "            mask_nonzero = y_true_level != 0\n",
    "            if np.sum(mask_nonzero) > 0:\n",
    "                mape = 100 * np.mean(np.abs((y_true_level[mask_nonzero] - y_pred_level[mask_nonzero]) / y_true_level[mask_nonzero]))\n",
    "            else:\n",
    "                mape = np.nan\n",
    "            \n",
    "            metrics_list.append({\n",
    "                'Model': model_name,\n",
    "                'Elevation Level': level_name,\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'R¬≤': r2\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"Modelo {model_name}, nivel {level_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, MAPE={mape:.2f}%, R¬≤={r2:.4f}\")\n",
    "    \n",
    "    # Crear DataFrame con m√©tricas\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df\n",
    "\n",
    "@trace(\"Evaluaci√≥n por percentiles\")\n",
    "def calculate_metrics_by_percentiles(predictions, ground_truth, percentiles):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas separadas por rangos de percentiles para todos los modelos.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Diccionario de predicciones por modelo\n",
    "        ground_truth: Valores reales de precipitaci√≥n\n",
    "        percentiles: Lista de percentiles para definir los rangos\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con m√©tricas para cada modelo y rango de percentil\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    metrics_list = []\n",
    "    \n",
    "    logger.info(f\"Calculando m√©tricas por percentiles: {percentiles}\")\n",
    "    \n",
    "    # Calcular umbrales de percentiles en los datos reales\n",
    "    y_true_flat = ground_truth.reshape(-1)\n",
    "    y_true_nonzero = y_true_flat[y_true_flat > 0]  # Solo valores positivos\n",
    "    \n",
    "    if len(y_true_nonzero) == 0:\n",
    "        logger.warning(\"No hay valores positivos para calcular percentiles. Omitiendo c√°lculo por percentiles.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calcular umbrales\n",
    "    thresholds = [np.percentile(y_true_nonzero, p) for p in percentiles]\n",
    "    logger.info(f\"Umbrales de percentiles: {thresholds}\")\n",
    "    \n",
    "    # Para cada rango de percentiles\n",
    "    for i in range(len(percentiles)-1):\n",
    "        lower_pct = percentiles[i]\n",
    "        upper_pct = percentiles[i+1]\n",
    "        lower_val = thresholds[i]\n",
    "        upper_val = thresholds[i+1]\n",
    "        \n",
    "        range_name = f\"P{lower_pct}-P{upper_pct}\"\n",
    "        logger.info(f\"Calculando m√©tricas para rango {range_name}: {lower_val:.4f} - {upper_val:.4f}\")\n",
    "        \n",
    "        # Para cada modelo\n",
    "        for model_name, preds in predictions.items():\n",
    "            # Aplanar arrays\n",
    "            y_true = ground_truth.reshape(-1)\n",
    "            y_pred = preds.reshape(-1)\n",
    "            \n",
    "            # Filtrar por rango de percentiles\n",
    "            if i == len(percentiles)-2:  # √öltimo rango, incluir el valor superior\n",
    "                mask_range = (y_true >= lower_val) & (y_true <= upper_val)\n",
    "            else:\n",
    "                mask_range = (y_true >= lower_val) & (y_true < upper_val)\n",
    "            \n",
    "            # Si no hay datos en este rango, continuar\n",
    "            if np.sum(mask_range) == 0:\n",
    "                logger.warning(f\"No hay datos en rango {range_name} para modelo {model_name}\")\n",
    "                continue\n",
    "                \n",
    "            # Filtrar datos para este rango\n",
    "            y_true_range = y_true[mask_range]\n",
    "            y_pred_range = y_pred[mask_range]\n",
    "            \n",
    "            # Filtrar NaN si existen\n",
    "            mask_valid = ~np.isnan(y_true_range) & ~np.isnan(y_pred_range)\n",
    "            if np.sum(mask_valid) < len(mask_valid):\n",
    "                logger.warning(f\"Modelo {model_name}, rango {range_name}: {len(mask_valid) - np.sum(mask_valid)} valores NaN detectados y excluidos\")\n",
    "                y_true_range = y_true_range[mask_valid]\n",
    "                y_pred_range = y_pred_range[mask_valid]\n",
    "            \n",
    "            # Calcular m√©tricas para este rango\n",
    "            mae = mean_absolute_error(y_true_range, y_pred_range)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true_range, y_pred_range))\n",
    "            r2 = r2_score(y_true_range, y_pred_range)\n",
    "            \n",
    "            # MAPE con manejo de divisiones por cero\n",
    "            mask_nonzero = y_true_range != 0\n",
    "            if np.sum(mask_nonzero) > 0:\n",
    "                mape = 100 * np.mean(np.abs((y_true_range[mask_nonzero] - y_pred_range[mask_nonzero]) / y_true_range[mask_nonzero]))\n",
    "            else:\n",
    "                mape = np.nan\n",
    "            \n",
    "            metrics_list.append({\n",
    "                'Model': model_name,\n",
    "                'Percentile Range': range_name,\n",
    "                'Value Range': f\"{lower_val:.4f} - {upper_val:.4f}\",\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'R¬≤': r2,\n",
    "                'Samples': np.sum(mask_range)\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"Modelo {model_name}, rango {range_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, MAPE={mape:.2f}%, R¬≤={r2:.4f}\")\n",
    "    \n",
    "    # Crear DataFrame con m√©tricas\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df\n",
    "\n",
    "@trace(\"Visualizaci√≥n de mapas de predicci√≥n\")\n",
    "def plot_all_model_maps(predictions, ground_truth, lat, lon, example_idx=0, horizon_idx=0):\n",
    "    \"\"\"\n",
    "    Genera mapas de predicciones para todos los modelos en un horizonte espec√≠fico.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Diccionario de predicciones por modelo\n",
    "        ground_truth: Valores reales de precipitaci√≥n\n",
    "        lat: Latitudes para el mapa\n",
    "        lon: Longitudes para el mapa\n",
    "        example_idx: √çndice del ejemplo a visualizar\n",
    "        horizon_idx: √çndice del horizonte a visualizar\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.feature as cfeature\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    \n",
    "    # Configurar visualizaci√≥n\n",
    "    n_models = len(predictions) + 1  # +1 para ground truth\n",
    "    n_cols = min(3, n_models)\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Crear figura\n",
    "    fig = plt.figure(figsize=(n_cols * 5, n_rows * 4))\n",
    "    \n",
    "    # Encontrar l√≠mites de colorbar consistentes para todos los mapas\n",
    "    vmin = ground_truth[example_idx, horizon_idx].min()\n",
    "    vmax = ground_truth[example_idx, horizon_idx].max()\n",
    "    \n",
    "    # Crear mapa para ground truth primero\n",
    "    ax = plt.subplot(n_rows, n_cols, 1, projection=ccrs.PlateCarree())\n",
    "    \n",
    "    # Reshape de datos para mapeo\n",
    "    ny, nx = len(lat), len(lon)\n",
    "    truth_map = ground_truth[example_idx, horizon_idx].reshape(ny, nx)\n",
    "    \n",
    "    # Configurar colormap para precipitaci√≥n\n",
    "    cmap = plt.cm.YlGnBu\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    # A√±adir caracter√≠sticas del mapa\n",
    "    ax.coastlines(resolution='10m')\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.RIVERS, linestyle='-', alpha=0.5)\n",
    "    \n",
    "    # Plotear datos\n",
    "    im = ax.pcolormesh(lon, lat, truth_map, cmap=cmap, norm=norm, transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # A√±adir colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, axes_class=plt.Axes)\n",
    "    plt.colorbar(im, cax=cax, orientation=\"vertical\", label=\"Precipitation\")\n",
    "    \n",
    "    # T√≠tulo y configuraci√≥n\n",
    "    ax.set_title(f\"Ground Truth (H+{horizon_idx+1})\")\n",
    "    ax.gridlines(draw_labels=True, alpha=0.3)\n",
    "    \n",
    "    # Crear mapas para cada modelo\n",
    "    for i, (model_name, preds) in enumerate(predictions.items(), 2):\n",
    "        ax = plt.subplot(n_rows, n_cols, i, projection=ccrs.PlateCarree())\n",
    "        \n",
    "        # Reshape de datos para mapeo\n",
    "        pred_map = preds[example_idx, horizon_idx].reshape(ny, nx)\n",
    "        \n",
    "        # A√±adir caracter√≠sticas del mapa\n",
    "        ax.coastlines(resolution='10m')\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        ax.add_feature(cfeature.RIVERS, linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Plotear datos\n",
    "        im = ax.pcolormesh(lon, lat, pred_map, cmap=cmap, norm=norm, transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # A√±adir colorbar\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, axes_class=plt.Axes)\n",
    "        plt.colorbar(im, cax=cax, orientation=\"vertical\", label=\"Precipitation\")\n",
    "        \n",
    "        # T√≠tulo y configuraci√≥n\n",
    "        ax.set_title(f\"{model_name} (H+{horizon_idx+1})\")\n",
    "        ax.gridlines(draw_labels=True, alpha=0.3)\n",
    "    \n",
    "    # Ajustar dise√±o y guardar\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{BASE}/models/output/prediction_maps_horizon{horizon_idx+1}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Mapas de predicci√≥n guardados para horizonte {horizon_idx+1}\")\n",
    "\n",
    "def visualize_process_tracker_results():\n",
    "    \"\"\"Visualiza los resultados del tracker de proceso\"\"\"\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Crear directorio de salida si no existe\n",
    "    vis_dir = Path(f\"{BASE}/models/output/visualizations\")\n",
    "    vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Visualizar tiempo por secci√≥n\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    section_names = list(tracker.section_times.keys())\n",
    "    section_times = list(tracker.section_times.values())\n",
    "    \n",
    "    # Ordenar por tiempo\n",
    "    indices = np.argsort(section_times)\n",
    "    section_names = [section_names[i] for i in indices]\n",
    "    section_times = [section_times[i] for i in indices]\n",
    "    \n",
    "    sns.barplot(x=section_times, y=section_names)\n",
    "    plt.title('Tiempo de ejecuci√≥n por secci√≥n')\n",
    "    plt.xlabel('Tiempo (segundos)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{vis_dir}/section_times.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Visualizar uso de recursos a lo largo del tiempo\n",
    "    if tracker.resources:\n",
    "        times = [(r['timestamp'] - tracker.start_time) for r in tracker.resources]\n",
    "        mem_pcts = [r['memory_percent'] for r in tracker.resources]\n",
    "        cpu_pcts = [r['cpu_percent'] for r in tracker.resources]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(times, mem_pcts, label='Memoria (%)')\n",
    "        plt.plot(times, cpu_pcts, label='CPU (%)')\n",
    "        plt.axhline(y=90, color='r', linestyle='--', alpha=0.7, label='L√≠mite cr√≠tico (90%)')\n",
    "        \n",
    "        # A√±adir marcas de checkpoints\n",
    "        for cp in tracker.checkpoints:\n",
    "            plt.axvline(x=cp['elapsed_total'], color='g', alpha=0.5, linestyle='-.')\n",
    "        \n",
    "        plt.title('Uso de recursos durante la ejecuci√≥n')\n",
    "        plt.xlabel('Tiempo (segundos)')\n",
    "        plt.ylabel('Uso (%)')\n",
    "        plt.ylim(0, 100)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f\"{vis_dir}/resource_usage.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    logger.info(f\"Visualizaciones del proceso guardadas en {vis_dir}\")\n",
    "\n",
    "def display_log_summary():\n",
    "    \"\"\"Muestra un resumen del archivo de log\"\"\"\n",
    "    log_file = LOG_DIR / log_filename\n",
    "    if not log_file.exists():\n",
    "        logger.warning(f\"No se encontr√≥ el archivo de log: {log_file}\")\n",
    "        return\n",
    "    \n",
    "    # Leer √∫ltimas l√≠neas\n",
    "    try:\n",
    "        with open(log_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        # Mostrar stats b√°sicos\n",
    "        total_lines = len(lines)\n",
    "        errors = sum(1 for line in lines if \" ERROR \" in line)\n",
    "        warnings = sum(1 for line in lines if \" WARNING \" in line)\n",
    "        infos = sum(1 for line in lines if \" INFO \" in line)\n",
    "        \n",
    "        print(f\"\\nüìã Resumen del log ({log_file.name}):\")\n",
    "        print(f\"  Total l√≠neas: {total_lines}\")\n",
    "        print(f\"  Informaci√≥n: {infos}\")\n",
    "        print(f\"  Advertencias: {warnings}\")\n",
    "        print(f\"  Errores: {errors}\")\n",
    "        \n",
    "        # Mostrar √∫ltimos errores\n",
    "        if errors > 0:\n",
    "            print(\"\\n‚ö†Ô∏è √öltimos errores:\")\n",
    "            error_lines = [line.strip() for line in lines if \" ERROR \" in line]\n",
    "            for line in error_lines[-min(5, len(error_lines)):]:\n",
    "                print(f\"  {line}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error procesando archivo de log: {str(e)}\")\n",
    "        \n",
    "@trace(\"Generaci√≥n de fusi√≥n optimizada\")\n",
    "def generate_optimized_fusion(ceemdan_data, tvfemd_data, fusion_weights, elevation_masks):\n",
    "    \"\"\"\n",
    "    Genera features de fusi√≥n optimizadas basadas en los pesos de importancia aprendidos\n",
    "    \n",
    "    Args:\n",
    "        ceemdan_data: Array de caracter√≠sticas CEEMDAN (T, ny, nx, 3)\n",
    "        tvfemd_data: Array de caracter√≠sticas TFV-EMD (T, ny, nx, 3)\n",
    "        fusion_weights: Diccionario de pesos por nivel y componente\n",
    "        elevation_masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        \n",
    "    Returns:\n",
    "        Array de fusi√≥n optimizada (T, ny, nx, 3)\n",
    "    \"\"\"\n",
    "    T, ny, nx, n_components = ceemdan_data.shape\n",
    "    logger.info(f\"Generando fusi√≥n optimizada: shape={T}√ó{ny}√ó{nx}√ó{n_components}\")\n",
    "    \n",
    "    # Inicializar array para fusi√≥n optimizada\n",
    "    fusion_optimized = np.zeros_like(ceemdan_data)\n",
    "    \n",
    "    # Para cada nivel de elevaci√≥n\n",
    "    for level_name, mask in elevation_masks.items():\n",
    "        # Convertir m√°scara 1D a 2D para aplicarla a los datos\n",
    "        level_mask_2d = mask.reshape(ny, nx)\n",
    "        \n",
    "        # Para cada componente\n",
    "        for comp_idx in range(n_components):\n",
    "            # Obtener pesos para este nivel y componente\n",
    "            if level_name in fusion_weights and fusion_weights[level_name][comp_idx] is not None:\n",
    "                ceemdan_weight, tvfemd_weight = fusion_weights[level_name][comp_idx]\n",
    "            else:\n",
    "                logger.warning(f\"No hay pesos para {level_name}, componente {comp_idx}. Usando pesos uniformes.\")\n",
    "                ceemdan_weight = tvfemd_weight = 0.5\n",
    "            \n",
    "            # Aplicar fusi√≥n ponderada para este nivel y componente\n",
    "            for t in range(T):\n",
    "                # Extraer solo las celdas para este nivel\n",
    "                fusion_optimized[t, level_mask_2d, comp_idx] = (\n",
    "                    ceemdan_weight * ceemdan_data[t, level_mask_2d, comp_idx] +\n",
    "                    tvfemd_weight * tvfemd_data[t, level_mask_2d, comp_idx]\n",
    "                )\n",
    "    \n",
    "    logger.info(f\"Fusi√≥n optimizada generada correctamente con shape {fusion_optimized.shape}\")\n",
    "    return fusion_optimized"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
