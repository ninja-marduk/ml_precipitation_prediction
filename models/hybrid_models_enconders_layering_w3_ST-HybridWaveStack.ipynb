{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/hybrid_models_ST-HybridWaveStack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7b96f3ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7b96f3ea",
        "outputId": "2a35e011-4fab-4fe2-a366-fbb70fa49f7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-19 17:37:19,690 INFO ⚙ CPU cores: 10, RAM libre: 3.0 GB\n",
            "2025-05-19 17:37:19,691 INFO 📂 Cargando datasets…\n",
            "2025-05-19 17:37:19,725 WARNING REF_DATE no hallado; usando último mes: 2025-02\n",
            "2025-05-19 17:37:19,726 INFO ▶ Procesando CEEMDAN_high\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▶️ Base path: /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 1.0134 - val_loss: 0.8062\n",
            "Epoch 2/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.7888 - val_loss: 0.7200\n",
            "Epoch 3/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.6007 - val_loss: 0.7267\n",
            "Epoch 4/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.5893 - val_loss: 0.7551\n",
            "Epoch 5/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.4737 - val_loss: 0.7128\n",
            "Epoch 6/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.4442 - val_loss: 0.6976\n",
            "Epoch 7/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.4166 - val_loss: 0.7586\n",
            "Epoch 8/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.4003 - val_loss: 0.6515\n",
            "Epoch 9/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.3680 - val_loss: 0.8289\n",
            "Epoch 10/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.3805 - val_loss: 0.7004\n",
            "Epoch 11/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.3930 - val_loss: 0.8003\n",
            "Epoch 12/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.3979 - val_loss: 0.6154\n",
            "Epoch 13/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.3388 - val_loss: 0.5631\n",
            "Epoch 14/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.3675 - val_loss: 0.7258\n",
            "Epoch 15/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.3475 - val_loss: 0.6760\n",
            "Epoch 16/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.3462 - val_loss: 0.6988\n",
            "Epoch 17/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.3640 - val_loss: 0.6482\n",
            "Epoch 18/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.3635 - val_loss: 0.7509\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-19 17:37:41,056 WARNING You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "2025-05-19 17:37:41,372 ERROR ‼ Error en CEEMDAN_high, continúo…\n",
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/83/c6n8lktn4qx_fwp7ksllkkhn0dhtn2/T/ipykernel_42911/781569034.py\", line 272, in <module>\n",
            "    pm = scY.inverse_transform(preds_s[h,0]).reshape(ny,nx)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/preprocessing/_data.py\", line 1106, in inverse_transform\n",
            "    X = check_array(\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1093, in check_array\n",
            "    raise ValueError(msg)\n",
            "ValueError: Expected 2D array, got 1D array instead:\n",
            "array=[-0.09601123 -0.13580438 -0.1794446  ...  0.7315675   0.69340205\n",
            "  0.64309657].\n",
            "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
            "2025-05-19 17:37:41,373 INFO ▶ Procesando CEEMDAN_medium\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - loss: 0.9550 - val_loss: 0.9645\n",
            "Epoch 2/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.8952 - val_loss: 1.0057\n",
            "Epoch 3/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.7863 - val_loss: 1.1768\n",
            "Epoch 4/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.6825 - val_loss: 0.8198\n",
            "Epoch 5/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.6333 - val_loss: 1.1406\n",
            "Epoch 6/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.5573 - val_loss: 0.9944\n",
            "Epoch 7/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.5086 - val_loss: 1.0982\n",
            "Epoch 8/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.4573 - val_loss: 1.4394\n",
            "Epoch 9/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.3940 - val_loss: 1.1079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-19 17:37:54,450 WARNING You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "2025-05-19 17:37:54,776 ERROR ‼ Error en CEEMDAN_medium, continúo…\n",
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/83/c6n8lktn4qx_fwp7ksllkkhn0dhtn2/T/ipykernel_42911/781569034.py\", line 272, in <module>\n",
            "    pm = scY.inverse_transform(preds_s[h,0]).reshape(ny,nx)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/preprocessing/_data.py\", line 1106, in inverse_transform\n",
            "    X = check_array(\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1093, in check_array\n",
            "    raise ValueError(msg)\n",
            "ValueError: Expected 2D array, got 1D array instead:\n",
            "array=[0.05129286 0.09772117 0.08344763 ... 0.47967255 0.4196765  0.44851854].\n",
            "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
            "2025-05-19 17:37:54,778 INFO ▶ Procesando CEEMDAN_low\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
            "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
            "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
            "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Entrenamiento Multi‐rama con GRU encoder–decoder y Transformer para low,\n",
        "validación y forecast parametrizables, meta‐modelo XGBoost,\n",
        "paralelización, trazabilidad y límites del departamento de Boyacá.\n",
        "\"\"\"\n",
        "\n",
        "# 0) Supresión de warnings irrelevantes\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "from cartopy.io import DownloadWarning\n",
        "warnings.filterwarnings(\"ignore\", category=DownloadWarning)\n",
        "\n",
        "# 1) Parámetros configurables\n",
        "INPUT_WINDOW   = 60          # número de meses en la ventana de entrada\n",
        "OUTPUT_HORIZON = 3           # meses de validación y forecast\n",
        "REF_DATE       = \"2025-03\"   # fecha de referencia (yyyy-mm)\n",
        "\n",
        "# 2) Detectar entorno (Local / Colab)\n",
        "import sys\n",
        "from pathlib import Path\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    BASE_PATH = Path(\"/content/drive/MyDrive/ml_precipitation_prediction\")\n",
        "    !pip install -q xarray netCDF4 optuna seaborn cartopy xgboost ace_tools_open\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p/\".git\").exists():\n",
        "            BASE_PATH = p\n",
        "            break\n",
        "print(f\"▶️ Base path: {BASE_PATH}\")\n",
        "\n",
        "# 3) Rutas y logger\n",
        "import logging\n",
        "MODEL_DIR   = BASE_PATH/\"models\"/\"output\"/\"trained_models\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FEATURES_NC = BASE_PATH/\"models\"/\"output\"/\"features_fusion_branches.nc\"\n",
        "FULL_NC     = BASE_PATH/\"data\"/\"output\"/\"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
        "SHP_USER    = Path(\"/mnt/data/MGN_Departamento.shp\")\n",
        "BOYACA_SHP  = SHP_USER if SHP_USER.exists() else BASE_PATH/\"data\"/\"input\"/\"shapes\"/\"MGN_Departamento.shp\"\n",
        "RESULTS_CSV = MODEL_DIR/f\"metrics_w{OUTPUT_HORIZON}_ref{REF_DATE}.csv\"\n",
        "IMAGE_DIR   = MODEL_DIR/\"images\"\n",
        "IMAGE_DIR.mkdir(exist_ok=True)\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 4) Imports principales\n",
        "import numpy            as np\n",
        "import pandas           as pd\n",
        "import xarray           as xr\n",
        "import geopandas        as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio.v2       as imageio\n",
        "import cartopy.crs      as ccrs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import psutil\n",
        "from joblib import cpu_count\n",
        "import tensorflow       as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add, Dense, Flatten, RepeatVector, Input, TimeDistributed, GRU\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 5) Recursos hardware\n",
        "CORES     = cpu_count()\n",
        "AVAIL_RAM = psutil.virtual_memory().available / (1024**3)\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "USE_GPU = bool(gpus)\n",
        "if USE_GPU:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    logger.info(f\"🖥 GPU disponible: {gpus[0].name}\")\n",
        "else:\n",
        "    tf.config.threading.set_inter_op_parallelism_threads(CORES)\n",
        "    tf.config.threading.set_intra_op_parallelism_threads(CORES)\n",
        "    logger.info(f\"⚙ CPU cores: {CORES}, RAM libre: {AVAIL_RAM:.1f} GB\")\n",
        "\n",
        "# 6) Modelos y utilitarios\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
        "    mae  = np.mean(np.abs(y_true - y_pred))\n",
        "    mape = np.mean(np.abs((y_true - y_pred)/(y_true + 1e-5)))*100\n",
        "    r2   = 1 - np.sum((y_true-y_pred)**2)/np.sum((y_true-np.mean(y_true))**2)\n",
        "    return rmse, mae, mape, r2\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, Y, batch_size=32):\n",
        "        self.X, self.Y = X.astype(np.float32), Y.astype(np.float32)\n",
        "        self.batch_size = batch_size\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X)/self.batch_size))\n",
        "    def __getitem__(self, idx):\n",
        "        sl = slice(idx*self.batch_size, (idx+1)*self.batch_size)\n",
        "        return self.X[sl], self.Y[sl]\n",
        "\n",
        "def build_gru_ed(input_shape, horizon, n_cells,\n",
        "                 latent=128, dropout=0.2):\n",
        "    inp = Input(shape=input_shape)\n",
        "    x   = GRU(latent, dropout=dropout)(inp)\n",
        "    x   = RepeatVector(horizon)(x)\n",
        "    x   = GRU(latent, dropout=dropout, return_sequences=True)(x)\n",
        "    out = TimeDistributed(Dense(n_cells))(x)\n",
        "    m   = Model(inp, out)\n",
        "    m.compile(\"adam\",\"mse\")\n",
        "    return m\n",
        "\n",
        "def build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                         head_size=64, num_heads=4, ff_dim=256, dropout=0.1):\n",
        "    inp = Input(shape=input_shape)                            # (window, n_feats)\n",
        "    attn = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inp, inp)\n",
        "    x    = Add()([inp, attn])\n",
        "    x    = LayerNormalization(epsilon=1e-6)(x)\n",
        "    ff   = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff   = Dense(input_shape[-1])(ff)\n",
        "    x    = Add()([x, ff])\n",
        "    x    = LayerNormalization(epsilon=1e-6)(x)\n",
        "    x    = Flatten()(x)\n",
        "    x    = Dense(horizon * n_cells)(x)\n",
        "    out  = layers.Reshape((horizon, n_cells))(x)\n",
        "    m    = Model(inp, out)\n",
        "    m.compile(\"adam\",\"mse\")\n",
        "    return m\n",
        "\n",
        "def build_gru_ed_low(input_shape, horizon, n_cells,\n",
        "                     latent=256, dropout=0.1, use_transformer=True):\n",
        "    if use_transformer:\n",
        "        return build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                                    head_size=64, num_heads=4, ff_dim=512, dropout=dropout)\n",
        "    else:\n",
        "        return build_gru_ed(input_shape, horizon, n_cells,\n",
        "                            latent=latent, dropout=dropout)\n",
        "\n",
        "# 7) Carga de datos y shapefile\n",
        "logger.info(\"📂 Cargando datasets…\")\n",
        "ds_full = xr.open_dataset(FULL_NC)\n",
        "ds_feat = xr.open_dataset(FEATURES_NC)\n",
        "boyaca_gdf = gpd.read_file(BOYACA_SHP)\n",
        "if boyaca_gdf.crs is None:\n",
        "    boyaca_gdf.set_crs(epsg=4326, inplace=True)\n",
        "else:\n",
        "    boyaca_gdf = boyaca_gdf.to_crs(epsg=4326)\n",
        "\n",
        "times = ds_full.time.values.astype(\"datetime64[M]\")\n",
        "ref   = np.datetime64(REF_DATE,\"M\")\n",
        "if ref not in times:\n",
        "    ref = times[-1]\n",
        "    logger.warning(f\"REF_DATE no hallado; usando último mes: {ref}\")\n",
        "idx_ref = int(np.where(times==ref)[0][0])\n",
        "\n",
        "lat = ds_full.latitude.values\n",
        "lon = ds_full.longitude.values\n",
        "METHODS  = [\"CEEMDAN\",\"TVFEMD\",\"FUSION\"]\n",
        "BRANCHES = [\"high\",\"medium\",\"low\"]\n",
        "\n",
        "all_metrics  = []\n",
        "preds_store  = {}\n",
        "true_store   = {}\n",
        "histories    = {}   # guardar history por modelo\n",
        "\n",
        "# 8) Bucle principal\n",
        "for method in METHODS:\n",
        "    for branch in BRANCHES:\n",
        "        name = f\"{method}_{branch}\"\n",
        "        if name not in ds_feat.data_vars:\n",
        "            logger.warning(f\"⚠ {name} no existe, saltando.\")\n",
        "            continue\n",
        "        logger.info(f\"▶ Procesando {name}\")\n",
        "        try:\n",
        "            # extraer y aplanar espacialmente\n",
        "            Xarr = ds_feat[name].values         # (T,ny,nx)\n",
        "            yarr = ds_full[\"total_precipitation\"].values\n",
        "            T, ny, nx = Xarr.shape\n",
        "            n_cells   = ny*nx\n",
        "\n",
        "            Xfull = Xarr.reshape(T, n_cells)\n",
        "            yfull = yarr.reshape(T, n_cells)\n",
        "\n",
        "            # ventanas deslizantes\n",
        "            Nw = T - INPUT_WINDOW - OUTPUT_HORIZON + 1\n",
        "            if Nw<=0:\n",
        "                logger.warning(\"❌ Ventanas insuficientes.\")\n",
        "                continue\n",
        "\n",
        "            # construir Xs, ys\n",
        "            Xs = np.stack([Xfull[i:i+INPUT_WINDOW] for i in range(Nw)], axis=0)      # (Nw, window, n_cells)\n",
        "            ys = np.stack([yfull[i+INPUT_WINDOW:i+INPUT_WINDOW+OUTPUT_HORIZON]\n",
        "                           for i in range(Nw)], axis=0)                             # (Nw, horizon, n_cells)\n",
        "\n",
        "            # para low-branch: agregar sin/cos estacionales\n",
        "            if branch==\"low\":\n",
        "                months = pd.to_datetime(times).month.values\n",
        "                sin_feat = np.sin(2*np.pi*months/12)\n",
        "                cos_feat = np.cos(2*np.pi*months/12)\n",
        "                Ssin = np.stack([sin_feat[i:i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                Scos = np.stack([cos_feat[i:i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                # tile\n",
        "                sin_tile = np.repeat(Ssin[:,:,None], n_cells, axis=2)\n",
        "                cos_tile = np.repeat(Scos[:,:,None], n_cells, axis=2)\n",
        "                Xs = np.concatenate([Xs, sin_tile, cos_tile], axis=2)  # nuevo n_feats\n",
        "                n_feats = Xs.shape[2]\n",
        "            else:\n",
        "                n_feats = n_cells\n",
        "\n",
        "            # escalado global\n",
        "            scX = StandardScaler().fit(Xs.reshape(-1,n_feats))\n",
        "            scY = StandardScaler().fit(ys.reshape(-1,n_cells))\n",
        "            Xs_s = scX.transform(Xs.reshape(-1,n_feats)).reshape(Xs.shape)\n",
        "            ys_s = scY.transform(ys.reshape(-1,n_cells)).reshape(ys.shape)\n",
        "\n",
        "            # determinar índices para validación centrados en REF_DATE\n",
        "            k_ref = idx_ref - INPUT_WINDOW + 1\n",
        "            k_ref = max(0, min(k_ref, Nw-1))\n",
        "            i0    = k_ref - (OUTPUT_HORIZON-1)\n",
        "            i0    = max(0, min(i0, Nw-OUTPUT_HORIZON))\n",
        "\n",
        "            # split train / validación\n",
        "            X_tr = Xs_s[:i0]\n",
        "            y_tr = ys_s[:i0]\n",
        "            X_va = Xs_s[i0:i0+OUTPUT_HORIZON]\n",
        "            y_va = ys_s[i0:i0+OUTPUT_HORIZON]\n",
        "\n",
        "            # cargar o entrenar modelo\n",
        "            if branch==\"low\":\n",
        "                model_dir = MODEL_DIR/f\"{name}_w{OUTPUT_HORIZON}_ref{ref}\"\n",
        "                saved_model_dir = model_dir/\"saved_model\"\n",
        "                if saved_model_dir.exists():\n",
        "                    model = tf.keras.models.load_model(saved_model_dir)\n",
        "                    logger.info(f\"⏩ Cargado low-branch SavedModel: {saved_model_dir}\")\n",
        "                else:\n",
        "                    model = build_gru_ed_low((INPUT_WINDOW,n_feats),\n",
        "                                             OUTPUT_HORIZON, n_cells,\n",
        "                                             latent=256, dropout=0.1,\n",
        "                                             use_transformer=True)\n",
        "                    hist = model.fit(\n",
        "                        DataGenerator(X_tr,y_tr),\n",
        "                        validation_data=DataGenerator(X_va,y_va),\n",
        "                        epochs=100,\n",
        "                        callbacks=[callbacks.EarlyStopping(\"val_loss\",patience=7,restore_best_weights=True)],\n",
        "                        verbose=1\n",
        "                    )\n",
        "                    # guardar\n",
        "                    model_dir.mkdir(parents=True, exist_ok=True)\n",
        "                    model.save(saved_model_dir)\n",
        "                    histories[name] = hist.history\n",
        "            else:\n",
        "                model_path = MODEL_DIR/f\"{name}_w{OUTPUT_HORIZON}_ref{ref}.h5\"\n",
        "                if model_path.exists():\n",
        "                    model = tf.keras.models.load_model(model_path)\n",
        "                    logger.info(f\"⏩ Cargado modelo: {model_path}\")\n",
        "                else:\n",
        "                    model = build_gru_ed((INPUT_WINDOW,n_feats), OUTPUT_HORIZON, n_cells) \\\n",
        "                            if branch==\"high\" else build_gru_ed((INPUT_WINDOW,n_feats), OUTPUT_HORIZON, n_cells)\n",
        "                    hist = model.fit(\n",
        "                        DataGenerator(X_tr,y_tr),\n",
        "                        validation_data=DataGenerator(X_va,y_va),\n",
        "                        epochs=100,\n",
        "                        callbacks=[callbacks.EarlyStopping(\"val_loss\",patience=5,restore_best_weights=True)],\n",
        "                        verbose=1\n",
        "                    )\n",
        "                    model.save(model_path)\n",
        "                    histories[name] = hist.history\n",
        "\n",
        "            # — Validación: H=1…H=OUTPUT_HORIZON\n",
        "            preds_s = model.predict(X_va, verbose=0)   # (H, H, n_cells) or (N_va, H, n_cells)\n",
        "            # asumimos X_va.shape[0]==OUTPUT_HORIZON\n",
        "            preds_s = preds_s.reshape(OUTPUT_HORIZON, OUTPUT_HORIZON, n_cells)\n",
        "            # invertimos escala\n",
        "            for h in range(OUTPUT_HORIZON):\n",
        "                date_val = str(times[i0+h+INPUT_WINDOW-1])\n",
        "                pm = scY.inverse_transform(preds_s[h,0]).reshape(ny,nx)\n",
        "                tm = scY.inverse_transform(y_va[h,0]).reshape(ny,nx)\n",
        "                rmse, mae, mape, r2 = evaluate_metrics(tm.ravel(), pm.ravel())\n",
        "                all_metrics.append({\n",
        "                    \"model\":name, \"branch\":branch,\n",
        "                    \"horizon\":h+1, \"type\":\"validation\",\n",
        "                    \"date\":date_val,\n",
        "                    \"RMSE\":rmse,\"MAE\":mae,\"MAPE\":mape,\"R2\":r2\n",
        "                })\n",
        "                preds_store [(name,date_val)] = pm\n",
        "                true_store  [(name,date_val)] = tm\n",
        "\n",
        "            # — Forecast tras REF_DATE\n",
        "            X_fc = Xs_s[k_ref:k_ref+1]\n",
        "            fc_s = model.predict(X_fc, verbose=0)[0]    # (H, n_cells)\n",
        "            FC   = scY.inverse_transform(fc_s)\n",
        "            for h in range(OUTPUT_HORIZON):\n",
        "                date_fc = str(times[idx_ref] + np.timedelta64(h+1,'M'))\n",
        "                all_metrics.append({\n",
        "                    \"model\":name, \"branch\":branch,\n",
        "                    \"horizon\":h+1, \"type\":\"forecast\",\n",
        "                    \"date\":date_fc,\n",
        "                    \"RMSE\":np.nan,\"MAE\":np.nan,\"MAPE\":np.nan,\"R2\":np.nan\n",
        "                })\n",
        "                preds_store[(name,date_fc)] = FC[h].reshape(ny,nx)\n",
        "\n",
        "        except Exception:\n",
        "            logger.exception(f\"‼ Error en {name}, continúo…\")\n",
        "            continue\n",
        "\n",
        "# 9) Guardar métricas y mostrar tabla\n",
        "dfm = pd.DataFrame(all_metrics)\n",
        "dfm.to_csv(RESULTS_CSV, index=False)\n",
        "import ace_tools_open as tools\n",
        "tools.display_dataframe_to_user(name=f\"Metrics_w{OUTPUT_HORIZON}_ref{ref}\", dataframe=dfm)\n",
        "\n",
        "# 10) Curvas de entrenamiento\n",
        "for name,hist in histories.items():\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(hist[\"loss\"], label=\"train\")\n",
        "    plt.plot(hist[\"val_loss\"], label=\"val\")\n",
        "    plt.title(f\"Loss curve: {name}\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE\")\n",
        "    plt.legend(); plt.show()\n",
        "\n",
        "# 11) Mapas 3×3 validación H=1\n",
        "dates_va = sorted({d for (_,d) in preds_store if\n",
        "                   any(r for r in all_metrics if r[\"date\"]==d and r[\"type\"]==\"validation\")})[:OUTPUT_HORIZON]\n",
        "for date_val in dates_va:\n",
        "    # gather global vmin/vmax\n",
        "    arrs = [preds_store[(f\"{m}_{b}\",date_val)].ravel()\n",
        "            for m in METHODS for b in BRANCHES\n",
        "            if (f\"{m}_{b}\",date_val) in preds_store]\n",
        "    vmin = np.min(arrs); vmax = np.max(arrs)\n",
        "    fig, axs = plt.subplots(3,3, figsize=(12,12),\n",
        "                             subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
        "    fig.suptitle(f\"Validación H=1, {date_val}\", fontsize=16)\n",
        "    for i, b in enumerate(BRANCHES):\n",
        "        for j, m in enumerate(METHODS):\n",
        "            ax = axs[i,j]\n",
        "            ax.add_geometries(boyaca_gdf.geometry, ccrs.PlateCarree(),\n",
        "                              edgecolor=\"black\", facecolor=\"none\", linewidth=1)\n",
        "            key = (f\"{m}_{b}\", date_val)\n",
        "            if key in preds_store:\n",
        "                pcm = ax.pcolormesh(lon, lat, preds_store[key],\n",
        "                                    vmin=vmin, vmax=vmax,\n",
        "                                    transform=ccrs.PlateCarree(), cmap=\"Blues\")\n",
        "            ax.set_title(f\"{m}_{b}\")\n",
        "    cb = fig.colorbar(pcm, ax=axs, orientation=\"horizontal\", fraction=0.05, pad=0.04,\n",
        "                      label=\"Precipitación (mm)\")\n",
        "    fig.savefig(IMAGE_DIR/f\"val_H1_{date_val}.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # MAPE\n",
        "    arrs_mape = []\n",
        "    for (m,b) in [(m,b) for m in METHODS for b in BRANCHES]:\n",
        "        key = (f\"{m}_{b}\",date_val)\n",
        "        if key in preds_store and key in true_store:\n",
        "            mape_map = np.clip(np.abs((true_store[key]-preds_store[key])/(true_store[key]+1e-5))*100,0,200)\n",
        "            arrs_mape.append(mape_map.ravel())\n",
        "    vmin2, vmax2 = 0, np.max(arrs_mape)\n",
        "    fig, axs = plt.subplots(3,3, figsize=(12,12),\n",
        "                             subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
        "    fig.suptitle(f\"MAPE H=1, {date_val}\", fontsize=16)\n",
        "    for i, b in enumerate(BRANCHES):\n",
        "        for j, m in enumerate(METHODS):\n",
        "            ax = axs[i,j]\n",
        "            ax.add_geometries(boyaca_gdf.geometry, ccrs.PlateCarree(),\n",
        "                              edgecolor=\"black\", facecolor=\"none\", linewidth=1)\n",
        "            key = (f\"{m}_{b}\", date_val)\n",
        "            if key in preds_store and key in true_store:\n",
        "                mape_map = np.clip(np.abs((true_store[key]-preds_store[key])/(true_store[key]+1e-5))*100,0,200)\n",
        "                pcm2 = ax.pcolormesh(lon, lat, mape_map,\n",
        "                                     vmin=vmin2, vmax=vmax2,\n",
        "                                     transform=ccrs.PlateCarree(), cmap=\"Reds\")\n",
        "            ax.set_title(f\"{m}_{b}\")\n",
        "    cb2 = fig.colorbar(pcm2, ax=axs, orientation=\"horizontal\", fraction=0.05, pad=0.04,\n",
        "                       label=\"MAPE (%)\")\n",
        "    fig.savefig(IMAGE_DIR/f\"mape_H1_{date_val}.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "# 12) Meta‐modelo XGBoost (low, h=3)\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_meta, y_meta = [], []\n",
        "for date in dates_va:\n",
        "    keys = [(f\"{m}_low\", date) for m in METHODS]\n",
        "    if all(k in preds_store for k in keys):\n",
        "        arrs = [preds_store[k].ravel() for k in keys]\n",
        "        X_meta.append(np.vstack(arrs).T)\n",
        "        y_meta.append(true_store[keys[0]].ravel())\n",
        "X_meta = np.concatenate(X_meta, axis=0)\n",
        "y_meta = np.concatenate(y_meta, axis=0)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(X_meta, y_meta, test_size=0.2, random_state=42)\n",
        "xgb = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=4, n_jobs=-1)\n",
        "xgb.fit(Xtr, ytr)\n",
        "yhat = xgb.predict(Xte)\n",
        "rmse_meta = np.sqrt(mean_squared_error(yte,yhat))\n",
        "print(f\"Meta‐modelo XGB (low, h=3) RMSE: {rmse_meta:.3f}\")\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(yte, yhat, alpha=0.3, s=2)\n",
        "lims = [min(yte.min(),yhat.min()), max(yte.max(),yhat.max())]\n",
        "plt.plot(lims, lims, 'k--')\n",
        "plt.xlabel(\"True\"); plt.ylabel(\"Predicted\")\n",
        "plt.title(\"Meta‐modelo XGB (low, h=3)\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "precipitation_prediction",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
