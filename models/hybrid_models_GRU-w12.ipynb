{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43cdeb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH = /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction\n",
      "\n",
      "📊  Resumen global de NaNs\n",
      "───────────────────────────────────────────────────────\n",
      "total_precipitation_lag1    :    3,965 / 2,101,450  ( 0.19%)\n",
      "total_precipitation_lag2    :    7,930 / 2,101,450  ( 0.38%)\n",
      "total_precipitation_lag12   :   47,580 / 2,101,450  ( 2.26%)\n",
      "\n",
      "🕒  Fechas con NaNs por variable\n",
      "───────────────────────────────────────────────────────\n",
      "\n",
      "total_precipitation_lag1\n",
      "      time  na_cells\n",
      "1981-01-01      3965\n",
      "      time  na_cells\n",
      "1981-01-01      3965\n",
      "   ⇢  última fecha con NaNs: 1981-01\n",
      "\n",
      "total_precipitation_lag2\n",
      "      time  na_cells\n",
      "1981-01-01      3965\n",
      "1981-02-01      3965\n",
      "      time  na_cells\n",
      "1981-01-01      3965\n",
      "1981-02-01      3965\n",
      "   ⇢  última fecha con NaNs: 1981-02\n",
      "\n",
      "total_precipitation_lag12\n",
      "      time  na_cells\n",
      "1981-01-01      3965\n",
      "1981-02-01      3965\n",
      "1981-03-01      3965\n",
      "   …\n",
      "      time  na_cells\n",
      "1981-10-01      3965\n",
      "1981-11-01      3965\n",
      "1981-12-01      3965\n",
      "   ⇢  última fecha con NaNs: 1981-12\n",
      "\n",
      "Primera fecha 100 % libre de NaNs en TODOS los lags: 1982-01\n",
      "🔎  Timestamps antes : 530\n",
      "🔎  Timestamps después: 518\n",
      "💾  Dataset sin 1981 guardado en /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction/data/output/complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc\n",
      "\n",
      "📊  NaNs restantes tras quitar 1981\n",
      "──────────────────────────────────────────────────\n",
      "total_precipitation_lag1    : 0 NaNs\n",
      "total_precipitation_lag2    : 0 NaNs\n",
      "total_precipitation_lag12   : 0 NaNs\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, ConvLSTM2D, GRU, Flatten, RepeatVector, Reshape, TimeDistributed,\n",
    "    Dense, MultiHeadAttention, Add, LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "## ╭─────────────────────────── Rutas ──────────────────────────╮\n",
    "# ▶️ Path configuration\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
    "        if (p / '.git').exists():\n",
    "            BASE_PATH = p; break\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "print('BASE_PATH =', BASE_PATH)\n",
    "\n",
    "# Dataset paths\n",
    "DATA_DIR = BASE_PATH/'data'/'output'\n",
    "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
    "MODEL_DIR = BASE_PATH/'models'/'output'/'HybridLSTMModels'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
    "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMAGE_DIR = MODEL_DIR/'images'\n",
    "IMAGE_DIR.mkdir(exist_ok=True)\n",
    "FULL_NC = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation.nc'\n",
    "FULL_NC_CLEAN = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "dept_gdf = gpd.read_file(MODEL_INPUT_DIR/'MGN_Departamento.shp')\n",
    "\n",
    "BASE_MODEL_DIR = MODEL_DIR\n",
    "GIF_DIR        = MODEL_DIR / \"gifs\"\n",
    "GIF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ╭──────────────────────── Dataset & Shapes ──────────────────╮\n",
    "ds          = xr.open_dataset(FULL_NC)\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "LAG_VARS = ['total_precipitation_lag1',\n",
    "            'total_precipitation_lag2',\n",
    "            'total_precipitation_lag12']\n",
    "\n",
    "# ============================================================\n",
    "print(\"\\n📊  Resumen global de NaNs\")\n",
    "print(\"─\"*55)\n",
    "for var in LAG_VARS:\n",
    "    arr    = ds[var].values\n",
    "    total  = arr.size\n",
    "    n_nans = int(np.isnan(arr).sum())\n",
    "    print(f\"{var:<28}: {n_nans:>8,} / {total:,}  ({n_nans/total:6.2%})\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"\\n🕒  Fechas con NaNs por variable\")\n",
    "print(\"─\"*55)\n",
    "for var in LAG_VARS:\n",
    "    arr         = ds[var].values\n",
    "    nan_per_ts  = np.isnan(arr).reshape(len(ds.time), -1).sum(axis=1)\n",
    "    if nan_per_ts.sum() == 0:\n",
    "        print(f\"{var}: sin NaNs ✔️\")\n",
    "        continue\n",
    "\n",
    "    df_nan = (pd\n",
    "              .DataFrame({\"time\": pd.to_datetime(ds.time.values),\n",
    "                          \"na_cells\": nan_per_ts})\n",
    "              .query(\"na_cells > 0\"))\n",
    "\n",
    "    # primeras 3 y últimas 3 fechas con NaNs\n",
    "    head = df_nan.head(3).to_string(index=False)\n",
    "    tail = df_nan.tail(3).to_string(index=False)\n",
    "    last = df_nan[\"time\"].iloc[-1].strftime(\"%Y-%m\")\n",
    "\n",
    "    print(f\"\\n{var}\")\n",
    "    print(head)\n",
    "    if len(df_nan) > 6:\n",
    "        print(\"   …\")\n",
    "    print(tail)\n",
    "    print(f\"   ⇢  última fecha con NaNs: {last}\")\n",
    "\n",
    "# ============================================================\n",
    "# Primera fecha en la que las TRES variables están 100 % limpias\n",
    "# ------------------------------------------------------------\n",
    "def last_nan_index(var: str) -> int:\n",
    "    \"\"\"Índice del último timestamp que contiene al menos un NaN en `var`.\"\"\"\n",
    "    nan_per_ts = np.isnan(ds[var].values).reshape(len(ds.time), -1).sum(axis=1)\n",
    "    idxs       = np.where(nan_per_ts > 0)[0]\n",
    "    return idxs[-1] if len(idxs) else -1\n",
    "\n",
    "last_nan_any = max(last_nan_index(v) for v in LAG_VARS)\n",
    "first_clean  = pd.to_datetime(ds.time.values[last_nan_any + 1])\n",
    "\n",
    "print(\"\\nPrimera fecha 100 % libre de NaNs en TODOS los lags:\",\n",
    "      first_clean.strftime(\"%Y-%m\"))\n",
    "\n",
    "ds_clean = ds.sel(time=~(ds['time.year'] == 1981))   # descarta TODO 1981\n",
    "\n",
    "print(\"🔎  Timestamps antes :\", len(ds.time))\n",
    "print(\"🔎  Timestamps después:\", len(ds_clean.time))\n",
    "\n",
    "# 3) Guarda nuevo archivo NetCDF\n",
    "ds_clean.to_netcdf(FULL_NC_CLEAN, mode='w')\n",
    "print(f\"💾  Dataset sin 1981 guardado en {FULL_NC_CLEAN}\")\n",
    "\n",
    "# 4) (-- opcional --)  verifica que ya no queden NaNs en los lags\n",
    "LAG_VARS = ['total_precipitation_lag1',\n",
    "            'total_precipitation_lag2',\n",
    "            'total_precipitation_lag12']\n",
    "\n",
    "print(\"\\n📊  NaNs restantes tras quitar 1981\")\n",
    "print(\"─\"*50)\n",
    "for var in LAG_VARS:\n",
    "    n_nan = int(np.isnan(ds_clean[var].values).sum())\n",
    "    print(f\"{var:<28}: {n_nan:,} NaNs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, ConvLSTM2D, GRU, Flatten, RepeatVector, Reshape, TimeDistributed,\n",
    "    Dense, MultiHeadAttention, Add, LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "## ╭─────────────────────────── Rutas ──────────────────────────╮\n",
    "# ▶️ Path configuration\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
    "        if (p / '.git').exists():\n",
    "            BASE_PATH = p; break\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "print('BASE_PATH =', BASE_PATH)\n",
    "\n",
    "# Dataset paths\n",
    "DATA_DIR = BASE_PATH/'data'/'output'\n",
    "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
    "MODEL_DIR = BASE_PATH/'models'/'output'/'HybridLSTMModels'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
    "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMAGE_DIR = MODEL_DIR/'images'\n",
    "IMAGE_DIR.mkdir(exist_ok=True)\n",
    "FULL_NC = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "dept_gdf = gpd.read_file(MODEL_INPUT_DIR/'MGN_Departamento.shp')\n",
    "\n",
    "BASE_MODEL_DIR = MODEL_DIR\n",
    "GIF_DIR        = MODEL_DIR / \"gifs\"\n",
    "GIF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "# ╭──────────────────────── Dataset & Shapes ──────────────────╮\n",
    "ds          = xr.open_dataset(FULL_NC)\n",
    "lat, lon    = len(ds.latitude), len(ds.longitude)\n",
    "cells       = lat * lon\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "# ╭──────────────────── Hyper‑parámetros globales ─────────────╮\n",
    "INPUT_WINDOW   = 60\n",
    "HORIZON        = 3\n",
    "TARGET_VAR     = 'total_precipitation'\n",
    "EPOCHS         = 12\n",
    "BATCH_SIZE     = 4           # tamaño pequeño → menor RAM GPU\n",
    "PATIENCE       = 10\n",
    "LR             = 1e-3\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "\n",
    "# ╭────────────────────── Modelo base ConvLSTM ────────────────╮\n",
    "\n",
    "def _build_convlstm_ed(*,input_window: int,output_horizon: int,spatial_height: int,spatial_width: int,n_features: int,n_filters: int = 64,n_heads: int = 4,use_attention: bool = True,lr: float = LR) -> Model:\n",
    "    \"\"\"Construye un Encoder‑Decoder ConvLSTM.\n",
    "\n",
    "    Si `use_attention=False` se omite la capa Multi‑Head Attention.\n",
    "    La salida es `(B, T_out, H, W, 1)`.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(input_window, spatial_height, spatial_width, n_features), name=\"enc_input\")\n",
    "\n",
    "    # ── Encoder ────────────────────────────────────────────\n",
    "    x = ConvLSTM2D(n_filters,   (3, 3), padding='same', return_sequences=True,  name=\"enc_lstm_1\")(inputs)\n",
    "    x = ConvLSTM2D(n_filters//2,(3, 3), padding='same', return_sequences=False, name=\"enc_lstm_2\")(x)\n",
    "\n",
    "    # ── Flatten + contexto temporal ────────────────────────\n",
    "    flat = Flatten(name=\"flatten_spatial\")(x)\n",
    "    ctx  = RepeatVector(output_horizon, name=\"context\")(flat)  # (B, T_out, H·W·C)\n",
    "\n",
    "    # ── Decoder GRU (temporal) ─────────────────────────────\n",
    "    dec = GRU(2*n_filters, return_sequences=True, name=\"dec_gru\")(ctx)\n",
    "\n",
    "    if use_attention:\n",
    "        attn = MultiHeadAttention(num_heads=n_heads, key_dim=n_filters, dropout=0.1, name=\"mha\")(dec, dec)\n",
    "        dec  = LayerNormalization(name=\"mha_norm\")(Add(name=\"mha_add\")([dec, attn]))\n",
    "\n",
    "    # ── Proyección + reshape a grid ───────────────────────\n",
    "    proj = TimeDistributed(Dense(spatial_height*spatial_width, activation='linear'), name=\"dense_proj\")(dec)\n",
    "    out  = Reshape((output_horizon, spatial_height, spatial_width, 1), name=\"reshape_out\")(proj)\n",
    "\n",
    "    model = Model(inputs, out, name=\"ConvLSTM_ED_Attn\" if use_attention else \"ConvLSTM_ED\")\n",
    "    model.compile(optimizer=Adam(lr), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Factories ---------------------------------------------------\n",
    "\n",
    "def factory_no_attn(**kw):\n",
    "    return _build_convlstm_ed(use_attention=False, **kw)\n",
    "\n",
    "def factory_attn(**kw):\n",
    "    return _build_convlstm_ed(use_attention=True, **kw)\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "# ╭────────────────────────── Métricas ────────────────────────╮\n",
    "\n",
    "def evaluate(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-5))) * 100\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    return rmse, mae, mape, r2\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "# ╭──────────────────────── Quick‑plot ────────────────────────╮\n",
    "\n",
    "def quick_plot(ax, data, cmap, title, date_label, vmin=None, vmax=None):\n",
    "    mesh = ax.pcolormesh(ds.longitude, ds.latitude, data, cmap=cmap, shading='nearest', vmin=vmin, vmax=vmax, transform=ccrs.PlateCarree())\n",
    "    ax.coastlines(); ax.add_geometries(dept_gdf.geometry, ccrs.PlateCarree(), edgecolor='black', facecolor='none', linewidth=1)\n",
    "    gl = ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
    "    ax.set_title(f\"{title}\\n{date_label}\", pad=12)\n",
    "    return mesh\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "# ╭────────────────────── Experiments & Folds ─────────────────╮\n",
    "# ▸ Solo mostramos los tres primeros niveles; añade los demás igual\n",
    "BASE_FEATURES = [\n",
    "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
    "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
    "    'elevation','slope','aspect'\n",
    "]\n",
    "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
    "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
    "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
    "\n",
    "FOLDS = {'F1': {'year': 2018,'active': True}}\n",
    "\n",
    "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
    "    'ConvLSTM-ED': {\n",
    "        'active': True,\n",
    "        'feature_list': BASE_FEATURES,\n",
    "        'builder': factory_attn, #factory_no_attn,\n",
    "        'n_filters': 64,\n",
    "        'n_heads'  : 4\n",
    "    },\n",
    "    'ConvLSTM-ED-KCE': {\n",
    "        'active': True,\n",
    "        'feature_list': KCE_FEATURES,\n",
    "        'builder': factory_attn,\n",
    "        'n_filters': 64,\n",
    "        'n_heads'  : 4,\n",
    "    },\n",
    "    'ConvLSTM-ED-KCE-PAFC': {\n",
    "        'active': True,\n",
    "        'feature_list': PAFC_FEATURES,\n",
    "        'builder': factory_attn,\n",
    "        'n_filters': 96,\n",
    "        'n_heads'  : 6,\n",
    "    },\n",
    "}\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "# ╭──────────────────── Ventanas deslizadas ───────────────────╮\n",
    "\n",
    "def make_windows(mask:np.ndarray, allow_past_context:bool)->tuple[np.ndarray,np.ndarray]:\n",
    "    \"\"\"Genera ventanas **descartando** las que contienen NaNs.  # 🔸 NEW\"\"\"\n",
    "    seq_X, seq_y = [], []\n",
    "    lim = len(mask) - INPUT_WINDOW - HORIZON + 1\n",
    "    for start in range(lim):\n",
    "        end_w = start + INPUT_WINDOW; end_y = end_w + HORIZON\n",
    "        if allow_past_context:\n",
    "            if not mask[end_w:end_y].all():\n",
    "                continue\n",
    "        else:\n",
    "            if not mask[start:end_y].all():\n",
    "                continue\n",
    "        Xw = Xarr[start:end_w]; yw = yarr[end_w:end_y]\n",
    "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
    "            continue  # 🔸 NEW — descarta ventana con NaNs\n",
    "        seq_X.append(Xw); seq_y.append(yw)\n",
    "    return np.array(seq_X), np.array(seq_y)\n",
    "\n",
    "\n",
    "# ╭────────────────── Bucle principal de entrenamiento ────────╮\n",
    "RESULTS: List[Dict[str, Any]] = []\n",
    "\n",
    "# 🔸 NEW helper ------------------------------------------------\n",
    "\n",
    "def _impute_nans(a:np.ndarray, per_feature_mean:np.ndarray|None=None, is_target:bool=False)->np.ndarray:\n",
    "    \"\"\"Imputa NaNs restantes (seguridad extra).\"\"\"\n",
    "    if not np.isnan(a).any():\n",
    "        return a\n",
    "    if is_target:\n",
    "        a[np.isnan(a)] = 0.0  # 🔸 NEW – 0 para y\n",
    "        return a\n",
    "    if per_feature_mean is None:\n",
    "        raise ValueError('per_feature_mean required for imputing X')\n",
    "    flat = a.reshape(-1, a.shape[-1])\n",
    "    nan_idx = np.isnan(flat)\n",
    "    for f in range(a.shape[-1]):\n",
    "        flat[nan_idx[:,f], f] = per_feature_mean[f]  # 🔸 NEW\n",
    "    return flat.reshape(a.shape)\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "def run_all_experiments():\n",
    "    times = pd.to_datetime(ds.time.values)\n",
    "    total = sum(e['active'] for e in EXPERIMENTS.values()) * sum(f['active'] for f in FOLDS.values())\n",
    "    cnt   = 0\n",
    "\n",
    "    for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "        if not exp_cfg['active']:\n",
    "            continue\n",
    "        vars_     = exp_cfg['feature_list']\n",
    "        builder   = exp_cfg['builder']      # fábrica específica\n",
    "        n_filters = exp_cfg.get('n_filters',64)\n",
    "        n_heads   = exp_cfg.get('n_heads',4)\n",
    "\n",
    "        # ─ Pre‑load features por experimento ─────────────────────\n",
    "        global Xarr, yarr\n",
    "        Xarr = ds[vars_].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
    "        yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
    "        feats = Xarr.shape[-1]\n",
    "\n",
    "        for fold_name, fold_cfg in FOLDS.items():\n",
    "            if not fold_cfg['active']:\n",
    "                continue\n",
    "            cnt += 1\n",
    "            year_val = fold_cfg['year']\n",
    "            print(f\"\\n▶️  [{cnt}/{total}] {exp_name} – {fold_name} (val={year_val})\")\n",
    "\n",
    "            mask_val = times.year == year_val\n",
    "            mask_tr  = ~mask_val\n",
    "            if mask_val.sum() < HORIZON:\n",
    "                print(\"⚠️ Año sin pasos suficientes → skip\"); continue\n",
    "\n",
    "            X_tr, y_tr = make_windows(mask_tr,  allow_past_context=False)\n",
    "            X_va, y_va = make_windows(mask_val, allow_past_context=True)\n",
    "            print(f\"Ventanas train: {len(X_tr)} · val: {len(X_va)}\")\n",
    "            if len(X_tr)==0 or len(X_va)==0:\n",
    "                print(\"⚠️ Sin ventanas válidas → skip\"); continue\n",
    "\n",
    "            # 🔸 NEW — Imputación de seguridad\n",
    "            feat_mean = np.nanmean(X_tr.reshape(-1,feats),axis=0)\n",
    "            X_tr = _impute_nans(X_tr,feat_mean); X_va=_impute_nans(X_va,feat_mean)\n",
    "            y_tr = _impute_nans(y_tr,is_target=True); y_va=_impute_nans(y_va,is_target=True)\n",
    "            \n",
    "            # ─ Scaling (fit solo en train) ─────────────────────\n",
    "            sx = StandardScaler().fit(X_tr.reshape(-1, feats))\n",
    "            sy = StandardScaler().fit(y_tr.reshape(-1, 1))\n",
    "            X_tr_sc = sx.transform(X_tr.reshape(-1, feats)).reshape(X_tr.shape)\n",
    "            X_va_sc = sx.transform(X_va.reshape(-1, feats)).reshape(X_va.shape)\n",
    "            y_tr_sc = sy.transform(y_tr.reshape(-1, 1)).reshape(y_tr.shape)[..., None]\n",
    "            y_va_sc = sy.transform(y_va.reshape(-1, 1)).reshape(y_va.shape)[..., None]\n",
    "\n",
    "            # ─ Build & train model (factory) ───────────────────\n",
    "            tag        = f\"{exp_name.replace('+','_')}_{fold_name}\"\n",
    "            model_path = BASE_MODEL_DIR / f\"{tag}.keras\"\n",
    "            if model_path.exists():\n",
    "                print(f\"⏩ {tag} ya existe → skip\"); continue\n",
    "\n",
    "            model = builder(\n",
    "                input_window=INPUT_WINDOW,\n",
    "                output_horizon=HORIZON,\n",
    "                spatial_height=lat,\n",
    "                spatial_width=lon,\n",
    "                n_features=feats,\n",
    "                n_filters=n_filters,\n",
    "                n_heads=n_heads,\n",
    "                lr=LR\n",
    "            )\n",
    "\n",
    "            es   = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "            hist = model.fit(X_tr_sc, y_tr_sc, validation_data=(X_va_sc, y_va_sc), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=1)\n",
    "\n",
    "            # ─ Evaluación ─────────────────────────────────────\n",
    "            y_hat_sc = model.predict(X_va_sc, verbose=0)\n",
    "            y_hat    = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(y_hat_sc.shape)\n",
    "            y_true   = sy.inverse_transform(y_va_sc.reshape(-1,1)).reshape(y_va_sc.shape)\n",
    "\n",
    "            rmse, mae, mape, r2 = evaluate(y_true.ravel(), y_hat.ravel())\n",
    "            RESULTS.append(dict(experiment=exp_name, fold=fold_name, RMSE=rmse, MAE=mae, MAPE=mape, R2=r2, epochs=len(hist.history['loss'])))\n",
    "\n",
    "            # ─ Guardado artefactos ────────────────────────────\n",
    "            model.save(model_path)\n",
    "            plt.figure(); plt.plot(hist.history['loss'], label='train'); plt.plot(hist.history['val_loss'], label='val'); plt.legend(); plt.title(tag); plt.savefig(IMAGE_DIR/f\"{tag}.png\"); plt.close()\n",
    "\n",
    "            _generate_gif(y_true[0], y_hat[0], tag)\n",
    "            print(f\"✅ Guardado {model_path.name}\")\n",
    "\n",
    "    # ─ Métricas globales ────────────────────────────────────\n",
    "    df = pd.DataFrame(RESULTS)\n",
    "    out_csv = BASE_MODEL_DIR / \"metrics_experiments_folds.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"\\n📑 Tabla de métricas en {out_csv}\")\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "# ╭──────────────────── Generador de GIF ──────────────────────╮\n",
    "\n",
    "def _generate_gif(y_true_sample, y_pred_sample, tag):\n",
    "    pcm_min, pcm_max = 0, np.max(y_pred_sample)\n",
    "    frames = []\n",
    "    for h in range(HORIZON):\n",
    "        pmap = y_pred_sample[h, ..., 0]\n",
    "        fig, ax = plt.subplots(1,1, figsize=(6,5), subplot_kw={'projection':ccrs.PlateCarree()})\n",
    "        mesh = ax.pcolormesh(ds.longitude, ds.latitude, pmap, cmap='Blues', shading='nearest', vmin=pcm_min, vmax=pcm_max, transform=ccrs.PlateCarree())\n",
    "        ax.coastlines(); ax.gridlines(draw_labels=True)\n",
    "        ax.set_title(f\"{tag} – H{h+1}\")\n",
    "        fig.colorbar(mesh, ax=ax, fraction=0.046, pad=0.04)\n",
    "        tmp = GIF_DIR/f\"tmp_{tag}_h{h}.png\"\n",
    "        fig.savefig(tmp, bbox_inches='tight'); plt.close(fig)\n",
    "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
    "    gif_path = GIF_DIR/f\"{tag}.gif\"\n",
    "    imageio.mimsave(gif_path, frames, fps=0.5)\n",
    "    print(f\"💾 GIF {gif_path.name} listo\")\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "# ╭────────────────────── Bucle principal ─────────────────────╮\n",
    "run_all_experiments()\n",
    "# ╰────────────────────────────────────────────────────────────╯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4de7eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH = /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction\n",
      "\n",
      "🔍 Evaluando ConvLSTM-ED-KCE-PAFC_F1 …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   · h=1: todos los valores son NaN/Inf → skip\n",
      "   · h=2: todos los valores son NaN/Inf → skip\n",
      "   · h=3: todos los valores son NaN/Inf → skip\n",
      "💾 GIF ConvLSTM-ED-KCE-PAFC_F1.gif creado\n",
      "\n",
      "🔍 Evaluando ConvLSTM-ED-KCE_F1 …\n",
      "💾 GIF ConvLSTM-ED-KCE_F1.gif creado\n",
      "\n",
      "🔍 Evaluando ConvLSTM-ED_F1 …\n",
      "💾 GIF ConvLSTM-ED_F1.gif creado\n",
      "📑 Métricas guardadas en /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction/models/output/HybridLSTMModels/metrics_eval.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 📈 **Evaluador para salidas espaciales ConvLSTM**\n",
    "\n",
    "# ───────── Imports ──────────\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, xarray as xr, tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt, geopandas as gpd, imageio.v2 as imageio\n",
    "import sys\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# ───────── Paths & Constantes ─────────\n",
    "# ▶️ Path configuration\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
    "        if (p / '.git').exists():\n",
    "            BASE_PATH = p; break\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "print('BASE_PATH =', BASE_PATH)\n",
    "\n",
    "# Dataset paths\n",
    "DATA_DIR = BASE_PATH/'data'/'output'\n",
    "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
    "MODEL_DIR = BASE_PATH/'models'/'output'/'HybridLSTMModels'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
    "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMAGE_DIR = MODEL_DIR/'images'\n",
    "IMAGE_DIR.mkdir(exist_ok=True)\n",
    "FULL_NC = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "departamentos = gpd.read_file(MODEL_INPUT_DIR/'MGN_Departamento.shp')\n",
    "\n",
    "BASE_MODEL_DIR = MODEL_DIR\n",
    "GIF_DIR        = MODEL_DIR / \"gifs\"\n",
    "GIF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ╭──────────────────── Hyper‑parámetros globales ─────────────╮\n",
    "INPUT_WINDOW   = 60\n",
    "HORIZON        = 3\n",
    "TARGET_VAR     = 'total_precipitation'\n",
    "EPOCHS         = 12\n",
    "BATCH_SIZE     = 4           # tamaño pequeño → menor RAM GPU\n",
    "PATIENCE       = 10\n",
    "LR             = 1e-3\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "\n",
    "# ───────── Dataset & shapes ─────────\n",
    "ds = xr.open_dataset(FULL_NC); lat,lon=len(ds.latitude),len(ds.longitude)\n",
    "\n",
    "#╭────────────────────── Experiments & Folds ─────────────────╮\n",
    "\n",
    "# Factories ---------------------------------------------------\n",
    "\n",
    "def factory_no_attn(**kw):\n",
    "    return _build_convlstm_ed(use_attention=False, **kw)\n",
    "\n",
    "def factory_attn(**kw):\n",
    "    return _build_convlstm_ed(use_attention=True, **kw)\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "# ▸ Solo mostramos los tres primeros niveles; añade los demás igual\n",
    "BASE_FEATURES = [\n",
    "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
    "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
    "    'elevation','slope','aspect'\n",
    "]\n",
    "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
    "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
    "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
    "\n",
    "FOLDS = {'F1': {'year': 2018,'active': True}}\n",
    "\n",
    "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
    "    'ConvLSTM-ED': {\n",
    "        'active': True,\n",
    "        'feature_list': BASE_FEATURES,\n",
    "        'builder': factory_attn, #factory_no_attn,\n",
    "        'n_filters': 64,\n",
    "        'n_heads'  : 4\n",
    "    },\n",
    "    'ConvLSTM-ED-KCE': {\n",
    "        'active': True,\n",
    "        'feature_list': KCE_FEATURES,\n",
    "        'builder': factory_attn,\n",
    "        'n_filters': 64,\n",
    "        'n_heads'  : 4,\n",
    "    },\n",
    "    'ConvLSTM-ED-KCE-PAFC': {\n",
    "        'active': True,\n",
    "        'feature_list': PAFC_FEATURES,\n",
    "        'builder': factory_attn,\n",
    "        'n_filters': 96,\n",
    "        'n_heads'  : 6,\n",
    "    },\n",
    "}\n",
    "# ╰────────────────────────────────────────────────────────────╯\n",
    "\n",
    "\n",
    "def quick_plot(ax,data,cmap,title,date_label,vmin=None,vmax=None):\n",
    "    mesh=ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
    "    ax.coastlines(); ax.add_geometries(departamentos.geometry,ccrs.PlateCarree(),edgecolor='black',facecolor='none',linewidth=1)\n",
    "    gl=ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
    "    ax.set_title(f\"{title}\\n{date_label}\",pad=10); return mesh\n",
    "\n",
    "# ───────── Recuperamos diccionario EXPERIMENTS (del bloque de entrenamiento) ─────────\n",
    "from typing import Dict\n",
    "EXPERIMENTS:Dict[str,Dict[str,Any]] = {\n",
    "    'ConvLSTM-ED':              {'feature_list': \"+\".join(BASE_FEATURES).split(\"+\")},\n",
    "    'ConvLSTM-ED-KCE':          {'feature_list': \"+\".join(KCE_FEATURES).split(\"+\")},\n",
    "    'ConvLSTM-ED-KCE-PAFC':     {'feature_list': \"+\".join(PAFC_FEATURES).split(\"+\")},\n",
    "    # otros experimentos\n",
    "}\n",
    "\n",
    "# ———————————————————— Evaluación ————————————————————\n",
    "all_metrics=[]; times=pd.to_datetime(ds.time.values)\n",
    "for mpath in sorted(BASE_MODEL_DIR.glob(\"*.keras\")):\n",
    "    tag   = mpath.stem                        # p.ej. ConvLSTM-ED_F1\n",
    "    parts = tag.split(\"_\")\n",
    "    fold  = parts[-1]                         # F1\n",
    "    exp_token = \"_\".join(parts[:-1])\n",
    "    exp_name  = exp_token.replace(\"_\",\"+\")  # vuelve al nombre original con +\n",
    "    if exp_name not in EXPERIMENTS:\n",
    "        print(\"⚠️ Exp no encontrado para\",tag); continue\n",
    "    feats = EXPERIMENTS[exp_name]['feature_list']\n",
    "    print(f\"\\n🔍 Evaluando {tag} …\")\n",
    "\n",
    "    # — Extracción de arrays —\n",
    "    Xarr = ds[feats].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
    "    yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
    "    T,_,_,F = Xarr.shape\n",
    "    Xfull = Xarr; yfull=yarr  # mantenemos (T,H,W,F)\n",
    "\n",
    "    # ventana final (idéntica lógica del cuaderno original)\n",
    "    start=T-INPUT_WINDOW-HORIZON; end_w=start+INPUT_WINDOW; end_y=end_w+HORIZON\n",
    "    X_eval = Xfull[start:end_w]                 # (60,H,W,F)\n",
    "    y_eval = yfull[end_w:end_y]                 # (3,H,W)\n",
    "\n",
    "    # — Scalers (fit incremental) —\n",
    "    sx,sy = StandardScaler(),StandardScaler()\n",
    "    for t in range(T):\n",
    "        sx.partial_fit(Xfull[t].reshape(-1,F))\n",
    "        sy.partial_fit(yfull[t].reshape(-1,1))\n",
    "    Xe_sc = sx.transform(X_eval.reshape(-1,F)).reshape(1,INPUT_WINDOW,lat,lon,F)\n",
    "    ye_sc = sy.transform(y_eval.reshape(-1,1)).reshape(1,HORIZON,lat,lon,1)\n",
    "\n",
    "    # — Carga modelo y predicción —\n",
    "    model=tf.keras.models.load_model(mpath,compile=False)\n",
    "    yhat_sc=model.predict(Xe_sc,verbose=0)      # (1,3,H,W,1)\n",
    "    yhat   = sy.inverse_transform(yhat_sc.reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
    "    ytrue  = y_eval\n",
    "\n",
    "    # — Métricas por horizonte —\n",
    "    for h in range(HORIZON):\n",
    "        yt = ytrue[h].ravel()\n",
    "        yp = yhat[h].ravel()\n",
    "\n",
    "        # ---------- filtro NaN / ±∞ ----------\n",
    "        mask = np.isfinite(yt) & np.isfinite(yp)\n",
    "        if mask.sum() == 0:          # ventana vacía → se ignora\n",
    "            print(f\"   · h={h+1}: todos los valores son NaN/Inf → skip\")\n",
    "            continue\n",
    "        yt, yp = yt[mask], yp[mask]\n",
    "        # -------------------------------------\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(yt, yp))\n",
    "        mae  = mean_absolute_error(yt, yp)\n",
    "        mape = np.mean(np.abs((yt - yp) / (yt + 1e-5))) * 100\n",
    "        r2   = r2_score(yt, yp)\n",
    "\n",
    "        all_metrics.append(dict(\n",
    "            model      = tag,\n",
    "            experiment = exp_name,\n",
    "            fold       = fold,\n",
    "            horizon    = h + 1,\n",
    "            RMSE       = rmse,\n",
    "            MAE        = mae,\n",
    "            MAPE       = mape,\n",
    "            R2         = r2\n",
    "        ))\n",
    "\n",
    "    # — Figura Real vs Pred vs MAPE —\n",
    "    fig,axes=plt.subplots(HORIZON,3,figsize=(14,4*HORIZON),subplot_kw={'projection':ccrs.PlateCarree()})\n",
    "    dates=pd.date_range(times[end_w],periods=HORIZON,freq='MS')\n",
    "    vmin=0; vmax=max(yhat.max(),ytrue.max())\n",
    "    for h in range(HORIZON):\n",
    "        quick_plot(axes[h,0],ytrue[h],'Blues',f\"Real h={h+1}\",dates[h].strftime('%Y-%m'),vmin,vmax)\n",
    "        quick_plot(axes[h,1],yhat [h],'Blues',f\"Pred h={h+1}\",dates[h].strftime('%Y-%m'),vmin,vmax)\n",
    "        err=np.clip(np.abs((ytrue[h]-yhat[h])/(ytrue[h]+1e-5))*100,0,100)\n",
    "        quick_plot(axes[h,2],err,'Reds',f\"MAPE% h={h+1}\",dates[h].strftime('%Y-%m'),0,100)\n",
    "    fig.suptitle(f\"{tag}  — Eval final ventana\",fontsize=16); fig.tight_layout();\n",
    "    fig.savefig(BASE_MODEL_DIR/f\"fig_{tag}.png\"); plt.close(fig)\n",
    "\n",
    "    # — GIF —\n",
    "    frames=[]; pcm_min,pcm_max=0,yhat.max()\n",
    "    for h in range(HORIZON):\n",
    "        figg,ax=plt.subplots(1,1,figsize=(6,5),subplot_kw={'projection':ccrs.PlateCarree()})\n",
    "        m=ax.pcolormesh(ds.longitude,ds.latitude,yhat[h],cmap='Blues',shading='nearest',vmin=pcm_min,vmax=pcm_max,transform=ccrs.PlateCarree())\n",
    "        ax.coastlines(); ax.set_title(f\"{tag} – H{h+1}\"); figg.colorbar(m,ax=ax,fraction=0.046,pad=0.04)\n",
    "        tmp=GIF_DIR/f\"tmp_{tag}_{h}.png\"; figg.savefig(tmp,bbox_inches='tight'); plt.close(figg)\n",
    "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
    "    imageio.mimsave(GIF_DIR/f\"{tag}.gif\",frames,fps=0.5)\n",
    "    print(\"💾 GIF\",f\"{tag}.gif\",\"creado\")\n",
    "\n",
    "# ——— Guardar tabla ———\n",
    "pd.DataFrame(all_metrics).to_csv(BASE_MODEL_DIR/'metrics_eval.csv',index=False)\n",
    "print(\"📑 Métricas guardadas en\",BASE_MODEL_DIR/'metrics_eval.csv')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
