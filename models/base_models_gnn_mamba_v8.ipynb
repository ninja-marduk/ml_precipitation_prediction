{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# V8: GNN-Mamba for Monthly Precipitation Prediction\n",
    "\n",
    "## State Space Models + Graph Neural Networks\n",
    "\n",
    "**Innovation**: First application of Mamba SSM to regional precipitation prediction\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "| Component | V4 (Baseline) | V8 (This Model) |\n",
    "|-----------|---------------|------------------|\n",
    "| Spatial Encoder | GNN (GAT) | GNN (GAT) |\n",
    "| Temporal Encoder | LSTM + Attention | **Mamba SSM** |\n",
    "| Complexity | O(T) LSTM | O(T) Mamba |\n",
    "| Memory | Fixed cells | **Selective** |\n",
    "| Fusion | Concatenation | **Cross-Attention** |\n",
    "\n",
    "### Expected Improvements\n",
    "- Better long-range dependencies (60 months input)\n",
    "- Automatic seasonal pattern capture\n",
    "- More efficient memory usage\n",
    "\n",
    "### Target Metrics\n",
    "| Metric | V4 Baseline | V8 Target |\n",
    "|--------|-------------|----------|\n",
    "| R2 | 0.596 | > 0.62 |\n",
    "| RMSE | 84.37 mm | < 82 mm |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "colab_setup"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base path: d:\\github.com\\ninja-marduk\\ml_precipitation_prediction\n",
      "Running in: Local Environment\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 1: ENVIRONMENT SETUP\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "SEED = 42\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    \n",
    "    # Install dependencies\n",
    "    import torch\n",
    "    TORCH_VERSION = torch.__version__.split('+')[0]\n",
    "    CUDA_VERSION = torch.version.cuda\n",
    "    if CUDA_VERSION:\n",
    "        CUDA_TAG = f\"cu{CUDA_VERSION.replace('.', '')}\"\n",
    "    else:\n",
    "        CUDA_TAG = 'cpu'\n",
    "    \n",
    "    print(f\"PyTorch: {TORCH_VERSION}\")\n",
    "    print(f\"CUDA: {CUDA_VERSION}\")\n",
    "    \n",
    "    # Install torch_geometric\n",
    "    try:\n",
    "        import torch_geometric\n",
    "        print(f\"PyG already installed: {torch_geometric.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"Installing PyTorch Geometric...\")\n",
    "        pyg_url = f\"https://data.pyg.org/whl/torch-{TORCH_VERSION}+{CUDA_TAG}.html\"\n",
    "        !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f $pyg_url\n",
    "        !pip install torch-geometric\n",
    "    \n",
    "    # Install Mamba\n",
    "    print(\"\\nInstalling Mamba SSM...\")\n",
    "    !pip install mamba-ssm causal-conv1d>=1.1.0 --quiet\n",
    "    \n",
    "    # Other dependencies\n",
    "    !pip install netCDF4 xarray dask h5netcdf --quiet\n",
    "    \n",
    "    print(\"\\nEnvironment versions:\")\n",
    "    print(sys.version)\n",
    "    print(torch.__version__)\n",
    "    print(torch.version.cuda)\n",
    "    \n",
    "    env_info = {\n",
    "        'python': sys.version,\n",
    "        'torch': torch.__version__,\n",
    "        'cuda': torch.version.cuda\n",
    "    }\n",
    "    try:\n",
    "        import torch_geometric\n",
    "        env_info['torch_geometric'] = torch_geometric.__version__\n",
    "        print(f\"PyG: {torch_geometric.__version__}\")\n",
    "    except Exception:\n",
    "        print(\"PyG: not available\")\n",
    "    try:\n",
    "        import mamba_ssm\n",
    "        env_info['mamba_ssm'] = mamba_ssm.__version__\n",
    "        print(f\"Mamba-SSM: {mamba_ssm.__version__}\")\n",
    "    except Exception:\n",
    "        print(\"Mamba-SSM: not available\")\n",
    "    \n",
    "    env_dir = BASE_PATH / 'models' / 'output' / 'env_versions'\n",
    "    env_dir.mkdir(parents=True, exist_ok=True)\n",
    "    env_path = env_dir / 'v8_env_versions.json'\n",
    "    with open(env_path, 'w') as f:\n",
    "        json.dump(env_info, f, indent=2)\n",
    "    print(f\"Saved environment versions to: {env_path}\")\n",
    "    \n",
    "else:\n",
    "    BASE_PATH = Path(r'd:\\github.com\\ninja-marduk\\ml_precipitation_prediction')\n",
    "\n",
    "print(f\"\\nBase path: {BASE_PATH}\")\n",
    "print(f\"Running in: {'Google Colab' if IN_COLAB else 'Local Environment'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "imports"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Official mamba-ssm not available, using custom implementation (slower; may trigger CUDA timeout on Windows)\n",
      "\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce GTX 960M\n",
      "Memory: 4.3 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.1: IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import gc\n",
    "import copy\n",
    "import warnings\n",
    "import math\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any, Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Matplotlib setup\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "\n",
    "# Mamba SSM\n",
    "PREFER_OFFICIAL_MAMBA = True\n",
    "REQUIRE_OFFICIAL_MAMBA = False\n",
    "USE_OFFICIAL_MAMBA = False\n",
    "MAMBA_BACKEND = \"custom\"\n",
    "try:\n",
    "    from mamba_ssm import Mamba\n",
    "    if PREFER_OFFICIAL_MAMBA:\n",
    "        USE_OFFICIAL_MAMBA = True\n",
    "        MAMBA_BACKEND = \"official\"\n",
    "        print(\"Using official mamba-ssm package for Mamba blocks\")\n",
    "    else:\n",
    "        print(\"Official mamba-ssm available but disabled; using custom implementation\")\n",
    "except ImportError as e:\n",
    "    if REQUIRE_OFFICIAL_MAMBA:\n",
    "        raise RuntimeError(\n",
    "            \"mamba-ssm not available but REQUIRE_OFFICIAL_MAMBA=True. \"\n",
    "            \"Install mamba-ssm or set REQUIRE_OFFICIAL_MAMBA=False.\"\n",
    "        ) from e\n",
    "    print(\"Official mamba-ssm not available, using custom implementation (slower; may trigger CUDA timeout on Windows)\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_section"
   },
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "config"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== V8 GNN-Mamba Configuration ===\n",
      "  Input window: 60 months\n",
      "  Horizon: 12 months\n",
      "  Hidden dim: 64\n",
      "  Mamba d_state: 16\n",
      "  Mamba bidirectional: True\n",
      "  GNN type: GAT\n",
      "  Batch size: 4\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 2: CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class V8Config:\n",
    "    \"\"\"V8 GNN-Mamba Configuration.\"\"\"\n",
    "    \n",
    "    # === Data Configuration ===\n",
    "    input_window: int = 60          # 5 years of monthly data\n",
    "    horizon: int = 12               # Predict 12 months ahead\n",
    "    train_val_split: float = 0.8\n",
    "    \n",
    "    # Light mode for testing\n",
    "    light_mode: bool = True\n",
    "    light_grid_size: int = 10\n",
    "    \n",
    "    # Enabled horizons\n",
    "    enabled_horizons: List[int] = field(default_factory=lambda: [12])\n",
    "    \n",
    "    # Grid dimensions (updated after loading)\n",
    "    n_lat: int = 61\n",
    "    n_lon: int = 65\n",
    "    n_nodes: int = 61 * 65\n",
    "    \n",
    "    # === Model Dimensions ===\n",
    "    hidden_dim: int = 64            # Main hidden dimension\n",
    "    n_features: int = 5             # Input features\n",
    "    \n",
    "    # === GNN Configuration (Spatial) ===\n",
    "    gnn_type: str = 'GAT'           # GAT, SAGE, or GCN\n",
    "    gnn_num_layers: int = 3\n",
    "    gnn_num_heads: int = 4\n",
    "    gnn_dropout: float = 0.1\n",
    "    \n",
    "    # === MAMBA Configuration (Temporal) ===\n",
    "    mamba_d_state: int = 16         # SSM state dimension\n",
    "    mamba_d_conv: int = 4           # Causal conv kernel size\n",
    "    mamba_expand: int = 2           # Inner dimension expansion\n",
    "    mamba_num_layers: int = 2       # Number of Mamba blocks\n",
    "    mamba_bidirectional: bool = True  # Process forward + backward\n",
    "    mamba_dropout: float = 0.1\n",
    "    mamba_node_chunk: int = 256    # Nodes per temporal chunk (0 = disable)\n",
    "    \n",
    "    # === Cross-Modal Attention ===\n",
    "    cross_attn_heads: int = 4\n",
    "    cross_attn_dropout: float = 0.1\n",
    "    \n",
    "    # === Training ===\n",
    "    epochs: int = 100\n",
    "    batch_size: int = 4\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-5\n",
    "    patience: int = 15\n",
    "    gradient_clip: float = 1.0\n",
    "    \n",
    "    # === Physics Loss (from V7) ===\n",
    "    use_physics_loss: bool = True\n",
    "    lambda_mass_conservation: float = 0.05\n",
    "    lambda_orographic: float = 0.1\n",
    "    high_elev_threshold: float = 3000.0\n",
    "    \n",
    "    # === Scheduler ===\n",
    "    scheduler_type: str = 'cosine'  # 'plateau' or 'cosine'\n",
    "    cosine_T0: int = 20\n",
    "    cosine_T_mult: int = 2\n",
    "    \n",
    "    # === Output ===\n",
    "    output_dir: str = 'V8_GNN_Mamba'\n",
    "    save_checkpoints: bool = True\n",
    "    \n",
    "    # === Data handling ===\n",
    "    allow_missing_features: bool = True\n",
    "\n",
    "    # === Outputs / Diagnostics ===\n",
    "    export_predictions: bool = True\n",
    "    export_history: bool = True\n",
    "    plot_graph_diagnostics: bool = True\n",
    "    plot_results_summary: bool = True\n",
    "    plot_metrics_table: bool = True\n",
    "    generate_map_plots: bool = True\n",
    "    map_export_horizon: int = 1\n",
    "    map_export_max_samples: int = 3\n",
    "\n",
    "    # === Quality checks / gates ===\n",
    "    enforce_quality_gates: bool = False\n",
    "    quality_neg_pred_max: float = 0.01\n",
    "    quality_nan_max: float = 0.0\n",
    "    quality_bias_pct_max: float = 25.0\n",
    "\n",
    "\n",
    "# Feature sets\n",
    "FEATURE_SETS = {\n",
    "    'BASIC': ['total_precipitation'],\n",
    "    'KCE': ['total_precipitation', 'elev_low', 'elev_med', 'elev_high'],\n",
    "    'FULL': ['total_precipitation', 'elevation', 'slope', 'aspect',\n",
    "             'elev_low', 'elev_med', 'elev_high']\n",
    "}\n",
    "\n",
    "# Initialize config\n",
    "CONFIG = V8Config()\n",
    "\n",
    "# Adjust for Colab memory constraints\n",
    "if IN_COLAB:\n",
    "    CONFIG.batch_size = 2\n",
    "    CONFIG.hidden_dim = 48\n",
    "    CONFIG.mamba_d_state = 12\n",
    "    CONFIG.gnn_num_heads = 2\n",
    "    print(\"Adjusted config for Colab memory constraints\")\n",
    "\n",
    "print(\"\\n=== V8 GNN-Mamba Configuration ===\")\n",
    "print(f\"  Input window: {CONFIG.input_window} months\")\n",
    "print(f\"  Horizon: {CONFIG.horizon} months\")\n",
    "print(f\"  Hidden dim: {CONFIG.hidden_dim}\")\n",
    "print(f\"  Mamba d_state: {CONFIG.mamba_d_state}\")\n",
    "print(f\"  Mamba bidirectional: {CONFIG.mamba_bidirectional}\")\n",
    "print(f\"  GNN type: {CONFIG.gnn_type}\")\n",
    "print(f\"  Batch size: {CONFIG.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mamba_implementation"
   },
   "source": [
    "## 3. Mamba Implementation\n",
    "\n",
    "### State Space Model Basics\n",
    "\n",
    "Mamba is based on the continuous-time state space model:\n",
    "\n",
    "$$h'(t) = Ah(t) + Bx(t)$$\n",
    "$$y(t) = Ch(t) + Dx(t)$$\n",
    "\n",
    "**Key innovation**: A, B, C are **input-dependent** (selective), allowing the model to:\n",
    "- Focus on relevant information\n",
    "- Forget irrelevant context\n",
    "- Maintain O(n) complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mamba_block"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Mamba implementation (custom)...\n",
      "  Input shape: torch.Size([2, 60, 64])\n",
      "  Output shape: torch.Size([2, 60, 64])\n",
      "  Parameters: 127,360\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 3: MAMBA IMPLEMENTATION\n",
    "# ============================================================\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Selective State Space Model Block.\n",
    "    \n",
    "    Based on: \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\"\n",
    "    \n",
    "    Key components:\n",
    "    - Input-dependent (selective) SSM parameters\n",
    "    - Gated architecture with SiLU activation\n",
    "    - Causal 1D convolution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 4,\n",
    "        expand: int = 2,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.d_inner = d_model * expand\n",
    "        \n",
    "        # Input projection (splits into x and z for gating)\n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n",
    "        \n",
    "        # Causal 1D convolution (depthwise)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            kernel_size=d_conv,\n",
    "            padding=d_conv - 1,\n",
    "            groups=self.d_inner  # Depthwise separable\n",
    "        )\n",
    "        \n",
    "        # SSM parameters projection (input-dependent B, C, dt)\n",
    "        self.x_proj = nn.Linear(self.d_inner, d_state * 2 + 1, bias=False)\n",
    "        \n",
    "        # A is log-parametrized for numerical stability\n",
    "        A = torch.arange(1, d_state + 1, dtype=torch.float32)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        \n",
    "        # D is the skip connection coefficient\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        \n",
    "        # Normalization and dropout\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            y: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch, seq_len, _ = x.shape\n",
    "        residual = x\n",
    "        \n",
    "        # Project and split for gating\n",
    "        xz = self.in_proj(x)  # (B, L, 2*d_inner)\n",
    "        x_branch, z = xz.chunk(2, dim=-1)  # Each: (B, L, d_inner)\n",
    "        \n",
    "        # Causal 1D convolution\n",
    "        x_conv = x_branch.transpose(1, 2)  # (B, d_inner, L)\n",
    "        x_conv = self.conv1d(x_conv)[:, :, :seq_len]  # Truncate to maintain causality\n",
    "        x_conv = x_conv.transpose(1, 2)  # (B, L, d_inner)\n",
    "        x_conv = F.silu(x_conv)\n",
    "        \n",
    "        # Selective SSM\n",
    "        y = self.ssm(x_conv)\n",
    "        \n",
    "        # Gating with z\n",
    "        z = F.silu(z)\n",
    "        output = y * z\n",
    "        \n",
    "        # Output projection + residual\n",
    "        output = self.out_proj(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.norm(output + residual)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def ssm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Selective State Space Model computation.\n",
    "        \n",
    "        Discretized SSM:\n",
    "            h_t = A_bar * h_{t-1} + B_bar * x_t\n",
    "            y_t = C * h_t + D * x_t\n",
    "        \"\"\"\n",
    "        batch, seq_len, d_inner = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Project to get input-dependent B, C, dt\n",
    "        x_proj = self.x_proj(x)  # (B, L, d_state*2 + 1)\n",
    "        \n",
    "        # Split projections\n",
    "        B = x_proj[:, :, :self.d_state]  # (B, L, d_state)\n",
    "        C = x_proj[:, :, self.d_state:2*self.d_state]  # (B, L, d_state)\n",
    "        dt = F.softplus(x_proj[:, :, -1:])  # (B, L, 1) - positive timestep\n",
    "        \n",
    "        # A from log-parametrization (negative for stability)\n",
    "        A = -torch.exp(self.A_log.float())  # (d_state,)\n",
    "        \n",
    "        # Discretize: A_bar = exp(A * dt)\n",
    "        A_bar = torch.exp(\n",
    "            A.view(1, 1, self.d_state) * dt\n",
    "        )  # (B, L, d_state)\n",
    "        \n",
    "        # B_bar = B * dt (simplified discretization)\n",
    "        B_bar = B * dt  # (B, L, d_state)\n",
    "        \n",
    "        # Sequential scan (can be parallelized with associative scan)\n",
    "        h = torch.zeros(batch, d_inner, self.d_state, device=device)\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # State update: h_t = A_bar * h_{t-1} + B_bar * x_t\n",
    "            h = h * A_bar[:, t, :].unsqueeze(1) + \\\n",
    "                x[:, t, :].unsqueeze(-1) * B_bar[:, t, :].unsqueeze(1)\n",
    "            # h: (B, d_inner, d_state)\n",
    "            \n",
    "            # Output: y_t = C * h_t\n",
    "            y_t = (h * C[:, t, :].unsqueeze(1)).sum(dim=-1)  # (B, d_inner)\n",
    "            outputs.append(y_t)\n",
    "        \n",
    "        y = torch.stack(outputs, dim=1)  # (B, L, d_inner)\n",
    "        \n",
    "        # Skip connection with D\n",
    "        y = y + x * self.D.view(1, 1, -1)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class OfficialMambaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around the official mamba-ssm Mamba module with residual + norm.\n",
    "    \n",
    "    Uses the official CUDA kernels when available.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 4,\n",
    "        expand: int = 2,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mamba = Mamba(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        y = self.mamba(x)\n",
    "        y = self.dropout(y)\n",
    "        return self.norm(y + residual)\n",
    "\n",
    "\n",
    "class MambaTemporalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer bidirectional Mamba encoder for temporal sequences.\n",
    "    \n",
    "    Replaces LSTM + Temporal Attention from V4.\n",
    "    Uses official mamba-ssm when available, otherwise falls back to the custom block.\n",
    "    \n",
    "    Advantages:\n",
    "    - O(n) complexity (same as LSTM)\n",
    "    - Selective memory (learns what to remember)\n",
    "    - Parallelizable during training\n",
    "    - Better long-range dependencies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 4,\n",
    "        expand: int = 2,\n",
    "        n_layers: int = 2,\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        self.n_layers = n_layers\n",
    "        self.using_official = USE_OFFICIAL_MAMBA\n",
    "        block_cls = OfficialMambaBlock if USE_OFFICIAL_MAMBA else MambaBlock\n",
    "        \n",
    "        # Forward Mamba layers\n",
    "        self.forward_layers = nn.ModuleList([\n",
    "            block_cls(\n",
    "                d_model=d_model,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Backward Mamba layers (if bidirectional)\n",
    "        if bidirectional:\n",
    "            self.backward_layers = nn.ModuleList([\n",
    "                block_cls(\n",
    "                    d_model=d_model,\n",
    "                    d_state=d_state,\n",
    "                    d_conv=d_conv,\n",
    "                    expand=expand,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ])\n",
    "            # Merge forward and backward\n",
    "            self.merge = nn.Linear(d_model * 2, d_model)\n",
    "            self.merge_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.output_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            h: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        h_fwd = x\n",
    "        for layer in self.forward_layers:\n",
    "            h_fwd = layer(h_fwd)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # Backward pass (flip, process, flip back)\n",
    "            h_bwd = torch.flip(x, dims=[1])\n",
    "            for layer in self.backward_layers:\n",
    "                h_bwd = layer(h_bwd)\n",
    "            h_bwd = torch.flip(h_bwd, dims=[1])\n",
    "            \n",
    "            # Merge forward and backward\n",
    "            h = self.merge(torch.cat([h_fwd, h_bwd], dim=-1))\n",
    "            h = self.merge_norm(h)\n",
    "        else:\n",
    "            h = h_fwd\n",
    "        \n",
    "        return self.output_norm(h)\n",
    "\n",
    "\n",
    "# Test Mamba implementation\n",
    "print(f\"Testing Mamba implementation ({MAMBA_BACKEND})...\")\n",
    "test_mamba = MambaTemporalEncoder(\n",
    "    d_model=CONFIG.hidden_dim,\n",
    "    d_state=CONFIG.mamba_d_state,\n",
    "    d_conv=CONFIG.mamba_d_conv,\n",
    "    expand=CONFIG.mamba_expand,\n",
    "    n_layers=CONFIG.mamba_num_layers,\n",
    "    bidirectional=CONFIG.mamba_bidirectional\n",
    ").to(device)\n",
    "\n",
    "test_input = torch.randn(2, CONFIG.input_window, CONFIG.hidden_dim).to(device)\n",
    "test_output = test_mamba(test_input)\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in test_mamba.parameters()):,}\")\n",
    "del test_mamba, test_input, test_output\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnn_section"
   },
   "source": [
    "## 4. Spatial GNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gnn_encoder"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpatialGNNEncoder defined successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 4: SPATIAL GNN ENCODER\n",
    "# ============================================================\n",
    "\n",
    "class SpatialGNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Neural Network for spatial encoding.\n",
    "    \n",
    "    Supports GAT, GraphSAGE, and GCN.\n",
    "    Based on V4 architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int = 3,\n",
    "        gnn_type: str = 'GAT',\n",
    "        num_heads: int = 4,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gnn_type = gnn_type\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # GNN layers\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if gnn_type == 'GAT':\n",
    "                # GAT: multi-head attention\n",
    "                self.gnn_layers.append(\n",
    "                    GATConv(\n",
    "                        in_channels=hidden_dim,\n",
    "                        out_channels=hidden_dim // num_heads,\n",
    "                        heads=num_heads,\n",
    "                        dropout=dropout,\n",
    "                        concat=True\n",
    "                    )\n",
    "                )\n",
    "            elif gnn_type == 'SAGE':\n",
    "                self.gnn_layers.append(\n",
    "                    SAGEConv(hidden_dim, hidden_dim)\n",
    "                )\n",
    "            else:  # GCN\n",
    "                self.gnn_layers.append(\n",
    "                    GCNConv(hidden_dim, hidden_dim)\n",
    "                )\n",
    "            \n",
    "            self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_weight: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (num_nodes, input_dim) or (batch*num_nodes, input_dim)\n",
    "            edge_index: (2, num_edges)\n",
    "            edge_weight: (num_edges,) optional\n",
    "        Returns:\n",
    "            h: (num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Input projection\n",
    "        h = self.input_proj(x)\n",
    "        \n",
    "        # GNN layers with residual connections\n",
    "        for i, (gnn, norm) in enumerate(zip(self.gnn_layers, self.norms)):\n",
    "            h_res = h\n",
    "            \n",
    "            # GNN forward\n",
    "            if self.gnn_type == 'GAT':\n",
    "                h = gnn(h, edge_index)\n",
    "            elif edge_weight is not None and self.gnn_type == 'GCN':\n",
    "                h = gnn(h, edge_index, edge_weight)\n",
    "            else:\n",
    "                h = gnn(h, edge_index)\n",
    "            \n",
    "            # Activation + Normalization + Residual\n",
    "            h = F.gelu(h)\n",
    "            h = norm(h)\n",
    "            h = self.dropout(h)\n",
    "            h = h + h_res  # Residual\n",
    "        \n",
    "        return self.output_norm(h)\n",
    "\n",
    "\n",
    "print(\"SpatialGNNEncoder defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "main_model"
   },
   "source": [
    "## 5. GNN-Mamba V8 Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gnn_mamba_v8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing GNN_Mamba_V8...\n",
      "  Input shape: torch.Size([2, 12, 100, 4])\n",
      "  Output shape: torch.Size([2, 100, 6])\n",
      "  Total parameters: 52,246\n",
      "\n",
      "GNN_Mamba_V8 test passed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 5: GNN-MAMBA V8 MAIN MODEL\n",
    "# ============================================================\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention between spatial (GNN) and temporal (Mamba) representations.\n",
    "    \n",
    "    Allows spatial features to attend to temporal context and vice versa.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int = 4,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key_value: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch, n_nodes, d_model) - spatial features\n",
    "            key_value: (batch, n_nodes, d_model) - temporal features\n",
    "        Returns:\n",
    "            output: (batch, n_nodes, d_model)\n",
    "        \"\"\"\n",
    "        # Cross-attention\n",
    "        attn_out, _ = self.attn(query, key_value, key_value)\n",
    "        x = self.norm1(query + attn_out)\n",
    "        \n",
    "        # Feed-forward\n",
    "        x = self.norm2(x + self.ffn(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class GNN_Mamba_V8(nn.Module):\n",
    "    \"\"\"\n",
    "    V8: Graph Neural Network + Mamba State Space Model\n",
    "    \n",
    "    Architecture:\n",
    "    1. Input projection\n",
    "    2. Parallel branches:\n",
    "       - GNN: spatial encoding per timestep\n",
    "       - Mamba: temporal encoding per node\n",
    "    3. Cross-modal attention fusion\n",
    "    4. Prediction head\n",
    "    \n",
    "    Improvements over V4:\n",
    "    - Mamba replaces LSTM + Attention\n",
    "    - Cross-modal attention instead of concatenation\n",
    "    - Better handling of long sequences (60 months)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: V8Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # === Input Projection ===\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(config.n_features, config.hidden_dim),\n",
    "            nn.LayerNorm(config.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.gnn_dropout)\n",
    "        )\n",
    "        \n",
    "        # === Spatial Branch: GNN ===\n",
    "        self.gnn_encoder = SpatialGNNEncoder(\n",
    "            input_dim=config.hidden_dim,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            num_layers=config.gnn_num_layers,\n",
    "            gnn_type=config.gnn_type,\n",
    "            num_heads=config.gnn_num_heads,\n",
    "            dropout=config.gnn_dropout\n",
    "        )\n",
    "        \n",
    "        # === Temporal Branch: Mamba ===\n",
    "        self.mamba_encoder = MambaTemporalEncoder(\n",
    "            d_model=config.hidden_dim,\n",
    "            d_state=config.mamba_d_state,\n",
    "            d_conv=config.mamba_d_conv,\n",
    "            expand=config.mamba_expand,\n",
    "            n_layers=config.mamba_num_layers,\n",
    "            bidirectional=config.mamba_bidirectional,\n",
    "            dropout=config.mamba_dropout\n",
    "        )\n",
    "        \n",
    "        # === Cross-Modal Fusion ===\n",
    "        self.cross_attention = CrossModalAttention(\n",
    "            d_model=config.hidden_dim,\n",
    "            num_heads=config.cross_attn_heads,\n",
    "            dropout=config.cross_attn_dropout\n",
    "        )\n",
    "        \n",
    "        # === Prediction Head ===\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.gnn_dropout),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.gnn_dropout),\n",
    "            nn.Linear(config.hidden_dim // 2, config.horizon)\n",
    "        )\n",
    "        \n",
    "        # Store dimensions for reshaping\n",
    "        self.n_lat = config.n_lat\n",
    "        self.n_lon = config.n_lon\n",
    "        self.n_nodes = config.n_nodes\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with Xavier/Kaiming.\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and param.dim() >= 2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "                \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_weight: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, seq_len, n_nodes, n_features) or\n",
    "               (batch, seq_len, lat, lon, n_features)\n",
    "            edge_index: (2, num_edges)\n",
    "            edge_weight: (num_edges,) optional\n",
    "            \n",
    "        Returns:\n",
    "            pred: (batch, n_nodes, horizon)\n",
    "            info: dict with intermediate representations\n",
    "        \"\"\"\n",
    "        # Handle different input shapes\n",
    "        if x.dim() == 5:  # (B, T, lat, lon, F)\n",
    "            batch_size, seq_len, n_lat, n_lon, n_features = x.shape\n",
    "            n_nodes = n_lat * n_lon\n",
    "            x = x.view(batch_size, seq_len, n_nodes, n_features)\n",
    "        else:  # (B, T, N, F)\n",
    "            batch_size, seq_len, n_nodes, n_features = x.shape\n",
    "        \n",
    "        # 1. Input projection\n",
    "        x = self.input_proj(x)  # (B, T, N, H)\n",
    "        \n",
    "        # 2. Spatial encoding (GNN per timestep)\n",
    "        h_spatial = self._encode_spatial(x, edge_index, edge_weight)\n",
    "        # (B, T, N, H)\n",
    "        \n",
    "        # 3. Temporal encoding (Mamba per node)\n",
    "        h_temporal = self._encode_temporal(x)\n",
    "        # (B, T, N, H)\n",
    "        \n",
    "        # 4. Cross-modal fusion (at final timestep)\n",
    "        h_fused = self._fuse_representations(h_spatial, h_temporal)\n",
    "        # (B, N, H)\n",
    "        \n",
    "        # 5. Prediction\n",
    "        pred = self.predictor(h_fused)  # (B, N, horizon)\n",
    "        \n",
    "        info = {\n",
    "            'h_spatial': h_spatial[:, -1, :, :],  # Last timestep\n",
    "            'h_temporal': h_temporal[:, -1, :, :],\n",
    "            'h_fused': h_fused\n",
    "        }\n",
    "        \n",
    "        return pred, info\n",
    "    \n",
    "    def _encode_spatial(self, x, edge_index, edge_weight):\n",
    "        \"\"\"Apply GNN to each timestep.\"\"\"\n",
    "        batch_size, seq_len, n_nodes, hidden_dim = x.shape\n",
    "        \n",
    "        # Process in chunks to save memory\n",
    "        chunk_size = 10  # timesteps per chunk\n",
    "        outputs = []\n",
    "        \n",
    "        for t_start in range(0, seq_len, chunk_size):\n",
    "            t_end = min(t_start + chunk_size, seq_len)\n",
    "            x_chunk = x[:, t_start:t_end, :, :]  # (B, chunk, N, H)\n",
    "            \n",
    "            chunk_len = t_end - t_start\n",
    "            x_flat = x_chunk.reshape(batch_size * chunk_len, n_nodes, hidden_dim)\n",
    "            \n",
    "            # Apply GNN to each graph in batch\n",
    "            h_list = []\n",
    "            for i in range(x_flat.shape[0]):\n",
    "                h_i = self.gnn_encoder(x_flat[i], edge_index, edge_weight)\n",
    "                h_list.append(h_i)\n",
    "            \n",
    "            h_chunk = torch.stack(h_list, dim=0)  # (B*chunk, N, H)\n",
    "            h_chunk = h_chunk.view(batch_size, chunk_len, n_nodes, hidden_dim)\n",
    "            outputs.append(h_chunk)\n",
    "        \n",
    "        return torch.cat(outputs, dim=1)  # (B, T, N, H)\n",
    "    \n",
    "    def _encode_temporal(self, x):\n",
    "        \"\"\"Apply Mamba to each node's temporal sequence.\"\"\"\n",
    "        batch_size, seq_len, n_nodes, hidden_dim = x.shape\n",
    "        \n",
    "        # Chunk nodes to avoid long CUDA kernels on large grids\n",
    "        chunk_size = getattr(self.config, 'mamba_node_chunk', 0) or n_nodes\n",
    "        outputs = []\n",
    "        \n",
    "        for n_start in range(0, n_nodes, chunk_size):\n",
    "            n_end = min(n_start + chunk_size, n_nodes)\n",
    "            x_chunk = x[:, :, n_start:n_end, :]  # (B, T, chunk, H)\n",
    "            \n",
    "            # Reshape: (B, T, chunk, H) -> (B*chunk, T, H)\n",
    "            x_temporal = x_chunk.permute(0, 2, 1, 3)\n",
    "            x_temporal = x_temporal.reshape(batch_size * (n_end - n_start), seq_len, hidden_dim)\n",
    "            \n",
    "            # Apply Mamba\n",
    "            h_temporal = self.mamba_encoder(x_temporal)  # (B*chunk, T, H)\n",
    "            \n",
    "            # Reshape back: (B*chunk, T, H) -> (B, T, chunk, H)\n",
    "            h_temporal = h_temporal.view(batch_size, n_end - n_start, seq_len, hidden_dim)\n",
    "            h_temporal = h_temporal.permute(0, 2, 1, 3)\n",
    "            outputs.append(h_temporal)\n",
    "        \n",
    "        return torch.cat(outputs, dim=2)  # (B, T, N, H)\n",
    "    \n",
    "    def _fuse_representations(self, h_spatial, h_temporal):\n",
    "        \"\"\"Fuse spatial and temporal representations at final timestep.\"\"\"\n",
    "        # Take final timestep\n",
    "        h_spatial_final = h_spatial[:, -1, :, :]  # (B, N, H)\n",
    "        h_temporal_final = h_temporal[:, -1, :, :]  # (B, N, H)\n",
    "        \n",
    "        # Cross-attention: spatial queries temporal\n",
    "        h_fused = self.cross_attention(h_spatial_final, h_temporal_final)\n",
    "        \n",
    "        return h_fused  # (B, N, H)\n",
    "\n",
    "\n",
    "# Test model\n",
    "print(\"\\nTesting GNN_Mamba_V8...\")\n",
    "test_config = V8Config(\n",
    "    n_features=4,\n",
    "    n_lat=10,\n",
    "    n_lon=10,\n",
    "    n_nodes=100,\n",
    "    hidden_dim=32,\n",
    "    mamba_d_state=8,\n",
    "    input_window=12,\n",
    "    horizon=6\n",
    ")\n",
    "\n",
    "test_model = GNN_Mamba_V8(test_config).to(device)\n",
    "test_x = torch.randn(2, 12, 100, 4).to(device)\n",
    "test_edge_index = torch.randint(0, 100, (2, 500)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred, test_info = test_model(test_x, test_edge_index)\n",
    "\n",
    "print(f\"  Input shape: {test_x.shape}\")\n",
    "print(f\"  Output shape: {test_pred.shape}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "\n",
    "del test_model, test_x, test_pred, test_info\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "print(\"\\nGNN_Mamba_V8 test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## 6. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "data_loading"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: d:\\github.com\\ninja-marduk\\ml_precipitation_prediction\\data\\output\\complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc\n",
      "Output path: d:\\github.com\\ninja-marduk\\ml_precipitation_prediction\\models\\output\\V8_GNN_Mamba\n",
      "Data exists: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 6: DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "OUTPUT_ROOT = BASE_PATH / 'models' / 'output' / CONFIG.output_dir\n",
    "DATA_OUT_DIR = OUTPUT_ROOT / 'data'\n",
    "FIG_OUT_DIR = OUTPUT_ROOT / 'figures'\n",
    "COMP_DIR = OUTPUT_ROOT / 'comparisons'\n",
    "TRAIN_LOG_DIR = OUTPUT_ROOT / 'training_metrics'\n",
    "MAP_OUT_DIR = OUTPUT_ROOT / 'map_exports'\n",
    "\n",
    "for d in [OUTPUT_ROOT, DATA_OUT_DIR, FIG_OUT_DIR, COMP_DIR, TRAIN_LOG_DIR, MAP_OUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_ROOT}\")\n",
    "print(f\"Data exists: {DATA_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "data_functions"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 6.1: DATA FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def validate_dataset(ds: xr.Dataset, config: V8Config) -> Tuple[str, str]:\n",
    "    \"\"\"Validate dataset dimensions.\"\"\"\n",
    "    lat_candidates = ['latitude', 'lat', 'y']\n",
    "    lon_candidates = ['longitude', 'lon', 'x']\n",
    "    \n",
    "    lat_dim = next((d for d in lat_candidates if d in ds.dims), None)\n",
    "    lon_dim = next((d for d in lon_candidates if d in ds.dims), None)\n",
    "    \n",
    "    if lat_dim is None or lon_dim is None:\n",
    "        raise ValueError(f\"Cannot find lat/lon dims in {list(ds.dims.keys())}\")\n",
    "    \n",
    "    if 'time' not in ds.dims:\n",
    "        raise ValueError(\"Dataset must have 'time' dimension\")\n",
    "    \n",
    "    # Update config\n",
    "    config.n_lat = ds.dims[lat_dim]\n",
    "    config.n_lon = ds.dims[lon_dim]\n",
    "    config.n_nodes = config.n_lat * config.n_lon\n",
    "    \n",
    "    return lat_dim, lon_dim\n",
    "\n",
    "\n",
    "def load_dataset(data_path: Path, config: V8Config) -> xr.Dataset:\n",
    "    \"\"\"Load and validate NetCDF dataset.\"\"\"\n",
    "    print(f\"Loading dataset from: {data_path}\")\n",
    "    ds = xr.open_dataset(data_path)\n",
    "    \n",
    "    lat_dim, lon_dim = validate_dataset(ds, config)\n",
    "    \n",
    "    print(f\"Dataset dimensions:\")\n",
    "    for dim, size in ds.dims.items():\n",
    "        print(f\"  - {dim}: {size}\")\n",
    "    \n",
    "    print(f\"Available variables: {list(ds.data_vars)}\")\n",
    "    \n",
    "    if config.light_mode:\n",
    "        ds = ds.isel({\n",
    "            lat_dim: slice(0, config.light_grid_size),\n",
    "            lon_dim: slice(0, config.light_grid_size)\n",
    "        })\n",
    "        config.n_lat = config.light_grid_size\n",
    "        config.n_lon = config.light_grid_size\n",
    "        config.n_nodes = config.n_lat * config.n_lon\n",
    "        print(f\"Light mode: using {config.light_grid_size}x{config.light_grid_size} grid\")\n",
    "    \n",
    "    return ds, lat_dim, lon_dim\n",
    "\n",
    "\n",
    "def create_elevation_clusters(ds: xr.Dataset, n_clusters: int = 3) -> xr.Dataset:\n",
    "    \"\"\"Add elevation cluster features (KCE).\"\"\"\n",
    "    if 'elevation' not in ds:\n",
    "        print(\"Warning: No elevation data, skipping clustering\")\n",
    "        return ds\n",
    "    \n",
    "    elevation = ds['elevation'].values\n",
    "    elev_dims = ds['elevation'].dims\n",
    "    \n",
    "    if elevation.ndim == 3:\n",
    "        elevation = elevation[0]\n",
    "        elev_dims = elev_dims[-2:]\n",
    "    \n",
    "    valid_mask = ~np.isnan(elevation)\n",
    "    elev_flat = elevation[valid_mask].reshape(-1, 1)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=10)\n",
    "    labels = np.full(elevation.shape, -1)\n",
    "    labels[valid_mask] = kmeans.fit_predict(elev_flat)\n",
    "    \n",
    "    for i, name in enumerate(['elev_low', 'elev_med', 'elev_high']):\n",
    "        cluster_data = np.zeros_like(elevation)\n",
    "        cluster_data[labels == i] = 1.0\n",
    "        ds[name] = xr.DataArray(data=cluster_data, dims=elev_dims)\n",
    "    \n",
    "    print(\"Added elevation clusters: elev_low, elev_med, elev_high\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def build_spatial_graph(\n",
    "    ds: xr.Dataset,\n",
    "    lat_dim: str,\n",
    "    lon_dim: str,\n",
    "    k_neighbors: int = 8,\n",
    "    max_edges: int = 500000\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Build spatial graph based on geographic proximity.\"\"\"\n",
    "    lat_vals = ds[lat_dim].values\n",
    "    lon_vals = ds[lon_dim].values\n",
    "    \n",
    "    n_lat, n_lon = len(lat_vals), len(lon_vals)\n",
    "    n_nodes = n_lat * n_lon\n",
    "    \n",
    "    # Create node positions\n",
    "    lat_grid, lon_grid = np.meshgrid(lat_vals, lon_vals, indexing='ij')\n",
    "    positions = np.stack([lat_grid.flatten(), lon_grid.flatten()], axis=1)\n",
    "    \n",
    "    # Build edges based on grid connectivity (8-connectivity)\n",
    "    edges = []\n",
    "    weights = []\n",
    "    \n",
    "    for i in range(n_lat):\n",
    "        for j in range(n_lon):\n",
    "            node_idx = i * n_lon + j\n",
    "            \n",
    "            # 8-connectivity neighbors\n",
    "            neighbors = [\n",
    "                (i-1, j-1), (i-1, j), (i-1, j+1),\n",
    "                (i, j-1),             (i, j+1),\n",
    "                (i+1, j-1), (i+1, j), (i+1, j+1)\n",
    "            ]\n",
    "            \n",
    "            for ni, nj in neighbors:\n",
    "                if 0 <= ni < n_lat and 0 <= nj < n_lon:\n",
    "                    neighbor_idx = ni * n_lon + nj\n",
    "                    \n",
    "                    # Distance-based weight\n",
    "                    dist = np.sqrt(\n",
    "                        (lat_vals[i] - lat_vals[ni])**2 +\n",
    "                        (lon_vals[j] - lon_vals[nj])**2\n",
    "                    )\n",
    "                    weight = 1.0 / (dist + 1e-6)\n",
    "                    \n",
    "                    edges.append([node_idx, neighbor_idx])\n",
    "                    weights.append(weight)\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_weight = torch.tensor(weights, dtype=torch.float32)\n",
    "    \n",
    "    # Normalize weights\n",
    "    edge_weight = edge_weight / edge_weight.max()\n",
    "    \n",
    "    # Limit edges if needed\n",
    "    if edge_index.shape[1] > max_edges:\n",
    "        top_k = torch.topk(edge_weight, max_edges).indices\n",
    "        edge_index = edge_index[:, top_k]\n",
    "        edge_weight = edge_weight[top_k]\n",
    "    \n",
    "    print(f\"Graph built: {n_nodes} nodes, {edge_index.shape[1]} edges\")\n",
    "    \n",
    "    return edge_index, edge_weight\n",
    "\n",
    "\n",
    "def plot_graph_diagnostics(\n",
    "    edge_index: torch.Tensor,\n",
    "    edge_weight: torch.Tensor,\n",
    "    output_dir: Path,\n",
    "    config: V8Config\n",
    ") -> None:\n",
    "    \"\"\"Plot simple graph diagnostics.\"\"\"\n",
    "    if edge_index is None or edge_weight is None:\n",
    "        print('Graph diagnostics skipped: missing edge data')\n",
    "        return\n",
    "\n",
    "    src = edge_index[0].cpu().numpy() if torch.is_tensor(edge_index) else np.asarray(edge_index[0])\n",
    "    weights = edge_weight.cpu().numpy() if torch.is_tensor(edge_weight) else np.asarray(edge_weight)\n",
    "    degrees = np.bincount(src, minlength=config.n_nodes)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].hist(degrees, bins=30, color='steelblue', edgecolor='black')\n",
    "    axes[0].set_title('Node Degree Distribution')\n",
    "    axes[0].set_xlabel('Degree')\n",
    "    axes[0].set_ylabel('Count')\n",
    "\n",
    "    axes[1].hist(weights, bins=30, color='darkorange', edgecolor='black')\n",
    "    axes[1].set_title('Edge Weight Distribution')\n",
    "    axes[1].set_xlabel('Weight')\n",
    "    axes[1].set_ylabel('Count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig_path = output_dir / 'v8_graph_diagnostics.png'\n",
    "    fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Graph diagnostics saved to: {fig_path}\")\n",
    "\n",
    "\n",
    "def extract_features(\n",
    "    ds: xr.Dataset,\n",
    "    feature_names: List[str],\n",
    "    config: V8Config\n",
    ") -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"Extract features from dataset.\"\"\"\n",
    "    features = []\n",
    "    missing = []\n",
    "    \n",
    "    for name in feature_names:\n",
    "        if name in ds.data_vars:\n",
    "            data = ds[name].values\n",
    "            if data.ndim == 2:  # Static feature\n",
    "                data = np.broadcast_to(data, (ds.dims['time'], *data.shape))\n",
    "            features.append(data)\n",
    "        elif config.allow_missing_features:\n",
    "            print(f\"Warning: Missing feature {name}\")\n",
    "            missing.append(name)\n",
    "        else:\n",
    "            raise ValueError(f\"Missing feature: {name}\")\n",
    "    \n",
    "    if not features:\n",
    "        raise ValueError(\"No features extracted\")\n",
    "    \n",
    "    features = np.stack(features, axis=-1)\n",
    "    features = np.nan_to_num(features, nan=0.0)\n",
    "    \n",
    "    config.n_features = features.shape[-1]\n",
    "    print(f\"Extracted features shape: {features.shape}\")\n",
    "    \n",
    "    return features.astype(np.float32), missing\n",
    "\n",
    "\n",
    "def extract_elevation_vector(ds: xr.Dataset) -> Optional[np.ndarray]:\n",
    "    \"\"\"Extract elevation as a flat vector if available.\"\"\"\n",
    "    if 'elevation' not in ds:\n",
    "        return None\n",
    "\n",
    "    elevation = ds['elevation'].values\n",
    "    if elevation.ndim == 3:\n",
    "        elevation = elevation[0]\n",
    "\n",
    "    elevation = np.nan_to_num(elevation, nan=0.0).astype(np.float32)\n",
    "    return elevation.reshape(-1)\n",
    "\n",
    "\n",
    "print(\"Data functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dataset_class"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 6.2: DATASET CLASS\n",
    "# ============================================================\n",
    "\n",
    "class V8Dataset(Dataset):\n",
    "    \"\"\"Dataset for V8 GNN-Mamba model.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        features: torch.Tensor,      # (time, lat, lon, n_features)\n",
    "        target: torch.Tensor,         # (time, lat, lon)\n",
    "        input_window: int,\n",
    "        horizon: int,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_weight: torch.Tensor,\n",
    "        start_idx: int,\n",
    "        end_idx: int,\n",
    "        elevation: Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.input_window = input_window\n",
    "        self.horizon = horizon\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_weight = edge_weight\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx\n",
    "        self.elevation = elevation\n",
    "        \n",
    "        self.n_lat = features.shape[1]\n",
    "        self.n_lon = features.shape[2]\n",
    "        self.n_nodes = self.n_lat * self.n_lon\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.end_idx - self.start_idx\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = self.start_idx + idx\n",
    "        \n",
    "        # Input sequence\n",
    "        x = self.features[i:i+self.input_window]  # (T, lat, lon, F)\n",
    "        x = x.reshape(self.input_window, self.n_nodes, -1)  # (T, N, F)\n",
    "        \n",
    "        # Target\n",
    "        y = self.target[i+self.input_window:i+self.input_window+self.horizon]\n",
    "        y = y.reshape(self.horizon, self.n_nodes)  # (H, N)\n",
    "        y = y.permute(1, 0)  # (N, H)\n",
    "        \n",
    "        sample = {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'edge_index': self.edge_index,\n",
    "            'edge_weight': self.edge_weight\n",
    "        }\n",
    "\n",
    "        if self.elevation is not None:\n",
    "            sample['elevation'] = self.elevation\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "def prepare_data(ds: xr.Dataset, config: V8Config, lat_dim: str, lon_dim: str, output_dir: Path):\n",
    "    \"\"\"Prepare train/val datasets.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Preparing data for V8 GNN-Mamba\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Add elevation clusters\n",
    "    ds = create_elevation_clusters(ds)\n",
    "    \n",
    "    # Extract features\n",
    "    features, missing_features = extract_features(ds, FEATURE_SETS['KCE'], config)\n",
    "    \n",
    "    # Target\n",
    "    target = ds['total_precipitation'].values.astype(np.float32)\n",
    "    target = np.nan_to_num(target, nan=0.0)\n",
    "    print(f\"Target shape: {target.shape}\")\n",
    "    \n",
    "    # Build graph\n",
    "    edge_index, edge_weight = build_spatial_graph(ds, lat_dim, lon_dim)\n",
    "    \n",
    "    if getattr(config, 'plot_graph_diagnostics', True):\n",
    "        plot_graph_diagnostics(edge_index, edge_weight, output_dir, config)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    features = torch.from_numpy(features)\n",
    "    target = torch.from_numpy(target)\n",
    "    \n",
    "    # Elevation vector for physics loss (optional)\n",
    "    elevation_vec = extract_elevation_vector(ds)\n",
    "    elevation = torch.from_numpy(elevation_vec) if elevation_vec is not None else None\n",
    "    \n",
    "    # Time split\n",
    "    n_time = features.shape[0]\n",
    "    max_start = n_time - config.input_window - config.horizon\n",
    "    \n",
    "    split_idx = int(max_start * config.train_val_split)\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"  Total samples: {max_start + 1}\")\n",
    "    print(f\"  Train: 0 to {split_idx}\")\n",
    "    print(f\"  Val: {split_idx} to {max_start + 1}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = V8Dataset(\n",
    "        features, target,\n",
    "        config.input_window, config.horizon,\n",
    "        edge_index, edge_weight,\n",
    "        start_idx=0, end_idx=split_idx,\n",
    "        elevation=elevation\n",
    "    )\n",
    "    \n",
    "    val_dataset = V8Dataset(\n",
    "        features, target,\n",
    "        config.input_window, config.horizon,\n",
    "        edge_index, edge_weight,\n",
    "        start_idx=split_idx, end_idx=max_start + 1,\n",
    "        elevation=elevation\n",
    "    )\n",
    "    \n",
    "    overlap_leakage = bool(split_idx < config.input_window)\n",
    "    data_report = {\n",
    "        'n_time': int(n_time),\n",
    "        'n_lat': int(config.n_lat),\n",
    "        'n_lon': int(config.n_lon),\n",
    "        'n_nodes': int(config.n_nodes),\n",
    "        'input_window': int(config.input_window),\n",
    "        'horizon': int(config.horizon),\n",
    "        'train_val_split': float(config.train_val_split),\n",
    "        'train_start_idx': 0,\n",
    "        'train_end_idx': int(split_idx - 1),\n",
    "        'val_start_idx': int(split_idx),\n",
    "        'val_end_idx': int(max_start),\n",
    "        'missing_features': missing_features,\n",
    "        'feature_set': FEATURE_SETS['KCE'],\n",
    "        'has_elevation': bool(elevation is not None),\n",
    "        'overlap_leakage': overlap_leakage,\n",
    "        'target_stats': {\n",
    "            'min': float(target.min().item()),\n",
    "            'max': float(target.max().item()),\n",
    "            'mean': float(target.mean().item()),\n",
    "            'std': float(target.std().item())\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return train_dataset, val_dataset, edge_index, edge_weight, data_report\n",
    "\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: d:\\github.com\\ninja-marduk\\ml_precipitation_prediction\\data\\output\\complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc\n",
      "Dataset dimensions:\n",
      "  - time: 518\n",
      "  - latitude: 61\n",
      "  - longitude: 65\n",
      "Available variables: ['total_precipitation', 'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos', 'year', 'month', 'elevation', 'slope', 'aspect', 'cluster_elevation', 'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag3', 'total_precipitation_lag4', 'total_precipitation_lag12', 'total_precipitation_lag24', 'total_precipitation_lag36', 'CEEMDAN_imf_1', 'CEEMDAN_imf_2', 'CEEMDAN_imf_3', 'CEEMDAN_imf_4', 'CEEMDAN_imf_5', 'CEEMDAN_imf_6', 'CEEMDAN_imf_7', 'CEEMDAN_imf_8', 'CEEMDAN_imf_9', 'TVFEMD_imf_1', 'TVFEMD_imf_2', 'TVFEMD_imf_3', 'TVFEMD_imf_4', 'TVFEMD_imf_5', 'TVFEMD_imf_6', 'TVFEMD_imf_7', 'TVFEMD_imf_8', 'elev_low', 'elev_med', 'elev_high']\n",
      "Light mode: using 10x10 grid\n",
      "\n",
      "============================================================\n",
      "Preparing data for V8 GNN-Mamba\n",
      "============================================================\n",
      "Added elevation clusters: elev_low, elev_med, elev_high\n",
      "Extracted features shape: (518, 10, 10, 4)\n",
      "Target shape: (518, 10, 10)\n",
      "Graph built: 100 nodes, 684 edges\n",
      "Graph diagnostics saved to: d:\\github.com\\ninja-marduk\\ml_precipitation_prediction\\models\\output\\V8_GNN_Mamba\\v8_graph_diagnostics.png\n",
      "\n",
      "Data split:\n",
      "  Total samples: 447\n",
      "  Train: 0 to 356\n",
      "  Val: 356 to 447\n",
      "\n",
      "Data report summary:\n",
      "  Missing features: []\n",
      "  Overlap leakage: False\n",
      "  Target mean: 116.019\n",
      "\n",
      "Data loaders created:\n",
      "  Train batches: 89\n",
      "  Val batches: 23\n",
      "\n",
      "Batch shapes:\n",
      "  x: torch.Size([4, 60, 100, 4])\n",
      "  y: torch.Size([4, 100, 12])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 6.3: LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "# Load dataset\n",
    "ds, lat_dim, lon_dim = load_dataset(DATA_PATH, CONFIG)\n",
    "\n",
    "# Prepare data\n",
    "train_dataset, val_dataset, edge_index, edge_weight, data_report = prepare_data(\n",
    "    ds, CONFIG, lat_dim, lon_dim, OUTPUT_ROOT\n",
    ")\n",
    "\n",
    "print(\"\\nData report summary:\")\n",
    "print(f\"  Missing features: {data_report.get('missing_features', [])}\")\n",
    "print(f\"  Overlap leakage: {data_report.get('overlap_leakage', False)}\")\n",
    "print(f\"  Target mean: {data_report['target_stats']['mean']:.3f}\")\n",
    "\n",
    "# Move graph to device\n",
    "edge_index = edge_index.to(device)\n",
    "edge_weight = edge_weight.to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nData loaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Test batch\n",
    "test_batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  x: {test_batch['x'].shape}\")\n",
    "print(f\"  y: {test_batch['y'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "loss_functions"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 7: LOSS FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def physics_informed_loss(\n",
    "    pred: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    elevation: Optional[torch.Tensor] = None,\n",
    "    config: V8Config = None\n",
    ") -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Physics-informed loss with mass conservation and orographic constraints.\n",
    "    \n",
    "    Args:\n",
    "        pred: (batch, n_nodes, horizon)\n",
    "        target: (batch, n_nodes, horizon)\n",
    "        elevation: (n_nodes,) optional\n",
    "        config: V8Config\n",
    "    \"\"\"\n",
    "    # Base MSE loss\n",
    "    mse_loss = F.mse_loss(pred, target)\n",
    "    \n",
    "    components = {'mse': mse_loss.item()}\n",
    "    total_loss = mse_loss\n",
    "    \n",
    "    if config and config.use_physics_loss:\n",
    "        # Mass conservation: total precipitation should be preserved\n",
    "        pred_sum = pred.sum(dim=1)  # (batch, horizon)\n",
    "        target_sum = target.sum(dim=1)\n",
    "        mass_loss = torch.abs(pred_sum - target_sum) / (target_sum.abs() + 1e-6)\n",
    "        mass_loss = mass_loss.mean()\n",
    "        \n",
    "        total_loss = total_loss + config.lambda_mass_conservation * mass_loss\n",
    "        components['mass'] = mass_loss.item()\n",
    "        \n",
    "        # Orographic constraint: high elevation should have higher precipitation\n",
    "        if elevation is not None:\n",
    "            elev = elevation\n",
    "            if elev.dim() == 2:\n",
    "                elev = elev[0]\n",
    "            high_elev_mask = elev > config.high_elev_threshold\n",
    "            if high_elev_mask.sum() > 0:\n",
    "                pred_high = pred[:, high_elev_mask, :].mean()\n",
    "                target_high = target[:, high_elev_mask, :].mean()\n",
    "                oro_loss = F.relu(target_high - pred_high)\n",
    "                \n",
    "                total_loss = total_loss + config.lambda_orographic * oro_loss\n",
    "                components['orographic'] = oro_loss.item()\n",
    "    \n",
    "    return total_loss, components\n",
    "\n",
    "\n",
    "print(\"Loss functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "training_loop"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 7.1: TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "def train_v8(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    config: V8Config,\n",
    "    output_dir: Path\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Training loop for V8 GNN-Mamba.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    if config.scheduler_type == 'cosine':\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=config.cosine_T0,\n",
    "            T_mult=config.cosine_T_mult\n",
    "        )\n",
    "    else:\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=5\n",
    "        )\n",
    "    \n",
    "    # History\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_mse': [],\n",
    "        'val_mse': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting V8 GNN-Mamba Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        # === Training ===\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "        train_mse_sum = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x = batch['x'].to(device)  # (B, T, N, F)\n",
    "            y = batch['y'].to(device)  # (B, N, H)\n",
    "            edge_idx = batch['edge_index'][0].to(device)\n",
    "            edge_wt = batch['edge_weight'][0].to(device)\n",
    "            elevation = batch['elevation'].to(device) if 'elevation' in batch else None\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            pred, info = model(x, edge_idx, edge_wt)\n",
    "            \n",
    "            # Loss\n",
    "            loss, components = physics_informed_loss(\n",
    "                pred, y, elevation=elevation, config=config\n",
    "            )\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if config.gradient_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(),\n",
    "                    config.gradient_clip\n",
    "                )\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            train_mse_sum += components['mse']\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_train_loss = train_loss_sum / n_batches\n",
    "        avg_train_mse = train_mse_sum / n_batches\n",
    "        \n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        val_mse_sum = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x = batch['x'].to(device)\n",
    "                y = batch['y'].to(device)\n",
    "                edge_idx = batch['edge_index'][0].to(device)\n",
    "                edge_wt = batch['edge_weight'][0].to(device)\n",
    "                elevation = batch['elevation'].to(device) if 'elevation' in batch else None\n",
    "                \n",
    "                pred, info = model(x, edge_idx, edge_wt)\n",
    "                loss, components = physics_informed_loss(\n",
    "                    pred, y, elevation=elevation, config=config\n",
    "                )\n",
    "                \n",
    "                val_loss_sum += loss.item()\n",
    "                val_mse_sum += components['mse']\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = val_loss_sum / val_batches\n",
    "        avg_val_mse = val_mse_sum / val_batches\n",
    "        \n",
    "        # Update scheduler\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if config.scheduler_type == 'cosine':\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_mse'].append(avg_train_mse)\n",
    "        history['val_mse'].append(avg_val_mse)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1:3d}/{config.epochs}: \"\n",
    "                f\"Train={avg_train_loss:.4f} Val={avg_val_loss:.4f} \"\n",
    "                f\"MSE={avg_val_mse:.4f} LR={current_lr:.2e}\"\n",
    "            )\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            if config.save_checkpoints:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'config': asdict(config)\n",
    "                }\n",
    "                torch.save(checkpoint, output_dir / 'v8_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config.patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    print(f\"\\nTraining complete. Best val loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"Training loop defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model initialized:\n",
      "  Total parameters: 201,964\n",
      "  Trainable: 201,964\n",
      "\n",
      "============================================================\n",
      "Starting V8 GNN-Mamba Training\n",
      "============================================================\n",
      "Epoch   1/100: Train=12053.7118 Val=6791.6243 MSE=6791.5989 LR=1.00e-03\n",
      "Epoch   5/100: Train=4165.7004 Val=4966.9071 MSE=4966.8788 LR=9.05e-04\n",
      "Epoch  10/100: Train=4139.2760 Val=4949.3326 MSE=4949.3042 LR=5.78e-04\n",
      "Epoch  15/100: Train=4110.3432 Val=4942.9687 MSE=4942.9402 LR=2.06e-04\n",
      "Epoch  20/100: Train=4106.9570 Val=4962.5241 MSE=4962.4960 LR=6.16e-06\n",
      "Epoch  25/100: Train=3449.3740 Val=4127.7026 MSE=4127.6792 LR=9.76e-04\n",
      "Epoch  30/100: Train=3328.9026 Val=4031.1919 MSE=4031.1692 LR=8.80e-04\n",
      "Epoch  35/100: Train=3275.3447 Val=3974.3096 MSE=3974.2867 LR=7.27e-04\n",
      "Epoch  40/100: Train=3124.7188 Val=3740.8049 MSE=3740.7826 LR=5.39e-04\n",
      "Epoch  45/100: Train=2545.4239 Val=3143.9940 MSE=3143.9754 LR=3.45e-04\n",
      "Epoch  50/100: Train=2436.7964 Val=3132.2454 MSE=3132.2279 LR=1.75e-04\n",
      "Epoch  55/100: Train=2411.0608 Val=3059.8437 MSE=3059.8261 LR=5.45e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Trainable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;241m.\u001b[39mrequires_grad)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_v8\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_ROOT\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 72\u001b[0m, in \u001b[0;36mtrain_v8\u001b[1;34m(model, train_loader, val_loader, config, output_dir)\u001b[0m\n\u001b[0;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m pred, info \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_wt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[0;32m     75\u001b[0m loss, components \u001b[38;5;241m=\u001b[39m physics_informed_loss(\n\u001b[0;32m     76\u001b[0m     pred, y, elevation\u001b[38;5;241m=\u001b[39melevation, config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[0;32m     77\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 176\u001b[0m, in \u001b[0;36mGNN_Mamba_V8.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    173\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_proj(x)  \u001b[38;5;66;03m# (B, T, N, H)\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# 2. Spatial encoding (GNN per timestep)\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m h_spatial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_spatial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# (B, T, N, H)\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# 3. Temporal encoding (Mamba per node)\u001b[39;00m\n\u001b[0;32m    180\u001b[0m h_temporal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_temporal(x)\n",
      "Cell \u001b[1;32mIn[6], line 216\u001b[0m, in \u001b[0;36mGNN_Mamba_V8._encode_spatial\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    214\u001b[0m h_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x_flat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m--> 216\u001b[0m     h_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_flat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     h_list\u001b[38;5;241m.\u001b[39mappend(h_i)\n\u001b[0;32m    219\u001b[0m h_chunk \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(h_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (B*chunk, N, H)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 83\u001b[0m, in \u001b[0;36mSpatialGNNEncoder.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# GNN forward\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGAT\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 83\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGCN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     85\u001b[0m     h \u001b[38;5;241m=\u001b[39m gnn(h, edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:347\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    345\u001b[0m         num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(num_nodes, x_dst\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    346\u001b[0m     num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(size) \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m num_nodes\n\u001b[1;32m--> 347\u001b[0m     edge_index, edge_attr \u001b[38;5;241m=\u001b[39m \u001b[43mremove_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m     edge_index, edge_attr \u001b[38;5;241m=\u001b[39m add_self_loops(\n\u001b[0;32m    350\u001b[0m         edge_index, edge_attr, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_value,\n\u001b[0;32m    351\u001b[0m         num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes)\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, SparseTensor):\n",
      "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\torch_geometric\\utils\\loop.py:115\u001b[0m, in \u001b[0;36mremove_self_loops\u001b[1;34m(edge_index, edge_attr)\u001b[0m\n\u001b[0;32m    112\u001b[0m mask \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    113\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m edge_index[:, mask]\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_scripting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, EdgeIndex):\n\u001b[0;32m    116\u001b[0m     edge_index\u001b[38;5;241m.\u001b[39m_is_undirected \u001b[38;5;241m=\u001b[39m is_undirected\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\torch\\_jit_internal.py:106\u001b[0m, in \u001b[0;36mis_scripting\u001b[1;34m()\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastingList\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m BroadcastingList1\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_scripting\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    Function that returns True when in compilation and False otherwise. This\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    is useful especially with the @unused decorator to leave code in your\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m                return unsupported_linear_op(x)\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SECTION 7.2: RUN TRAINING\n",
    "# ============================================================\n",
    "\n",
    "# Initialize model\n",
    "model = GNN_Mamba_V8(CONFIG)\n",
    "print(f\"\\nModel initialized:\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Train\n",
    "history = train_v8(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=CONFIG,\n",
    "    output_dir=OUTPUT_ROOT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_section"
   },
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 8: EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    config: V8Config\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate model and compute metrics per horizon.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x = batch['x'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            edge_idx = batch['edge_index'][0].to(device)\n",
    "            edge_wt = batch['edge_weight'][0].to(device)\n",
    "            \n",
    "            pred, _ = model(x, edge_idx, edge_wt)\n",
    "            \n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(all_preds, axis=0)  # (N_samples, N_nodes, H)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    # Compute metrics per horizon\n",
    "    results = []\n",
    "    for h in range(config.horizon):\n",
    "        pred_h = preds[:, :, h].flatten()\n",
    "        target_h = targets[:, :, h].flatten()\n",
    "        \n",
    "        # RMSE\n",
    "        rmse = np.sqrt(np.mean((pred_h - target_h) ** 2))\n",
    "        \n",
    "        # MAE\n",
    "        mae = np.mean(np.abs(pred_h - target_h))\n",
    "        \n",
    "        # R2\n",
    "        ss_res = np.sum((target_h - pred_h) ** 2)\n",
    "        ss_tot = np.sum((target_h - target_h.mean()) ** 2)\n",
    "        r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "        \n",
    "        # Bias\n",
    "        bias = np.mean(pred_h - target_h)\n",
    "        \n",
    "        results.append({\n",
    "            'H': h + 1,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R^2': r2,\n",
    "            'Bias': bias\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'metrics': results,\n",
    "        'predictions': preds,\n",
    "        'targets': targets\n",
    "    }\n",
    "\n",
    "\n",
    "def run_quality_checks(\n",
    "    preds: np.ndarray,\n",
    "    targets: np.ndarray,\n",
    "    config: V8Config,\n",
    "    data_report: Optional[Dict[str, Any]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Simple quality checks to catch obvious issues.\"\"\"\n",
    "    stats = {\n",
    "        'neg_pred_frac': float((preds < 0).mean()),\n",
    "        'nan_pred_frac': float(np.isnan(preds).mean()),\n",
    "        'nan_target_frac': float(np.isnan(targets).mean()),\n",
    "        'bias_mm': float(np.mean(preds - targets)),\n",
    "        'bias_pct': float(\n",
    "            100.0 * np.mean(preds - targets) / (np.mean(targets) + 1e-6)\n",
    "        )\n",
    "    }\n",
    "\n",
    "    issues = []\n",
    "    if stats['neg_pred_frac'] > config.quality_neg_pred_max:\n",
    "        issues.append('neg_pred')\n",
    "    if stats['nan_pred_frac'] > config.quality_nan_max:\n",
    "        issues.append('nan_pred')\n",
    "    if stats['nan_target_frac'] > config.quality_nan_max:\n",
    "        issues.append('nan_target')\n",
    "    if abs(stats['bias_pct']) > config.quality_bias_pct_max:\n",
    "        issues.append('bias_pct')\n",
    "    if data_report and data_report.get('overlap_leakage', False):\n",
    "        issues.append('overlap_leakage')\n",
    "\n",
    "    return {'stats': stats, 'issues': issues}\n",
    "\n",
    "\n",
    "# Load best model\n",
    "if 'model' not in globals():\n",
    "    model = GNN_Mamba_V8(CONFIG)\n",
    "    print('Model not initialized; created new model for evaluation.')\n",
    "model = model.to(device)\n",
    "best_checkpoint = OUTPUT_ROOT / 'v8_best.pt'\n",
    "if best_checkpoint.exists():\n",
    "    checkpoint = torch.load(best_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"V8 GNN-Mamba Evaluation Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "eval_results = evaluate_model(model, val_loader, CONFIG)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics by Horizon:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'H':>3} {'RMSE':>10} {'MAE':>10} {'R2':>10} {'Bias':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for m in eval_results['metrics']:\n",
    "    print(\n",
    "        f\"{m['H']:>3} \"\n",
    "        f\"{m['RMSE']:>10.2f} \"\n",
    "        f\"{m['MAE']:>10.2f} \"\n",
    "        f\"{m['R^2']:>10.4f} \"\n",
    "        f\"{m['Bias']:>10.2f}\"\n",
    "    )\n",
    "\n",
    "# Summary\n",
    "avg_rmse = np.mean([m['RMSE'] for m in eval_results['metrics']])\n",
    "avg_mae = np.mean([m['MAE'] for m in eval_results['metrics']])\n",
    "avg_r2 = np.mean([m['R^2'] for m in eval_results['metrics']])\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'AVG':>3} {avg_rmse:>10.2f} {avg_mae:>10.2f} {avg_r2:>10.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Compare with V4 baseline\n",
    "print(\"=\" * 50)\n",
    "print(\"Comparison with V4 Baseline\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  V4 Baseline: R2=0.596, RMSE=84.37mm\")\n",
    "print(f\"  V8 Mamba:    R2={avg_r2:.3f}, RMSE={avg_rmse:.2f}mm\")\n",
    "\n",
    "r2_change = (avg_r2 - 0.596) / 0.596 * 100\n",
    "rmse_change = (84.37 - avg_rmse) / 84.37 * 100\n",
    "\n",
    "print(f\"\\n  R2 change:   {r2_change:+.2f}%\")\n",
    "print(f\"  RMSE change: {rmse_change:+.2f}%\")\n",
    "\n",
    "# Quality checks\n",
    "report_ref = data_report if 'data_report' in globals() else None\n",
    "quality = run_quality_checks(\n",
    "    eval_results['predictions'],\n",
    "    eval_results['targets'],\n",
    "    CONFIG,\n",
    "    data_report=report_ref\n",
    ")\n",
    "\n",
    "print(\"\\nQuality checks:\")\n",
    "print(f\"  neg_pred_frac: {quality['stats']['neg_pred_frac']:.4f}\")\n",
    "print(f\"  nan_pred_frac: {quality['stats']['nan_pred_frac']:.4f}\")\n",
    "print(f\"  nan_target_frac: {quality['stats']['nan_target_frac']:.4f}\")\n",
    "print(f\"  bias_pct: {quality['stats']['bias_pct']:.2f}%\")\n",
    "\n",
    "if quality['issues']:\n",
    "    print(f\"  Issues: {', '.join(quality['issues'])}\")\n",
    "    if CONFIG.enforce_quality_gates:\n",
    "        raise ValueError(f\"Quality gates failed: {quality['issues']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualization"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 8.1: VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "# Ensure required artifacts are initialized\n",
    "if 'history' not in globals():\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mse': [], 'val_mse': [], 'lr': []}\n",
    "    history_path = TRAIN_LOG_DIR / 'v8_training_log.csv'\n",
    "    if history_path.exists():\n",
    "        history_df = pd.read_csv(history_path)\n",
    "        n_epochs = len(history_df)\n",
    "        history = {\n",
    "            'train_loss': history_df['train_loss'].tolist(),\n",
    "            'val_loss': history_df['val_loss'].tolist(),\n",
    "            'train_mse': history_df['train_mse'].tolist() if 'train_mse' in history_df else [float('nan')] * n_epochs,\n",
    "            'val_mse': history_df['val_mse'].tolist() if 'val_mse' in history_df else [float('nan')] * n_epochs,\n",
    "            'lr': history_df['lr'].tolist()\n",
    "        }\n",
    "        print(f\"Loaded training history from: {history_path}\")\n",
    "    else:\n",
    "        print('Training history not found; plots will use empty series.')\n",
    "\n",
    "if 'eval_results' not in globals():\n",
    "    metrics_path = COMP_DIR / 'v8_metrics.csv'\n",
    "    preds_path = DATA_OUT_DIR / 'predictions.npy'\n",
    "    targets_path = DATA_OUT_DIR / 'targets.npy'\n",
    "    if metrics_path.exists() and preds_path.exists() and targets_path.exists():\n",
    "        metrics_df = pd.read_csv(metrics_path)\n",
    "        eval_results = {\n",
    "            'metrics': metrics_df.to_dict(orient='records'),\n",
    "            'predictions': np.load(preds_path),\n",
    "            'targets': np.load(targets_path)\n",
    "        }\n",
    "        print(f\"Loaded eval_results from: {metrics_path}\")\n",
    "    else:\n",
    "        raise RuntimeError('eval_results not found. Run evaluation before visualization.')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Training curves\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['train_loss'], label='Train Loss', alpha=0.8)\n",
    "ax1.plot(history['val_loss'], label='Val Loss', alpha=0.8)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Curves')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. RMSE by horizon\n",
    "ax2 = axes[0, 1]\n",
    "horizons = [m['H'] for m in eval_results['metrics']]\n",
    "rmses = [m['RMSE'] for m in eval_results['metrics']]\n",
    "ax2.bar(horizons, rmses, color='steelblue', edgecolor='black')\n",
    "ax2.axhline(y=84.37, color='red', linestyle='--', label='V4 Baseline')\n",
    "ax2.set_xlabel('Forecast Horizon (months)')\n",
    "ax2.set_ylabel('RMSE (mm)')\n",
    "ax2.set_title('RMSE by Forecast Horizon')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. R2 by horizon\n",
    "ax3 = axes[1, 0]\n",
    "r2s = [m['R^2'] for m in eval_results['metrics']]\n",
    "ax3.bar(horizons, r2s, color='forestgreen', edgecolor='black')\n",
    "ax3.axhline(y=0.596, color='red', linestyle='--', label='V4 Baseline')\n",
    "ax3.set_xlabel('Forecast Horizon (months)')\n",
    "ax3.set_ylabel('R2')\n",
    "ax3.set_title('R2 by Forecast Horizon')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Learning rate schedule\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(history['lr'], color='orange')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Learning Rate')\n",
    "ax4.set_title('Learning Rate Schedule')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('V8 GNN-Mamba Training Results', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "fig_path = FIG_OUT_DIR / 'v8_training_results.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"Figure saved to: {fig_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Results summary plot\n",
    "metrics_df = pd.DataFrame(eval_results['metrics'])\n",
    "if getattr(CONFIG, 'plot_results_summary', True):\n",
    "    pred_flat = eval_results['predictions'].reshape(-1)\n",
    "    targ_flat = eval_results['targets'].reshape(-1)\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    sample_size = min(50000, pred_flat.size)\n",
    "    sample_idx = rng.choice(pred_flat.size, size=sample_size, replace=False)\n",
    "    pred_s = pred_flat[sample_idx]\n",
    "    targ_s = targ_flat[sample_idx]\n",
    "    err = pred_flat - targ_flat\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.scatter(targ_s, pred_s, s=5, alpha=0.2, color='steelblue')\n",
    "    lims = [min(targ_s.min(), pred_s.min()), max(targ_s.max(), pred_s.max())]\n",
    "    ax1.plot(lims, lims, 'k--', linewidth=1)\n",
    "    ax1.set_xlabel('Target (mm)')\n",
    "    ax1.set_ylabel('Prediction (mm)')\n",
    "    ax1.set_title('Pred vs Target (sample)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(err, bins=50, color='slategray', edgecolor='black')\n",
    "    ax2.set_xlabel('Prediction Error (mm)')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_title('Error Distribution')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(metrics_df['H'], metrics_df['RMSE'], marker='o', label='RMSE')\n",
    "    ax3.plot(metrics_df['H'], metrics_df['MAE'], marker='o', label='MAE')\n",
    "    ax3.set_xlabel('Forecast Horizon (months)')\n",
    "    ax3.set_ylabel('Error (mm)')\n",
    "    ax3.set_title('RMSE and MAE by Horizon')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.bar(metrics_df['H'], metrics_df['Bias'], color='indianred', edgecolor='black')\n",
    "    ax4.axhline(y=0.0, color='black', linewidth=0.8)\n",
    "    ax4.set_xlabel('Forecast Horizon (months)')\n",
    "    ax4.set_ylabel('Bias (mm)')\n",
    "    ax4.set_title('Bias by Horizon')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('V8 GNN-Mamba Results Summary', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    fig_path = FIG_OUT_DIR / 'v8_results_summary.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Results summary saved to: {fig_path}\")\n",
    "else:\n",
    "    print('Results summary plot disabled')\n",
    "\n",
    "# Metrics summary table\n",
    "if getattr(CONFIG, 'plot_metrics_table', True):\n",
    "    display_df = metrics_df.copy()\n",
    "    display_df['RMSE'] = display_df['RMSE'].map(lambda x: f\"{x:.2f}\")\n",
    "    display_df['MAE'] = display_df['MAE'].map(lambda x: f\"{x:.2f}\")\n",
    "    display_df['R^2'] = display_df['R^2'].map(lambda x: f\"{x:.4f}\")\n",
    "    display_df['Bias'] = display_df['Bias'].map(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 0.6 * len(display_df) + 1.5))\n",
    "    ax.axis('off')\n",
    "    table = ax.table(\n",
    "        cellText=display_df.values.tolist(),\n",
    "        colLabels=display_df.columns.tolist(),\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.2)\n",
    "    fig_path = COMP_DIR / 'v8_metrics_summary_table.png'\n",
    "    fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"Metrics summary table saved to: {fig_path}\")\n",
    "else:\n",
    "    print('Metrics table plot disabled')\n",
    "\n",
    "# Map exports\n",
    "if getattr(CONFIG, 'generate_map_plots', True):\n",
    "    if CONFIG.n_lat * CONFIG.n_lon != eval_results['predictions'].shape[1]:\n",
    "        print('Map export skipped: grid shape does not match n_nodes')\n",
    "    else:\n",
    "        horizon = max(1, min(CONFIG.map_export_horizon, CONFIG.horizon)) - 1\n",
    "        max_samples = min(CONFIG.map_export_max_samples, eval_results['predictions'].shape[0])\n",
    "        export_dir = MAP_OUT_DIR / f\"H{horizon + 1}\"\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for idx in range(max_samples):\n",
    "            pred_map = eval_results['predictions'][idx, :, horizon].reshape(CONFIG.n_lat, CONFIG.n_lon)\n",
    "            targ_map = eval_results['targets'][idx, :, horizon].reshape(CONFIG.n_lat, CONFIG.n_lon)\n",
    "            err_map = np.abs(pred_map - targ_map)\n",
    "\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "            axes[0].imshow(targ_map, cmap='Blues')\n",
    "            axes[0].set_title('Target')\n",
    "            axes[1].imshow(pred_map, cmap='Blues')\n",
    "            axes[1].set_title('Prediction')\n",
    "            axes[2].imshow(err_map, cmap='Reds')\n",
    "            axes[2].set_title('Absolute Error')\n",
    "\n",
    "            for ax in axes:\n",
    "                ax.axis('off')\n",
    "\n",
    "            fig_path = export_dir / f\"sample_{idx + 1}_H{horizon + 1}.png\"\n",
    "            fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "        print(f\"Map exports saved to: {export_dir}\")\n",
    "else:\n",
    "    print('Map export disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 8.2: SAVE RESULTS\n",
    "# ============================================================\n",
    "\n",
    "# Ensure evaluation artifacts are available\n",
    "if 'eval_results' not in globals():\n",
    "    metrics_path = COMP_DIR / 'v8_metrics.csv'\n",
    "    preds_path = DATA_OUT_DIR / 'predictions.npy'\n",
    "    targets_path = DATA_OUT_DIR / 'targets.npy'\n",
    "    if metrics_path.exists() and preds_path.exists() and targets_path.exists():\n",
    "        metrics_df = pd.read_csv(metrics_path)\n",
    "        eval_results = {\n",
    "            'metrics': metrics_df.to_dict(orient='records'),\n",
    "            'predictions': np.load(preds_path),\n",
    "            'targets': np.load(targets_path)\n",
    "        }\n",
    "        print(f\"Loaded eval_results from: {metrics_path}\")\n",
    "    else:\n",
    "        raise RuntimeError('eval_results not found. Run evaluation before saving results.')\n",
    "\n",
    "history_available = 'history' in globals()\n",
    "if not history_available and getattr(CONFIG, 'export_history', True):\n",
    "    history_path = TRAIN_LOG_DIR / 'v8_training_log.csv'\n",
    "    if history_path.exists():\n",
    "        history_df = pd.read_csv(history_path)\n",
    "        n_epochs = len(history_df)\n",
    "        history = {\n",
    "            'train_loss': history_df['train_loss'].tolist(),\n",
    "            'val_loss': history_df['val_loss'].tolist(),\n",
    "            'train_mse': history_df['train_mse'].tolist() if 'train_mse' in history_df else [float('nan')] * n_epochs,\n",
    "            'val_mse': history_df['val_mse'].tolist() if 'val_mse' in history_df else [float('nan')] * n_epochs,\n",
    "            'lr': history_df['lr'].tolist()\n",
    "        }\n",
    "        history_available = True\n",
    "        print(f\"Loaded training history from: {history_path}\")\n",
    "    else:\n",
    "        print('Training history not found; skipping training log export.')\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame(eval_results['metrics'])\n",
    "metrics_df.to_csv(COMP_DIR / 'v8_metrics.csv', index=False)\n",
    "\n",
    "if 'avg_rmse' not in globals():\n",
    "    avg_rmse = float(np.mean([m['RMSE'] for m in eval_results['metrics']]))\n",
    "if 'avg_mae' not in globals():\n",
    "    avg_mae = float(np.mean([m['MAE'] for m in eval_results['metrics']]))\n",
    "if 'avg_r2' not in globals():\n",
    "    avg_r2 = float(np.mean([m['R^2'] for m in eval_results['metrics']]))\n",
    "if 'r2_change' not in globals():\n",
    "    r2_change = (avg_r2 - 0.596) / 0.596 * 100\n",
    "if 'rmse_change' not in globals():\n",
    "    rmse_change = (84.37 - avg_rmse) / 84.37 * 100\n",
    "\n",
    "# Save training history\n",
    "if getattr(CONFIG, 'export_history', True) and history_available:\n",
    "    history_df = pd.DataFrame({\n",
    "        'epoch': np.arange(1, len(history['train_loss']) + 1),\n",
    "        'train_loss': history['train_loss'],\n",
    "        'val_loss': history['val_loss'],\n",
    "        'train_mse': history['train_mse'],\n",
    "        'val_mse': history['val_mse'],\n",
    "        'lr': history['lr']\n",
    "    })\n",
    "    history_df.to_csv(TRAIN_LOG_DIR / 'v8_training_log.csv', index=False)\n",
    "\n",
    "# Save predictions and targets\n",
    "if getattr(CONFIG, 'export_predictions', True):\n",
    "    np.save(DATA_OUT_DIR / 'predictions.npy', eval_results['predictions'])\n",
    "    np.save(DATA_OUT_DIR / 'targets.npy', eval_results['targets'])\n",
    "\n",
    "    meta = {\n",
    "        'n_samples': int(eval_results['predictions'].shape[0]),\n",
    "        'n_nodes': int(eval_results['predictions'].shape[1]),\n",
    "        'horizon': int(CONFIG.horizon),\n",
    "        'n_lat': int(CONFIG.n_lat),\n",
    "        'n_lon': int(CONFIG.n_lon)\n",
    "    }\n",
    "    with open(DATA_OUT_DIR / 'metadata.json', 'w') as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "\n",
    "# Save data report\n",
    "if 'data_report' in globals():\n",
    "    with open(OUTPUT_ROOT / 'v8_data_report.json', 'w') as f:\n",
    "        json.dump(data_report, f, indent=2)\n",
    "\n",
    "# Save quality checks\n",
    "if 'quality' in globals():\n",
    "    with open(OUTPUT_ROOT / 'v8_quality_checks.json', 'w') as f:\n",
    "        json.dump(quality, f, indent=2)\n",
    "\n",
    "# Save config\n",
    "with open(OUTPUT_ROOT / 'v8_config.json', 'w') as f:\n",
    "    json.dump(asdict(CONFIG), f, indent=2)\n",
    "\n",
    "# Save environment info\n",
    "env_info = {\n",
    "    'python': sys.version,\n",
    "    'torch': torch.__version__,\n",
    "    'cuda': torch.version.cuda\n",
    "}\n",
    "try:\n",
    "    import torch_geometric\n",
    "    env_info['torch_geometric'] = torch_geometric.__version__\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    import mamba_ssm\n",
    "    env_info['mamba_ssm'] = mamba_ssm.__version__\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "with open(OUTPUT_ROOT / 'v8_env.json', 'w') as f:\n",
    "    json.dump(env_info, f, indent=2)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'model': 'V8 GNN-Mamba',\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'config': asdict(CONFIG),\n",
    "    'results': {\n",
    "        'avg_rmse': float(avg_rmse),\n",
    "        'avg_mae': float(avg_mae),\n",
    "        'avg_r2': float(avg_r2),\n",
    "        'metrics_by_horizon': eval_results['metrics']\n",
    "    },\n",
    "    'comparison': {\n",
    "        'v4_baseline': {'r2': 0.596, 'rmse': 84.37},\n",
    "        'v8_mamba': {'r2': float(avg_r2), 'rmse': float(avg_rmse)},\n",
    "        'improvement': {\n",
    "            'r2_percent': float(r2_change),\n",
    "            'rmse_percent': float(rmse_change)\n",
    "        }\n",
    "    },\n",
    "    'quality_checks': quality if 'quality' in globals() else None,\n",
    "    'data_report': data_report if 'data_report' in globals() else None\n",
    "}\n",
    "\n",
    "with open(OUTPUT_ROOT / 'v8_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nResults saved to: {OUTPUT_ROOT}\")\n",
    "print('  - v8_metrics.csv')\n",
    "print('  - v8_summary.json')\n",
    "print('  - v8_config.json')\n",
    "print('  - v8_env.json')\n",
    "print('  - v8_data_report.json')\n",
    "print('  - v8_quality_checks.json')\n",
    "print('  - v8_training_log.csv')\n",
    "print('  - v8_training_results.png')\n",
    "print('  - v8_results_summary.png')\n",
    "print('  - v8_metrics_summary_table.png')\n",
    "print('  - v8_graph_diagnostics.png')\n",
    "print('  - predictions.npy')\n",
    "print('  - targets.npy')\n",
    "print('  - metadata.json')\n",
    "print('  - v8_best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_disconnect"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 8.3: COLAB DISCONNECT\n",
    "# ============================================================\n",
    "\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import runtime\n",
    "        runtime.unassign()\n",
    "        print('Colab runtime disconnected.')\n",
    "    except Exception as exc:\n",
    "        print(f\"Colab runtime disconnect failed: {exc}\")\n",
    "else:\n",
    "    print('Not running in Colab; skip disconnect.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "### V8 GNN-Mamba Summary\n",
    "\n",
    "This notebook implemented the V8 architecture combining:\n",
    "- **Graph Neural Networks** for spatial encoding\n",
    "- **Mamba State Space Models** for temporal encoding\n",
    "- **Cross-Modal Attention** for feature fusion\n",
    "\n",
    "### Key Innovations\n",
    "1. First application of Mamba SSM to regional precipitation prediction\n",
    "2. Selective memory mechanism for capturing seasonal patterns\n",
    "3. Efficient O(n) complexity for 60-month input sequences\n",
    "\n",
    "### Next Steps\n",
    "1. Ablation studies (Mamba vs LSTM, bidirectional vs unidirectional)\n",
    "2. Physics loss refinement\n",
    "3. Ensemble with V4 baseline"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
