{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4e7925",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/main/models/base_models_STHyMOUNTAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3744f4",
   "metadata": {
    "id": "af3744f4"
   },
   "source": [
    "# ğŸ“˜ Entrenamiento de Modelos Baseline para PredicciÃ³n Espaciotemporal de PrecipitaciÃ³n Mensual STHyMOUNTAIN\n",
    "\n",
    "Este notebook implementa modelos baseline para la predicciÃ³n de precipitaciones usando datos espaciotemporales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994be36",
   "metadata": {},
   "source": [
    "## ğŸ” ImplementaciÃ³n de Modelos Avanzados y TÃ©cnicas de ValidaciÃ³n\n",
    "\n",
    "AdemÃ¡s de los modelos tabulares baseline, implementaremos:\n",
    "\n",
    "1. **Modelos de Deep Learning** para capturar patrones espaciales y temporales:\n",
    "   - Redes CNN para patrones espaciales\n",
    "   - Redes ConvLSTM para patrones temporales\n",
    "   - Redes ConvGRU para patrones temporales\n",
    "\n",
    "El objetivo es proporcionar una evaluaciÃ³n completa de diferentes enfoques de modelado para la predicciÃ³n de precipitaciÃ³n en regiones montaÃ±osas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06416284",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06416284",
    "outputId": "a8e4c864-34e9-41b2-d5c3-e6ccaaf3699e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH = /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction\n",
      "Dataset â†’ time=518, lat=61, lon=65\n",
      "\n",
      "=== Experimento BASIC (12 feats) ===\n",
      "â†’ ConvLSTM\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueÃ³ al ejecutar cÃ³digo en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el cÃ³digo de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquÃ­</a> para obtener mÃ¡s informaciÃ³n. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener mÃ¡s detalles."
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import sys, os, gc, warnings, shutil\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, ConvLSTM2D, SimpleRNN, BatchNormalization, MaxPooling2D,\n",
    "    Dropout, Flatten, Dense, Reshape, TimeDistributed, Add, LayerNormalization\n",
    ")\n",
    "# â”€â”€ ConvGRU2D: intento nativoÂ â†’Â fallback custom si no existe â”€â”€\n",
    "try:\n",
    "    from tensorflow.keras.layers import ConvGRU2D  # TFÂ â‰¥Â 2.8\n",
    "except ImportError:  # â¬…ï¸Â define capa manualmente (TFÂ <Â 2.8)\n",
    "    from tensorflow.python.keras.layers.recurrent import RNN\n",
    "    from tensorflow.python.keras.layers.convolutional_recurrent import ConvRNN2D\n",
    "    from tensorflow.python.keras.utils import conv_utils\n",
    "    from tensorflow.python.keras import backend as K, activations, initializers, regularizers, constraints\n",
    "    from tensorflow.python.ops import array_ops\n",
    "\n",
    "    class _ConvGRU2DCell(tf.keras.layers.Layer):\n",
    "        def __init__(self, filters, kernel_size, activation=\"tanh\", recurrent_activation=\"sigmoid\", **kw):\n",
    "            super().__init__(**kw)\n",
    "            self.filters = filters\n",
    "            self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, \"kernel_size\")\n",
    "            self.activation = activations.get(activation)\n",
    "            self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        def build(self, input_shape):\n",
    "            channel_axis = -1\n",
    "            input_dim = int(input_shape[channel_axis])\n",
    "            kernel_shape = self.kernel_size + (input_dim, self.filters * 3)\n",
    "            rec_shape    = self.kernel_size + (self.filters, self.filters * 3)\n",
    "            self.kernel = self.add_weight(\"kernel\", shape=kernel_shape, initializer=\"glorot_uniform\")\n",
    "            self.recurrent_kernel = self.add_weight(\"recurrent_kernel\", shape=rec_shape, initializer=\"orthogonal\")\n",
    "            self.bias = self.add_weight(\"bias\", shape=(self.filters * 3,), initializer=\"zeros\")\n",
    "            self.built = True\n",
    "        def call(self, inputs, states):\n",
    "            h_prev = states[0]\n",
    "            k_z, k_r, k_h = array_ops.split(self.kernel, 3, axis=3)\n",
    "            rk_z, rk_r, rk_h = array_ops.split(self.recurrent_kernel, 3, axis=3)\n",
    "            b_z, b_r, b_h = array_ops.split(self.bias, 3)\n",
    "            x_z = K.conv2d(inputs, k_z, padding=\"same\") + b_z\n",
    "            x_r = K.conv2d(inputs, k_r, padding=\"same\") + b_r\n",
    "            x_h = K.conv2d(inputs, k_h, padding=\"same\") + b_h\n",
    "            h_z = K.conv2d(h_prev, rk_z, padding=\"same\")\n",
    "            h_r = K.conv2d(h_prev, rk_r, padding=\"same\")\n",
    "            h_h = K.conv2d(h_prev, rk_h, padding=\"same\")\n",
    "            z = self.recurrent_activation(x_z + h_z)\n",
    "            r = self.recurrent_activation(x_r + h_r)\n",
    "            hh = self.activation(x_h + r * h_h)\n",
    "            h = (1 - z) * h_prev + z * hh\n",
    "            return h, [h]\n",
    "        @property\n",
    "        def state_size(self):\n",
    "            return (self.filters, None, None)\n",
    "    class ConvGRU2D(ConvRNN2D):\n",
    "        def __init__(self, filters, kernel_size, **kw):\n",
    "            cell = _ConvGRU2DCell(filters, kernel_size, **kw)\n",
    "            super().__init__(cell, **kw)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid'); sns.set_context('notebook')\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ENTORNO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    if not (Path('/content')/'ml_precipitation_prediction').exists():\n",
    "        !git clone -q https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
    "    %cd /content/ml_precipitation_prediction || true\n",
    "    !pip install -q -r requirements.txt xarray netCDF4 seaborn scikit-learn cartopy geopandas imageio\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    for p in [BASE_PATH,*BASE_PATH.parents]:\n",
    "        if (p/'.git').exists(): BASE_PATH=p; break\n",
    "print('BASE_PATH =', BASE_PATH)\n",
    "\n",
    "import matplotlib.pyplot as plt, seaborn as sns, geopandas as gpd, imageio.v2 as imageio\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PATHS & CONST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATA_FILE = BASE_PATH/'data'/'output'/(\n",
    "    'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc')\n",
    "OUT_ROOT  = BASE_PATH/'models'/'output'/'Spatial_CONVRNN'\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SHAPE_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
    "DEPT_GDF   = gpd.read_file(SHAPE_DIR/'MGN_Departamento.shp')\n",
    "\n",
    "INPUT_WINDOW = 60; HORIZON = 3; EPOCHS = 50; BATCH = 4; LR = 1e-3\n",
    "PATIENCE = 6\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FEATURE SETS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE_FEATS = ['year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
    "              'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
    "              'elevation','slope','aspect']\n",
    "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
    "KCE_FEATS = BASE_FEATS + ELEV_CLUSTER\n",
    "PAFC_FEATS= KCE_FEATS + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'BASIC': BASE_FEATS,\n",
    "    'KCE'  : KCE_FEATS,\n",
    "    'PAFC' : PAFC_FEATS\n",
    "}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DATASET â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ds = xr.open_dataset(DATA_FILE)\n",
    "lat, lon = len(ds.latitude), len(ds.longitude)\n",
    "print(f\"Dataset â†’ time={len(ds.time)}, lat={lat}, lon={lon}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def windowed_arrays(X:np.ndarray, y:np.ndarray):\n",
    "    \"\"\"Crea ventanas deslizantes 5â€‘D para X y 4â€‘D para y.\"\"\"\n",
    "    seq_X, seq_y = [], []\n",
    "    T = len(X)\n",
    "    for start in range(T-INPUT_WINDOW-HORIZON+1):\n",
    "        end_w = start+INPUT_WINDOW; end_y=end_w+HORIZON\n",
    "        Xw = X[start:end_w]; yw = y[end_w:end_y]\n",
    "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
    "            continue\n",
    "        seq_X.append(Xw); seq_y.append(yw)\n",
    "    return np.array(seq_X,dtype=np.float32), np.array(seq_y,dtype=np.float32)\n",
    "\n",
    "\n",
    "def quick_plot(ax,data,cmap,title,vmin=None,vmax=None):\n",
    "    mesh = ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',\n",
    "                         vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
    "    ax.coastlines(); ax.add_geometries(DEPT_GDF.geometry,ccrs.PlateCarree(),edgecolor='black',facecolor='none',linewidth=1)\n",
    "    ax.gridlines(draw_labels=True,top_labels=False,right_labels=False)\n",
    "    ax.set_title(title,fontsize=9); return mesh\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MODEL FACTORIES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def build_conv_lstm(n_feats:int):\n",
    "    inp = Input(shape=(INPUT_WINDOW,lat,lon,n_feats))\n",
    "    x   = ConvLSTM2D(64,(3,3),padding='same',return_sequences=True)(inp)\n",
    "    x   = ConvLSTM2D(32,(3,3),padding='same')(x)\n",
    "    out = Dense(HORIZON*lat*lon)(Flatten()(x))\n",
    "    out = Reshape((HORIZON,lat,lon,1))(out)\n",
    "    m   = Model(inp,out,'ConvLSTM'); m.compile('adam','mse'); return m\n",
    "\n",
    "\n",
    "def build_conv_gru(n_feats:int):\n",
    "    inp = Input(shape=(INPUT_WINDOW,lat,lon,n_feats))\n",
    "    x = ConvGRU2D(64,(3,3),padding='same',return_sequences=True)(inp)\n",
    "    x = ConvGRU2D(32,(3,3),padding='same')(x)\n",
    "    out = Dense(HORIZON*lat*lon)(Flatten()(x))\n",
    "    out = Reshape((HORIZON,lat,lon,1))(out)\n",
    "    m = Model(inp,out,'ConvGRU'); m.compile('adam','mse'); return m\n",
    "\n",
    "\n",
    "def build_conv_rnn(n_feats:int):\n",
    "    inp = Input(shape=(INPUT_WINDOW,lat,lon,n_feats))\n",
    "    x   = Flatten()(inp)\n",
    "    x   = SimpleRNN(128,activation='tanh')(x)\n",
    "    out = Dense(HORIZON*lat*lon)(x)\n",
    "    out = Reshape((HORIZON,lat,lon,1))(out)\n",
    "    m = Model(inp,out,'ConvRNN'); m.compile('adam','mse'); return m\n",
    "\n",
    "MODELS = {\n",
    "    'ConvLSTM': build_conv_lstm,\n",
    "    'ConvGRU' : build_conv_gru,\n",
    "    'ConvRNN' : build_conv_rnn\n",
    "}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TRAIN + EVAL LOOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results=[]\n",
    "for exp, feat_list in EXPERIMENTS.items():\n",
    "    print(f\"\\n=== Experimento {exp} ({len(feat_list)} feats) ===\")\n",
    "    Xarr = ds[feat_list].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
    "    yarr = ds['total_precipitation'].values.astype(np.float32)[...,None]\n",
    "    X, y = windowed_arrays(Xarr, yarr)\n",
    "    split=int(0.8*len(X))\n",
    "\n",
    "    sx = StandardScaler().fit(X[:split].reshape(-1,len(feat_list)))\n",
    "    sy = StandardScaler().fit(y[:split].reshape(-1,1))\n",
    "    X_sc = sx.transform(X.reshape(-1,len(feat_list))).reshape(X.shape)\n",
    "    y_sc = sy.transform(y.reshape(-1,1)).reshape(y.shape)\n",
    "    X_tr, X_va = X_sc[:split], X_sc[split:]\n",
    "    y_tr, y_va = y_sc[:split], y_sc[split:]\n",
    "\n",
    "    OUT_EXP = OUT_ROOT/exp; OUT_EXP.mkdir(exist_ok=True)\n",
    "\n",
    "    for mdl_name,builder in MODELS.items():\n",
    "        print(f\"â†’ {mdl_name}\")\n",
    "        model_path = OUT_EXP/f\"{mdl_name.lower()}_best.keras\"\n",
    "        if model_path.exists():\n",
    "            print(\"â© already trained â€“ loading\"); model=tf.keras.models.load_model(model_path,compile=False)\n",
    "        else:\n",
    "            model = builder(n_feats=len(feat_list))\n",
    "            cb=[EarlyStopping('val_loss',patience=PATIENCE,restore_best_weights=True),\n",
    "                ModelCheckpoint(model_path,save_best_only=True)]\n",
    "            model.fit(X_tr,y_tr,validation_data=(X_va,y_va),epochs=EPOCHS,batch_size=BATCH,callbacks=cb,verbose=0)\n",
    "        # â”€ Pred Ãºltima ventanaÂ validaciÃ³n â”€\n",
    "        y_hat_sc = model.predict(X_va[-1:],verbose=0)\n",
    "        y_hat = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
    "        y_true= sy.inverse_transform(y_va[-1:].reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
    "\n",
    "        # â”€ Mapas & GIF â”€\n",
    "        vmin,vmax=0,max(y_true.max(),y_hat.max()); frames=[]\n",
    "        dates=pd.date_range(ds.time.values[-HORIZON],periods=HORIZON,freq='MS')\n",
    "        for h in range(HORIZON):\n",
    "            err=np.clip(np.abs((y_true[h]-y_hat[h])/(y_true[h]+1e-5))*100,0,100)\n",
    "            fig,axs=plt.subplots(1,3,figsize=(12,4),subplot_kw={'projection':ccrs.PlateCarree()})\n",
    "            quick_plot(axs[0],y_true[h],'Blues',f\"Real h={h+1}\",vmin,vmax)\n",
    "            quick_plot(axs[1],y_hat[h],'Blues',f\"{mdl_name} h={h+1}\",vmin,vmax)\n",
    "            quick_plot(axs[2],err,'Reds',f\"MAPE% h={h+1}\",0,100)\n",
    "            fig.suptitle(f\"{mdl_name} â€“ {exp} â€“ {dates[h].strftime('%Y-%m')}\")\n",
    "            png=OUT_EXP/f\"{mdl_name}_{h+1}.png\"; fig.savefig(png,bbox_inches='tight'); plt.close(fig)\n",
    "            frames.append(imageio.imread(png))\n",
    "        imageio.mimsave(OUT_EXP/f\"{mdl_name}.gif\",frames,fps=0.5)\n",
    "\n",
    "        # â”€ MÃ©tricas â”€\n",
    "        for h in range(HORIZON):\n",
    "            results.append({\n",
    "                'Experiment':exp,'Model':mdl_name,'H':h+1,\n",
    "                'RMSE':np.sqrt(mean_squared_error(y_true[h].ravel(),y_hat[h].ravel())),\n",
    "                'MAE': mean_absolute_error(y_true[h].ravel(),y_hat[h].ravel()),\n",
    "                'R2' : r2_score(y_true[h].ravel(),y_hat[h].ravel())\n",
    "            })\n",
    "        tf.keras.backend.clear_session(); gc.collect()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CSV FINAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "res_df=pd.DataFrame(results)\n",
    "res_df.to_csv(OUT_ROOT/'metrics_spatial.csv',index=False)\n",
    "print(\"\\nğŸ“‘ Metrics saved â†’\", OUT_ROOT/'metrics_spatial.csv')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
