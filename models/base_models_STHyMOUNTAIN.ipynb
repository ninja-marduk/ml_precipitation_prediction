{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4e7925",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/main/models/base_models_STHyMOUNTAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3744f4",
   "metadata": {
    "id": "af3744f4"
   },
   "source": [
    "# 📘 Entrenamiento de Modelos Baseline para Predicción Espaciotemporal de Precipitación Mensual STHyMOUNTAIN\n",
    "\n",
    "Este notebook implementa modelos baseline para la predicción de precipitaciones usando datos espaciotemporales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994be36",
   "metadata": {},
   "source": [
    "## 🔍 Implementación de Modelos Avanzados y Técnicas de Validación\n",
    "\n",
    "Además de los modelos tabulares baseline, implementaremos:\n",
    "\n",
    "1. **Optimización avanzada con Optuna** para los modelos tabulares XGBoost y LightGBM\n",
    "2. **Validación robusta** mediante:\n",
    "   - Hold-Out Validation (ya implementada)\n",
    "   - Cross-Validation (k=5)\n",
    "   - Bootstrapping (100 muestras)\n",
    "3. **Modelos de Deep Learning** para capturar patrones espaciales y temporales:\n",
    "   - Redes CNN para patrones espaciales\n",
    "   - Redes ConvLSTM para patrones espaciotemporales\n",
    "\n",
    "El objetivo es proporcionar una evaluación completa de diferentes enfoques de modelado para la predicción de precipitación en regiones montañosas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06416284",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06416284",
    "outputId": "a8e4c864-34e9-41b2-d5c3-e6ccaaf3699e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entorno configurado. Usando ruta base: ..\n",
      "Directorio para salida de modelos creado: ../models/output\n",
      "Error al cargar el modelo RandomForest: name 'pickle' is not defined\n",
      "Error al cargar el modelo XGBoost: name 'pickle' is not defined\n",
      "Error al cargar el modelo LightGBM: name 'pickle' is not defined\n",
      "No se encontró el archivo de respaldo en: ../models/output/cnn_model.h5\n",
      "No se encontró el archivo de respaldo en: ../models/output/convlstm_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Configuración del entorno (compatible con Colab y local)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Detectar si estamos en Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')   \n",
    "    # Si estamos en Colab, clonar el repositorio\n",
    "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
    "    %cd ml_precipitation_prediction\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn\n",
    "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
    "else:\n",
    "    # Si estamos en local, usar la ruta actual\n",
    "    if '/models' in os.getcwd():\n",
    "        BASE_PATH = Path('..')\n",
    "    else:\n",
    "        BASE_PATH = Path('.')\n",
    "\n",
    "print(f\"Entorno configurado. Usando ruta base: {BASE_PATH}\")\n",
    "\n",
    "# Si BASE_PATH viene como string, lo convertimos\n",
    "BASE_PATH = Path(BASE_PATH)\n",
    "\n",
    "# Ahora puedes concatenar correctamente\n",
    "model_output_dir = BASE_PATH / 'models' / 'output'\n",
    "model_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directorio para salida de modelos creado: {model_output_dir}\")\n",
    "\n",
    "# Implementación de resiliencia para interacción con Google Drive y restauración de datos\n",
    "def backup_dataframe(df, backup_path):\n",
    "    \"\"\"Guarda un DataFrame como respaldo en formato Parquet.\"\"\"\n",
    "    try:\n",
    "        df.to_parquet(backup_path, index=False)\n",
    "        print(f\"Respaldo del DataFrame guardado en: {backup_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar respaldo del DataFrame: {e}\")\n",
    "\n",
    "def restore_dataframe(backup_path):\n",
    "    \"\"\"Restaura un DataFrame desde un archivo de respaldo en formato Parquet.\"\"\"\n",
    "    try:\n",
    "        if backup_path.exists():\n",
    "            df_restored = pd.read_parquet(backup_path)\n",
    "            print(f\"DataFrame restaurado desde: {backup_path}\")\n",
    "            return df_restored\n",
    "        else:\n",
    "            print(f\"No se encontró el archivo de respaldo en: {backup_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al restaurar el DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "# Ruta para respaldo temporal del DataFrame\n",
    "temp_dir = BASE_PATH / 'data' / 'output' / 'temp'\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "temp_file_path = temp_dir / 'dataframe_backup.parquet'\n",
    "\n",
    "# Respaldo inicial del DataFrame principal\n",
    "if 'df' in locals() and df is not None:\n",
    "    backup_dataframe(df, temp_file_path)\n",
    "\n",
    "# Modificar interacción con Google Drive para reintentos\n",
    "max_retries = 3\n",
    "retry_delay = 5  # segundos\n",
    "\n",
    "def mount_google_drive():\n",
    "    \"\"\"Intenta montar Google Drive con reintentos.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"Google Drive montado exitosamente.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error al montar Google Drive (intento {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "    print(\"No se pudo montar Google Drive después de varios intentos.\")\n",
    "    return False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not mount_google_drive():\n",
    "        print(\"Usando datos en memoria o restaurando desde respaldo local.\")\n",
    "        df = restore_dataframe(temp_file_path)\n",
    "\n",
    "# Restaurar modelos guardados en caso de fallo\n",
    "model_files = {\n",
    "    'RandomForest': model_output_dir / 'RandomForest.pkl',\n",
    "    'XGBoost': model_output_dir / 'XGBoost.pkl',\n",
    "    'LightGBM': model_output_dir / 'LightGBM.pkl'\n",
    "}\n",
    "\n",
    "def load_saved_model(model_name, model_path):\n",
    "    \"\"\"Carga un modelo guardado desde disco.\"\"\"\n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            print(f\"Modelo {model_name} cargado desde: {model_path}\")\n",
    "            return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Inicializar `modelos_base` como un diccionario vacío\n",
    "modelos_base = {}\n",
    "\n",
    "# Intentar cargar modelos guardados\n",
    "for model_name, model_path in model_files.items():\n",
    "    if model_name not in modelos_base:\n",
    "        modelos_base[model_name] = load_saved_model(model_name, model_path)\n",
    "\n",
    "# Implementación de resiliencia para modelos CNN y ConvLSTM\n",
    "\n",
    "# Respaldo y restauración de modelos CNN y ConvLSTM\n",
    "cnn_model_path = model_output_dir / 'cnn_model.h5'\n",
    "convlstm_model_path = model_output_dir / 'convlstm_model.h5'\n",
    "\n",
    "def backup_model(model, model_path):\n",
    "    \"\"\"Guarda un modelo de Keras como respaldo.\"\"\"\n",
    "    try:\n",
    "        model.save(model_path)\n",
    "        print(f\"Modelo respaldado en: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar respaldo del modelo: {e}\")\n",
    "\n",
    "def restore_model(model_path):\n",
    "    \"\"\"Restaura un modelo de Keras desde un archivo de respaldo.\"\"\"\n",
    "    try:\n",
    "        if model_path.exists():\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "            print(f\"Modelo restaurado desde: {model_path}\")\n",
    "            return model\n",
    "        else:\n",
    "            print(f\"No se encontró el archivo de respaldo en: {model_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al restaurar el modelo: {e}\")\n",
    "        return None\n",
    "\n",
    "# Respaldo inicial de modelos si existen\n",
    "if 'cnn_model' in locals() and cnn_model is not None:\n",
    "    backup_model(cnn_model, cnn_model_path)\n",
    "if 'convlstm_model' in locals() and convlstm_model is not None:\n",
    "    backup_model(convlstm_model, convlstm_model_path)\n",
    "\n",
    "# Restaurar modelos en caso de fallo\n",
    "if 'cnn_model' not in locals() or cnn_model is None:\n",
    "    cnn_model = restore_model(cnn_model_path)\n",
    "if 'convlstm_model' not in locals() or convlstm_model is None:\n",
    "    convlstm_model = restore_model(convlstm_model_path)\n",
    "\n",
    "# Modificar interacción con Google Drive para reintentos\n",
    "max_retries = 3\n",
    "retry_delay = 5  # segundos\n",
    "\n",
    "def mount_google_drive():\n",
    "    \"\"\"Intenta montar Google Drive con reintentos.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"Google Drive montado exitosamente.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error al montar Google Drive (intento {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "    print(\"No se pudo montar Google Drive después de varios intentos.\")\n",
    "    return False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not mount_google_drive():\n",
    "        print(\"Usando datos en memoria o restaurando desde respaldo local para modelos CNN y ConvLSTM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e47fb555",
   "metadata": {
    "id": "e47fb555"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/dask/dataframe/_pyarrow_compat.py:15: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 13.0.0. Please consider upgrading.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Importaciones necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import optuna\n",
    "import pickle\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importaciones para barras de progreso y mejora de visualización\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import time\n",
    "\n",
    "# Configurar visualización más atractiva\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313434be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow versión: 2.18.0\n",
      "No se detectó GPU. Usando CPU.\n"
     ]
    }
   ],
   "source": [
    "# Importaciones adicionales para Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, save_model, load_model\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Conv2D, Conv3D, ConvLSTM2D, BatchNormalization, \n",
    "                                   MaxPooling2D, Flatten, Input, concatenate, Reshape, TimeDistributed, UpSampling2D)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(\"TensorFlow versión:\", tf.__version__)\n",
    "\n",
    "# Configurar GPU si está disponible\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(f\"GPU disponible: {physical_devices}\")\n",
    "    # Permitir crecimiento de memoria según sea necesario\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "else:\n",
    "    print(\"No se detectó GPU. Usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26215d90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "26215d90",
    "outputId": "ccb06926-e151-453f-a306-999f15566bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando archivo en: ../data/output/complete_dataset_with_features.nc\n",
      "Intentando cargar el archivo: ../data/output/complete_dataset_with_features.nc\n",
      "Archivo cargado exitosamente con xarray\n",
      "\n",
      "Información del dataset:\n",
      "xarray.Dataset {\n",
      "dimensions:\n",
      "\ttime = 530 ;\n",
      "\tlatitude = 62 ;\n",
      "\tlongitude = 66 ;\n",
      "\n",
      "variables:\n",
      "\tdatetime64[ns] time(time) ;\n",
      "\tfloat32 latitude(latitude) ;\n",
      "\tfloat32 longitude(longitude) ;\n",
      "\tfloat32 total_precipitation(time, latitude, longitude) ;\n",
      "\tfloat32 max_daily_precipitation(time, latitude, longitude) ;\n",
      "\tfloat32 min_daily_precipitation(time, latitude, longitude) ;\n",
      "\tfloat32 daily_precipitation_std(time, latitude, longitude) ;\n",
      "\tfloat32 month_sin(time, latitude, longitude) ;\n",
      "\tfloat32 month_cos(time, latitude, longitude) ;\n",
      "\tfloat32 doy_sin(time, latitude, longitude) ;\n",
      "\tfloat32 doy_cos(time, latitude, longitude) ;\n",
      "\tfloat64 elevation(latitude, longitude) ;\n",
      "\tfloat32 slope(latitude, longitude) ;\n",
      "\tfloat32 aspect(latitude, longitude) ;\n",
      "\n",
      "// global attributes:\n",
      "\t:description = ST-HyMOUNTAIN-Net ready dataset with CHIRPS monthly precipitation and DEM variables ;\n",
      "\t:source = CHIRPS v2.0 & DEM Boyacá ;\n",
      "\t:created_at = 2025-04-27 19:02:24 ;\n",
      "}None\n",
      "\n",
      "Variables disponibles:\n",
      "- total_precipitation: (530, 62, 66)\n",
      "- max_daily_precipitation: (530, 62, 66)\n",
      "- min_daily_precipitation: (530, 62, 66)\n",
      "- daily_precipitation_std: (530, 62, 66)\n",
      "- month_sin: (530, 62, 66)\n",
      "- month_cos: (530, 62, 66)\n",
      "- doy_sin: (530, 62, 66)\n",
      "- doy_cos: (530, 62, 66)\n",
      "- elevation: (62, 66)\n",
      "- slope: (62, 66)\n",
      "- aspect: (62, 66)\n",
      "Archivo cargado exitosamente con xarray\n",
      "\n",
      "Información del dataset:\n",
      "xarray.Dataset {\n",
      "dimensions:\n",
      "\ttime = 530 ;\n",
      "\tlatitude = 62 ;\n",
      "\tlongitude = 66 ;\n",
      "\n",
      "variables:\n",
      "\tdatetime64[ns] time(time) ;\n",
      "\tfloat32 latitude(latitude) ;\n",
      "\tfloat32 longitude(longitude) ;\n",
      "\tfloat32 total_precipitation(time, latitude, longitude) ;\n",
      "\tfloat32 max_daily_precipitation(time, latitude, longitude) ;\n",
      "\tfloat32 min_daily_precipitation(time, latitude, longitude) ;\n",
      "\tfloat32 daily_precipitation_std(time, latitude, longitude) ;\n",
      "\tfloat32 month_sin(time, latitude, longitude) ;\n",
      "\tfloat32 month_cos(time, latitude, longitude) ;\n",
      "\tfloat32 doy_sin(time, latitude, longitude) ;\n",
      "\tfloat32 doy_cos(time, latitude, longitude) ;\n",
      "\tfloat64 elevation(latitude, longitude) ;\n",
      "\tfloat32 slope(latitude, longitude) ;\n",
      "\tfloat32 aspect(latitude, longitude) ;\n",
      "\n",
      "// global attributes:\n",
      "\t:description = ST-HyMOUNTAIN-Net ready dataset with CHIRPS monthly precipitation and DEM variables ;\n",
      "\t:source = CHIRPS v2.0 & DEM Boyacá ;\n",
      "\t:created_at = 2025-04-27 19:02:24 ;\n",
      "}None\n",
      "\n",
      "Variables disponibles:\n",
      "- total_precipitation: (530, 62, 66)\n",
      "- max_daily_precipitation: (530, 62, 66)\n",
      "- min_daily_precipitation: (530, 62, 66)\n",
      "- daily_precipitation_std: (530, 62, 66)\n",
      "- month_sin: (530, 62, 66)\n",
      "- month_cos: (530, 62, 66)\n",
      "- doy_sin: (530, 62, 66)\n",
      "- doy_cos: (530, 62, 66)\n",
      "- elevation: (62, 66)\n",
      "- slope: (62, 66)\n",
      "- aspect: (62, 66)\n",
      "Dataset cargado con éxito. Dimensiones: (2168760, 14)\n",
      "\n",
      "Primeras filas del DataFrame:\n",
      "Dataset cargado con éxito. Dimensiones: (2168760, 14)\n",
      "\n",
      "Primeras filas del DataFrame:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "time",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "latitude",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "longitude",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "total_precipitation",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "max_daily_precipitation",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "min_daily_precipitation",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "daily_precipitation_std",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "month_sin",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "month_cos",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "doy_sin",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "doy_cos",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "elevation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "slope",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "aspect",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2e87289d-76e2-4334-b10b-14fdaa5483cd",
       "rows": [
        [
         "0",
         "1981-01-01 00:00:00",
         "4.324997",
         "-74.975006",
         "47.38105",
         "24.706928",
         "0.0",
         "5.8257756",
         "0.5",
         "0.8660254",
         "0.017201575",
         "0.99985206",
         "493.78455182073014",
         "89.53955",
         "102.0445"
        ],
        [
         "1",
         "1981-01-01 00:00:00",
         "4.324997",
         "-74.925",
         "40.750824",
         "21.819195",
         "0.0",
         "5.0190454",
         "0.5",
         "0.8660254",
         "0.017201575",
         "0.99985206",
         "519.7501066909579",
         "89.86702",
         "73.481674"
        ],
        [
         "2",
         "1981-01-01 00:00:00",
         "4.324997",
         "-74.87501",
         "46.338623",
         "26.092327",
         "0.0",
         "5.7402234",
         "0.5",
         "0.8660254",
         "0.017201575",
         "0.99985206",
         "248.7760453427361",
         "89.72222",
         "65.91682"
        ],
        [
         "3",
         "1981-01-01 00:00:00",
         "4.324997",
         "-74.825005",
         "48.779938",
         "29.42145",
         "0.0",
         "5.611738",
         "0.5",
         "0.8660254",
         "0.017201575",
         "0.99985206",
         "351.4157280671193",
         "86.98613",
         "140.916"
        ],
        [
         "4",
         "1981-01-01 00:00:00",
         "4.324997",
         "-74.775",
         "38.932945",
         "18.48306",
         "0.0",
         "3.7335742",
         "0.5",
         "0.8660254",
         "0.017201575",
         "0.99985206",
         "278.2619223660964",
         "88.27329",
         "18.43994"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>total_precipitation</th>\n",
       "      <th>max_daily_precipitation</th>\n",
       "      <th>min_daily_precipitation</th>\n",
       "      <th>daily_precipitation_std</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>doy_sin</th>\n",
       "      <th>doy_cos</th>\n",
       "      <th>elevation</th>\n",
       "      <th>slope</th>\n",
       "      <th>aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.975006</td>\n",
       "      <td>47.381050</td>\n",
       "      <td>24.706928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.825776</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>493.784552</td>\n",
       "      <td>89.539551</td>\n",
       "      <td>102.044502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.925003</td>\n",
       "      <td>40.750824</td>\n",
       "      <td>21.819195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.019045</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>519.750107</td>\n",
       "      <td>89.867020</td>\n",
       "      <td>73.481674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.875008</td>\n",
       "      <td>46.338623</td>\n",
       "      <td>26.092327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.740223</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>248.776045</td>\n",
       "      <td>89.722221</td>\n",
       "      <td>65.916817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.825005</td>\n",
       "      <td>48.779938</td>\n",
       "      <td>29.421450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.611738</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>351.415728</td>\n",
       "      <td>86.986130</td>\n",
       "      <td>140.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.775002</td>\n",
       "      <td>38.932945</td>\n",
       "      <td>18.483061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.733574</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>278.261922</td>\n",
       "      <td>88.273293</td>\n",
       "      <td>18.439939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time  latitude  longitude  total_precipitation  \\\n",
       "0 1981-01-01  4.324997 -74.975006            47.381050   \n",
       "1 1981-01-01  4.324997 -74.925003            40.750824   \n",
       "2 1981-01-01  4.324997 -74.875008            46.338623   \n",
       "3 1981-01-01  4.324997 -74.825005            48.779938   \n",
       "4 1981-01-01  4.324997 -74.775002            38.932945   \n",
       "\n",
       "   max_daily_precipitation  min_daily_precipitation  daily_precipitation_std  \\\n",
       "0                24.706928                      0.0                 5.825776   \n",
       "1                21.819195                      0.0                 5.019045   \n",
       "2                26.092327                      0.0                 5.740223   \n",
       "3                29.421450                      0.0                 5.611738   \n",
       "4                18.483061                      0.0                 3.733574   \n",
       "\n",
       "   month_sin  month_cos   doy_sin   doy_cos   elevation      slope      aspect  \n",
       "0        0.5   0.866025  0.017202  0.999852  493.784552  89.539551  102.044502  \n",
       "1        0.5   0.866025  0.017202  0.999852  519.750107  89.867020   73.481674  \n",
       "2        0.5   0.866025  0.017202  0.999852  248.776045  89.722221   65.916817  \n",
       "3        0.5   0.866025  0.017202  0.999852  351.415728  86.986130  140.916000  \n",
       "4        0.5   0.866025  0.017202  0.999852  278.261922  88.273293   18.439939  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Cargar el dataset NetCDF\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Carga un archivo NetCDF y lo convierte a pandas DataFrame\"\"\"\n",
    "    try:\n",
    "        # Cargar el archivo NetCDF con xarray\n",
    "        print(f\"Intentando cargar el archivo: {file_path}\")\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        print(\"Archivo cargado exitosamente con xarray\")\n",
    "\n",
    "        # Mostrar información del dataset cargado\n",
    "        print(\"\\nInformación del dataset:\")\n",
    "        print(ds.info())\n",
    "        print(\"\\nVariables disponibles:\")\n",
    "        for var_name in ds.data_vars:\n",
    "            print(f\"- {var_name}: {ds[var_name].shape}\")\n",
    "\n",
    "        # Convertir a DataFrame\n",
    "        df = ds.to_dataframe().reset_index()\n",
    "        return df, ds\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el archivo NetCDF: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Ruta al dataset\n",
    "data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features.nc'\n",
    "print(f\"Buscando archivo en: {data_file}\")\n",
    "\n",
    "# Cargar el dataset\n",
    "df, ds_original = load_dataset(data_file)\n",
    "\n",
    "# Verificar si se cargó correctamente\n",
    "if df is not None:\n",
    "    print(f\"Dataset cargado con éxito. Dimensiones: {df.shape}\")\n",
    "    print(\"\\nPrimeras filas del DataFrame:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"No se pudo cargar el dataset. Verificar la ruta y el formato del archivo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f0aebbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f0aebbc",
    "outputId": "48e82987-ff47-41e8-9f40-57e53cf90cf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna objetivo identificada: total_precipitation\n",
      "Filas antes de eliminar NaN: 2168760\n",
      "Filas después de eliminar NaN: 2168760\n",
      "\n",
      "Features seleccionadas (12):\n",
      "['latitude', 'longitude', 'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos', 'elevation', 'slope', 'aspect']\n",
      "\n",
      "Variable objetivo: total_precipitation\n"
     ]
    }
   ],
   "source": [
    "# 3. Preparación de los datos\n",
    "if df is not None:\n",
    "    # Identificar la columna objetivo (precipitación)\n",
    "    target_column = 'total_precipitation'  # Ajustar si tiene otro nombre en tu dataset\n",
    "\n",
    "    # Ver si existe 'precip_target' o usar 'total_precipitation'\n",
    "    if 'total_precipitation' in df.columns:\n",
    "        target_column = 'total_precipitation'\n",
    "\n",
    "    print(f\"Columna objetivo identificada: {target_column}\")\n",
    "\n",
    "    # Separar variables predictoras y variable objetivo\n",
    "    feature_cols = [col for col in df.columns if col != target_column and not pd.isna(df[col]).all()]\n",
    "\n",
    "    # Eliminar columnas no numéricas para los modelos (como fechas o coordenadas si no se usan como features)\n",
    "    non_feature_cols = ['time', 'spatial_ref']\n",
    "    feature_cols = [col for col in feature_cols if col not in non_feature_cols]\n",
    "\n",
    "    # Eliminar filas con valores NaN\n",
    "    print(f\"Filas antes de eliminar NaN: {df.shape[0]}\")\n",
    "    df_clean = df.dropna(subset=[target_column] + feature_cols)\n",
    "    print(f\"Filas después de eliminar NaN: {df_clean.shape[0]}\")\n",
    "\n",
    "    # Separar features y target\n",
    "    X = df_clean[feature_cols]\n",
    "    y = df_clean[target_column]\n",
    "\n",
    "    print(f\"\\nFeatures seleccionadas ({len(feature_cols)}):\\n{feature_cols}\")\n",
    "    print(f\"\\nVariable objetivo: {target_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da222af5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da222af5",
    "outputId": "579fbd57-a790-42dd-807a-e61fc1daacbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del conjunto de entrenamiento: (1735008, 12)\n",
      "Dimensiones del conjunto de prueba: (433752, 12)\n",
      "Escalador guardado en models/output/scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# 4. División del conjunto de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dimensiones del conjunto de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Dimensiones del conjunto de prueba: {X_test.shape}\")\n",
    "\n",
    "# 5. Estandarización de variables predictoras\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Guardar el scaler para uso futuro\n",
    "with open(model_output_dir / 'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Escalador guardado en models/output/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ba053b",
   "metadata": {
    "id": "c5ba053b"
   },
   "outputs": [],
   "source": [
    "# 6. Funciones de evaluación y entrenamiento\n",
    "def evaluar_modelo(y_true, y_pred):\n",
    "    \"\"\"Evalúa el rendimiento de un modelo usando múltiples métricas\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "def entrenar_y_evaluar_modelo(modelo, nombre, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Entrena un modelo y evalúa su rendimiento con visualización del progreso\"\"\"\n",
    "    # Crear widget para mostrar información del proceso\n",
    "    display(HTML(f'<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\">' +\n",
    "                 f'<h3>🔄 Entrenando modelo: {nombre}</h3>' +\n",
    "                 f'<div id=\"status_{nombre}\">Estado: Iniciando entrenamiento...</div>' +\n",
    "                 f'</div>'))\n",
    "    \n",
    "    # Tiempo de inicio\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Entrenar el modelo con seguimiento visual según el tipo\n",
    "    if hasattr(modelo, 'fit_generator') or nombre in ['XGBoost', 'XGBoost_Optuna', 'LightGBM', 'LightGBM_Optuna']:\n",
    "        # Para modelos que soportan entrenamiento por lotes como XGBoost, LightGBM\n",
    "        print(f\"Entrenando {nombre} con visualización de progreso...\")\n",
    "        if hasattr(modelo, 'n_estimators'):\n",
    "            n_estimators = modelo.n_estimators\n",
    "            for i in tqdm(range(n_estimators), desc=f\"Entrenando {nombre}\"):\n",
    "                if i == 0:\n",
    "                    # Primera iteración, ajuste inicial\n",
    "                    if nombre.startswith('LightGBM'):\n",
    "                        # LightGBM tiene parámetro verbose\n",
    "                        temp_modelo = type(modelo)(n_estimators=1, **{k:v for k,v in modelo.get_params().items() \n",
    "                                                                 if k != 'n_estimators' and k != 'verbose'}, verbose=-1)\n",
    "                    else:\n",
    "                        temp_modelo = type(modelo)(n_estimators=1, **{k:v for k,v in modelo.get_params().items() \n",
    "                                                                if k != 'n_estimators'})\n",
    "                    temp_modelo.fit(X_train, y_train)\n",
    "                elif i == n_estimators - 1:\n",
    "                    # Última iteración, ajuste completo\n",
    "                    modelo.fit(X_train, y_train)\n",
    "                \n",
    "                # Actualizar progreso visual\n",
    "                if i % max(1, n_estimators // 10) == 0:\n",
    "                    clear_output(wait=True)\n",
    "                    display(HTML(f'<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\">' +\n",
    "                                f'<h3>🔄 Entrenando modelo: {nombre}</h3>' +\n",
    "                                f'<div id=\"status_{nombre}\">Estado: Progreso {i+1}/{n_estimators} estimadores ({((i+1)/n_estimators*100):.1f}%)</div>' +\n",
    "                                f'</div>'))\n",
    "                    time.sleep(0.1)  # Pequeña pausa para actualización visual\n",
    "        else:\n",
    "            # Si no tiene n_estimators, entrenamiento directo\n",
    "            modelo.fit(X_train, y_train)\n",
    "    else:\n",
    "        # Para modelos estándar como RandomForest\n",
    "        modelo.fit(X_train, y_train)\n",
    "    \n",
    "    # Tiempo de entrenamiento\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Visualizar tiempo de entrenamiento\n",
    "    display(HTML(f'<div style=\"background-color:#e6ffe6; padding:10px; border-radius:5px;\">' +\n",
    "                f'<h3>✅ Entrenamiento completado: {nombre}</h3>' +\n",
    "                f'<div>Tiempo de entrenamiento: {training_time:.2f} segundos</div>' +\n",
    "                f'</div>'))\n",
    "    \n",
    "    print(f\"Evaluando rendimiento de {nombre}...\")\n",
    "    predicciones = modelo.predict(X_test)\n",
    "    rmse, mae, r2 = evaluar_modelo(y_test, predicciones)\n",
    "    \n",
    "    # Visualizar métricas con estilo\n",
    "    display(HTML(f'<div style=\"background-color:#f5f5dc; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
    "                f'<h3>📊 Métricas para {nombre}</h3>' +\n",
    "                f'<table style=\"width:100%; text-align:left;\">' +\n",
    "                f'<tr><th>Métrica</th><th>Valor</th></tr>' +\n",
    "                f'<tr><td>RMSE</td><td>{rmse:.4f}</td></tr>' +\n",
    "                f'<tr><td>MAE</td><td>{mae:.4f}</td></tr>' +\n",
    "                f'<tr><td>R²</td><td>{r2:.4f}</td></tr>' +\n",
    "                f'</table></div>'))\n",
    "    \n",
    "    return modelo, (rmse, mae, r2)\n",
    "\n",
    "def guardar_modelo(modelo, nombre):\n",
    "    \"\"\"Guarda un modelo entrenado en disco\"\"\"\n",
    "    # timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # filename = f\"{nombre}_{timestamp}.pkl\"\n",
    "    filename = f\"{nombre}.pkl\"\n",
    "    with open(model_output_dir / filename, 'wb') as f:\n",
    "        pickle.dump(modelo, f)\n",
    "    \n",
    "    # Visualizar confirmación de guardado\n",
    "    display(HTML(f'<div style=\"background-color:#e6ffee; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
    "                f'<h3>💾 Modelo guardado</h3>' +\n",
    "                f'<div>Modelo <b>{nombre}</b> guardado como: {filename}</div>' +\n",
    "                f'</div>'))\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f9865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo RandomForest encontrado en ../models/output/RandomForest.pkl. Cargando...\n"
     ]
    }
   ],
   "source": [
    "# 🔁 1. Importaciones necesarias\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "# ⚙️ 2. Diccionarios de almacenamiento\n",
    "resultados_base = {}\n",
    "modelos_base = {}\n",
    "modelos_guardados = {}\n",
    "\n",
    "# 📁 3. Rutas de modelos guardados\n",
    "cnn_model_path = model_output_dir / 'cnn_model.h5'\n",
    "convlstm_model_path = model_output_dir / 'convlstm_model.h5'\n",
    "\n",
    "model_paths = {\n",
    "    'RandomForest': model_output_dir / 'RandomForest.pkl',\n",
    "    'XGBoost': model_output_dir / 'XGBoost.pkl',\n",
    "    'LightGBM': model_output_dir / 'LightGBM.pkl',\n",
    "    'CNN': cnn_model_path,\n",
    "    'ConvLSTM': convlstm_model_path\n",
    "}\n",
    "\n",
    "# ✅ 4. Carga y evaluación de modelos\n",
    "for model_name, model_path in model_paths.items():\n",
    "    if model_path.exists():\n",
    "        print(f\"Modelo {model_name} encontrado en {model_path}. Cargando...\")\n",
    "        if model_name in ['RandomForest', 'XGBoost', 'LightGBM']:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                modelo = pickle.load(f)\n",
    "                modelos_base[model_name] = modelo\n",
    "                y_pred = modelo.predict(X_test_scaled)\n",
    "        elif model_name == 'CNN':\n",
    "            cnn_model = tf.keras.models.load_model(model_path)\n",
    "            print(\"✅ CNN cargado.\")\n",
    "            y_pred = cnn_model.predict(X_test_spatial).squeeze()\n",
    "        elif model_name == 'ConvLSTM':\n",
    "            convlstm_model = tf.keras.models.load_model(model_path)\n",
    "            print(\"✅ ConvLSTM cargado.\")\n",
    "            y_pred = convlstm_model.predict(X_test_spatial).squeeze()\n",
    "\n",
    "        # Evaluar métricas si hay predicción\n",
    "        if 'y_pred' in locals():\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            resultados_base[model_name] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}\n",
    "            print(f\"✅ {model_name} evaluado: RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}\")\n",
    "            del y_pred\n",
    "\n",
    "# 📊 5. Visualización de resultados\n",
    "print(\"\\n🔍 Comparación de modelos base sin optimización:\")\n",
    "temp_df = pd.DataFrame(resultados_base, index=['RMSE', 'MAE', 'R2']).T\n",
    "\n",
    "# Mostrar tabla ordenada por RMSE\n",
    "print(\"\\nOrdenados por RMSE (menor es mejor):\")\n",
    "display(temp_df.sort_values('RMSE'))\n",
    "\n",
    "# Gráfico de comparación de RMSE\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=temp_df.index, y=temp_df['RMSE'], palette='coolwarm')\n",
    "plt.title('Comparación de RMSE - Modelos Base')\n",
    "plt.ylabel('RMSE (menor es mejor)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'baseline_rmse_comparison_full.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 14:52:21,503] Using an existing study with name 'RandomForest_memory_optimized' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Ejecutando optimización para RandomForest...\n",
      "\n",
      "📊 Iniciando optimización adaptativa para RandomForest...\n",
      "Memoria RAM disponible: 2.78 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 14:56:52,815] Trial 29 finished with value: 42.98332560013569 and parameters: {'n_estimators': 178, 'max_depth': 11, 'min_samples_split': 5, 'min_samples_leaf': 8, 'max_features': None, 'bootstrap': False}. Best is trial 0 with value: 41.690542968034165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [RandomForest] Trials completados: 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:00:00,616] Trial 30 finished with value: 44.47966299214686 and parameters: {'n_estimators': 138, 'max_depth': 10, 'min_samples_split': 19, 'min_samples_leaf': 7, 'max_features': None, 'bootstrap': False}. Best is trial 0 with value: 41.690542968034165.\n",
      "[I 2025-04-29 15:00:37,801] Trial 31 finished with value: 41.73937211042662 and parameters: {'n_estimators': 107, 'max_depth': 12, 'min_samples_split': 13, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: 41.690542968034165.\n",
      "[I 2025-04-29 15:00:53,826] Trial 32 finished with value: 51.669319127683885 and parameters: {'n_estimators': 67, 'max_depth': 7, 'min_samples_split': 13, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: 41.690542968034165.\n",
      "[I 2025-04-29 15:01:09,832] Trial 33 finished with value: 69.14284001491328 and parameters: {'n_estimators': 146, 'max_depth': 3, 'min_samples_split': 16, 'min_samples_leaf': 10, 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: 41.690542968034165.\n",
      "[I 2025-04-29 15:01:49,008] Trial 34 finished with value: 41.786152771739346 and parameters: {'n_estimators': 113, 'max_depth': 12, 'min_samples_split': 13, 'min_samples_leaf': 6, 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: 41.690542968034165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [RandomForest] Trials completados: 35/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:02:24,908] Trial 35 finished with value: 43.10660452299993 and parameters: {'n_estimators': 110, 'max_depth': 11, 'min_samples_split': 11, 'min_samples_leaf': 5, 'max_features': 'log2', 'bootstrap': True}. Best is trial 0 with value: 41.690542968034165.\n",
      "[I 2025-04-29 15:03:21,729] Trial 36 finished with value: 41.683672656261486 and parameters: {'n_estimators': 167, 'max_depth': 12, 'min_samples_split': 12, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:04:21,181] Trial 37 finished with value: 43.04178345394798 and parameters: {'n_estimators': 186, 'max_depth': 11, 'min_samples_split': 14, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:05:09,896] Trial 38 finished with value: 44.630415483659775 and parameters: {'n_estimators': 162, 'max_depth': 10, 'min_samples_split': 12, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:05:55,422] Trial 39 finished with value: 41.7352751389159 and parameters: {'n_estimators': 134, 'max_depth': 12, 'min_samples_split': 16, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [RandomForest] Trials completados: 40/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:06:38,386] Trial 40 finished with value: 43.03216589051541 and parameters: {'n_estimators': 133, 'max_depth': 11, 'min_samples_split': 14, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:07:46,516] Trial 41 finished with value: 41.745456939921255 and parameters: {'n_estimators': 194, 'max_depth': 12, 'min_samples_split': 12, 'min_samples_leaf': 3, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:08:26,089] Trial 42 finished with value: 58.02529881035575 and parameters: {'n_estimators': 232, 'max_depth': 5, 'min_samples_split': 16, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:09:08,464] Trial 43 finished with value: 48.535359158106345 and parameters: {'n_estimators': 165, 'max_depth': 8, 'min_samples_split': 11, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:09:54,548] Trial 44 finished with value: 41.71048595636075 and parameters: {'n_estimators': 134, 'max_depth': 12, 'min_samples_split': 16, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [RandomForest] Trials completados: 45/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:10:45,619] Trial 45 finished with value: 41.73158265051698 and parameters: {'n_estimators': 147, 'max_depth': 12, 'min_samples_split': 15, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:11:36,391] Trial 46 finished with value: 43.08872618698756 and parameters: {'n_estimators': 152, 'max_depth': 11, 'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:11:55,299] Trial 47 finished with value: 62.73258896525744 and parameters: {'n_estimators': 133, 'max_depth': 4, 'min_samples_split': 19, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:12:54,934] Trial 48 finished with value: 41.68737461255988 and parameters: {'n_estimators': 174, 'max_depth': 12, 'min_samples_split': 16, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:13:56,576] Trial 49 finished with value: 44.60306566247338 and parameters: {'n_estimators': 209, 'max_depth': 10, 'min_samples_split': 18, 'min_samples_leaf': 9, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [RandomForest] Trials completados: 50/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:14:51,702] Trial 50 finished with value: 43.034264811422354 and parameters: {'n_estimators': 173, 'max_depth': 11, 'min_samples_split': 15, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:15:52,820] Trial 51 finished with value: 41.70940824893633 and parameters: {'n_estimators': 182, 'max_depth': 12, 'min_samples_split': 14, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:16:34,175] Trial 52 finished with value: 54.302124087006376 and parameters: {'n_estimators': 192, 'max_depth': 6, 'min_samples_split': 14, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:17:50,214] Trial 53 finished with value: 41.73262119750279 and parameters: {'n_estimators': 224, 'max_depth': 12, 'min_samples_split': 16, 'min_samples_leaf': 9, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 36 with value: 41.683672656261486.\n",
      "[I 2025-04-29 15:18:46,084] Trial 54 finished with value: 41.65832422347339 and parameters: {'n_estimators': 173, 'max_depth': 12, 'min_samples_split': 14, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 54 with value: 41.65832422347339.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [RandomForest] Trials completados: 55/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:19:46,453] Trial 55 finished with value: 41.65832422347339 and parameters: {'n_estimators': 173, 'max_depth': 12, 'min_samples_split': 12, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 54 with value: 41.65832422347339.\n",
      "[I 2025-04-29 15:20:47,517] Trial 56 finished with value: 43.02993937265686 and parameters: {'n_estimators': 177, 'max_depth': 11, 'min_samples_split': 12, 'min_samples_leaf': 8, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 54 with value: 41.65832422347339.\n",
      "[I 2025-04-29 15:21:57,385] Trial 57 finished with value: 41.67212978813097 and parameters: {'n_estimators': 201, 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 54 with value: 41.65832422347339.\n",
      "[I 2025-04-29 15:23:05,247] Trial 58 finished with value: 43.03089382235649 and parameters: {'n_estimators': 200, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 7, 'max_features': 'sqrt', 'bootstrap': True}. Best is trial 54 with value: 41.65832422347339.\n",
      "[I 2025-04-29 15:29:04,589] Using an existing study with name 'XGBoost_memory_optimized' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ RandomForest optimizado:\n",
      "RMSE: 41.6583\n",
      "MAE: 29.3069\n",
      "R²: 0.8911\n",
      "\n",
      "🚀 Ejecutando optimización para XGBoost...\n",
      "\n",
      "📊 Iniciando optimización adaptativa para XGBoost...\n",
      "Memoria RAM disponible: 2.95 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:29:15,129] Trial 30 finished with value: 36.62548661152845 and parameters: {'n_estimators': 139, 'max_depth': 12, 'learning_rate': 0.05680841237567405, 'subsample': 0.6255423735245499, 'colsample_bytree': 0.6658087572237882, 'min_child_weight': 2, 'gamma': 0.07797399829228002}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:29:30,060] Trial 31 finished with value: 35.80503098924263 and parameters: {'n_estimators': 249, 'max_depth': 12, 'learning_rate': 0.07639209457951664, 'subsample': 0.6856154283737833, 'colsample_bytree': 0.9585633716823388, 'min_child_weight': 3, 'gamma': 3.4547323210100664}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:29:41,807] Trial 32 finished with value: 35.99836896804319 and parameters: {'n_estimators': 240, 'max_depth': 11, 'learning_rate': 0.10605708857340211, 'subsample': 0.6997625821405725, 'colsample_bytree': 0.9339879990337439, 'min_child_weight': 4, 'gamma': 3.3702110962631133}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:29:54,744] Trial 33 finished with value: 35.76857888358947 and parameters: {'n_estimators': 210, 'max_depth': 12, 'learning_rate': 0.07597694840826469, 'subsample': 0.6792952686720702, 'colsample_bytree': 0.9950348476322967, 'min_child_weight': 3, 'gamma': 2.837885340636238}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:30:06,211] Trial 34 finished with value: 36.77854008443222 and parameters: {'n_estimators': 211, 'max_depth': 10, 'learning_rate': 0.027068851641358287, 'subsample': 0.6485806716671252, 'colsample_bytree': 0.9849618943540825, 'min_child_weight': 3, 'gamma': 2.8108812843031035}. Best is trial 22 with value: 35.711163960942415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [XGBoost] Trials completados: 35/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:30:15,981] Trial 35 finished with value: 35.93187455971369 and parameters: {'n_estimators': 191, 'max_depth': 11, 'learning_rate': 0.08690529832041757, 'subsample': 0.6616132068894329, 'colsample_bytree': 0.9995983016398858, 'min_child_weight': 2, 'gamma': 2.095580068958296}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:30:31,162] Trial 36 finished with value: 36.5276626900978 and parameters: {'n_estimators': 205, 'max_depth': 12, 'learning_rate': 0.04071437512734325, 'subsample': 0.6785830832653815, 'colsample_bytree': 0.596609562772596, 'min_child_weight': 4, 'gamma': 3.8834816984958227}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:30:42,624] Trial 37 finished with value: 36.445274819112356 and parameters: {'n_estimators': 178, 'max_depth': 11, 'learning_rate': 0.03167666597394982, 'subsample': 0.6630468829597461, 'colsample_bytree': 0.8962708604716907, 'min_child_weight': 2, 'gamma': 2.652286695452191}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:30:53,342] Trial 38 finished with value: 36.441675696090904 and parameters: {'n_estimators': 267, 'max_depth': 9, 'learning_rate': 0.11633185649566023, 'subsample': 0.6119676707535835, 'colsample_bytree': 0.9158782952134779, 'min_child_weight': 5, 'gamma': 2.8986049155255293}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:31:08,217] Trial 39 finished with value: 35.73023963555328 and parameters: {'n_estimators': 228, 'max_depth': 12, 'learning_rate': 0.05438064562693795, 'subsample': 0.6391415567219676, 'colsample_bytree': 0.9662209611185965, 'min_child_weight': 3, 'gamma': 4.534951428733521}. Best is trial 22 with value: 35.711163960942415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [XGBoost] Trials completados: 40/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:31:14,056] Trial 40 finished with value: 41.35424800579742 and parameters: {'n_estimators': 151, 'max_depth': 7, 'learning_rate': 0.021042180299414088, 'subsample': 0.6438136102662341, 'colsample_bytree': 0.8880997147420849, 'min_child_weight': 1, 'gamma': 4.357097180179197}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:31:27,766] Trial 41 finished with value: 35.71505544650428 and parameters: {'n_estimators': 224, 'max_depth': 12, 'learning_rate': 0.06369751553038497, 'subsample': 0.689405221576034, 'colsample_bytree': 0.9633425507197392, 'min_child_weight': 3, 'gamma': 4.702439357154193}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:31:42,045] Trial 42 finished with value: 35.721521518677996 and parameters: {'n_estimators': 230, 'max_depth': 12, 'learning_rate': 0.060489366224316374, 'subsample': 0.6248065870358512, 'colsample_bytree': 0.9570165692196, 'min_child_weight': 4, 'gamma': 4.754522846338015}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:31:56,858] Trial 43 finished with value: 35.75602959423417 and parameters: {'n_estimators': 228, 'max_depth': 12, 'learning_rate': 0.05253512646464307, 'subsample': 0.6001790390210191, 'colsample_bytree': 0.9574416612538619, 'min_child_weight': 3, 'gamma': 4.7257259609132}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:32:10,232] Trial 44 finished with value: 35.8694974819745 and parameters: {'n_estimators': 273, 'max_depth': 11, 'learning_rate': 0.06393228717627751, 'subsample': 0.6250439690336747, 'colsample_bytree': 0.9576366402958628, 'min_child_weight': 3, 'gamma': 4.9614312025961}. Best is trial 22 with value: 35.711163960942415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [XGBoost] Trials completados: 45/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:32:15,306] Trial 45 finished with value: 43.24834319893798 and parameters: {'n_estimators': 257, 'max_depth': 3, 'learning_rate': 0.042975671890912434, 'subsample': 0.6328151299281151, 'colsample_bytree': 0.9719356550867797, 'min_child_weight': 6, 'gamma': 4.438817260819888}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:32:35,709] Trial 46 finished with value: 35.84132511414024 and parameters: {'n_estimators': 286, 'max_depth': 12, 'learning_rate': 0.030285223306482843, 'subsample': 0.5900694922078336, 'colsample_bytree': 0.9153853241137007, 'min_child_weight': 4, 'gamma': 4.591642060349627}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:32:46,410] Trial 47 finished with value: 36.253475048952204 and parameters: {'n_estimators': 235, 'max_depth': 10, 'learning_rate': 0.05952105055143628, 'subsample': 0.6410912488542901, 'colsample_bytree': 0.8905184273603719, 'min_child_weight': 2, 'gamma': 4.29209915622836}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:32:52,927] Trial 48 finished with value: 38.58869842951917 and parameters: {'n_estimators': 220, 'max_depth': 6, 'learning_rate': 0.05240040744413739, 'subsample': 0.6897243670126477, 'colsample_bytree': 0.9746498295403634, 'min_child_weight': 5, 'gamma': 4.78763643091129}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:33:04,968] Trial 49 finished with value: 36.02288599529037 and parameters: {'n_estimators': 255, 'max_depth': 11, 'learning_rate': 0.0913451127314643, 'subsample': 0.6937701854624787, 'colsample_bytree': 0.8405457157570864, 'min_child_weight': 6, 'gamma': 3.970149365334482}. Best is trial 22 with value: 35.711163960942415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [XGBoost] Trials completados: 50/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:33:20,378] Trial 50 finished with value: 35.7468704240971 and parameters: {'n_estimators': 233, 'max_depth': 12, 'learning_rate': 0.04634598796674968, 'subsample': 0.5776016894687577, 'colsample_bytree': 0.9245070293038141, 'min_child_weight': 10, 'gamma': 4.256964777240955}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:33:35,802] Trial 51 finished with value: 35.73542883502922 and parameters: {'n_estimators': 236, 'max_depth': 12, 'learning_rate': 0.046760639495998194, 'subsample': 0.5803843285105978, 'colsample_bytree': 0.921796745693267, 'min_child_weight': 10, 'gamma': 4.61418837348294}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:33:49,229] Trial 52 finished with value: 35.82714203568731 and parameters: {'n_estimators': 219, 'max_depth': 12, 'learning_rate': 0.06482965517991178, 'subsample': 0.5543289074291777, 'colsample_bytree': 0.9499428593173277, 'min_child_weight': 7, 'gamma': 4.984087626288539}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:34:01,397] Trial 53 finished with value: 37.45122641985924 and parameters: {'n_estimators': 203, 'max_depth': 11, 'learning_rate': 0.03812819928336224, 'subsample': 0.5747820981104603, 'colsample_bytree': 0.5338509616649421, 'min_child_weight': 9, 'gamma': 4.611300564384116}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:34:14,265] Trial 54 finished with value: 36.06162328332052 and parameters: {'n_estimators': 226, 'max_depth': 11, 'learning_rate': 0.04695007512352506, 'subsample': 0.6283073207392276, 'colsample_bytree': 0.8785967009318646, 'min_child_weight': 8, 'gamma': 4.566497527962416}. Best is trial 22 with value: 35.711163960942415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [XGBoost] Trials completados: 55/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:34:27,714] Trial 55 finished with value: 35.74347419132092 and parameters: {'n_estimators': 189, 'max_depth': 12, 'learning_rate': 0.054163319298659474, 'subsample': 0.6104048260205589, 'colsample_bytree': 0.9728762474196093, 'min_child_weight': 2, 'gamma': 4.779352692117119}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:34:40,557] Trial 56 finished with value: 36.54383036965252 and parameters: {'n_estimators': 242, 'max_depth': 10, 'learning_rate': 0.03340496451153928, 'subsample': 0.602703154669325, 'colsample_bytree': 0.911423475156453, 'min_child_weight': 4, 'gamma': 4.177650673822405}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:34:55,645] Trial 57 finished with value: 35.9219565571062 and parameters: {'n_estimators': 260, 'max_depth': 12, 'learning_rate': 0.08245574707021573, 'subsample': 0.5414650696346721, 'colsample_bytree': 0.9404457656737991, 'min_child_weight': 10, 'gamma': 0.6654401031387573}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:35:04,664] Trial 58 finished with value: 36.42888077462973 and parameters: {'n_estimators': 171, 'max_depth': 11, 'learning_rate': 0.1305704928666805, 'subsample': 0.6200772245407548, 'colsample_bytree': 0.7201199681908232, 'min_child_weight': 4, 'gamma': 3.771686525735328}. Best is trial 22 with value: 35.711163960942415.\n",
      "[I 2025-04-29 15:35:16,087] Trial 59 finished with value: 42.978273092433724 and parameters: {'n_estimators': 215, 'max_depth': 9, 'learning_rate': 0.01018739106267007, 'subsample': 0.6564646046004304, 'colsample_bytree': 0.8024481031192191, 'min_child_weight': 5, 'gamma': 4.503286293676885}. Best is trial 22 with value: 35.711163960942415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [XGBoost] Trials completados: 60/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 15:35:28,179] A new study created in RDB with name: LightGBM_memory_optimized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ XGBoost optimizado:\n",
      "RMSE: 35.7112\n",
      "MAE: 24.5218\n",
      "R²: 0.9200\n",
      "\n",
      "🚀 Ejecutando optimización para LightGBM...\n",
      "\n",
      "📊 Iniciando optimización adaptativa para LightGBM...\n",
      "Memoria RAM disponible: 3.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:35:32,199] Trial 0 finished with value: 38.652224429277986 and parameters: {'n_estimators': 144, 'max_depth': 12, 'learning_rate': 0.1205712628744377, 'subsample': 0.6197316968394073, 'colsample_bytree': 0.5780093202212182, 'min_child_samples': 19, 'reg_alpha': 0.05808361216819946, 'reg_lambda': 0.8661761457749352}. Best is trial 0 with value: 38.652224429277986.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:35:38,502] Trial 1 finished with value: 48.04119962241277 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.010725209743171997, 'subsample': 0.6939819704323988, 'colsample_bytree': 0.9162213204002109, 'min_child_samples': 25, 'reg_alpha': 0.18182496720710062, 'reg_lambda': 0.18340450985343382}. Best is trial 0 with value: 38.652224429277986.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:35:42,377] Trial 2 finished with value: 40.49368537625996 and parameters: {'n_estimators': 126, 'max_depth': 8, 'learning_rate': 0.04345454109729477, 'subsample': 0.5582458280396083, 'colsample_bytree': 0.8059264473611898, 'min_child_samples': 18, 'reg_alpha': 0.29214464853521815, 'reg_lambda': 0.3663618432936917}. Best is trial 0 with value: 38.652224429277986.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:35:47,480] Trial 3 finished with value: 43.24097541962213 and parameters: {'n_estimators': 164, 'max_depth': 10, 'learning_rate': 0.019721610970574007, 'subsample': 0.6028468876827223, 'colsample_bytree': 0.7962072844310213, 'min_child_samples': 9, 'reg_alpha': 0.6075448519014384, 'reg_lambda': 0.17052412368729153}. Best is trial 0 with value: 38.652224429277986.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:35:49,316] Trial 4 finished with value: 38.90785553010555 and parameters: {'n_estimators': 66, 'max_depth': 12, 'learning_rate': 0.26690431824362526, 'subsample': 0.6616794696232922, 'colsample_bytree': 0.6523068845866853, 'min_child_samples': 14, 'reg_alpha': 0.6842330265121569, 'reg_lambda': 0.4401524937396013}. Best is trial 0 with value: 38.652224429277986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [LightGBM] Trials completados: 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:35:51,754] Trial 5 finished with value: 71.77272885862774 and parameters: {'n_estimators': 80, 'max_depth': 7, 'learning_rate': 0.011240768803005551, 'subsample': 0.6818640804157564, 'colsample_bytree': 0.6293899908000085, 'min_child_samples': 68, 'reg_alpha': 0.31171107608941095, 'reg_lambda': 0.5200680211778108}. Best is trial 0 with value: 38.652224429277986.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:35:54,634] Trial 6 finished with value: 38.83237118231734 and parameters: {'n_estimators': 187, 'max_depth': 4, 'learning_rate': 0.2705166881899928, 'subsample': 0.6550265646722229, 'colsample_bytree': 0.9697494707820946, 'min_child_samples': 90, 'reg_alpha': 0.5978999788110851, 'reg_lambda': 0.9218742350231168}. Best is trial 0 with value: 38.652224429277986.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:35:56,390] Trial 7 finished with value: 77.09149700565877 and parameters: {'n_estimators': 72, 'max_depth': 4, 'learning_rate': 0.011662890273931383, 'subsample': 0.5650660661526529, 'colsample_bytree': 0.6943386448447411, 'min_child_samples': 31, 'reg_alpha': 0.8287375091519293, 'reg_lambda': 0.3567533266935893}. Best is trial 0 with value: 38.652224429277986.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:35:59,707] Trial 8 finished with value: 52.22237256706204 and parameters: {'n_estimators': 120, 'max_depth': 8, 'learning_rate': 0.016149614799999188, 'subsample': 0.6604393961508079, 'colsample_bytree': 0.5372753218398854, 'min_child_samples': 99, 'reg_alpha': 0.7722447692966574, 'reg_lambda': 0.1987156815341724}. Best is trial 0 with value: 38.652224429277986.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:36:01,422] Trial 9 finished with value: 40.362124418694265 and parameters: {'n_estimators': 51, 'max_depth': 11, 'learning_rate': 0.11069143219393454, 'subsample': 0.6458014336081974, 'colsample_bytree': 0.8856351733429728, 'min_child_samples': 12, 'reg_alpha': 0.3584657285442726, 'reg_lambda': 0.11586905952512971}. Best is trial 0 with value: 38.652224429277986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [LightGBM] Trials completados: 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:36:07,848] Trial 10 finished with value: 38.367019671664 and parameters: {'n_estimators': 274, 'max_depth': 6, 'learning_rate': 0.09315744902122478, 'subsample': 0.503592375122964, 'colsample_bytree': 0.5076838686640521, 'min_child_samples': 47, 'reg_alpha': 0.015144237102756836, 'reg_lambda': 0.9761398998579952}. Best is trial 10 with value: 38.367019671664.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:36:14,561] Trial 11 finished with value: 38.25754754413511 and parameters: {'n_estimators': 287, 'max_depth': 6, 'learning_rate': 0.1027064040029392, 'subsample': 0.5034372300537918, 'colsample_bytree': 0.5123526693065024, 'min_child_samples': 46, 'reg_alpha': 0.017030089686359176, 'reg_lambda': 0.968432213810875}. Best is trial 11 with value: 38.25754754413511.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:36:21,159] Trial 12 finished with value: 38.52162600857122 and parameters: {'n_estimators': 294, 'max_depth': 6, 'learning_rate': 0.07319036091699212, 'subsample': 0.5007281490616222, 'colsample_bytree': 0.5225230855336822, 'min_child_samples': 50, 'reg_alpha': 0.034540749310195976, 'reg_lambda': 0.7493535939227458}. Best is trial 11 with value: 38.25754754413511.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:36:29,043] Trial 13 finished with value: 39.17433896039017 and parameters: {'n_estimators': 298, 'max_depth': 6, 'learning_rate': 0.03807795492819243, 'subsample': 0.5020284941874823, 'colsample_bytree': 0.5937663157166836, 'min_child_samples': 46, 'reg_alpha': 0.9993624386415516, 'reg_lambda': 0.7132794126223531}. Best is trial 11 with value: 38.25754754413511.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:36:32,174] Trial 14 finished with value: 40.96464307164093 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.1509159640289796, 'subsample': 0.5319061796613693, 'colsample_bytree': 0.5065701578589041, 'min_child_samples': 65, 'reg_alpha': 0.15234683307338875, 'reg_lambda': 0.9960169893612137}. Best is trial 11 with value: 38.25754754413511.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [LightGBM] Trials completados: 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:36:36,974] Trial 15 finished with value: 39.076688140868896 and parameters: {'n_estimators': 246, 'max_depth': 5, 'learning_rate': 0.06689802001564377, 'subsample': 0.5377454454873054, 'colsample_bytree': 0.7261631564388042, 'min_child_samples': 41, 'reg_alpha': 0.42410875059584285, 'reg_lambda': 0.6728112095643421}. Best is trial 11 with value: 38.25754754413511.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:36:41,599] Trial 16 finished with value: 37.98968281683715 and parameters: {'n_estimators': 248, 'max_depth': 7, 'learning_rate': 0.16873111205747265, 'subsample': 0.5263259811095404, 'colsample_bytree': 0.5707921208261756, 'min_child_samples': 64, 'reg_alpha': 0.004740119653089181, 'reg_lambda': 0.827787259457541}. Best is trial 16 with value: 37.98968281683715.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:36:45,708] Trial 17 finished with value: 37.83296187595987 and parameters: {'n_estimators': 217, 'max_depth': 9, 'learning_rate': 0.17441159401168893, 'subsample': 0.5708214970691061, 'colsample_bytree': 0.5937119280839002, 'min_child_samples': 67, 'reg_alpha': 0.17329167518661676, 'reg_lambda': 0.8249744927727954}. Best is trial 17 with value: 37.83296187595987.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:36:49,939] Trial 18 finished with value: 37.80883615926837 and parameters: {'n_estimators': 230, 'max_depth': 9, 'learning_rate': 0.18104829110098108, 'subsample': 0.5661407116718896, 'colsample_bytree': 0.6656894543543169, 'min_child_samples': 68, 'reg_alpha': 0.19384513036005002, 'reg_lambda': 0.5922461881109746}. Best is trial 18 with value: 37.80883615926837.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:36:53,857] Trial 19 finished with value: 37.75187295463515 and parameters: {'n_estimators': 222, 'max_depth': 9, 'learning_rate': 0.2022787940976506, 'subsample': 0.5739622208497583, 'colsample_bytree': 0.6985984530929163, 'min_child_samples': 80, 'reg_alpha': 0.19589963108466257, 'reg_lambda': 0.5751898281386826}. Best is trial 19 with value: 37.75187295463515.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [LightGBM] Trials completados: 20/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:36:57,703] Trial 20 finished with value: 37.63227014322227 and parameters: {'n_estimators': 222, 'max_depth': 9, 'learning_rate': 0.21227334472098058, 'subsample': 0.588474409474484, 'colsample_bytree': 0.7860103944969721, 'min_child_samples': 83, 'reg_alpha': 0.44632200845003633, 'reg_lambda': 0.5822209281175389}. Best is trial 20 with value: 37.63227014322227.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:37:01,871] Trial 21 finished with value: 37.68909944322988 and parameters: {'n_estimators': 226, 'max_depth': 9, 'learning_rate': 0.2199426419054362, 'subsample': 0.58799613308381, 'colsample_bytree': 0.7653123195315842, 'min_child_samples': 81, 'reg_alpha': 0.4560400754472178, 'reg_lambda': 0.5896163955173216}. Best is trial 20 with value: 37.63227014322227.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:37:05,384] Trial 22 finished with value: 37.465303437931425 and parameters: {'n_estimators': 204, 'max_depth': 9, 'learning_rate': 0.293782756663976, 'subsample': 0.5945204656920496, 'colsample_bytree': 0.7828324310145387, 'min_child_samples': 82, 'reg_alpha': 0.489487908773735, 'reg_lambda': 0.5727495385642619}. Best is trial 22 with value: 37.465303437931425.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:37:08,356] Trial 23 finished with value: 37.66828441561713 and parameters: {'n_estimators': 171, 'max_depth': 10, 'learning_rate': 0.2731707714035988, 'subsample': 0.5939629668550541, 'colsample_bytree': 0.7890333204216629, 'min_child_samples': 81, 'reg_alpha': 0.48853571956096964, 'reg_lambda': 0.6333140141298547}. Best is trial 22 with value: 37.465303437931425.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:37:12,245] Trial 24 finished with value: 37.67067888207164 and parameters: {'n_estimators': 170, 'max_depth': 10, 'learning_rate': 0.28009936164163796, 'subsample': 0.6242580807367039, 'colsample_bytree': 0.8397057264767822, 'min_child_samples': 82, 'reg_alpha': 0.5358551083815833, 'reg_lambda': 0.4412847646061133}. Best is trial 22 with value: 37.465303437931425.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [LightGBM] Trials completados: 25/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:37:14,970] Trial 25 finished with value: 37.72521177316875 and parameters: {'n_estimators': 151, 'max_depth': 11, 'learning_rate': 0.2987555052583498, 'subsample': 0.6004910811725761, 'colsample_bytree': 0.831116804437942, 'min_child_samples': 93, 'reg_alpha': 0.5118327733311046, 'reg_lambda': 0.6379311772229769}. Best is trial 22 with value: 37.465303437931425.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:37:18,871] Trial 26 finished with value: 38.04635658911274 and parameters: {'n_estimators': 196, 'max_depth': 11, 'learning_rate': 0.1369677241217024, 'subsample': 0.6202123670855653, 'colsample_bytree': 0.7509826968682635, 'min_child_samples': 75, 'reg_alpha': 0.4115875383954207, 'reg_lambda': 0.765049861788444}. Best is trial 22 with value: 37.465303437931425.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:37:22,639] Trial 27 finished with value: 37.74089851077237 and parameters: {'n_estimators': 206, 'max_depth': 8, 'learning_rate': 0.22546569961365356, 'subsample': 0.5880487960320194, 'colsample_bytree': 0.863381547827285, 'min_child_samples': 58, 'reg_alpha': 0.643040946530429, 'reg_lambda': 0.48374541228636875}. Best is trial 22 with value: 37.465303437931425.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:37:26,960] Trial 28 finished with value: 37.34593941374425 and parameters: {'n_estimators': 265, 'max_depth': 10, 'learning_rate': 0.21819535879434604, 'subsample': 0.6327290529453772, 'colsample_bytree': 0.783753233521487, 'min_child_samples': 89, 'reg_alpha': 0.5421287003864963, 'reg_lambda': 0.3234251989603163}. Best is trial 28 with value: 37.34593941374425.\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-04-29 15:37:32,065] Trial 29 finished with value: 37.72418418027439 and parameters: {'n_estimators': 269, 'max_depth': 12, 'learning_rate': 0.12659698083155912, 'subsample': 0.6276986347072635, 'colsample_bytree': 0.9164087998653863, 'min_child_samples': 91, 'reg_alpha': 0.7368005145838271, 'reg_lambda': 0.32568187686628597}. Best is trial 28 with value: 37.34593941374425.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 [LightGBM] Trials completados: 30/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ LightGBM optimizado:\n",
      "RMSE: 37.3459\n",
      "MAE: 25.9670\n",
      "R²: 0.9125\n",
      "\n",
      "📊 Comparación de modelos optimizados:\n",
      "Modelo\t\tRMSE\t\tMAE\t\tR2\n",
      "RandomForest\t41.6583\t29.3069\t0.8911\n",
      "XGBoost    \t35.7112\t24.5218\t0.9200\n",
      "LightGBM   \t37.3459\t25.9670\t0.9125\n"
     ]
    }
   ],
   "source": [
    "# ✅ Versión corregida y funcional de la optimización adaptativa con Optuna (sin warnings)\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import warnings\n",
    "import pickle\n",
    "import optuna\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "resultados_base = {}\n",
    "model_paths = {}  # Para guardar rutas de modelos optimizados\n",
    "\n",
    "# Suprimir warnings de XGBoost innecesarios\n",
    "def suppress_specific_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xgboost\")\n",
    "    os.environ[\"XGBOOST_DISABLE_USE_LABEL_ENCODER\"] = \"1\"\n",
    "\n",
    "suppress_specific_warnings()\n",
    "\n",
    "class OptimizationProgressCallback:\n",
    "    def __init__(self, total_trials, model_name=\"Modelo\"):\n",
    "        self.total_trials = total_trials\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        completed_trials = len(study.trials)\n",
    "        if completed_trials % 5 == 0 or completed_trials == self.total_trials:\n",
    "            print(f\"📈 [{self.model_name}] Trials completados: {completed_trials}/{self.total_trials}\")\n",
    "\n",
    "\n",
    "def run_memory_efficient_optimization(model_type, X_train, y_train, X_test, y_test):\n",
    "    print(f\"\\n📊 Iniciando optimización adaptativa para {model_type}...\")\n",
    "\n",
    "    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
    "    print(f\"Memoria RAM disponible: {available_memory_gb:.2f} GB\")\n",
    "\n",
    "    model_types = {\n",
    "        'RandomForest': RandomForestRegressor,\n",
    "        'XGBoost': XGBRegressor,\n",
    "        'LightGBM': LGBMRegressor\n",
    "    }\n",
    "\n",
    "    if model_type not in model_types:\n",
    "        raise ValueError(f\"Tipo de modelo no soportado: {model_type}.\")\n",
    "\n",
    "    if available_memory_gb < 2.0:\n",
    "        n_trials, max_estimators, max_depth, subsample = 10, 50, 6, 0.5\n",
    "    elif available_memory_gb < 8.0:\n",
    "        n_trials, max_estimators, max_depth, subsample = 30, 300, 12, 0.7\n",
    "    else:\n",
    "        n_trials, max_estimators, max_depth, subsample = 50, 500, 20, 0.9\n",
    "\n",
    "    def objective(trial):\n",
    "        common_params = {'random_state': 42}\n",
    "\n",
    "        if model_type == 'RandomForest':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, max_estimators),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, max_depth),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "        elif model_type == 'XGBoost':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, max_estimators),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, max_depth),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, subsample),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "                'tree_method': 'hist',\n",
    "                'n_jobs': -1,\n",
    "                'verbosity': 0\n",
    "            }\n",
    "        elif model_type == 'LightGBM':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, max_estimators),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, max_depth),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, subsample),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "                'boosting_type': 'gbdt',\n",
    "                'n_jobs': -1,\n",
    "                'verbose': -1\n",
    "            }\n",
    "\n",
    "        params.update(common_params)\n",
    "        gc.collect()\n",
    "        try:\n",
    "            model = model_types[model_type](**params)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            return rmse\n",
    "        except Exception as e:\n",
    "            print(f\"Error con parámetros: {params}\\n{e}\")\n",
    "            return float('inf')\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=f\"{model_type}_memory_optimized\",\n",
    "        storage=f\"sqlite:///{model_output_dir}/{model_type}_study.db\",\n",
    "        direction='minimize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5),\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=n_trials, callbacks=[OptimizationProgressCallback(n_trials, model_type)])\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_params['random_state'] = 42\n",
    "    if model_type == 'XGBoost':\n",
    "        best_params['tree_method'] = 'hist'\n",
    "\n",
    "    best_model = model_types[model_type](**best_params)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n✅ {model_type} optimizado:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "    resultados_base[f\"{model_type}_Optuna\"] = (rmse, mae, r2)\n",
    "\n",
    "    model_path = model_output_dir / f\"{model_type}_optimized.pkl\"\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    model_paths[f\"{model_type}_Optuna\"] = model_path\n",
    "\n",
    "    return best_params, best_model, (rmse, mae, r2)\n",
    "\n",
    "# ✅ Ejecutar optimización adaptativa para todos los modelos base\n",
    "if 'X_train_scaled' in globals() and 'y_train' in globals() and 'X_test_scaled' in globals() and 'y_test' in globals():\n",
    "    print(\"\\n🚀 Ejecutando optimización para RandomForest...\")\n",
    "    rf_params, rf_model, rf_metrics = run_memory_efficient_optimization('RandomForest', X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "    print(\"\\n🚀 Ejecutando optimización para XGBoost...\")\n",
    "    xgb_params, xgb_model, xgb_metrics = run_memory_efficient_optimization('XGBoost', X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "    print(\"\\n🚀 Ejecutando optimización para LightGBM...\")\n",
    "    lgb_params, lgb_model, lgb_metrics = run_memory_efficient_optimization('LightGBM', X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "    # 📊 Comparación de métricas\n",
    "    print(\"\\n📊 Comparación de modelos optimizados:\")\n",
    "    print(\"Modelo\\t\\tRMSE\\t\\tMAE\\t\\tR2\")\n",
    "    print(f\"RandomForest\\t{rf_metrics[0]:.4f}\\t{rf_metrics[1]:.4f}\\t{rf_metrics[2]:.4f}\")\n",
    "    print(f\"XGBoost    \\t{xgb_metrics[0]:.4f}\\t{xgb_metrics[1]:.4f}\\t{xgb_metrics[2]:.4f}\")\n",
    "    print(f\"LightGBM   \\t{lgb_metrics[0]:.4f}\\t{lgb_metrics[1]:.4f}\\t{lgb_metrics[2]:.4f}\")\n",
    "else:\n",
    "    print(\"❌ No se encontraron las variables X_train_scaled, y_train, X_test_scaled o y_test en el entorno actual.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f81dd",
   "metadata": {},
   "source": [
    "## 🧠 Implementación de Modelos de Deep Learning\n",
    "\n",
    "A continuación implementaremos modelos basados en redes neuronales profundas para capturar patrones espaciales y temporales en los datos de precipitación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09e57e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Preparando datos para modelos CNN...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🔍 Preparando datos para modelos CNN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Verificar si tenemos columnas de coordenadas en nuestros datos\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m coord_cols \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m feature_cols \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(coord_cols) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnas de coordenadas encontradas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoord_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_cols' is not defined"
     ]
    }
   ],
   "source": [
    "# Implementación de modelo CNN para predicción espacial\n",
    "import gc  # Para liberar memoria\n",
    "\n",
    "print(\"\\n🔍 Preparando datos para modelos CNN...\")\n",
    "\n",
    "# Verificar si tenemos columnas de coordenadas en nuestros datos\n",
    "coord_cols = [col for col in feature_cols if col in ['x', 'y', 'latitude', 'longitude', 'lat', 'lon']]\n",
    "\n",
    "if len(coord_cols) >= 2:\n",
    "    print(f\"Columnas de coordenadas encontradas: {coord_cols}\")\n",
    "    \n",
    "    # Mapeo de nombres de columnas comunes\n",
    "    lat_names = ['latitude', 'lat', 'y']\n",
    "    lon_names = ['longitude', 'lon', 'x']\n",
    "    \n",
    "    # Identificar columnas de latitud y longitud\n",
    "    lat_col = next((col for col in coord_cols if col in lat_names), None)\n",
    "    lon_col = next((col for col in coord_cols if col in lon_names), None)\n",
    "    \n",
    "    if lat_col and lon_col:\n",
    "        print(f\"Usando {lat_col} y {lon_col} como coordenadas para CNN\")\n",
    "        \n",
    "        # Convertir datos a formato espacial para CNN\n",
    "        def prepare_spatial_data(X_data, y_data, lat_col, lon_col):\n",
    "            \"\"\"Prepara datos espaciales para CNN\"\"\"\n",
    "            try:\n",
    "                # Extraer coordenadas únicas en orden\n",
    "                lats = sorted(X_data[lat_col].unique())\n",
    "                lons = sorted(X_data[lon_col].unique())\n",
    "                \n",
    "                # Crear diccionarios de mapeo para índices\n",
    "                lat_to_idx = {lat: idx for idx, lat in enumerate(lats)}\n",
    "                lon_to_idx = {lon: idx for idx, lon in enumerate(lons)}\n",
    "                \n",
    "                # Dimensiones de la grilla\n",
    "                grid_height = len(lats)\n",
    "                grid_width = len(lons)\n",
    "                n_features = X_data.shape[1] - 2  # Restar las dos columnas de coordenadas\n",
    "                \n",
    "                # Inicializar arrays\n",
    "                X_grid = np.zeros((len(X_data), grid_height, grid_width, n_features), dtype=np.float32)\n",
    "                y_grid = np.zeros((len(y_data), grid_height, grid_width, 1), dtype=np.float32)\n",
    "                \n",
    "                # Recorrer todos los datos y ubicarlos en la grilla\n",
    "                non_coord_cols = [col for col in X_data.columns if col != lat_col and col != lon_col]\n",
    "                \n",
    "                for idx in range(len(X_data)):\n",
    "                    lat = X_data.iloc[idx][lat_col]\n",
    "                    lon = X_data.iloc[idx][lon_col]\n",
    "                    \n",
    "                    lat_idx = lat_to_idx[lat]\n",
    "                    lon_idx = lon_to_idx[lon]\n",
    "                    \n",
    "                    # Colocar características en la grilla\n",
    "                    for i, col in enumerate(non_coord_cols):\n",
    "                        X_grid[idx, lat_idx, lon_idx, i] = X_data.iloc[idx][col]\n",
    "                    \n",
    "                    # Colocar valor objetivo\n",
    "                    y_grid[idx, lat_idx, lon_idx, 0] = y_data.iloc[idx]\n",
    "                \n",
    "                return X_grid, y_grid\n",
    "            except Exception as e:\n",
    "                print(f\"Error preparando datos espaciales: {e}\")\n",
    "                return None, None\n",
    "        \n",
    "        # Convertir datos de entrenamiento a formato espacial\n",
    "        print(\"Convirtiendo datos a formato espacial...\")\n",
    "        try:\n",
    "            X_train_df = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "            X_test_df = pd.DataFrame(X_test_scaled, columns=feature_cols)\n",
    "            \n",
    "            X_train_spatial, y_train_spatial = prepare_spatial_data(X_train_df, y_train, lat_col, lon_col)\n",
    "            X_test_spatial, y_test_spatial = prepare_spatial_data(X_test_df, y_test, lat_col, lon_col)\n",
    "            \n",
    "            if X_train_spatial is None or X_test_spatial is None:\n",
    "                raise ValueError(\"Error al preparar datos espaciales para CNN.\")\n",
    "            \n",
    "            print(f\"Datos espaciales preparados:\")\n",
    "            print(f\"X_train_spatial: {X_train_spatial.shape}\")\n",
    "            print(f\"y_train_spatial: {y_train_spatial.shape}\")\n",
    "            print(f\"X_test_spatial: {X_test_spatial.shape}\")\n",
    "            print(f\"y_test_spatial: {y_test_spatial.shape}\")\n",
    "            \n",
    "            # Liberar memoria innecesaria\n",
    "            del X_train_df, X_test_df\n",
    "            gc.collect()\n",
    "            \n",
    "            # Modelo CNN para predicción de precipitación\n",
    "            def create_cnn_model(input_shape):\n",
    "                \"\"\"Crea un modelo CNN para predicción espacial\"\"\"\n",
    "                inputs = Input(shape=input_shape)\n",
    "                x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
    "                x = BatchNormalization()(x)\n",
    "                x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "                x = Dropout(0.25)(x)\n",
    "                x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "                x = Dropout(0.25)(x)\n",
    "                outputs = Conv2D(1, kernel_size=(1, 1), activation='linear', padding='same')(x)\n",
    "                model = Model(inputs=inputs, outputs=outputs)\n",
    "                model.compile(loss='mse', optimizer=Adam(learning_rate=0.001), metrics=['mae'])\n",
    "                return model\n",
    "            \n",
    "            print(\"\\n🧠 Creando y entrenando modelo CNN...\")\n",
    "            input_shape = X_train_spatial.shape[1:]\n",
    "            cnn_model = create_cnn_model(input_shape)\n",
    "            cnn_model.summary()\n",
    "            \n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
    "                ModelCheckpoint(filepath=model_output_dir / 'cnn_model_best.h5', save_best_only=True, monitor='val_loss')\n",
    "            ]\n",
    "            \n",
    "            history = cnn_model.fit(\n",
    "                X_train_spatial, y_train_spatial,\n",
    "                validation_split=0.2,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            print(\"\\n📊 Evaluando modelo CNN...\")\n",
    "            cnn_metrics = cnn_model.evaluate(X_test_spatial, y_test_spatial)\n",
    "            print(f\"Loss (MSE): {cnn_metrics[0]:.4f}\")\n",
    "            print(f\"MAE: {cnn_metrics[1]:.4f}\")\n",
    "            \n",
    "            cnn_model.save(model_output_dir / 'cnn_model_final.h5')\n",
    "            print(\"Modelo CNN guardado correctamente.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparando datos espaciales para CNN: {e}\")\n",
    "    else:\n",
    "        print(\"No se pudieron identificar columnas de latitud y longitud.\")\n",
    "else:\n",
    "    print(\"No se encontraron suficientes columnas de coordenadas para implementar CNN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de modelo ConvLSTM para predicción espaciotemporal\n",
    "print(\"\\n🔍 Preparando datos para modelo ConvLSTM...\")\n",
    "\n",
    "# Verificar si tenemos el DataFrame disponible\n",
    "if 'df' not in locals() or df is None:\n",
    "    print(\"DataFrame no disponible, intentando recargarlo...\")\n",
    "    try:\n",
    "        # Recargar el dataset si no está disponible\n",
    "        data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features.nc'\n",
    "        print(f\"Recargando archivo desde: {data_file}\")\n",
    "        df, ds_original = load_dataset(data_file)\n",
    "        \n",
    "        if df is None:\n",
    "            print(\"Error: No se pudo recargar el DataFrame. Verificar la ruta del archivo.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al recargar el DataFrame: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Verificar si tenemos las variables necesarias\n",
    "if 'feature_cols' not in locals() or 'target_column' not in locals():\n",
    "    print(\"Variables necesarias no definidas, intentando redefinirlas...\")\n",
    "    if df is not None:\n",
    "        # Identificar la columna objetivo (precipitación)\n",
    "        target_column = 'total_precipitation'  # Ajustar si tiene otro nombre en tu dataset\n",
    "        \n",
    "        # Ver si existe 'precip_target' o usar 'total_precipitation'\n",
    "        if 'total_precipitation' in df.columns:\n",
    "            target_column = 'total_precipitation'\n",
    "        \n",
    "        # Separar variables predictoras y variable objetivo\n",
    "        feature_cols = [col for col in df.columns if col != target_column and not pd.isna(df[col]).all()]\n",
    "        \n",
    "        # Eliminar columnas no numéricas para los modelos\n",
    "        non_feature_cols = ['time', 'spatial_ref']\n",
    "        feature_cols = [col for col in feature_cols if col not in non_feature_cols]\n",
    "\n",
    "# Si el DataFrame está disponible, continuar con la preparación de datos\n",
    "if df is not None:\n",
    "    # Para ConvLSTM necesitamos datos con dimensión temporal\n",
    "    time_cols = [col for col in df.columns if col in ['time', 'date', 'month', 'year', 'day']]\n",
    "\n",
    "    if len(time_cols) > 0 and len(coord_cols) >= 2:\n",
    "        print(f\"Columnas temporales encontradas: {time_cols}\")\n",
    "        time_col = time_cols[0]\n",
    "        \n",
    "        # Función para preparar datos espaciotemporales\n",
    "        def prepare_spatiotemporal_data(df, feature_cols, target_column, lat_col, lon_col, time_col, \n",
    "                                        sequence_length=3):\n",
    "            \"\"\"Prepara datos para ConvLSTM con dimensión espaciotemporal\"\"\"\n",
    "            print(\"Preparando datos espaciotemporales para ConvLSTM...\")\n",
    "            try:\n",
    "                # Asegurarnos que la columna temporal está ordenada\n",
    "                # Verificar el tipo de la columna temporal\n",
    "                time_dtype = df[time_col].dtype\n",
    "                print(f\"Tipo de dato de columna temporal: {time_dtype}\")\n",
    "                \n",
    "                if pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "                    # Ya es datetime, ordenamos\n",
    "                    df_sorted = df.sort_values(by=time_col)\n",
    "                else:\n",
    "                    # Intentar convertir a datetime\n",
    "                    try:\n",
    "                        df[time_col] = pd.to_datetime(df[time_col])\n",
    "                        df_sorted = df.sort_values(by=time_col)\n",
    "                    except Exception as e:\n",
    "                        print(f\"No se pudo convertir columna temporal a datetime: {e}\")\n",
    "                        # Si no podemos convertir, asumimos que ya está ordenado\n",
    "                        df_sorted = df\n",
    "                \n",
    "                # Extraer coordenadas únicas\n",
    "                lats = sorted(df_sorted[lat_col].unique())\n",
    "                lons = sorted(df_sorted[lon_col].unique())\n",
    "                time_steps = sorted(df_sorted[time_col].unique())\n",
    "                \n",
    "                print(f\"Dimensiones espaciotemporales:\")\n",
    "                print(f\"- Latitudes (filas): {len(lats)}\")\n",
    "                print(f\"- Longitudes (columnas): {len(lons)}\")\n",
    "                print(f\"- Pasos temporales: {len(time_steps)}\")\n",
    "                \n",
    "                # Crear mapeos para índices\n",
    "                lat_to_idx = {lat: idx for idx, lat in enumerate(lats)}\n",
    "                lon_to_idx = {lon: idx for idx, lon in enumerate(lons)}\n",
    "                time_to_idx = {time: idx for idx, time in enumerate(time_steps)}\n",
    "                \n",
    "                # Filtrar columnas feature eliminando coordenadas y tiempo\n",
    "                feature_cols_filtered = [col for col in feature_cols if col != lat_col and col != lon_col and col != time_col]\n",
    "                n_features = len(feature_cols_filtered)\n",
    "                \n",
    "                # Dimensiones de la grilla espaciotemporal\n",
    "                grid_height = len(lats)\n",
    "                grid_width = len(lons)\n",
    "                n_timesteps = len(time_steps)\n",
    "                \n",
    "                print(f\"Características a usar: {n_features}\")\n",
    "                \n",
    "                # Crear un DataFrame indexado para acceso rápido\n",
    "                df_indexed = df_sorted.set_index([time_col, lat_col, lon_col])\n",
    "                \n",
    "                # Crear matrices 3D para cada paso temporal\n",
    "                # Las dimensiones son: [tiempo, altura, ancho, features]\n",
    "                X_spatiotemporal = np.zeros((n_timesteps, grid_height, grid_width, n_features))\n",
    "                y_spatiotemporal = np.zeros((n_timesteps, grid_height, grid_width, 1))\n",
    "                \n",
    "                # Llenar matrices con datos disponibles\n",
    "                for t_idx, t in enumerate(time_steps):\n",
    "                    for lat_idx, lat in enumerate(lats):\n",
    "                        for lon_idx, lon in enumerate(lons):\n",
    "                            try:\n",
    "                                # Obtener datos para esta coordenada y tiempo\n",
    "                                data = df_indexed.loc[(t, lat, lon)]\n",
    "                                \n",
    "                                # Llenar características\n",
    "                                for f_idx, feat in enumerate(feature_cols_filtered):\n",
    "                                    X_spatiotemporal[t_idx, lat_idx, lon_idx, f_idx] = data[feat]\n",
    "                                \n",
    "                                # Llenar target\n",
    "                                y_spatiotemporal[t_idx, lat_idx, lon_idx, 0] = data[target_column]\n",
    "                            except KeyError:\n",
    "                                # Este punto espaciotemporal no existe en los datos\n",
    "                                pass\n",
    "                \n",
    "                # Crear secuencias para ConvLSTM\n",
    "                # Para cada paso temporal t, usaremos t-sequence_length hasta t-1 para predecir t\n",
    "                n_sequences = n_timesteps - sequence_length\n",
    "                \n",
    "                if n_sequences <= 0:\n",
    "                    print(\"No hay suficientes pasos temporales para crear secuencias. Ajustando sequence_length.\")\n",
    "                    sequence_length = max(1, n_timesteps // 2)\n",
    "                    n_sequences = n_timesteps - sequence_length\n",
    "                    print(f\"Nuevo sequence_length: {sequence_length}, n_sequences: {n_sequences}\")\n",
    "                \n",
    "                # Crear arrays para secuencias\n",
    "                X_sequences = np.zeros((n_sequences, sequence_length, grid_height, grid_width, n_features))\n",
    "                y_sequences = np.zeros((n_sequences, grid_height, grid_width, 1))\n",
    "                \n",
    "                for i in range(n_sequences):\n",
    "                    X_sequences[i] = X_spatiotemporal[i:i+sequence_length]\n",
    "                    y_sequences[i] = y_spatiotemporal[i+sequence_length]\n",
    "                \n",
    "                print(f\"Secuencias creadas:\")\n",
    "                print(f\"X_sequences: {X_sequences.shape}\")\n",
    "                print(f\"y_sequences: {y_sequences.shape}\")\n",
    "                \n",
    "                # Dividir en train/test\n",
    "                train_size = int(0.8 * n_sequences)\n",
    "                X_train = X_sequences[:train_size]\n",
    "                y_train = y_sequences[:train_size]\n",
    "                X_test = X_sequences[train_size:]\n",
    "                y_test = y_sequences[train_size:]\n",
    "                \n",
    "                return X_train, y_train, X_test, y_test\n",
    "            except Exception as e:\n",
    "                print(f\"Error preparando datos espaciotemporales: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                return None, None, None, None\n",
    "        \n",
    "        # Intentar preparar datos espaciotemporales\n",
    "        try:\n",
    "            X_train_convlstm, y_train_convlstm, X_test_convlstm, y_test_convlstm = prepare_spatiotemporal_data(\n",
    "                df, feature_cols, target_column, lat_col, lon_col, time_col, sequence_length=3\n",
    "            )\n",
    "            \n",
    "            # Si los datos se preparan correctamente, crear y entrenar modelo ConvLSTM\n",
    "            if X_train_convlstm is not None:\n",
    "                print(\"\\n🧠 Creando y entrenando modelo ConvLSTM...\")\n",
    "                \n",
    "                def create_convlstm_model(input_shape):\n",
    "                    \"\"\"Crea un modelo ConvLSTM para predicción espaciotemporal\"\"\"\n",
    "                    model = Sequential([\n",
    "                        # Capa ConvLSTM\n",
    "                        ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same',\n",
    "                                  return_sequences=True, activation='tanh',\n",
    "                                  input_shape=input_shape),\n",
    "                        BatchNormalization(),\n",
    "                        \n",
    "                        # Segunda capa ConvLSTM\n",
    "                        ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same',\n",
    "                                   return_sequences=False, activation='tanh'),\n",
    "                        BatchNormalization(),\n",
    "                        \n",
    "                        # Capa convolucional para reducir mapas de características\n",
    "                        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "                        BatchNormalization(),\n",
    "                        MaxPooling2D(pool_size=(2, 2)),\n",
    "                        \n",
    "                        # Capas finales\n",
    "                        Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "                        UpSampling2D(size=(2, 2)),  # Restaurar dimensión original\n",
    "                        Conv2D(filters=1, kernel_size=(3, 3), activation='linear', padding='same')\n",
    "                    ])\n",
    "                    \n",
    "                    # Compilar modelo\n",
    "                    model.compile(\n",
    "                        loss='mse',\n",
    "                        optimizer=Adam(learning_rate=0.001),\n",
    "                        metrics=['mae']\n",
    "                    )\n",
    "                    \n",
    "                    return model\n",
    "                \n",
    "                # Crear modelo ConvLSTM\n",
    "                input_shape = X_train_convlstm.shape[1:]  # (sequence_length, height, width, features)\n",
    "                convlstm_model = create_convlstm_model(input_shape)\n",
    "                \n",
    "                # Mostrar resumen del modelo\n",
    "                convlstm_model.summary()\n",
    "                \n",
    "                # Callbacks para entrenamiento\n",
    "                callbacks = [\n",
    "                    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "                    ModelCheckpoint(filepath=model_output_dir / 'convlstm_model_best.h5',\n",
    "                                  save_best_only=True, monitor='val_loss')\n",
    "                ]\n",
    "                \n",
    "                # Entrenar modelo\n",
    "                history = convlstm_model.fit(\n",
    "                    X_train_convlstm, y_train_convlstm,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=100,\n",
    "                    batch_size=16,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1\n",
    "                )\n",
    "                \n",
    "                # Evaluar modelo\n",
    "                print(\"\\n📊 Evaluando modelo ConvLSTM...\")\n",
    "                convlstm_metrics = convlstm_model.evaluate(X_test_convlstm, y_test_convlstm)\n",
    "                print(f\"Loss (MSE): {convlstm_metrics[0]:.4f}\")\n",
    "                print(f\"MAE: {convlstm_metrics[1]:.4f}\")\n",
    "                \n",
    "                # Predecir con el modelo\n",
    "                y_pred_convlstm = convlstm_model.predict(X_test_convlstm)\n",
    "                \n",
    "                # Aplanar las predicciones para calcular métricas\n",
    "                y_test_flat = y_test_convlstm.flatten()\n",
    "                y_pred_flat = y_pred_convlstm.flatten()\n",
    "                \n",
    "                # Filtrar valores donde y_test_flat > 0 (presumiblemente donde hay datos)\n",
    "                valid_indices = y_test_flat > 0\n",
    "                y_test_valid = y_test_flat[valid_indices]\n",
    "                y_pred_valid = y_pred_flat[valid_indices]\n",
    "                \n",
    "                # Calcular métricas\n",
    "                convlstm_rmse = np.sqrt(mean_squared_error(y_test_valid, y_pred_valid))\n",
    "                convlstm_mae = mean_absolute_error(y_test_valid, y_pred_valid)\n",
    "                convlstm_r2 = r2_score(y_test_valid, y_pred_valid)\n",
    "                \n",
    "                print(f\"RMSE: {convlstm_rmse:.4f}\")\n",
    "                print(f\"MAE: {convlstm_mae:.4f}\")\n",
    "                print(f\"R²: {convlstm_r2:.4f}\")\n",
    "                \n",
    "                # Guardar modelo\n",
    "                convlstm_model.save(model_output_dir / 'convlstm_model_final.h5')\n",
    "                print(\"Modelo ConvLSTM guardado como 'convlstm_model_final.h5'\")\n",
    "                \n",
    "                # Visualizar la historia del entrenamiento\n",
    "                plt.figure(figsize=(12, 5))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.plot(history.history['loss'])\n",
    "                plt.plot(history.history['val_loss'])\n",
    "                plt.title('Pérdida del modelo ConvLSTM')\n",
    "                plt.ylabel('Pérdida')\n",
    "                plt.xlabel('Época')\n",
    "                plt.legend(['Entrenamiento', 'Validación'], loc='upper right')\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.plot(history.history['mae'])\n",
    "                plt.plot(history.history['val_mae'])\n",
    "                plt.title('Error absoluto medio ConvLSTM')\n",
    "                plt.ylabel('MAE')\n",
    "                plt.xlabel('Época')\n",
    "                plt.legend(['Entrenamiento', 'Validación'], loc='upper right')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(model_output_dir / 'convlstm_training_history.png')\n",
    "                plt.show()\n",
    "                \n",
    "                # Añadir resultados a nuestro diccionario de comparación\n",
    "                resultados_base['ConvLSTM'] = (convlstm_rmse, convlstm_mae, convlstm_r2)\n",
    "            else:\n",
    "                print(\"No se pudieron preparar datos para ConvLSTM.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al ejecutar preparación de datos para ConvLSTM: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"No se encontraron columnas temporales o espaciales suficientes para implementar ConvLSTM.\")\n",
    "        print(\"El modelo ConvLSTM requiere al menos una columna temporal y dos columnas espaciales.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87873507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar optimización adaptativa de memoria RAM para modelos base\n",
    "print(\"\\n🔍 Ejecutando optimización adaptativa de memoria RAM para Random Forest...\")\n",
    "rf_params, rf_model_opt, rf_metrics_opt = run_memory_efficient_optimization('RandomForest', X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(\"\\n🔍 Ejecutando optimización adaptativa de memoria RAM para XGBoost...\")\n",
    "xgb_params, xgb_model_opt, xgb_metrics_opt = run_memory_efficient_optimization('XGBoost', X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(\"\\n🔍 Ejecutando optimización adaptativa de memoria RAM para LightGBM...\")\n",
    "lgbm_params, lgbm_model_opt, lgbm_metrics_opt = run_memory_efficient_optimization('LightGBM', X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Resumen de parámetros óptimos encontrados\n",
    "print(\"\\n📊 Mejores parámetros encontrados para cada modelo:\")\n",
    "print(f\"\\nRandom Forest: {rf_params}\")\n",
    "print(f\"\\nXGBoost: {xgb_params}\")\n",
    "print(f\"\\nLightGBM: {lgbm_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejora de visibilidad en el entrenamiento y errores para modelos CNN y ConvLSTM\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Visualiza la historia del entrenamiento de un modelo.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Pérdida\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label='Validación')\n",
    "    plt.title(f'Pérdida del modelo {model_name}')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.xlabel('Época')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Entrenamiento')\n",
    "    plt.plot(history.history['val_mae'], label='Validación')\n",
    "    plt.title(f'Error absoluto medio (MAE) - {model_name}')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Época')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_model_metrics(metrics, model_name):\n",
    "    \"\"\"Muestra las métricas de evaluación de un modelo de forma visual.\"\"\"\n",
    "    rmse, mae, r2 = metrics\n",
    "    display(HTML(f'<div style=\"background-color:#f5f5dc; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
    "                 f'<h3>📊 Métricas para {model_name}</h3>' +\n",
    "                 f'<table style=\"width:100%; text-align:left;\">' +\n",
    "                 f'<tr><th>Métrica</th><th>Valor</th></tr>' +\n",
    "                 f'<tr><td>RMSE</td><td>{rmse:.4f}</td></tr>' +\n",
    "                 f'<tr><td>MAE</td><td>{mae:.4f}</td></tr>' +\n",
    "                 f'<tr><td>R²</td><td>{r2:.4f}</td></tr>' +\n",
    "                 f'</table></div>'))\n",
    "\n",
    "# Aplicar mejoras de visibilidad al modelo CNN\n",
    "if 'cnn_model' in locals() and 'history' in locals():\n",
    "    print(\"\\n📈 Visualizando historia del entrenamiento para modelo CNN...\")\n",
    "    plot_training_history(history, 'CNN')\n",
    "\n",
    "if 'cnn_metrics' in locals():\n",
    "    print(\"\\n📊 Mostrando métricas para modelo CNN...\")\n",
    "    display_model_metrics((cnn_rmse, cnn_mae, cnn_r2), 'CNN')\n",
    "\n",
    "# Aplicar mejoras de visibilidad al modelo ConvLSTM\n",
    "if 'convlstm_model' in locals() and 'history' in locals():\n",
    "    print(\"\\n📈 Visualizando historia del entrenamiento para modelo ConvLSTM...\")\n",
    "    plot_training_history(history, 'ConvLSTM')\n",
    "\n",
    "if 'convlstm_metrics' in locals():\n",
    "    print(\"\\n📊 Mostrando métricas para modelo ConvLSTM...\")\n",
    "    display_model_metrics((convlstm_rmse, convlstm_mae, convlstm_r2), 'ConvLSTM')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
