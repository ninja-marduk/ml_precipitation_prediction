{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab-badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/main/models/base_models_gnn_convlstm_stacking_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# V5 GNN-ConvLSTM Stacking: Meta-Learner Ensemble for Spatiotemporal Precipitation Prediction\n",
        "\n",
        "## Novel Hybrid Architecture - Dual-Branch Stacking with Interpretable Meta-Learner\n",
        "\n",
        "**Version:** 5.0  \n",
        "**Date:** January 2026  \n",
        "**Author:** Manuel Perez  \n",
        "**Institution:** UPTC - Doctoral Thesis in Engineering  \n",
        "\n",
        "---\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "V5 combines the complementary strengths of V2 (ConvLSTM) and V4 (GNN-TAT) through a stacking ensemble:\n",
        "\n",
        "| Component | Description | Innovation |\n",
        "|-----------|-------------|------------|\n",
        "| **Branch 1: ConvLSTM** | Euclidean spatial patterns (BASIC features) | Local spatiotemporal extraction |\n",
        "| **Branch 2: GNN-TAT** | Non-Euclidean topographic relations (KCE features) | Graph-based orographic modeling |\n",
        "| **Grid-Graph Fusion** | Cross-attention between representations | Novel grid\u2194graph alignment |\n",
        "| **Meta-Learner** | Interpretable weighted fusion | Context-dependent branch weighting |\n",
        "\n",
        "### Target Performance (from spec.md)\n",
        "\n",
        "| Metric | V4 Baseline | V5 Target | V5 Excellent |\n",
        "|--------|-------------|-----------|---------------|\n",
        "| R\u00b2 (H1-H6) | 0.628 | > 0.65 | > 0.70 |\n",
        "| RMSE (mm) | 92.12 | < 85 | < 80 |\n",
        "| Parameters | 98K | < 200K | < 180K |\n",
        "\n",
        "### Innovation Status\n",
        "**NOVEL CONTRIBUTION:** No existing Q1 publications combine GNN and ConvLSTM in stacking ensemble for precipitation prediction (verified January 2026)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-1"
      },
      "source": [
        "## 1. Environment Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup-environment"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in Google Colab: False\n",
            "Output directory: d:\\github.com\\ninja-marduk\\ml_precipitation_prediction\\models\\output\\V5_GNN_ConvLSTM_Stacking\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# SECTION 1.1: ENVIRONMENT DETECTION AND SETUP\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Install required packages (match PyTorch/CUDA build)\n",
        "    import torch\n",
        "\n",
        "    torch_version = torch.__version__.split('+')[0]\n",
        "    cuda_version = torch.version.cuda\n",
        "    if cuda_version:\n",
        "        cuda_tag = f\"cu{cuda_version.replace('.', '')}\"\n",
        "    else:\n",
        "        cuda_tag = \"cpu\"\n",
        "    pyg_url = f\"https://data.pyg.org/whl/torch-{torch_version}+{cuda_tag}.html\"\n",
        "    print(f\"Installing PyG wheels from: {pyg_url}\")\n",
        "    !pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f {pyg_url}\n",
        "    !pip install -q netCDF4 xarray dask h5netcdf\n",
        "\n",
        "    # Set base paths for Colab\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "    DRIVE_DATA_FILE = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "    LOCAL_DATA_FILE = Path('/content/complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc')\n",
        "    OUTPUT_ROOT = BASE_PATH / 'models' / 'output' / 'V5_GNN_ConvLSTM_Stacking'\n",
        "\n",
        "    # Copy dataset to local for faster access\n",
        "    if not LOCAL_DATA_FILE.exists():\n",
        "        !cp \"{DRIVE_DATA_FILE}\" \"{LOCAL_DATA_FILE}\"\n",
        "        print(\"Dataset copied to local storage for faster access\")\n",
        "    DATA_FILE = LOCAL_DATA_FILE\n",
        "else:\n",
        "    # Local paths\n",
        "    BASE_PATH = Path('d:/github.com/ninja-marduk/ml_precipitation_prediction')\n",
        "    DATA_FILE = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "    OUTPUT_ROOT = BASE_PATH / 'models' / 'output' / 'V5_GNN_ConvLSTM_Stacking'\n",
        "\n",
        "# Create output directories\n",
        "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output directory: {OUTPUT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "imports"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA GeForce GTX 960M\n",
            "GPU Memory: 4.3 GB\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# SECTION 1.2: IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "import json\n",
        "import gc\n",
        "import copy\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# PyTorch Geometric\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
        "from torch_geometric.data import Data, Batch\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy import stats\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-2"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "config"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "V5 Configuration initialized:\n",
            "  - Light mode: True\n",
            "  - GNN type: GAT\n",
            "  - Enabled horizons: [1, 3, 6, 12]\n",
            "  - Epochs: 200\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# SECTION 2: V5 CONFIGURATION (from spec.md Section 3.3)\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class V5Config:\n",
        "    \"\"\"Complete V5 GNN-ConvLSTM Stacking configuration.\"\"\"\n",
        "\n",
        "    # Data configuration\n",
        "    input_window: int = 60      # Input months\n",
        "    horizon: int = 12           # Prediction months\n",
        "    train_val_split: float = 0.8\n",
        "\n",
        "    # Light mode for testing\n",
        "    light_mode: bool = True     # Set to False for full grid\n",
        "    light_grid_size: int = 5    # Grid subset size when light_mode=True\n",
        "\n",
        "    # Enabled horizons for experiments\n",
        "    enabled_horizons: List[int] = field(default_factory=lambda: [1, 3, 6, 12])\n",
        "\n",
        "    # Branch 1: ConvLSTM configuration (V2 Enhanced)\n",
        "    convlstm_filters: List[int] = field(default_factory=lambda: [32, 16])\n",
        "    convlstm_kernel_size: int = 3\n",
        "    convlstm_attention: bool = True\n",
        "    convlstm_bidirectional: bool = True\n",
        "    convlstm_residual: bool = True\n",
        "    convlstm_output_dim: int = 64\n",
        "\n",
        "    # Branch 2: GNN-TAT configuration (V4)\n",
        "    gnn_type: str = 'GAT'           # GAT, SAGE, or GCN\n",
        "    gnn_hidden_dim: int = 64\n",
        "    gnn_num_layers: int = 2\n",
        "    gnn_num_heads: int = 4          # For GAT\n",
        "    gnn_temporal_heads: int = 4\n",
        "    gnn_lstm_hidden: int = 64\n",
        "    gnn_lstm_layers: int = 2\n",
        "    gnn_output_dim: int = 64\n",
        "    gnn_dropout: float = 0.1\n",
        "\n",
        "    # Graph construction\n",
        "    edge_threshold: float = 0.3\n",
        "    max_neighbors: int = 8\n",
        "    use_distance_edges: bool = True\n",
        "    use_elevation_edges: bool = True\n",
        "    use_correlation_edges: bool = True\n",
        "    distance_scale_km: float = 10.0\n",
        "    elevation_weight: float = 0.3\n",
        "    correlation_weight: float = 0.5\n",
        "    elevation_scale_m: float = 500.0\n",
        "    min_edge_weight: float = 0.01\n",
        "\n",
        "    # Grid-Graph Fusion\n",
        "    fusion_type: str = 'auto'       # cross_attention, gated, or auto\n",
        "    fusion_heads: int = 4\n",
        "    fusion_hidden_dim: int = 64\n",
        "    fusion_dropout: float = 0.1\n",
        "    fusion_max_nodes: int = 1024\n",
        "\n",
        "    # Meta-Learner\n",
        "    meta_hidden_dim: int = 128\n",
        "    meta_dropout: float = 0.1\n",
        "    meta_use_context_features: bool = True\n",
        "    meta_context_features: List[str] = field(\n",
        "        default_factory=lambda: ['mean_elevation', 'elevation_cluster', 'temporal_regime']\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 200\n",
        "    batch_size: int = 4\n",
        "    learning_rate: float = 0.0005\n",
        "    weight_decay: float = 1e-4\n",
        "    patience: int = 60\n",
        "    gradient_clip: float = 1.0\n",
        "\n",
        "# Initialize config\n",
        "CONFIG = V5Config()\n",
        "print(\"V5 Configuration initialized:\")\n",
        "print(f\"  - Light mode: {CONFIG.light_mode}\")\n",
        "print(f\"  - GNN type: {CONFIG.gnn_type}\")\n",
        "print(f\"  - Enabled horizons: {CONFIG.enabled_horizons}\")\n",
        "print(f\"  - Epochs: {CONFIG.epochs}\")\n",
        "print(f\"  - Fusion type: {CONFIG.fusion_type}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-3"
      },
      "source": [
        "## 3. Data Loading and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "data-loading"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset from: d:\\github.com\\ninja-marduk\\ml_precipitation_prediction\\data\\processed\\dataset_monthly_boyaca.nc\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'd:\\\\github.com\\\\ninja-marduk\\\\ml_precipitation_prediction\\\\data\\\\processed\\\\dataset_monthly_boyaca.nc'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\xarray\\backends\\file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\xarray\\backends\\lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('d:\\\\github.com\\\\ninja-marduk\\\\ml_precipitation_prediction\\\\data\\\\processed\\\\dataset_monthly_boyaca.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '20b6a66c-dd67-45ba-b33e-e157f18c61c7']",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 96\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m ds \u001b[38;5;241m=\u001b[39m create_elevation_clusters(ds)\n",
            "Cell \u001b[1;32mIn[4], line 30\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(data_path, config)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load and validate the NetCDF dataset.\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading dataset from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Print dataset info\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataset dimensions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\xarray\\backends\\api.py:687\u001b[0m, in \u001b[0;36mopen_dataset\u001b[1;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[0;32m    676\u001b[0m     decode_cf,\n\u001b[0;32m    677\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    683\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[0;32m    684\u001b[0m )\n\u001b[0;32m    686\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 687\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[0;32m    688\u001b[0m     filename_or_obj,\n\u001b[0;32m    689\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[0;32m    690\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    692\u001b[0m )\n\u001b[0;32m    693\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[0;32m    694\u001b[0m     backend_ds,\n\u001b[0;32m    695\u001b[0m     filename_or_obj,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    706\u001b[0m )\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
            "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:666\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[1;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, auto_complex, lock, autoclose)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    646\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m ReadBuffer \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    663\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    664\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[0;32m    665\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[1;32m--> 666\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    679\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
            "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:452\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[1;34m(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\u001b[0m\n\u001b[0;32m    448\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_complex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m auto_complex\n\u001b[0;32m    449\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[0;32m    450\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[0;32m    451\u001b[0m )\n\u001b[1;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:393\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[1;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
            "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:461\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:455\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[0;32m    456\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
            "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\xarray\\backends\\file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
            "File \u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\xarray\\backends\\file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[1;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opener(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[1;32msrc/netCDF4/_netCDF4.pyx:2521\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32msrc/netCDF4/_netCDF4.pyx:2158\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'd:\\\\github.com\\\\ninja-marduk\\\\ml_precipitation_prediction\\\\data\\\\processed\\\\dataset_monthly_boyaca.nc'"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# SECTION 3: DATA LOADING AND FEATURE EXTRACTION\n",
        "# =============================================================================\n",
        "\n",
        "# Feature set definitions (from CLAUDE.md Section 8)\n",
        "FEATURE_SETS = {\n",
        "    'BASIC': [\n",
        "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
        "        'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
        "        'elevation', 'slope', 'aspect'\n",
        "    ],\n",
        "    'KCE': [\n",
        "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
        "        'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
        "        'elevation', 'slope', 'aspect',\n",
        "        'elev_high', 'elev_med', 'elev_low'\n",
        "    ],\n",
        "    'PAFC': [\n",
        "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
        "        'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
        "        'elevation', 'slope', 'aspect',\n",
        "        'elev_high', 'elev_med', 'elev_low',\n",
        "        'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag12'\n",
        "    ]\n",
        "}\n",
        "\n",
        "def load_dataset(data_path: Path, config: V5Config) -> xr.Dataset:\n",
        "    \"\"\"Load and validate the NetCDF dataset.\"\"\"\n",
        "    print(f\"Loading dataset from: {data_path}\")\n",
        "    ds = xr.open_dataset(data_path)\n",
        "    \n",
        "    # Print dataset info\n",
        "    print(f\"\\nDataset dimensions:\")\n",
        "    for dim, size in ds.dims.items():\n",
        "        print(f\"  - {dim}: {size}\")\n",
        "    \n",
        "    print(f\"\\nAvailable variables: {list(ds.data_vars)}\")\n",
        "    \n",
        "    # Apply light mode if enabled\n",
        "    if config.light_mode:\n",
        "        lat_slice = slice(0, config.light_grid_size)\n",
        "        lon_slice = slice(0, config.light_grid_size)\n",
        "        ds = ds.isel(lat=lat_slice, lon=lon_slice)\n",
        "        print(f\"\\nLight mode enabled: using {config.light_grid_size}x{config.light_grid_size} grid\")\n",
        "    \n",
        "    return ds\n",
        "\n",
        "def create_elevation_clusters(ds: xr.Dataset, n_clusters: int = 3) -> xr.Dataset:\n",
        "    \"\"\"Add elevation cluster features (KCE) to dataset.\"\"\"\n",
        "    elevation = ds['elevation'].values\n",
        "    valid_mask = ~np.isnan(elevation)\n",
        "    \n",
        "    # Flatten for clustering\n",
        "    elev_flat = elevation[valid_mask].reshape(-1, 1)\n",
        "    \n",
        "    # K-means clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=10)\n",
        "    labels = np.full(elevation.shape, -1)\n",
        "    labels[valid_mask] = kmeans.fit_predict(elev_flat)\n",
        "    \n",
        "    # Create one-hot encoded features\n",
        "    for i, name in enumerate(['elev_low', 'elev_med', 'elev_high']):\n",
        "        cluster_data = np.zeros_like(elevation)\n",
        "        cluster_data[labels == i] = 1.0\n",
        "        ds[name] = xr.DataArray(\n",
        "            data=cluster_data,\n",
        "            dims=['lat', 'lon'],\n",
        "            attrs={'description': f'Elevation cluster {name}'}\n",
        "        )\n",
        "    \n",
        "    print(f\"Added elevation clusters: elev_low, elev_med, elev_high\")\n",
        "    return ds\n",
        "\n",
        "def extract_features(ds: xr.Dataset, feature_names: List[str]) -> np.ndarray:\n",
        "    \"\"\"Extract features from dataset into numpy array.\"\"\"\n",
        "    features = []\n",
        "    \n",
        "    for name in feature_names:\n",
        "        if name in ds.data_vars:\n",
        "            data = ds[name].values\n",
        "            # Handle different dimensions\n",
        "            if data.ndim == 2:  # (lat, lon) - static features\n",
        "                # Broadcast to (time, lat, lon)\n",
        "                data = np.broadcast_to(data, (ds.dims['time'], *data.shape))\n",
        "            features.append(data)\n",
        "        else:\n",
        "            print(f\"Warning: Feature '{name}' not found in dataset\")\n",
        "    \n",
        "    # Stack features: (time, lat, lon, n_features)\n",
        "    features = np.stack(features, axis=-1)\n",
        "    print(f\"Extracted features shape: {features.shape}\")\n",
        "    \n",
        "    return features.astype(np.float32)\n",
        "\n",
        "# Load dataset\n",
        "ds = load_dataset(DATA_FILE, CONFIG)\n",
        "ds = create_elevation_clusters(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-4"
      },
      "source": [
        "## 4. Graph Construction for GNN Branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "graph-construction"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 4: GRAPH CONSTRUCTION\n",
        "# =============================================================================\n",
        "\n",
        "class SpatialGraphBuilder:\n",
        "    \"\"\"Build spatial graph for GNN branch based on geographic and topographic similarity.\"\"\"\n",
        "\n",
        "    def __init__(self, config: V5Config):\n",
        "        self.config = config\n",
        "\n",
        "    @staticmethod\n",
        "    def _safe_correlation(ts_a: np.ndarray, ts_b: np.ndarray) -> float:\n",
        "        \"\"\"Compute correlation robustly (returns 0.0 for invalid cases).\"\"\"\n",
        "        mask = np.isfinite(ts_a) & np.isfinite(ts_b)\n",
        "        if mask.sum() < 2:\n",
        "            return 0.0\n",
        "        a = ts_a[mask]\n",
        "        b = ts_b[mask]\n",
        "        a = a - a.mean()\n",
        "        b = b - b.mean()\n",
        "        denom = np.sqrt(np.sum(a * a)) * np.sqrt(np.sum(b * b))\n",
        "        if denom < 1e-6:\n",
        "            return 0.0\n",
        "        corr = float(np.sum(a * b) / denom)\n",
        "        if not np.isfinite(corr):\n",
        "            return 0.0\n",
        "        return corr\n",
        "\n",
        "    def build_graph(self, ds: xr.Dataset) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Build graph structure from dataset.\n",
        "\n",
        "        Returns:\n",
        "            edge_index: (2, num_edges) tensor of edge indices\n",
        "            edge_weight: (num_edges,) tensor of edge weights\n",
        "        \"\"\"\n",
        "        lat = ds['lat'].values\n",
        "        lon = ds['lon'].values\n",
        "        elevation = ds['elevation'].values\n",
        "\n",
        "        n_lat, n_lon = len(lat), len(lon)\n",
        "        n_nodes = n_lat * n_lon\n",
        "\n",
        "        print(f\"Building graph for {n_lat}x{n_lon} = {n_nodes} nodes\")\n",
        "\n",
        "        # Create node positions in km for distance scaling\n",
        "        lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')\n",
        "        mean_lat = float(np.mean(lat))\n",
        "        km_per_deg_lat = 111.32\n",
        "        km_per_deg_lon = 111.32 * np.cos(np.deg2rad(mean_lat))\n",
        "        positions = np.stack(\n",
        "            [lat_grid.flatten() * km_per_deg_lat, lon_grid.flatten() * km_per_deg_lon],\n",
        "            axis=1\n",
        "        )\n",
        "        elev_flat = elevation.flatten()\n",
        "\n",
        "        # Precompute precipitation time series for correlation edges\n",
        "        precip_flat = None\n",
        "        if self.config.use_correlation_edges:\n",
        "            precip = ds['total_precipitation'].values.astype(np.float32)\n",
        "            precip_flat = precip.reshape(precip.shape[0], n_nodes)\n",
        "\n",
        "        edges = []\n",
        "        weights = []\n",
        "\n",
        "        distance_coeff = 1.0\n",
        "        if self.config.use_elevation_edges or self.config.use_correlation_edges:\n",
        "            distance_coeff = max(\n",
        "                0.0,\n",
        "                1.0 - self.config.elevation_weight - self.config.correlation_weight\n",
        "            )\n",
        "\n",
        "        for i in range(n_nodes):\n",
        "            # Calculate distances to all other nodes\n",
        "            distances = np.sqrt(np.sum((positions - positions[i])**2, axis=1))\n",
        "\n",
        "            # Get k nearest neighbors (excluding self)\n",
        "            distances[i] = np.inf\n",
        "            nearest_idx = np.argsort(distances)[:self.config.max_neighbors]\n",
        "            ts_i = precip_flat[:, i] if precip_flat is not None else None\n",
        "\n",
        "            for j in nearest_idx:\n",
        "                if distances[j] == np.inf:\n",
        "                    continue\n",
        "\n",
        "                component_weights = []\n",
        "\n",
        "                if self.config.use_distance_edges:\n",
        "                    dist_weight = np.exp(-distances[j] / self.config.distance_scale_km)\n",
        "                    component_weights.append((dist_weight, distance_coeff))\n",
        "\n",
        "                if self.config.use_elevation_edges:\n",
        "                    if not np.isnan(elev_flat[i]) and not np.isnan(elev_flat[j]):\n",
        "                        elev_diff = np.abs(elev_flat[i] - elev_flat[j])\n",
        "                        elev_weight = np.exp(-elev_diff / self.config.elevation_scale_m)\n",
        "                    else:\n",
        "                        elev_weight = 0.5\n",
        "                    component_weights.append((elev_weight, self.config.elevation_weight))\n",
        "\n",
        "                if self.config.use_correlation_edges:\n",
        "                    if precip_flat is not None:\n",
        "                        corr = self._safe_correlation(ts_i, precip_flat[:, j])\n",
        "                    else:\n",
        "                        corr = 0.0\n",
        "                    corr_weight = max(0.0, corr)\n",
        "                    component_weights.append((corr_weight, self.config.correlation_weight))\n",
        "\n",
        "                if not component_weights:\n",
        "                    continue\n",
        "\n",
        "                coeff_sum = sum(weight for _, weight in component_weights)\n",
        "                if coeff_sum <= 0:\n",
        "                    coeff_sum = len(component_weights)\n",
        "                    combined_weight = sum(val for val, _ in component_weights) / coeff_sum\n",
        "                else:\n",
        "                    combined_weight = sum(val * weight for val, weight in component_weights) / coeff_sum\n",
        "\n",
        "                threshold = max(self.config.edge_threshold, self.config.min_edge_weight)\n",
        "                if combined_weight >= threshold:\n",
        "                    edges.append([i, j])\n",
        "                    weights.append(combined_weight)\n",
        "\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        edge_weight = torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "        print(f\"Graph built: {n_nodes} nodes, {edge_index.shape[1]} edges\")\n",
        "        print(f\"Average edges per node: {edge_index.shape[1] / n_nodes:.1f}\")\n",
        "\n",
        "        return edge_index, edge_weight\n",
        "\n",
        "# Build graph\n",
        "graph_builder = SpatialGraphBuilder(CONFIG)\n",
        "edge_index, edge_weight = graph_builder.build_graph(ds)\n",
        "print(f\"Edge index shape: {edge_index.shape}\")\n",
        "print(f\"Edge weight shape: {edge_weight.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-5"
      },
      "source": [
        "## 5. Data Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocessing"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 5: DATA PREPROCESSING AND DATASET CLASS\n",
        "# =============================================================================\n",
        "\n",
        "def create_temporal_windows(\n",
        "    features: np.ndarray,\n",
        "    target: np.ndarray,\n",
        "    input_window: int,\n",
        "    horizon: int\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Create temporal windows for sequence-to-sequence prediction.\n",
        "\n",
        "    Args:\n",
        "        features: (time, lat, lon, n_features)\n",
        "        target: (time, lat, lon)\n",
        "        input_window: Number of input timesteps\n",
        "        horizon: Number of output timesteps\n",
        "\n",
        "    Returns:\n",
        "        X: (n_samples, input_window, lat, lon, n_features)\n",
        "        Y: (n_samples, horizon, lat, lon)\n",
        "    \"\"\"\n",
        "    n_time = features.shape[0]\n",
        "    n_samples = n_time - input_window - horizon + 1\n",
        "\n",
        "    if n_samples <= 0:\n",
        "        raise ValueError(f\"Not enough timesteps: {n_time} < {input_window + horizon}\")\n",
        "\n",
        "    X_list = []\n",
        "    Y_list = []\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        X_list.append(features[i:i+input_window])\n",
        "        Y_list.append(target[i+input_window:i+input_window+horizon])\n",
        "\n",
        "    X = np.stack(X_list, axis=0)\n",
        "    Y = np.stack(Y_list, axis=0)\n",
        "\n",
        "    print(f\"Created {n_samples} samples\")\n",
        "    print(f\"  X shape: {X.shape}\")\n",
        "    print(f\"  Y shape: {Y.shape}\")\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "class V5DualBranchDataset(Dataset):\n",
        "    \"\"\"Dataset for V5 dual-branch model (ConvLSTM + GNN).\n",
        "\n",
        "    Provides both grid (for ConvLSTM) and graph (for GNN) representations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        X_basic: np.ndarray,    # BASIC features for ConvLSTM\n",
        "        X_kce: np.ndarray,      # KCE features for GNN\n",
        "        Y: np.ndarray,\n",
        "        edge_index: torch.Tensor,\n",
        "        edge_weight: torch.Tensor\n",
        "    ):\n",
        "        self.X_basic = torch.tensor(X_basic, dtype=torch.float32)\n",
        "        self.X_kce = torch.tensor(X_kce, dtype=torch.float32)\n",
        "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
        "        self.edge_index = edge_index\n",
        "        self.edge_weight = edge_weight\n",
        "\n",
        "        # Grid dimensions\n",
        "        self.n_lat = X_basic.shape[2]\n",
        "        self.n_lon = X_basic.shape[3]\n",
        "        self.n_nodes = self.n_lat * self.n_lon\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_basic)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Grid format for ConvLSTM: (seq, lat, lon, features)\n",
        "        x_grid = self.X_basic[idx]\n",
        "\n",
        "        # Graph format for GNN: (seq, n_nodes, features)\n",
        "        x_graph = self.X_kce[idx].reshape(\n",
        "            self.X_kce.shape[1], self.n_nodes, -1\n",
        "        )\n",
        "\n",
        "        # Target: (horizon, lat, lon)\n",
        "        y = self.Y[idx]\n",
        "\n",
        "        return {\n",
        "            'x_grid': x_grid,           # For ConvLSTM branch\n",
        "            'x_graph': x_graph,         # For GNN branch\n",
        "            'y': y,\n",
        "            'edge_index': self.edge_index,\n",
        "            'edge_weight': self.edge_weight\n",
        "        }\n",
        "\n",
        "def prepare_data(ds: xr.Dataset, config: V5Config, edge_index: torch.Tensor, edge_weight: torch.Tensor):\n",
        "    \"\"\"Prepare data for training.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Preparing data for V5 dual-branch model\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Extract features for both branches\n",
        "    print(\"\\nExtracting BASIC features (ConvLSTM branch):\")\n",
        "    features_basic = extract_features(ds, FEATURE_SETS['BASIC'])\n",
        "\n",
        "    print(\"\\nExtracting KCE features (GNN branch):\")\n",
        "    features_kce = extract_features(ds, FEATURE_SETS['KCE'])\n",
        "\n",
        "    # Target variable\n",
        "    target = ds['total_precipitation'].values.astype(np.float32)\n",
        "    print(f\"\\nTarget shape: {target.shape}\")\n",
        "\n",
        "    # Handle NaN values\n",
        "    features_basic = np.nan_to_num(features_basic, nan=0.0)\n",
        "    features_kce = np.nan_to_num(features_kce, nan=0.0)\n",
        "    target = np.nan_to_num(target, nan=0.0)\n",
        "\n",
        "    # Create temporal windows (raw)\n",
        "    print(\"\\nCreating temporal windows:\")\n",
        "    X_basic_raw, Y = create_temporal_windows(\n",
        "        features_basic, target,\n",
        "        config.input_window, config.horizon\n",
        "    )\n",
        "    X_kce_raw, _ = create_temporal_windows(\n",
        "        features_kce, target,\n",
        "        config.input_window, config.horizon\n",
        "    )\n",
        "\n",
        "    # Train/val split\n",
        "    n_train = int(len(X_basic_raw) * config.train_val_split)\n",
        "\n",
        "    # Normalize using training split only (avoid leakage)\n",
        "    print(\"\\nNormalizing features using training split only:\")\n",
        "    X_basic_train = X_basic_raw[:n_train]\n",
        "    X_kce_train = X_kce_raw[:n_train]\n",
        "\n",
        "    basic_mean = X_basic_train.mean(axis=(0, 1, 2, 3), keepdims=True)\n",
        "    basic_std = X_basic_train.std(axis=(0, 1, 2, 3), keepdims=True)\n",
        "    basic_std = np.where(basic_std > 1e-6, basic_std, 1.0)\n",
        "\n",
        "    kce_mean = X_kce_train.mean(axis=(0, 1, 2, 3), keepdims=True)\n",
        "    kce_std = X_kce_train.std(axis=(0, 1, 2, 3), keepdims=True)\n",
        "    kce_std = np.where(kce_std > 1e-6, kce_std, 1.0)\n",
        "\n",
        "    X_basic = ((X_basic_raw - basic_mean) / basic_std).astype(np.float32)\n",
        "    X_kce = ((X_kce_raw - kce_mean) / kce_std).astype(np.float32)\n",
        "\n",
        "    train_dataset = V5DualBranchDataset(\n",
        "        X_basic[:n_train], X_kce[:n_train], Y[:n_train],\n",
        "        edge_index, edge_weight\n",
        "    )\n",
        "    val_dataset = V5DualBranchDataset(\n",
        "        X_basic[n_train:], X_kce[n_train:], Y[n_train:],\n",
        "        edge_index, edge_weight\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataset split:\")\n",
        "    print(f\"  Training samples: {len(train_dataset)}\")\n",
        "    print(f\"  Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    return train_dataset, val_dataset, features_basic.shape[-1], features_kce.shape[-1]\n",
        "\n",
        "# Prepare data\n",
        "train_dataset, val_dataset, n_features_basic, n_features_kce = prepare_data(\n",
        "    ds, CONFIG, edge_index, edge_weight\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG.batch_size, shuffle=False)\n",
        "\n",
        "print(f\"\\nData loaders created:\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-6"
      },
      "source": [
        "## 6. V5 Model Architecture\n",
        "\n",
        "### 6.1 Branch 1: ConvLSTM (Euclidean Spatial Encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convlstm-branch"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 6.1: CONVLSTM BRANCH (V2 Enhanced Architecture)\n",
        "# =============================================================================\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    \"\"\"Single ConvLSTM cell with spatial convolutions.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, kernel_size: int):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        padding = kernel_size // 2\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=input_dim + hidden_dim,\n",
        "            out_channels=4 * hidden_dim,  # i, f, o, g gates\n",
        "            kernel_size=kernel_size,\n",
        "            padding=padding,\n",
        "            bias=True\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]):\n",
        "        h, c = state\n",
        "        combined = torch.cat([x, h], dim=1)\n",
        "        gates = self.conv(combined)\n",
        "\n",
        "        i, f, o, g = torch.split(gates, self.hidden_dim, dim=1)\n",
        "        i = torch.sigmoid(i)\n",
        "        f = torch.sigmoid(f)\n",
        "        o = torch.sigmoid(o)\n",
        "        g = torch.tanh(g)\n",
        "\n",
        "        c_new = f * c + i * g\n",
        "        h_new = o * torch.tanh(c_new)\n",
        "\n",
        "        return h_new, c_new\n",
        "\n",
        "    def init_hidden(self, batch_size: int, height: int, width: int, device: torch.device):\n",
        "        return (\n",
        "            torch.zeros(batch_size, self.hidden_dim, height, width, device=device),\n",
        "            torch.zeros(batch_size, self.hidden_dim, height, width, device=device)\n",
        "        )\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    \"\"\"Spatial attention mechanism for ConvLSTM output.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        attention = torch.sigmoid(self.conv(x))\n",
        "        return x * attention\n",
        "\n",
        "class ConvLSTMBranch(nn.Module):\n",
        "    \"\"\"Branch 1: ConvLSTM encoder for Euclidean spatial patterns.\n",
        "\n",
        "    Based on V2 Enhanced architecture with attention and residual connections.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: V5Config, n_features: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Input projection\n",
        "        self.input_proj = nn.Conv2d(\n",
        "            n_features, config.convlstm_filters[0],\n",
        "            kernel_size=1\n",
        "        )\n",
        "\n",
        "        # ConvLSTM layers\n",
        "        self.convlstm_cells_fw = nn.ModuleList()\n",
        "        self.convlstm_cells_bw = nn.ModuleList() if config.convlstm_bidirectional else None\n",
        "        in_dim = config.convlstm_filters[0]\n",
        "        for out_dim in config.convlstm_filters:\n",
        "            self.convlstm_cells_fw.append(\n",
        "                ConvLSTMCell(in_dim, out_dim, config.convlstm_kernel_size)\n",
        "            )\n",
        "            if config.convlstm_bidirectional:\n",
        "                self.convlstm_cells_bw.append(\n",
        "                    ConvLSTMCell(in_dim, out_dim, config.convlstm_kernel_size)\n",
        "                )\n",
        "            in_dim = out_dim\n",
        "\n",
        "        # Spatial attention\n",
        "        attn_in_dim = config.convlstm_filters[-1] * (2 if config.convlstm_bidirectional else 1)\n",
        "        if config.convlstm_attention:\n",
        "            self.attention = SpatialAttention(attn_in_dim)\n",
        "        else:\n",
        "            self.attention = None\n",
        "\n",
        "        # Output projection to match GNN branch\n",
        "        self.output_proj = nn.Conv2d(\n",
        "            config.convlstm_filters[-1] * (2 if config.convlstm_bidirectional else 1),\n",
        "            config.convlstm_output_dim,\n",
        "            kernel_size=1\n",
        "        )\n",
        "\n",
        "        # Residual connection\n",
        "        if config.convlstm_residual:\n",
        "            self.residual_proj = nn.Conv2d(n_features, config.convlstm_output_dim, kernel_size=1)\n",
        "        else:\n",
        "            self.residual_proj = None\n",
        "\n",
        "    def _run_convlstm(self, x: torch.Tensor, cells: nn.ModuleList) -> torch.Tensor:\n",
        "        \"\"\"Run ConvLSTM stack and return last hidden state.\"\"\"\n",
        "        batch_size, seq_len, _, h, w = x.shape\n",
        "        layer_input = x\n",
        "        for layer_idx, cell in enumerate(cells):\n",
        "            h_state, c_state = cell.init_hidden(batch_size, h, w, x.device)\n",
        "            outputs = []\n",
        "            for t in range(seq_len):\n",
        "                x_t = layer_input[:, t]\n",
        "                if layer_idx == 0:\n",
        "                    x_t = self.input_proj(x_t)\n",
        "                h_state, c_state = cell(x_t, (h_state, c_state))\n",
        "                outputs.append(h_state)\n",
        "            layer_input = torch.stack(outputs, dim=1)\n",
        "        return h_state\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, lat, lon, features)\n",
        "\n",
        "        Returns:\n",
        "            output: (batch, lat, lon, output_dim)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, h, w, _ = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # Reshape for Conv2d: (batch, seq, features, h, w)\n",
        "        x_reshaped = x.permute(0, 1, 4, 2, 3)\n",
        "\n",
        "        # Store residual\n",
        "        if self.residual_proj is not None:\n",
        "            residual = self.residual_proj(x_reshaped[:, -1])  # Use last timestep\n",
        "\n",
        "        # Forward direction\n",
        "        h_forward = self._run_convlstm(x_reshaped, self.convlstm_cells_fw)\n",
        "\n",
        "        if self.config.convlstm_bidirectional:\n",
        "            # Backward direction\n",
        "            x_backward = x_reshaped.flip(1)\n",
        "            h_backward = self._run_convlstm(x_backward, self.convlstm_cells_bw)\n",
        "            output = torch.cat([h_forward, h_backward], dim=1)\n",
        "        else:\n",
        "            output = h_forward\n",
        "\n",
        "        # Apply attention\n",
        "        if self.attention is not None:\n",
        "            output = self.attention(output)\n",
        "\n",
        "        # Project to output dimension\n",
        "        output = self.output_proj(output)  # (batch, output_dim, h, w)\n",
        "\n",
        "        # Add residual\n",
        "        if self.residual_proj is not None:\n",
        "            output = output + residual\n",
        "\n",
        "        # Reshape to (batch, h, w, output_dim)\n",
        "        output = output.permute(0, 2, 3, 1)\n",
        "\n",
        "        return output\n",
        "\n",
        "print(\"ConvLSTM Branch defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-6-2"
      },
      "source": [
        "### 6.2 Branch 2: GNN-TAT (Non-Euclidean Spatial Encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnn-branch"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 6.2: GNN-TAT BRANCH (V4 Architecture)\n",
        "# =============================================================================\n",
        "\n",
        "class TemporalAttention(nn.Module):\n",
        "    \"\"\"Multi-head temporal self-attention.\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_dim: int, num_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_dim,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"x: (batch, seq_len, hidden_dim)\"\"\"\n",
        "        attended, _ = self.attention(x, x, x)\n",
        "        x = self.norm(x + self.dropout(attended))\n",
        "        return x\n",
        "\n",
        "class SpatialGNNEncoder(nn.Module):\n",
        "    \"\"\"GNN encoder for non-Euclidean spatial patterns.\"\"\"\n",
        "    \n",
        "    def __init__(self, config: V5Config, n_features: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        # Input projection\n",
        "        self.input_proj = nn.Linear(n_features, config.gnn_hidden_dim)\n",
        "        \n",
        "        # GNN layers\n",
        "        self.gnn_layers = nn.ModuleList()\n",
        "        for i in range(config.gnn_num_layers):\n",
        "            if config.gnn_type == 'GAT':\n",
        "                self.gnn_layers.append(\n",
        "                    GATConv(\n",
        "                        config.gnn_hidden_dim,\n",
        "                        config.gnn_hidden_dim // config.gnn_num_heads,\n",
        "                        heads=config.gnn_num_heads,\n",
        "                        dropout=config.gnn_dropout,\n",
        "                        concat=True\n",
        "                    )\n",
        "                )\n",
        "            elif config.gnn_type == 'SAGE':\n",
        "                self.gnn_layers.append(\n",
        "                    SAGEConv(config.gnn_hidden_dim, config.gnn_hidden_dim)\n",
        "                )\n",
        "            else:  # GCN\n",
        "                self.gnn_layers.append(\n",
        "                    GCNConv(config.gnn_hidden_dim, config.gnn_hidden_dim)\n",
        "                )\n",
        "        \n",
        "        self.dropout = nn.Dropout(config.gnn_dropout)\n",
        "        self.norm = nn.LayerNorm(config.gnn_hidden_dim)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_weight: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (n_nodes, n_features)\n",
        "            edge_index: (2, n_edges)\n",
        "            edge_weight: (n_edges,)\n",
        "        \n",
        "        Returns:\n",
        "            x: (n_nodes, hidden_dim)\n",
        "        \"\"\"\n",
        "        x = self.input_proj(x)\n",
        "        \n",
        "        for gnn in self.gnn_layers:\n",
        "            if self.config.gnn_type == 'GCN' and edge_weight is not None:\n",
        "                x_new = gnn(x, edge_index, edge_weight)\n",
        "            else:\n",
        "                x_new = gnn(x, edge_index)\n",
        "            x_new = F.relu(x_new)\n",
        "            x_new = self.dropout(x_new)\n",
        "            x = self.norm(x + x_new)  # Residual\n",
        "        \n",
        "        return x\n",
        "\n",
        "class GNNTATBranch(nn.Module):\n",
        "    \"\"\"Branch 2: GNN-TAT encoder for non-Euclidean spatial patterns.\n",
        "    \n",
        "    Based on V4 architecture: GNN + Temporal Attention + LSTM.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: V5Config, n_features: int, n_nodes: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_nodes = n_nodes\n",
        "        \n",
        "        # Spatial GNN encoder\n",
        "        self.spatial_encoder = SpatialGNNEncoder(config, n_features)\n",
        "        \n",
        "        # Temporal attention\n",
        "        self.temporal_attention = TemporalAttention(\n",
        "            config.gnn_hidden_dim,\n",
        "            config.gnn_temporal_heads,\n",
        "            config.gnn_dropout\n",
        "        )\n",
        "        \n",
        "        # Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=config.gnn_hidden_dim,\n",
        "            hidden_size=config.gnn_lstm_hidden,\n",
        "            num_layers=config.gnn_lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=config.gnn_dropout if config.gnn_lstm_layers > 1 else 0\n",
        "        )\n",
        "        \n",
        "        # Output projection\n",
        "        self.output_proj = nn.Linear(\n",
        "            config.gnn_lstm_hidden * 2,  # Bidirectional\n",
        "            config.gnn_output_dim\n",
        "        )\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        edge_index: torch.Tensor,\n",
        "        edge_weight: torch.Tensor = None\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, n_nodes, n_features)\n",
        "            edge_index: (2, n_edges)\n",
        "            edge_weight: (n_edges,)\n",
        "        \n",
        "        Returns:\n",
        "            output: (batch, n_nodes, output_dim)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, n_nodes, _ = x.shape\n",
        "        \n",
        "        # Process each timestep through GNN\n",
        "        spatial_outputs = []\n",
        "        for t in range(seq_len):\n",
        "            # Process all nodes for this timestep\n",
        "            x_t = x[:, t]  # (batch, n_nodes, features)\n",
        "            \n",
        "            # Flatten batch and process\n",
        "            batch_outputs = []\n",
        "            for b in range(batch_size):\n",
        "                h = self.spatial_encoder(x_t[b], edge_index, edge_weight)\n",
        "                batch_outputs.append(h)\n",
        "            \n",
        "            spatial_out = torch.stack(batch_outputs, dim=0)  # (batch, n_nodes, hidden)\n",
        "            spatial_outputs.append(spatial_out)\n",
        "        \n",
        "        # Stack temporal: (batch, seq_len, n_nodes, hidden)\n",
        "        spatial_outputs = torch.stack(spatial_outputs, dim=1)\n",
        "        \n",
        "        # Apply temporal attention per node\n",
        "        # Reshape: (batch*n_nodes, seq_len, hidden)\n",
        "        x_temporal = spatial_outputs.permute(0, 2, 1, 3).reshape(\n",
        "            batch_size * n_nodes, seq_len, -1\n",
        "        )\n",
        "        x_temporal = self.temporal_attention(x_temporal)\n",
        "        \n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(x_temporal)  # (batch*n_nodes, seq_len, hidden*2)\n",
        "        \n",
        "        # Take last timestep\n",
        "        last_hidden = lstm_out[:, -1]  # (batch*n_nodes, hidden*2)\n",
        "        \n",
        "        # Project to output dimension\n",
        "        output = self.output_proj(last_hidden)  # (batch*n_nodes, output_dim)\n",
        "        \n",
        "        # Reshape back: (batch, n_nodes, output_dim)\n",
        "        output = output.reshape(batch_size, n_nodes, -1)\n",
        "        \n",
        "        return output\n",
        "\n",
        "print(\"GNN-TAT Branch defined successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-6-3"
      },
      "source": [
        "### 6.3 Grid-Graph Fusion Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fusion-module"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 6.3: GRID-GRAPH FUSION MODULE (NOVEL INNOVATION)\n",
        "# =============================================================================\n",
        "\n",
        "class GridGraphFusion(nn.Module):\n",
        "    \"\"\"Novel Grid-Graph Fusion via Cross-Attention.\n",
        "\n",
        "    Bridges Euclidean (ConvLSTM grid) and Non-Euclidean (GNN graph) representations.\n",
        "    This is a key innovation in the V5 architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: V5Config, n_lat: int, n_lon: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_lat = n_lat\n",
        "        self.n_lon = n_lon\n",
        "        self.n_nodes = n_lat * n_lon\n",
        "        self._warned_large = False\n",
        "\n",
        "        hidden_dim = config.fusion_hidden_dim\n",
        "\n",
        "        # Project both branches to same dimension\n",
        "        self.grid_proj = nn.Linear(config.convlstm_output_dim, hidden_dim)\n",
        "        self.graph_proj = nn.Linear(config.gnn_output_dim, hidden_dim)\n",
        "\n",
        "        # Cross-attention: grid attends to graph\n",
        "        self.grid_to_graph_attn = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_dim,\n",
        "            num_heads=config.fusion_heads,\n",
        "            dropout=config.fusion_dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Cross-attention: graph attends to grid\n",
        "        self.graph_to_grid_attn = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_dim,\n",
        "            num_heads=config.fusion_heads,\n",
        "            dropout=config.fusion_dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Gated fusion (lighter fallback)\n",
        "        self.gate_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.fusion_dropout),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Layer norms\n",
        "        self.norm_grid = nn.LayerNorm(hidden_dim)\n",
        "        self.norm_graph = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Final fusion\n",
        "        self.fusion_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.fusion_dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "    def _select_fusion_mode(self) -> str:\n",
        "        if self.config.fusion_type == 'auto':\n",
        "            if self.n_nodes > self.config.fusion_max_nodes:\n",
        "                return 'gated'\n",
        "            return 'cross_attention'\n",
        "        return self.config.fusion_type\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        grid_features: torch.Tensor,   # From ConvLSTM: (batch, lat, lon, dim)\n",
        "        graph_features: torch.Tensor   # From GNN: (batch, n_nodes, dim)\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Fuse grid and graph representations through cross-attention.\n",
        "\n",
        "        Returns:\n",
        "            fused_grid: (batch, lat, lon, hidden_dim)\n",
        "            fused_graph: (batch, n_nodes, hidden_dim)\n",
        "        \"\"\"\n",
        "        batch_size = grid_features.shape[0]\n",
        "\n",
        "        # Flatten grid to sequence: (batch, n_nodes, dim)\n",
        "        grid_flat = grid_features.reshape(batch_size, self.n_nodes, -1)\n",
        "\n",
        "        # Project to common dimension\n",
        "        grid_proj = self.grid_proj(grid_flat)\n",
        "        graph_proj = self.graph_proj(graph_features)\n",
        "\n",
        "        mode = self._select_fusion_mode()\n",
        "        if mode == 'cross_attention':\n",
        "            if self.n_nodes > self.config.fusion_max_nodes and not self._warned_large:\n",
        "                print(\"Warning: cross-attention on large grids may OOM. Consider fusion_type='gated'.\")\n",
        "                self._warned_large = True\n",
        "\n",
        "            # Cross-attention: grid attends to graph\n",
        "            grid_attended, _ = self.grid_to_graph_attn(\n",
        "                query=grid_proj,\n",
        "                key=graph_proj,\n",
        "                value=graph_proj\n",
        "            )\n",
        "            grid_fused = self.norm_grid(grid_proj + grid_attended)\n",
        "\n",
        "            # Cross-attention: graph attends to grid\n",
        "            graph_attended, _ = self.graph_to_grid_attn(\n",
        "                query=graph_proj,\n",
        "                key=grid_proj,\n",
        "                value=grid_proj\n",
        "            )\n",
        "            graph_fused = self.norm_graph(graph_proj + graph_attended)\n",
        "        elif mode == 'gated':\n",
        "            gate_input = torch.cat([grid_proj, graph_proj], dim=-1)\n",
        "            gate = self.gate_mlp(gate_input)\n",
        "            grid_fused = self.norm_grid(gate * grid_proj)\n",
        "            graph_fused = self.norm_graph((1.0 - gate) * graph_proj)\n",
        "        else:\n",
        "            grid_fused = self.norm_grid(grid_proj)\n",
        "            graph_fused = self.norm_graph(graph_proj)\n",
        "\n",
        "        # Combine fused features\n",
        "        combined = torch.cat([grid_fused, graph_fused], dim=-1)\n",
        "        fused = self.fusion_mlp(combined)\n",
        "\n",
        "        # Reshape grid back to spatial\n",
        "        fused_grid = fused.reshape(batch_size, self.n_lat, self.n_lon, -1)\n",
        "        fused_graph = fused  # Keep as (batch, n_nodes, hidden_dim)\n",
        "\n",
        "        return fused_grid, fused_graph\n",
        "\n",
        "print(\"Grid-Graph Fusion Module defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-6-4"
      },
      "source": [
        "### 6.4 Meta-Learner (Interpretable Weighted Fusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meta-learner"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 6.4: META-LEARNER (INTERPRETABLE BRANCH WEIGHTING)\n",
        "# =============================================================================\n",
        "\n",
        "class MetaLearner(nn.Module):\n",
        "    \"\"\"Interpretable Meta-Learner for branch weighting.\n",
        "\n",
        "    Learns context-dependent weights for ConvLSTM and GNN predictions:\n",
        "        output = w1(context) * ConvLSTM + w2(context) * GNN\n",
        "\n",
        "    Enables analysis of which branch contributes more for different contexts\n",
        "    (elevation regimes, temporal patterns, etc.).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: V5Config, n_lat: int, n_lon: int, horizon: int, context_dim: int = 0):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_lat = n_lat\n",
        "        self.n_lon = n_lon\n",
        "        self.n_nodes = n_lat * n_lon\n",
        "        self.horizon = horizon\n",
        "        self.context_dim = context_dim\n",
        "\n",
        "        hidden_dim = config.fusion_hidden_dim\n",
        "        meta_dim = config.meta_hidden_dim\n",
        "\n",
        "        # Prediction heads for each branch\n",
        "        self.convlstm_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, meta_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.meta_dropout),\n",
        "            nn.Linear(meta_dim, horizon)\n",
        "        )\n",
        "\n",
        "        self.gnn_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, meta_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.meta_dropout),\n",
        "            nn.Linear(meta_dim, horizon)\n",
        "        )\n",
        "\n",
        "        # Context-dependent weight network\n",
        "        # Input: concatenated fused features (+ optional context)\n",
        "        self.weight_network = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2 + context_dim, meta_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.meta_dropout),\n",
        "            nn.Linear(meta_dim, 2),  # 2 weights: w_convlstm, w_gnn\n",
        "            nn.Softmax(dim=-1)       # Ensure weights sum to 1\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        fused_grid: torch.Tensor,    # (batch, lat, lon, hidden_dim)\n",
        "        fused_graph: torch.Tensor,   # (batch, n_nodes, hidden_dim)\n",
        "        context_features: Optional[torch.Tensor] = None  # (batch, n_nodes, context_dim)\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Generate predictions with interpretable branch weighting.\n",
        "\n",
        "        Returns:\n",
        "            predictions: (batch, horizon, lat, lon)\n",
        "            weights: (batch, n_nodes, 2) - weights for each branch per location\n",
        "        \"\"\"\n",
        "        batch_size = fused_grid.shape[0]\n",
        "\n",
        "        # Flatten grid to match graph\n",
        "        grid_flat = fused_grid.reshape(batch_size, self.n_nodes, -1)\n",
        "\n",
        "        # Generate predictions from each branch\n",
        "        pred_convlstm = self.convlstm_head(grid_flat)   # (batch, n_nodes, horizon)\n",
        "        pred_gnn = self.gnn_head(fused_graph)          # (batch, n_nodes, horizon)\n",
        "\n",
        "        # Compute context-dependent weights\n",
        "        # Context is the concatenation of both fused representations\n",
        "        if context_features is not None:\n",
        "            context = torch.cat([grid_flat, fused_graph, context_features], dim=-1)\n",
        "        else:\n",
        "            context = torch.cat([grid_flat, fused_graph], dim=-1)\n",
        "        weights = self.weight_network(context)  # (batch, n_nodes, 2)\n",
        "\n",
        "        # Weighted combination\n",
        "        w_convlstm = weights[..., 0:1]  # (batch, n_nodes, 1)\n",
        "        w_gnn = weights[..., 1:2]       # (batch, n_nodes, 1)\n",
        "\n",
        "        predictions = w_convlstm * pred_convlstm + w_gnn * pred_gnn\n",
        "\n",
        "        # Reshape to spatial grid: (batch, horizon, lat, lon)\n",
        "        predictions = predictions.reshape(batch_size, self.n_lat, self.n_lon, self.horizon)\n",
        "        predictions = predictions.permute(0, 3, 1, 2)  # (batch, horizon, lat, lon)\n",
        "\n",
        "        return predictions, weights\n",
        "\n",
        "print(\"Meta-Learner defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-6-5"
      },
      "source": [
        "### 6.5 Complete V5 Stacking Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5-model"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 6.5: COMPLETE V5 STACKING MODEL\n",
        "# =============================================================================\n",
        "\n",
        "class V5StackingModel(nn.Module):\n",
        "    \"\"\"V5 GNN-ConvLSTM Stacking Ensemble.\n",
        "\n",
        "    Novel hybrid architecture combining:\n",
        "    - Branch 1: ConvLSTM for Euclidean spatial patterns (BASIC features)\n",
        "    - Branch 2: GNN-TAT for non-Euclidean topographic relations (KCE features)\n",
        "    - Grid-Graph Fusion via cross-attention\n",
        "    - Interpretable Meta-Learner for weighted combination\n",
        "\n",
        "    Innovation: First work to combine GNN and ConvLSTM in stacking ensemble\n",
        "    for precipitation prediction (verified January 2026).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: V5Config,\n",
        "        n_features_basic: int,\n",
        "        n_features_kce: int,\n",
        "        n_lat: int,\n",
        "        n_lon: int,\n",
        "        horizon: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_lat = n_lat\n",
        "        self.n_lon = n_lon\n",
        "        self.n_nodes = n_lat * n_lon\n",
        "        self.horizon = horizon\n",
        "        self.use_context_features = config.meta_use_context_features\n",
        "\n",
        "        # Context feature indices (from KCE)\n",
        "        self.context_indices = {}\n",
        "        self.context_dim = 0\n",
        "        if self.use_context_features:\n",
        "            try:\n",
        "                kce_features = FEATURE_SETS['KCE']\n",
        "                self.context_indices = {\n",
        "                    'elevation': kce_features.index('elevation'),\n",
        "                    'elev_low': kce_features.index('elev_low'),\n",
        "                    'elev_med': kce_features.index('elev_med'),\n",
        "                    'elev_high': kce_features.index('elev_high'),\n",
        "                    'month_sin': kce_features.index('month_sin'),\n",
        "                    'month_cos': kce_features.index('month_cos')\n",
        "                }\n",
        "                # mean_elevation (1) + elevation_cluster (3) + temporal_regime (2)\n",
        "                self.context_dim = 6\n",
        "            except Exception as exc:\n",
        "                print(f\"Warning: context features unavailable ({exc}); disabling meta context.\")\n",
        "                self.use_context_features = False\n",
        "                self.context_dim = 0\n",
        "\n",
        "        # Branch 1: ConvLSTM (Euclidean)\n",
        "        self.convlstm_branch = ConvLSTMBranch(config, n_features_basic)\n",
        "\n",
        "        # Branch 2: GNN-TAT (Non-Euclidean)\n",
        "        self.gnn_branch = GNNTATBranch(config, n_features_kce, self.n_nodes)\n",
        "\n",
        "        # Grid-Graph Fusion\n",
        "        self.fusion = GridGraphFusion(config, n_lat, n_lon)\n",
        "\n",
        "        # Meta-Learner\n",
        "        self.meta_learner = MetaLearner(config, n_lat, n_lon, horizon, context_dim=self.context_dim)\n",
        "\n",
        "    def _extract_context(self, x_graph: torch.Tensor) -> Optional[torch.Tensor]:\n",
        "        \"\"\"Extract per-node context features for the meta-learner.\"\"\"\n",
        "        if not self.use_context_features:\n",
        "            return None\n",
        "\n",
        "        idx = self.context_indices\n",
        "\n",
        "        # (batch, seq, n_nodes, features)\n",
        "        elevation = x_graph[:, -1, :, idx['elevation']].unsqueeze(-1)\n",
        "        elev_clusters = x_graph[:, -1, :, [idx['elev_low'], idx['elev_med'], idx['elev_high']]]\n",
        "\n",
        "        # Temporal regime from mean seasonal encoding\n",
        "        month_sin = x_graph[:, :, :, idx['month_sin']].mean(dim=1)\n",
        "        month_cos = x_graph[:, :, :, idx['month_cos']].mean(dim=1)\n",
        "        temporal = torch.stack([month_sin, month_cos], dim=-1)\n",
        "\n",
        "        context = torch.cat([elevation, elev_clusters, temporal], dim=-1)\n",
        "        return context\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x_grid: torch.Tensor,       # (batch, seq, lat, lon, features_basic)\n",
        "        x_graph: torch.Tensor,      # (batch, seq, n_nodes, features_kce)\n",
        "        edge_index: torch.Tensor,   # (2, n_edges)\n",
        "        edge_weight: torch.Tensor   # (n_edges,)\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass through V5 stacking model.\n",
        "\n",
        "        Returns:\n",
        "            predictions: (batch, horizon, lat, lon) - precipitation predictions\n",
        "            weights: (batch, n_nodes, 2) - branch weights for interpretability\n",
        "        \"\"\"\n",
        "        # Branch 1: ConvLSTM processing\n",
        "        convlstm_out = self.convlstm_branch(x_grid)  # (batch, lat, lon, dim)\n",
        "\n",
        "        # Branch 2: GNN-TAT processing\n",
        "        gnn_out = self.gnn_branch(x_graph, edge_index, edge_weight)  # (batch, n_nodes, dim)\n",
        "\n",
        "        # Grid-Graph Fusion\n",
        "        fused_grid, fused_graph = self.fusion(convlstm_out, gnn_out)\n",
        "\n",
        "        # Meta-Learner for final predictions\n",
        "        context = self._extract_context(x_graph)\n",
        "        predictions, weights = self.meta_learner(fused_grid, fused_graph, context)\n",
        "\n",
        "        return predictions, weights\n",
        "\n",
        "    def count_parameters(self) -> Dict[str, int]:\n",
        "        \"\"\"Count parameters per component.\"\"\"\n",
        "        counts = {\n",
        "            'convlstm_branch': sum(p.numel() for p in self.convlstm_branch.parameters()),\n",
        "            'gnn_branch': sum(p.numel() for p in self.gnn_branch.parameters()),\n",
        "            'fusion': sum(p.numel() for p in self.fusion.parameters()),\n",
        "            'meta_learner': sum(p.numel() for p in self.meta_learner.parameters())\n",
        "        }\n",
        "        counts['total'] = sum(counts.values())\n",
        "        return counts\n",
        "\n",
        "# Test model instantiation\n",
        "n_lat, n_lon = (CONFIG.light_grid_size, CONFIG.light_grid_size) if CONFIG.light_mode else (ds.dims['lat'], ds.dims['lon'])\n",
        "\n",
        "model = V5StackingModel(\n",
        "    config=CONFIG,\n",
        "    n_features_basic=n_features_basic,\n",
        "    n_features_kce=n_features_kce,\n",
        "    n_lat=n_lat,\n",
        "    n_lon=n_lon,\n",
        "    horizon=CONFIG.horizon\n",
        ").to(device)\n",
        "\n",
        "# Print parameter counts\n",
        "param_counts = model.count_parameters()\n",
        "print(\"\\nV5 Model Parameter Counts:\")\n",
        "print(\"=\"*40)\n",
        "for name, count in param_counts.items():\n",
        "    print(f\"  {name}: {count:,}\")\n",
        "print(f\"\\nTotal: {param_counts['total']:,} parameters\")\n",
        "print(f\"Target: < 200,000 parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-7"
      },
      "source": [
        "## 7. Training Infrastructure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trainer"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 7: TRAINING INFRASTRUCTURE\n",
        "# =============================================================================\n",
        "\n",
        "class V5Trainer:\n",
        "    \"\"\"Training infrastructure for V5 model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: V5StackingModel,\n",
        "        config: V5Config,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        device: torch.device\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.config = config\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.device = device\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.scheduler = ReduceLROnPlateau(\n",
        "            self.optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=20,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "\n",
        "        # Training history\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'train_mae': [],\n",
        "            'val_mae': [],\n",
        "            'lr': [],\n",
        "            'branch_weights': []  # Track meta-learner weights\n",
        "        }\n",
        "\n",
        "        # Early stopping\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.patience_counter = 0\n",
        "        self.best_epoch = 0\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def train_epoch(self) -> Tuple[float, float]:\n",
        "        \"\"\"Train for one epoch.\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0.0\n",
        "        total_mae = 0.0\n",
        "        n_batches = 0\n",
        "\n",
        "        for batch in self.train_loader:\n",
        "            x_grid = batch['x_grid'].to(self.device)\n",
        "            x_graph = batch['x_graph'].to(self.device)\n",
        "            y = batch['y'].to(self.device)\n",
        "            edge_index = batch['edge_index'][0].to(self.device)\n",
        "            edge_weight = batch['edge_weight'][0].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predictions, weights = self.model(x_grid, x_graph, edge_index, edge_weight)\n",
        "\n",
        "            # Loss computation\n",
        "            loss = self.criterion(predictions, y)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                self.model.parameters(),\n",
        "                self.config.gradient_clip\n",
        "            )\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Metrics\n",
        "            total_loss += loss.item()\n",
        "            total_mae += torch.mean(torch.abs(predictions - y)).item()\n",
        "            n_batches += 1\n",
        "\n",
        "        return total_loss / n_batches, total_mae / n_batches\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self) -> Tuple[float, float, np.ndarray]:\n",
        "        \"\"\"Validate model.\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        total_mae = 0.0\n",
        "        n_batches = 0\n",
        "        all_weights = []\n",
        "\n",
        "        for batch in self.val_loader:\n",
        "            x_grid = batch['x_grid'].to(self.device)\n",
        "            x_graph = batch['x_graph'].to(self.device)\n",
        "            y = batch['y'].to(self.device)\n",
        "            edge_index = batch['edge_index'][0].to(self.device)\n",
        "            edge_weight = batch['edge_weight'][0].to(self.device)\n",
        "\n",
        "            predictions, weights = self.model(x_grid, x_graph, edge_index, edge_weight)\n",
        "\n",
        "            loss = self.criterion(predictions, y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_mae += torch.mean(torch.abs(predictions - y)).item()\n",
        "            all_weights.append(weights.cpu().numpy())\n",
        "            n_batches += 1\n",
        "\n",
        "        avg_weights = np.concatenate(all_weights, axis=0).mean(axis=(0, 1))\n",
        "\n",
        "        return total_loss / n_batches, total_mae / n_batches, avg_weights\n",
        "\n",
        "    def train(self, output_dir: Path = None) -> Dict:\n",
        "        \"\"\"Full training loop.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"Starting V5 Training\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        for epoch in range(self.config.epochs):\n",
        "            # Train\n",
        "            train_loss, train_mae = self.train_epoch()\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_mae, weights = self.validate()\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "            # Record history\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['train_mae'].append(train_mae)\n",
        "            self.history['val_mae'].append(val_mae)\n",
        "            self.history['lr'].append(current_lr)\n",
        "            self.history['branch_weights'].append(weights.tolist())\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.best_epoch = epoch\n",
        "                self.patience_counter = 0\n",
        "                self.best_model_state = copy.deepcopy(self.model.state_dict())\n",
        "\n",
        "                # Save checkpoint\n",
        "                if output_dir:\n",
        "                    checkpoint_path = output_dir / 'v5_stacking_best.pt'\n",
        "                    torch.save({\n",
        "                        'epoch': epoch,\n",
        "                        'model_state_dict': self.model.state_dict(),\n",
        "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                        'val_loss': val_loss,\n",
        "                        'config': asdict(self.config)\n",
        "                    }, checkpoint_path)\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "\n",
        "            # Logging\n",
        "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "                w_conv, w_gnn = weights\n",
        "                print(f\"Epoch {epoch+1:3d}/{self.config.epochs} | \"\n",
        "                      f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "                      f\"LR: {current_lr:.2e} | W_conv: {w_conv:.2f} W_gnn: {w_gnn:.2f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if self.patience_counter >= self.config.patience:\n",
        "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Restore best model\n",
        "        if self.best_model_state:\n",
        "            self.model.load_state_dict(self.best_model_state)\n",
        "\n",
        "        elapsed = datetime.now() - start_time\n",
        "\n",
        "        print(f\"\\nTraining completed in {elapsed}\")\n",
        "        print(f\"Best validation loss: {self.best_val_loss:.4f} at epoch {self.best_epoch+1}\")\n",
        "\n",
        "        return self.history\n",
        "\n",
        "print(\"V5 Trainer defined successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "metrics-evaluation"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 7.2: METRICS EVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(\n",
        "    model: V5StackingModel,\n",
        "    data_loader: DataLoader,\n",
        "    device: torch.device\n",
        ") -> Dict:\n",
        "    \"\"\"Evaluate model and compute metrics per horizon.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_weights = []\n",
        "    \n",
        "    for batch in data_loader:\n",
        "        x_grid = batch['x_grid'].to(device)\n",
        "        x_graph = batch['x_graph'].to(device)\n",
        "        y = batch['y'].to(device)\n",
        "        edge_index = batch['edge_index'][0].to(device)\n",
        "        edge_weight = batch['edge_weight'][0].to(device)\n",
        "        \n",
        "        predictions, weights = model(x_grid, x_graph, edge_index, edge_weight)\n",
        "        \n",
        "        all_preds.append(predictions.cpu().numpy())\n",
        "        all_targets.append(y.cpu().numpy())\n",
        "        all_weights.append(weights.cpu().numpy())\n",
        "    \n",
        "    preds = np.concatenate(all_preds, axis=0)\n",
        "    targets = np.concatenate(all_targets, axis=0)\n",
        "    weights = np.concatenate(all_weights, axis=0)\n",
        "    \n",
        "    # Compute metrics per horizon\n",
        "    metrics = {}\n",
        "    n_horizons = preds.shape[1]\n",
        "    \n",
        "    for h in range(n_horizons):\n",
        "        pred_h = preds[:, h].flatten()\n",
        "        target_h = targets[:, h].flatten()\n",
        "        \n",
        "        # Remove NaN values\n",
        "        valid_mask = ~np.isnan(pred_h) & ~np.isnan(target_h)\n",
        "        pred_h = pred_h[valid_mask]\n",
        "        target_h = target_h[valid_mask]\n",
        "        \n",
        "        if len(pred_h) == 0:\n",
        "            continue\n",
        "        \n",
        "        # RMSE\n",
        "        rmse = np.sqrt(np.mean((pred_h - target_h) ** 2))\n",
        "        \n",
        "        # MAE\n",
        "        mae = np.mean(np.abs(pred_h - target_h))\n",
        "        \n",
        "        # R\u00b2\n",
        "        ss_res = np.sum((target_h - pred_h) ** 2)\n",
        "        ss_tot = np.sum((target_h - np.mean(target_h)) ** 2)\n",
        "        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
        "        \n",
        "        # Bias\n",
        "        mean_true = np.mean(target_h)\n",
        "        mean_pred = np.mean(pred_h)\n",
        "        bias_mm = mean_pred - mean_true\n",
        "        bias_pct = 100 * bias_mm / mean_true if mean_true != 0 else 0\n",
        "        \n",
        "        metrics[f'H{h+1}'] = {\n",
        "            'RMSE': float(rmse),\n",
        "            'MAE': float(mae),\n",
        "            'R^2': float(r2),\n",
        "            'Mean_True_mm': float(mean_true),\n",
        "            'Mean_Pred_mm': float(mean_pred),\n",
        "            'mean_bias_mm': float(bias_mm),\n",
        "            'mean_bias_pct': float(bias_pct)\n",
        "        }\n",
        "    \n",
        "    # Average branch weights\n",
        "    avg_weights = weights.mean(axis=(0, 1))\n",
        "    metrics['branch_weights'] = {\n",
        "        'w_convlstm': float(avg_weights[0]),\n",
        "        'w_gnn': float(avg_weights[1])\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "print(\"Metrics evaluation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-8"
      },
      "source": [
        "## 8. Main Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main-training"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 8: MAIN TRAINING LOOP\n",
        "# =============================================================================\n",
        "\n",
        "def run_v5_experiments(\n",
        "    ds: xr.Dataset,\n",
        "    config: V5Config,\n",
        "    edge_index: torch.Tensor,\n",
        "    edge_weight: torch.Tensor,\n",
        "    output_root: Path\n",
        "):\n",
        "    \"\"\"Run V5 experiments across multiple horizons.\"\"\"\n",
        "    \n",
        "    all_metrics = []\n",
        "    experiment_state = {\n",
        "        'config': asdict(config),\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'results': {}\n",
        "    }\n",
        "    \n",
        "    for horizon in config.enabled_horizons:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"TRAINING V5 STACKING MODEL - HORIZON H{horizon}\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # Update horizon in config\n",
        "        current_config = V5Config(**{**asdict(config), 'horizon': horizon})\n",
        "        \n",
        "        # Prepare data\n",
        "        train_dataset, val_dataset, n_basic, n_kce = prepare_data(\n",
        "            ds, current_config, edge_index, edge_weight\n",
        "        )\n",
        "        \n",
        "        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "        \n",
        "        # Get grid dimensions\n",
        "        n_lat = train_dataset.n_lat\n",
        "        n_lon = train_dataset.n_lon\n",
        "        \n",
        "        # Initialize model\n",
        "        model = V5StackingModel(\n",
        "            config=current_config,\n",
        "            n_features_basic=n_basic,\n",
        "            n_features_kce=n_kce,\n",
        "            n_lat=n_lat,\n",
        "            n_lon=n_lon,\n",
        "            horizon=horizon\n",
        "        ).to(device)\n",
        "        \n",
        "        # Create output directory for this horizon\n",
        "        horizon_dir = output_root / f'h{horizon}' / 'BASIC_KCE' / 'training_metrics'\n",
        "        horizon_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Train\n",
        "        trainer = V5Trainer(model, current_config, train_loader, val_loader, device)\n",
        "        history = trainer.train(horizon_dir)\n",
        "        \n",
        "        # Evaluate\n",
        "        metrics = evaluate_model(model, val_loader, device)\n",
        "        \n",
        "        # Save training history\n",
        "        history_data = {\n",
        "            'model_name': 'V5_STACKING',\n",
        "            'experiment': 'BASIC_KCE',\n",
        "            'horizon': horizon,\n",
        "            'best_epoch': trainer.best_epoch,\n",
        "            'best_val_loss': trainer.best_val_loss,\n",
        "            'final_train_loss': history['train_loss'][-1],\n",
        "            'final_val_loss': history['val_loss'][-1],\n",
        "            'total_epochs': len(history['train_loss']),\n",
        "            'parameters': model.count_parameters()['total'],\n",
        "            'branch_weights_final': history['branch_weights'][-1] if history['branch_weights'] else [0.5, 0.5]\n",
        "        }\n",
        "        \n",
        "        with open(horizon_dir / 'v5_stacking_history.json', 'w') as f:\n",
        "            json.dump(history_data, f, indent=2)\n",
        "        \n",
        "        # Save training log CSV\n",
        "        log_df = pd.DataFrame({\n",
        "            'epoch': range(1, len(history['train_loss']) + 1),\n",
        "            'train_loss': history['train_loss'],\n",
        "            'val_loss': history['val_loss'],\n",
        "            'train_mae': history['train_mae'],\n",
        "            'val_mae': history['val_mae'],\n",
        "            'lr': history['lr']\n",
        "        })\n",
        "        log_df.to_csv(horizon_dir / f'v5_stacking_training_log_h{horizon}.csv', index=False)\n",
        "        \n",
        "        # Collect metrics for CSV\n",
        "        for h_key, h_metrics in metrics.items():\n",
        "            if h_key.startswith('H'):\n",
        "                h_num = int(h_key[1:])\n",
        "                row = {\n",
        "                    'TotalHorizon': horizon,\n",
        "                    'Experiment': 'BASIC_KCE',\n",
        "                    'Model': 'V5_STACKING',\n",
        "                    'H': h_num,\n",
        "                    **h_metrics\n",
        "                }\n",
        "                all_metrics.append(row)\n",
        "        \n",
        "        # Store results\n",
        "        experiment_state['results'][f'H{horizon}'] = {\n",
        "            'metrics': metrics,\n",
        "            'history': history_data\n",
        "        }\n",
        "        \n",
        "        # Print summary\n",
        "        print(f\"\\nH{horizon} Results Summary:\")\n",
        "        for h_key, h_metrics in metrics.items():\n",
        "            if h_key.startswith('H'):\n",
        "                print(f\"  {h_key}: RMSE={h_metrics['RMSE']:.2f}mm, \"\n",
        "                      f\"MAE={h_metrics['MAE']:.2f}mm, R\u00b2={h_metrics['R^2']:.4f}\")\n",
        "        print(f\"  Branch weights: ConvLSTM={metrics['branch_weights']['w_convlstm']:.2%}, \"\n",
        "              f\"GNN={metrics['branch_weights']['w_gnn']:.2%}\")\n",
        "        \n",
        "        # Cleanup\n",
        "        del model, trainer\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    # Save consolidated metrics CSV\n",
        "    if all_metrics:\n",
        "        metrics_df = pd.DataFrame(all_metrics)\n",
        "        metrics_df.to_csv(output_root / 'metrics_spatial_v5_all_horizons.csv', index=False)\n",
        "        print(f\"\\nConsolidated metrics saved to: {output_root / 'metrics_spatial_v5_all_horizons.csv'}\")\n",
        "    \n",
        "    # Save experiment state\n",
        "    with open(output_root / 'experiment_state_v5.json', 'w') as f:\n",
        "        json.dump(experiment_state, f, indent=2, default=str)\n",
        "    \n",
        "    return experiment_state, metrics_df if all_metrics else None\n",
        "\n",
        "print(\"Main training function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-training"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 8.2: EXECUTE TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "# Run experiments\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"#\" + \" \"*20 + \"V5 GNN-ConvLSTM STACKING\" + \" \"*20 + \"#\")\n",
        "print(\"#\"*70)\n",
        "\n",
        "if CONFIG.light_mode:\n",
        "    print(\"\\n>>> LIGHT MODE ENABLED - Using reduced grid for testing <<<\")\n",
        "    print(f\">>> Grid size: {CONFIG.light_grid_size}x{CONFIG.light_grid_size} <<<\\n\")\n",
        "\n",
        "experiment_state, metrics_df = run_v5_experiments(\n",
        "    ds=ds,\n",
        "    config=CONFIG,\n",
        "    edge_index=edge_index,\n",
        "    edge_weight=edge_weight,\n",
        "    output_root=OUTPUT_ROOT\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"V5 TRAINING COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-9"
      },
      "source": [
        "## 9. Results Export and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 9: RESULTS VISUALIZATION (700 DPI)\n",
        "# =============================================================================\n",
        "\n",
        "def plot_v5_results(metrics_df: pd.DataFrame, experiment_state: Dict, output_dir: Path):\n",
        "    \"\"\"Generate V5 results visualizations at 700 DPI.\"\"\"\n",
        "    \n",
        "    if metrics_df is None or len(metrics_df) == 0:\n",
        "        print(\"No metrics to plot\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "    \n",
        "    # 1. RMSE by Horizon\n",
        "    ax1 = axes[0, 0]\n",
        "    horizons = sorted(metrics_df['H'].unique())\n",
        "    rmse_by_h = metrics_df.groupby('H')['RMSE'].mean()\n",
        "    ax1.bar(rmse_by_h.index, rmse_by_h.values, color='steelblue', edgecolor='black')\n",
        "    ax1.set_xlabel('Prediction Horizon (months)', fontsize=12)\n",
        "    ax1.set_ylabel('RMSE (mm)', fontsize=12)\n",
        "    ax1.set_title('V5 Stacking: RMSE by Forecast Horizon', fontsize=14, fontweight='bold')\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add target line\n",
        "    ax1.axhline(y=85, color='red', linestyle='--', label='Target: 85mm')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # 2. R\u00b2 by Horizon\n",
        "    ax2 = axes[0, 1]\n",
        "    r2_by_h = metrics_df.groupby('H')['R^2'].mean()\n",
        "    ax2.bar(r2_by_h.index, r2_by_h.values, color='forestgreen', edgecolor='black')\n",
        "    ax2.set_xlabel('Prediction Horizon (months)', fontsize=12)\n",
        "    ax2.set_ylabel('R\u00b2', fontsize=12)\n",
        "    ax2.set_title('V5 Stacking: R\u00b2 by Forecast Horizon', fontsize=14, fontweight='bold')\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add target line\n",
        "    ax2.axhline(y=0.65, color='red', linestyle='--', label='Target: 0.65')\n",
        "    ax2.legend()\n",
        "    \n",
        "    # 3. Bias by Horizon\n",
        "    ax3 = axes[1, 0]\n",
        "    bias_by_h = metrics_df.groupby('H')['mean_bias_mm'].mean()\n",
        "    colors = ['red' if b < 0 else 'blue' for b in bias_by_h.values]\n",
        "    ax3.bar(bias_by_h.index, bias_by_h.values, color=colors, edgecolor='black')\n",
        "    ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax3.set_xlabel('Prediction Horizon (months)', fontsize=12)\n",
        "    ax3.set_ylabel('Mean Bias (mm)', fontsize=12)\n",
        "    ax3.set_title('V5 Stacking: Bias by Forecast Horizon', fontsize=14, fontweight='bold')\n",
        "    ax3.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # 4. Branch Weights Evolution\n",
        "    ax4 = axes[1, 1]\n",
        "    \n",
        "    # Extract branch weights from experiment state\n",
        "    horizon_labels = []\n",
        "    w_convlstm = []\n",
        "    w_gnn = []\n",
        "    \n",
        "    for h_key, results in experiment_state['results'].items():\n",
        "        if 'metrics' in results and 'branch_weights' in results['metrics']:\n",
        "            horizon_labels.append(h_key)\n",
        "            w_convlstm.append(results['metrics']['branch_weights']['w_convlstm'])\n",
        "            w_gnn.append(results['metrics']['branch_weights']['w_gnn'])\n",
        "    \n",
        "    x = np.arange(len(horizon_labels))\n",
        "    width = 0.35\n",
        "    \n",
        "    ax4.bar(x - width/2, w_convlstm, width, label='ConvLSTM', color='coral', edgecolor='black')\n",
        "    ax4.bar(x + width/2, w_gnn, width, label='GNN-TAT', color='teal', edgecolor='black')\n",
        "    \n",
        "    ax4.set_xlabel('Horizon', fontsize=12)\n",
        "    ax4.set_ylabel('Branch Weight', fontsize=12)\n",
        "    ax4.set_title('V5 Meta-Learner: Branch Weight Distribution', fontsize=14, fontweight='bold')\n",
        "    ax4.set_xticks(x)\n",
        "    ax4.set_xticklabels(horizon_labels)\n",
        "    ax4.legend()\n",
        "    ax4.grid(axis='y', alpha=0.3)\n",
        "    ax4.set_ylim(0, 1)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save at 700 DPI\n",
        "    fig_path = output_dir / 'v5_results_summary.png'\n",
        "    fig.savefig(fig_path, dpi=700, bbox_inches='tight', facecolor='white')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nVisualization saved to: {fig_path} (700 DPI)\")\n",
        "\n",
        "# Generate visualizations\n",
        "plot_v5_results(metrics_df, experiment_state, OUTPUT_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training-curves"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 9.2: TRAINING CURVES VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "def plot_training_curves(experiment_state: Dict, output_dir: Path):\n",
        "    \"\"\"Plot training curves for all horizons.\"\"\"\n",
        "    \n",
        "    n_horizons = len(experiment_state['results'])\n",
        "    if n_horizons == 0:\n",
        "        print(\"No training results to plot\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    colors = ['steelblue', 'coral', 'forestgreen', 'purple']\n",
        "    \n",
        "    for idx, (h_key, results) in enumerate(experiment_state['results'].items()):\n",
        "        if idx >= 4:\n",
        "            break\n",
        "            \n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Load training log\n",
        "        h_num = h_key.replace('H', '')\n",
        "        log_path = output_dir / f'h{h_num}' / 'BASIC_KCE' / 'training_metrics' / f'v5_stacking_training_log_h{h_num}.csv'\n",
        "        \n",
        "        if log_path.exists():\n",
        "            log_df = pd.read_csv(log_path)\n",
        "            \n",
        "            ax.plot(log_df['epoch'], log_df['train_loss'], label='Train Loss', color=colors[idx], alpha=0.8)\n",
        "            ax.plot(log_df['epoch'], log_df['val_loss'], label='Val Loss', color=colors[idx], linestyle='--', alpha=0.8)\n",
        "            \n",
        "            # Mark best epoch\n",
        "            best_epoch = results['history']['best_epoch'] + 1\n",
        "            best_val = results['history']['best_val_loss']\n",
        "            ax.scatter([best_epoch], [best_val], color='red', s=100, zorder=5, marker='*', label=f'Best: {best_val:.4f}')\n",
        "        \n",
        "        ax.set_xlabel('Epoch', fontsize=10)\n",
        "        ax.set_ylabel('Loss', fontsize=10)\n",
        "        ax.set_title(f'Training Curves - {h_key}', fontsize=12, fontweight='bold')\n",
        "        ax.legend(fontsize=8)\n",
        "        ax.grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save at 700 DPI\n",
        "    fig_path = output_dir / 'v5_training_curves.png'\n",
        "    fig.savefig(fig_path, dpi=700, bbox_inches='tight', facecolor='white')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Training curves saved to: {fig_path} (700 DPI)\")\n",
        "\n",
        "# Plot training curves\n",
        "plot_training_curves(experiment_state, OUTPUT_ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-10"
      },
      "source": [
        "## 10. Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "summary"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 10: SUMMARY AND RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"V5 GNN-ConvLSTM STACKING - EXPERIMENT SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Print configuration\n",
        "print(\"\\n[Configuration]\")\n",
        "print(f\"  Light Mode: {CONFIG.light_mode}\")\n",
        "if CONFIG.light_mode:\n",
        "    print(f\"  Grid Size: {CONFIG.light_grid_size}x{CONFIG.light_grid_size}\")\n",
        "print(f\"  GNN Type: {CONFIG.gnn_type}\")\n",
        "print(f\"  Horizons: {CONFIG.enabled_horizons}\")\n",
        "print(f\"  Epochs: {CONFIG.epochs}\")\n",
        "print(f\"  Batch Size: {CONFIG.batch_size}\")\n",
        "\n",
        "# Print results summary\n",
        "if metrics_df is not None and len(metrics_df) > 0:\n",
        "    print(\"\\n[Results Summary]\")\n",
        "    print(\"-\"*70)\n",
        "    print(metrics_df[['TotalHorizon', 'H', 'RMSE', 'MAE', 'R^2', 'mean_bias_mm']].to_string(index=False))\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    # Overall averages\n",
        "    print(\"\\n[Overall Averages]\")\n",
        "    print(f\"  RMSE: {metrics_df['RMSE'].mean():.2f} mm (Target: < 85 mm)\")\n",
        "    print(f\"  MAE: {metrics_df['MAE'].mean():.2f} mm\")\n",
        "    print(f\"  R\u00b2: {metrics_df['R^2'].mean():.4f} (Target: > 0.65)\")\n",
        "    print(f\"  Bias: {metrics_df['mean_bias_mm'].mean():.2f} mm\")\n",
        "\n",
        "# Print branch weights\n",
        "print(\"\\n[Branch Weights (Meta-Learner)]\")\n",
        "for h_key, results in experiment_state['results'].items():\n",
        "    if 'metrics' in results and 'branch_weights' in results['metrics']:\n",
        "        weights = results['metrics']['branch_weights']\n",
        "        print(f\"  {h_key}: ConvLSTM={weights['w_convlstm']:.1%}, GNN={weights['w_gnn']:.1%}\")\n",
        "\n",
        "# Output files\n",
        "print(\"\\n[Output Files]\")\n",
        "print(f\"  Output Directory: {OUTPUT_ROOT}\")\n",
        "for path in OUTPUT_ROOT.glob('*'):\n",
        "    if path.is_file():\n",
        "        print(f\"  - {path.name}\")\n",
        "\n",
        "# Next steps\n",
        "print(\"\\n[Next Steps]\")\n",
        "if CONFIG.light_mode:\n",
        "    print(\"  1. Set light_mode=False for full grid validation (61x65)\")\n",
        "    print(\"  2. Run full experiments on Colab GPU\")\n",
        "print(\"  3. Compare results with V4 baseline (R\u00b2=0.628, RMSE=92.12mm)\")\n",
        "print(\"  4. Analyze branch contributions by elevation regime\")\n",
        "print(\"  5. Run ablation studies (A1-A6 from spec.md)\")\n",
        "print(\"  6. Integrate with Paper-5 documentation\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"V5 NOTEBOOK EXECUTION COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 10.2: CLEANUP\n",
        "# =============================================================================\n",
        "\n",
        "# Memory cleanup\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"GPU Memory freed. Current usage: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
        "\n",
        "print(\"\\nNotebook execution completed successfully.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}