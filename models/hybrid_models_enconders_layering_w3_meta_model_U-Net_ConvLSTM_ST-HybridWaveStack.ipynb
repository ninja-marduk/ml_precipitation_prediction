{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "827c19f7",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/hybrid_models_enconders_layering_w3_ST-HybridWaveStack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7b96f3ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7b96f3ea",
        "outputId": "975c1075-a5e6-4368-a96e-62b0c2abe62a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▶️ Base path: /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-19 23:11:28,831 INFO ⚙ CPU cores: 10, RAM libre: 2.7 GB\n",
            "2025-05-19 23:11:28,833 INFO 📂 Cargando datasets…\n",
            "2025-05-19 23:11:30,185 INFO REF_DATE fuera de rango; usando último mes: 2025-02\n",
            "2025-05-19 23:11:30,187 INFO ▶ Procesando CEEMDAN_high\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 0.9856 - val_loss: 0.8132 - learning_rate: 0.0010\n",
            "Epoch 2/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 0.6379 - val_loss: 0.7613 - learning_rate: 0.0010\n",
            "Epoch 3/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 0.5127 - val_loss: 0.6941 - learning_rate: 0.0010\n",
            "Epoch 4/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.4712 - val_loss: 0.7173 - learning_rate: 0.0010\n",
            "Epoch 5/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.4667 - val_loss: 0.7911 - learning_rate: 0.0010\n",
            "Epoch 6/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.4334 - val_loss: 0.6176 - learning_rate: 0.0010\n",
            "Epoch 7/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 0.4242 - val_loss: 0.7683 - learning_rate: 0.0010\n",
            "Epoch 8/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 0.4400 - val_loss: 0.5735 - learning_rate: 0.0010\n",
            "Epoch 9/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.4250 - val_loss: 0.6609 - learning_rate: 0.0010\n",
            "Epoch 10/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 0.4191 - val_loss: 0.8318 - learning_rate: 0.0010\n",
            "Epoch 11/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 0.3984 - val_loss: 0.6258 - learning_rate: 0.0010\n",
            "Epoch 12/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 0.3861 - val_loss: 0.7171 - learning_rate: 0.0010\n",
            "Epoch 13/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.4186 - val_loss: 0.6305 - learning_rate: 0.0010\n",
            "Epoch 14/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 0.4044 - val_loss: 0.5917 - learning_rate: 0.0010\n",
            "Epoch 15/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.4022 - val_loss: 0.6576 - learning_rate: 0.0010\n",
            "Epoch 16/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 0.3697 - val_loss: 0.5624 - learning_rate: 0.0010\n",
            "Epoch 17/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 0.3557 - val_loss: 0.5342 - learning_rate: 0.0010\n",
            "Epoch 18/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 0.3795 - val_loss: 0.7159 - learning_rate: 0.0010\n",
            "Epoch 19/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 0.3469 - val_loss: 0.6691 - learning_rate: 0.0010\n",
            "Epoch 20/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 0.3143 - val_loss: 0.6639 - learning_rate: 0.0010\n",
            "Epoch 21/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.3035 - val_loss: 0.5448 - learning_rate: 0.0010\n",
            "Epoch 22/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.3331 - val_loss: 0.5305 - learning_rate: 0.0010\n",
            "Epoch 23/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 0.3207 - val_loss: 0.8002 - learning_rate: 0.0010\n",
            "Epoch 24/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 0.3339 - val_loss: 0.6297 - learning_rate: 0.0010\n",
            "Epoch 25/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - loss: 0.3383 - val_loss: 0.7211 - learning_rate: 0.0010\n",
            "Epoch 26/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 0.3000 - val_loss: 0.5860 - learning_rate: 0.0010\n",
            "Epoch 27/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 0.3141 - val_loss: 0.8393 - learning_rate: 0.0010\n",
            "Epoch 28/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.3122 - val_loss: 0.7502 - learning_rate: 0.0010\n",
            "Epoch 29/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.2933 - val_loss: 0.8037 - learning_rate: 0.0010\n",
            "Epoch 30/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 0.3143 - val_loss: 0.7281 - learning_rate: 0.0010\n",
            "Epoch 31/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.3184 - val_loss: 0.7466 - learning_rate: 0.0010\n",
            "Epoch 32/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.2869 - val_loss: 0.7341 - learning_rate: 0.0010\n",
            "Epoch 33/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - loss: 0.2894 - val_loss: 0.7092 - learning_rate: 5.0000e-04\n",
            "Epoch 34/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.2738 - val_loss: 0.7530 - learning_rate: 5.0000e-04\n",
            "Epoch 35/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 0.2966 - val_loss: 0.8320 - learning_rate: 5.0000e-04\n",
            "Epoch 36/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.2778 - val_loss: 0.8104 - learning_rate: 5.0000e-04\n",
            "Epoch 37/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.2822 - val_loss: 0.8006 - learning_rate: 5.0000e-04\n",
            "Epoch 38/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 0.2596 - val_loss: 0.7820 - learning_rate: 5.0000e-04\n",
            "Epoch 39/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - loss: 0.2542 - val_loss: 0.7889 - learning_rate: 5.0000e-04\n",
            "Epoch 40/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - loss: 0.2597 - val_loss: 0.7674 - learning_rate: 5.0000e-04\n",
            "Epoch 41/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.2469 - val_loss: 0.7865 - learning_rate: 5.0000e-04\n",
            "Epoch 42/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - loss: 0.2378 - val_loss: 0.7812 - learning_rate: 5.0000e-04\n",
            "Epoch 43/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 0.2554 - val_loss: 0.7778 - learning_rate: 2.5000e-04\n",
            "Epoch 44/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 0.2411 - val_loss: 0.7580 - learning_rate: 2.5000e-04\n",
            "Epoch 45/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.2432 - val_loss: 0.7596 - learning_rate: 2.5000e-04\n",
            "Epoch 46/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 0.2403 - val_loss: 0.7354 - learning_rate: 2.5000e-04\n",
            "Epoch 47/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 0.2374 - val_loss: 0.7859 - learning_rate: 2.5000e-04\n",
            "Epoch 48/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 0.2305 - val_loss: 0.7696 - learning_rate: 2.5000e-04\n",
            "Epoch 49/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.2374 - val_loss: 0.7738 - learning_rate: 2.5000e-04\n",
            "Epoch 50/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.2424 - val_loss: 0.7665 - learning_rate: 2.5000e-04\n",
            "Epoch 51/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 0.2328 - val_loss: 0.7710 - learning_rate: 2.5000e-04\n",
            "Epoch 52/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.2251 - val_loss: 0.7754 - learning_rate: 2.5000e-04\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-19 23:13:53,063 INFO ▶ Procesando CEEMDAN_medium\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - loss: 1.0133 - val_loss: 1.4045 - learning_rate: 0.0010\n",
            "Epoch 2/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.8065 - val_loss: 0.8336 - learning_rate: 0.0010\n",
            "Epoch 3/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.7130 - val_loss: 1.4256 - learning_rate: 0.0010\n",
            "Epoch 4/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.6434 - val_loss: 1.2344 - learning_rate: 0.0010\n",
            "Epoch 5/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 0.5437 - val_loss: 1.2226 - learning_rate: 0.0010\n",
            "Epoch 6/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 0.4841 - val_loss: 1.3690 - learning_rate: 0.0010\n",
            "Epoch 7/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 0.4579 - val_loss: 2.1632 - learning_rate: 0.0010\n",
            "Epoch 8/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 0.4425 - val_loss: 1.4615 - learning_rate: 0.0010\n",
            "Epoch 9/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - loss: 0.4370 - val_loss: 1.7299 - learning_rate: 0.0010\n",
            "Epoch 10/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.4148 - val_loss: 1.5099 - learning_rate: 0.0010\n",
            "Epoch 11/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 0.4113 - val_loss: 1.5278 - learning_rate: 0.0010\n",
            "Epoch 12/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 83ms/step - loss: 0.3411 - val_loss: 1.4839 - learning_rate: 0.0010\n",
            "Epoch 13/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 0.3508 - val_loss: 1.5759 - learning_rate: 5.0000e-04\n",
            "Epoch 14/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 0.3031 - val_loss: 1.3828 - learning_rate: 5.0000e-04\n",
            "Epoch 15/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - loss: 0.3072 - val_loss: 1.7580 - learning_rate: 5.0000e-04\n",
            "Epoch 16/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.3272 - val_loss: 1.5635 - learning_rate: 5.0000e-04\n",
            "Epoch 17/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 0.2760 - val_loss: 1.6816 - learning_rate: 5.0000e-04\n",
            "Epoch 18/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - loss: 0.2932 - val_loss: 1.5944 - learning_rate: 5.0000e-04\n",
            "Epoch 19/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 0.2828 - val_loss: 1.7409 - learning_rate: 5.0000e-04\n",
            "Epoch 20/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - loss: 0.2963 - val_loss: 1.8111 - learning_rate: 5.0000e-04\n",
            "Epoch 21/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.2757 - val_loss: 1.7956 - learning_rate: 5.0000e-04\n",
            "Epoch 22/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 0.2564 - val_loss: 1.7220 - learning_rate: 5.0000e-04\n",
            "Epoch 23/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.2699 - val_loss: 1.7568 - learning_rate: 2.5000e-04\n",
            "Epoch 24/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 0.2489 - val_loss: 1.7515 - learning_rate: 2.5000e-04\n",
            "Epoch 25/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 0.2435 - val_loss: 1.8118 - learning_rate: 2.5000e-04\n",
            "Epoch 26/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - loss: 0.2361 - val_loss: 1.8950 - learning_rate: 2.5000e-04\n",
            "Epoch 27/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.2469 - val_loss: 1.8849 - learning_rate: 2.5000e-04\n",
            "Epoch 28/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.2277 - val_loss: 1.8609 - learning_rate: 2.5000e-04\n",
            "Epoch 29/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - loss: 0.2456 - val_loss: 1.9112 - learning_rate: 2.5000e-04\n",
            "Epoch 30/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 0.2363 - val_loss: 1.7185 - learning_rate: 2.5000e-04\n",
            "Epoch 31/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 0.2587 - val_loss: 1.7708 - learning_rate: 2.5000e-04\n",
            "Epoch 32/300\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - loss: 0.2337 - val_loss: 1.7896 - learning_rate: 2.5000e-04\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-19 23:15:18,911 INFO ▶ Procesando CEEMDAN_low\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
            "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
            "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
            "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Entrenamiento Multi‐rama con GRU encoder–decoder y Transformer para low,\n",
        "validación y forecast parametrizables, meta‐modelo U-Net + ConvLSTM (stacking low, medium, high),\n",
        "paralelización, trazabilidad y límites del departamento de Boyacá.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "# 0) Detectar entorno (Local / Colab)\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    BASE_PATH = Path(\"/content/drive/MyDrive/ml_precipitation_prediction\")\n",
        "    !pip install -q xarray netCDF4 optuna seaborn cartopy ace_tools_open\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p/\".git\").exists():\n",
        "            BASE_PATH = p\n",
        "            break\n",
        "print(f\"▶️ Base path: {BASE_PATH}\")\n",
        "\n",
        "# 1) Suprimir warnings irrelevantes\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "from cartopy.io import DownloadWarning\n",
        "warnings.filterwarnings(\"ignore\", category=DownloadWarning)\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# 2) Parámetros configurables\n",
        "INPUT_WINDOW    = 60          # meses de entrada\n",
        "OUTPUT_HORIZON  = 3           # meses de validación y forecast\n",
        "REF_DATE        = \"2025-03\"   # fecha de referencia yyyy-mm\n",
        "MAX_EPOCHS      = 300\n",
        "PATIENCE_ES     = 30\n",
        "LR_FACTOR       = 0.5\n",
        "LR_PATIENCE     = 10\n",
        "DROPOUT         = 0.1\n",
        "BATCH_SIZE      = 16\n",
        "\n",
        "# 3) Rutas y logger\n",
        "MODEL_DIR    = BASE_PATH/\"models\"/\"output\"/\"trained_models\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FEATURES_NC  = BASE_PATH/\"models\"/\"output\"/\"features_fusion_branches.nc\"\n",
        "FULL_NC      = BASE_PATH/\"data\"/\"output\"/\"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
        "SHP_USER     = Path(\"/mnt/data/MGN_Departamento.shp\")\n",
        "BOYACA_SHP   = SHP_USER if SHP_USER.exists() else BASE_PATH/\"data\"/\"input\"/\"shapes\"/\"MGN_Departamento.shp\"\n",
        "RESULTS_CSV  = MODEL_DIR/f\"metrics_w{OUTPUT_HORIZON}_ref{REF_DATE}.csv\"\n",
        "IMAGE_DIR    = MODEL_DIR/\"images\"\n",
        "IMAGE_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 4) Imports principales\n",
        "import numpy            as np\n",
        "import pandas           as pd\n",
        "import xarray           as xr\n",
        "import geopandas        as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs      as ccrs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import psutil\n",
        "from joblib import cpu_count\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, GRU, RepeatVector, TimeDistributed, Dense,\n",
        "    MultiHeadAttention, Add, LayerNormalization, Flatten,\n",
        "    Conv2D, ConvLSTM2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "# 5) Recursos hardware\n",
        "CORES     = cpu_count()\n",
        "AVAIL_RAM = psutil.virtual_memory().available / (1024**3)\n",
        "gpus      = tf.config.list_physical_devices(\"GPU\")\n",
        "USE_GPU   = bool(gpus)\n",
        "if USE_GPU:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    logger.info(f\"🖥 GPU disponible: {gpus[0].name}\")\n",
        "else:\n",
        "    tf.config.threading.set_inter_op_parallelism_threads(CORES)\n",
        "    tf.config.threading.set_intra_op_parallelism_threads(CORES)\n",
        "    logger.info(f\"⚙ CPU cores: {CORES}, RAM libre: {AVAIL_RAM:.1f} GB\")\n",
        "\n",
        "# 6) Modelos y utilitarios\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
        "    mae  = np.mean(np.abs(y_true - y_pred))\n",
        "    mape = np.mean(np.abs((y_true - y_pred)/(y_true + 1e-5))) * 100\n",
        "    r2   = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "    return rmse, mae, mape, r2\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, Y, batch_size=BATCH_SIZE, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.X, self.Y = X.astype(np.float32), Y.astype(np.float32)\n",
        "        self.batch_size = batch_size\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / self.batch_size))\n",
        "    def __getitem__(self, idx):\n",
        "        sl = slice(idx*self.batch_size, (idx+1)*self.batch_size)\n",
        "        return self.X[sl], self.Y[sl]\n",
        "\n",
        "def build_gru_ed(input_shape, horizon, n_cells, latent=128, dropout=DROPOUT):\n",
        "    inp = Input(shape=input_shape)\n",
        "    x = GRU(latent, dropout=dropout)(inp)\n",
        "    x = RepeatVector(horizon)(x)\n",
        "    x = GRU(latent, dropout=dropout, return_sequences=True)(x)\n",
        "    out = TimeDistributed(Dense(n_cells))(x)\n",
        "    m = Model(inp, out)\n",
        "    m.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    return m\n",
        "\n",
        "def build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                         head_size=64, num_heads=4, ff_dim=256, dropout=0.1):\n",
        "    inp = Input(shape=input_shape)\n",
        "    attn = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inp, inp)\n",
        "    x = Add()([inp, attn])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    ff = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = Dense(input_shape[-1])(ff)\n",
        "    x = Add()([x, ff])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(horizon * n_cells)(x)\n",
        "    out = tf.reshape(x, (-1, horizon, n_cells))\n",
        "    m = Model(inp, out)\n",
        "    m.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    return m\n",
        "\n",
        "def build_gru_ed_low(input_shape, horizon, n_cells,\n",
        "                     latent=256, dropout=0.1, use_transformer=True):\n",
        "    if use_transformer:\n",
        "        try:\n",
        "            return build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                                        head_size=64, num_heads=4,\n",
        "                                        ff_dim=512, dropout=dropout)\n",
        "        except tf.errors.ResourceExhaustedError:\n",
        "            logger.warning(\"OOM Transformer → usando GRU‐ED para low-branch\")\n",
        "    return build_gru_ed(input_shape, horizon, n_cells,\n",
        "                        latent=latent, dropout=dropout)\n",
        "\n",
        "# NUEVO: modelo Meta U-Net + ConvLSTM para predicción espaciotemporal\n",
        "def build_unet_convlstm_meta(input_shape, output_horizon, ny, nx, channels=3):\n",
        "    \"\"\"\n",
        "    input_shape: (num_models, height, width, channels) — ej. (9, ny, nx, 1)\n",
        "    output_horizon: número de meses a predecir (e.g., 3)\n",
        "    ny, nx: dimensiones espaciales\n",
        "    channels: canales de entrada por modelo (normalmente 1)\n",
        "\n",
        "    Retorna modelo que predice secuencia (output_horizon, ny, nx, 1)\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)  # e.g. (9, ny, nx, 1)\n",
        "\n",
        "    # Fusionar los 9 mapas concatenando canales\n",
        "    # reshape para concatenar todos mapas en canales: (ny, nx, 9*channels)\n",
        "    x = tf.reshape(inputs, (-1, ny, nx, input_shape[0]*channels))\n",
        "\n",
        "    # Encoder convolucional (simples bloques conv + maxpool)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(2)(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(2)(conv2)\n",
        "\n",
        "    # Bottleneck ConvLSTM\n",
        "    # Reshape para ConvLSTM: (batch_size, time_steps=output_horizon, height, width, channels=128)\n",
        "    bottleneck = tf.reshape(pool2, (-1, output_horizon, pool2.shape[1], pool2.shape[2], 128))\n",
        "    convlstm = ConvLSTM2D(64, kernel_size=3, padding='same', return_sequences=True)(bottleneck)\n",
        "\n",
        "    # Decoder convolucional con upsampling + skip connections\n",
        "    up1 = UpSampling2D(2)(convlstm[:, -1])  # último frame de secuencia\n",
        "    concat1 = Concatenate()([up1, conv2])\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(concat1)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(128, 3, activation='relu', padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "\n",
        "    up2 = UpSampling2D(2)(conv3)\n",
        "    concat2 = Concatenate()([up2, conv1])\n",
        "    conv4 = Conv2D(64, 3, activation='relu', padding='same')(concat2)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Conv2D(64, 3, activation='relu', padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "\n",
        "    # Capa final para predicción (secuencia de meses)\n",
        "    outputs = TimeDistributed(Conv2D(1, 1, activation='linear'))(tf.expand_dims(convlstm, axis=2))\n",
        "    # outputs.shape = (batch, time_steps, 1, ny, nx, 1)\n",
        "    # Reorganizar a (batch, time_steps, ny, nx, 1)\n",
        "    outputs = tf.squeeze(outputs, axis=2)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "\n",
        "# 7) Carga datos y shapefile\n",
        "logger.info(\"📂 Cargando datasets…\")\n",
        "ds_full = xr.open_dataset(FULL_NC)\n",
        "ds_feat = xr.open_dataset(FEATURES_NC)\n",
        "boyaca_gdf = gpd.read_file(BOYACA_SHP)\n",
        "if boyaca_gdf.crs is None:\n",
        "    boyaca_gdf.set_crs(epsg=4326, inplace=True)\n",
        "else:\n",
        "    boyaca_gdf = boyaca_gdf.to_crs(epsg=4326)\n",
        "\n",
        "times      = ds_full.time.values.astype(\"datetime64[M]\")\n",
        "user_ref   = np.datetime64(REF_DATE, \"M\")\n",
        "last_avail = times[-1]\n",
        "if user_ref > last_avail:\n",
        "    ref = last_avail\n",
        "    logger.info(f\"REF_DATE fuera de rango; usando último mes: {ref}\")\n",
        "else:\n",
        "    ref = user_ref\n",
        "\n",
        "# fechas explícitas para validación y forecast\n",
        "val_dates = [\n",
        "    str(ref),\n",
        "    str((ref - np.timedelta64(1,'M')).astype(\"datetime64[M]\")),\n",
        "    str((ref - np.timedelta64(2,'M')).astype(\"datetime64[M]\"))\n",
        "]\n",
        "fc_dates  = [\n",
        "    str((ref + np.timedelta64(i+1,'M')).astype(\"datetime64[M]\"))\n",
        "    for i in range(OUTPUT_HORIZON)\n",
        "]\n",
        "\n",
        "idx_ref = int(np.where(times == ref)[0][0])\n",
        "lat     = ds_full.latitude.values\n",
        "lon     = ds_full.longitude.values\n",
        "METHODS   = [\"CEEMDAN\",\"TVFEMD\",\"FUSION\"]\n",
        "BRANCHES  = [\"high\",\"medium\",\"low\"]\n",
        "\n",
        "all_metrics = []\n",
        "preds_store = {}\n",
        "true_store  = {}\n",
        "histories   = {}\n",
        "\n",
        "# callbacks\n",
        "es_cb = callbacks.EarlyStopping(\"val_loss\", patience=PATIENCE_ES, restore_best_weights=True)\n",
        "lr_cb = callbacks.ReduceLROnPlateau(\"val_loss\", factor=LR_FACTOR, patience=LR_PATIENCE, min_lr=1e-6)\n",
        "\n",
        "# 8) Entrenamiento modelos base (low, medium, high)\n",
        "for method in METHODS:\n",
        "    for branch in BRANCHES:\n",
        "        name = f\"{method}_{branch}\"\n",
        "        if name not in ds_feat.data_vars:\n",
        "            logger.warning(f\"⚠ {name} no existe, salto.\")\n",
        "            continue\n",
        "        logger.info(f\"▶ Procesando {name}\")\n",
        "        try:\n",
        "            Xarr = ds_feat[name].values\n",
        "            Yarr = ds_full[\"total_precipitation\"].values\n",
        "            T, ny, nx = Xarr.shape\n",
        "            n_cells = ny * nx\n",
        "\n",
        "            Xfull = Xarr.reshape(T, n_cells)\n",
        "            yfull = Yarr.reshape(T, n_cells)\n",
        "\n",
        "            Nw = T - INPUT_WINDOW - OUTPUT_HORIZON + 1\n",
        "            if Nw <= 0:\n",
        "                logger.warning(\"❌ Ventanas insuficientes.\")\n",
        "                continue\n",
        "\n",
        "            Xs = np.stack([Xfull[i : i + INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "            ys = np.stack([yfull[i + INPUT_WINDOW : i + INPUT_WINDOW + OUTPUT_HORIZON] for i in range(Nw)], axis=0)\n",
        "\n",
        "            if branch == \"low\":\n",
        "                months = pd.to_datetime(ds_full.time.values).month.values\n",
        "                s = np.sin(2 * np.pi * months / 12)\n",
        "                c = np.cos(2 * np.pi * months / 12)\n",
        "                Ss = np.stack([s[i : i + INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                Cs = np.stack([c[i : i + INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                Ss = np.repeat(Ss[:, :, None], n_cells, axis=2)\n",
        "                Cs = np.repeat(Cs[:, :, None], n_cells, axis=2)\n",
        "                Xs = np.concatenate([Xs, Ss, Cs], axis=2)\n",
        "                n_feats = Xs.shape[2]\n",
        "            else:\n",
        "                n_feats = n_cells\n",
        "\n",
        "            scX = StandardScaler().fit(Xs.reshape(-1, n_feats))\n",
        "            scY = StandardScaler().fit(ys.reshape(-1, n_cells))\n",
        "            Xs_s = scX.transform(Xs.reshape(-1, n_feats)).reshape(Xs.shape)\n",
        "            ys_s = scY.transform(ys.reshape(-1, n_cells)).reshape(ys.shape)\n",
        "\n",
        "            k_ref = np.clip(idx_ref - INPUT_WINDOW + 1, 0, Nw - 1)\n",
        "            i0 = np.clip(k_ref - (OUTPUT_HORIZON - 1), 0, Nw - OUTPUT_HORIZON)\n",
        "\n",
        "            X_tr, y_tr = Xs_s[:i0], ys_s[:i0]\n",
        "            X_va, y_va = Xs_s[i0 : i0 + OUTPUT_HORIZON], ys_s[i0 : i0 + OUTPUT_HORIZON]\n",
        "\n",
        "            model_path = MODEL_DIR / f\"{name}_w{OUTPUT_HORIZON}_ref{ref}.keras\"\n",
        "            if model_path.exists():\n",
        "                model = tf.keras.models.load_model(str(model_path), compile=False)\n",
        "                model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "                logger.info(f\"⏩ Cargado modelo: {model_path.name}\")\n",
        "            else:\n",
        "                if branch == \"low\":\n",
        "                    model = build_gru_ed_low((INPUT_WINDOW, n_feats), OUTPUT_HORIZON, n_cells)\n",
        "                else:\n",
        "                    model = build_gru_ed((INPUT_WINDOW, n_feats), OUTPUT_HORIZON, n_cells)\n",
        "                hist = model.fit(\n",
        "                    DataGenerator(X_tr, y_tr),\n",
        "                    validation_data=DataGenerator(X_va, y_va),\n",
        "                    epochs=MAX_EPOCHS,\n",
        "                    callbacks=[es_cb, lr_cb],\n",
        "                    verbose=1,\n",
        "                )\n",
        "                model.save(str(model_path))\n",
        "                histories[name] = hist.history\n",
        "\n",
        "            preds = model.predict(X_va, verbose=0).reshape(OUTPUT_HORIZON, OUTPUT_HORIZON, n_cells)\n",
        "            for h in range(OUTPUT_HORIZON):\n",
        "                date_val = val_dates[h]\n",
        "                pm_flat = preds[h, 0]\n",
        "                tm_flat = y_va[h, 0]\n",
        "                pm = scY.inverse_transform(pm_flat.reshape(1, -1))[0].reshape(ny, nx)\n",
        "                tm = scY.inverse_transform(tm_flat.reshape(1, -1))[0].reshape(ny, nx)\n",
        "                rmse, mae, mape, r2 = evaluate_metrics(tm.ravel(), pm.ravel())\n",
        "                all_metrics.append(\n",
        "                    {\n",
        "                        \"model\": name,\n",
        "                        \"branch\": branch,\n",
        "                        \"horizon\": h + 1,\n",
        "                        \"type\": \"validation\",\n",
        "                        \"date\": date_val,\n",
        "                        \"RMSE\": rmse,\n",
        "                        \"MAE\": mae,\n",
        "                        \"MAPE\": mape,\n",
        "                        \"R2\": r2,\n",
        "                    }\n",
        "                )\n",
        "                preds_store[(name, date_val)] = pm\n",
        "                true_store[(name, date_val)] = tm\n",
        "\n",
        "            X_fc = Xs_s[k_ref : k_ref + 1]\n",
        "            fc_s = model.predict(X_fc, verbose=0)[0]\n",
        "            FC = scY.inverse_transform(fc_s)\n",
        "            for h in range(OUTPUT_HORIZON):\n",
        "                date_fc = fc_dates[h]\n",
        "                all_metrics.append(\n",
        "                    {\n",
        "                        \"model\": name,\n",
        "                        \"branch\": branch,\n",
        "                        \"horizon\": h + 1,\n",
        "                        \"type\": \"forecast\",\n",
        "                        \"date\": date_fc,\n",
        "                        \"RMSE\": np.nan,\n",
        "                        \"MAE\": np.nan,\n",
        "                        \"MAPE\": np.nan,\n",
        "                        \"R2\": np.nan,\n",
        "                    }\n",
        "                )\n",
        "                preds_store[(name, date_fc)] = FC[h].reshape(ny, nx)\n",
        "\n",
        "        except Exception:\n",
        "            logger.exception(f\"‼ Error en {name}, continuo…\")\n",
        "            continue\n",
        "\n",
        "# 9) Guardar métricas y mostrar tabla\n",
        "dfm = pd.DataFrame(all_metrics)\n",
        "dfm.to_csv(RESULTS_CSV, index=False)\n",
        "import ace_tools_open as tools\n",
        "\n",
        "tools.display_dataframe_to_user(name=f\"Metrics_w{OUTPUT_HORIZON}_ref{ref}\", dataframe=dfm)\n",
        "\n",
        "# 10) Curvas de entrenamiento\n",
        "for name, hist in histories.items():\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(hist[\"loss\"], label=\"train\")\n",
        "    plt.plot(hist[\"val_loss\"], label=\"val\")\n",
        "    plt.title(f\"Loss curve: {name}\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"MSE\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 10bis) True vs Predicted por rama y horizonte\n",
        "for branch in BRANCHES:\n",
        "    for h in range(1, OUTPUT_HORIZON + 1):\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        for method in METHODS:\n",
        "            key = f\"{method}_{branch}\"\n",
        "            date_val = val_dates[h - 1]\n",
        "            if (key, date_val) in preds_store and (key, date_val) in true_store:\n",
        "                y_true = true_store[(key, date_val)].ravel()\n",
        "                y_pred = preds_store[(key, date_val)].ravel()\n",
        "                plt.scatter(y_true, y_pred, alpha=0.3, s=2, label=method)\n",
        "        lims = [0, max(plt.xlim()[1], plt.ylim()[1])]\n",
        "        plt.plot(lims, lims, \"k--\")\n",
        "        plt.xlabel(\"True\")\n",
        "        plt.ylabel(\"Predicted\")\n",
        "        plt.title(f\"True vs Pred — {branch}, H={h}\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# 11) Mapas 3×3 validación H=1\n",
        "xmin, ymin, xmax, ymax = boyaca_gdf.total_bounds\n",
        "for date_val in val_dates:\n",
        "    arrs = [\n",
        "        preds_store[(f\"{m}_{b}\", date_val)].ravel()\n",
        "        for m in METHODS\n",
        "        for b in BRANCHES\n",
        "        if (f\"{m}_{b}\", date_val) in preds_store\n",
        "    ]\n",
        "    if not arrs:\n",
        "        logger.warning(f\"No hay predicciones para {date_val}, salto plot.\")\n",
        "        continue\n",
        "    vmin, vmax = np.min(arrs), np.max(arrs)\n",
        "    fig, axs = plt.subplots(3, 3, figsize=(12, 12), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
        "    fig.suptitle(f\"Validación H=1 — {date_val}\", fontsize=16)\n",
        "    for i, b in enumerate(BRANCHES):\n",
        "        for j, m in enumerate(METHODS):\n",
        "            ax = axs[i, j]\n",
        "            ax.set_extent([xmin, xmax, ymin, ymax], ccrs.PlateCarree())\n",
        "            ax.add_geometries(boyaca_gdf.geometry, ccrs.PlateCarree(), edgecolor=\"black\", facecolor=\"none\", linewidth=1)\n",
        "            key = (f\"{m}_{b}\", date_val)\n",
        "            if key in preds_store:\n",
        "                pcm = ax.pcolormesh(\n",
        "                    lon, lat, preds_store[key], vmin=vmin, vmax=vmax, transform=ccrs.PlateCarree(), cmap=\"Blues\"\n",
        "                )\n",
        "            ax.set_title(f\"{m}_{b}\")\n",
        "    fig.colorbar(pcm, ax=axs, orientation=\"horizontal\", fraction=0.05, pad=0.04, label=\"Precipitación (mm)\")\n",
        "    fig.savefig(IMAGE_DIR / f\"val_H1_{date_val}.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    arrs_mape = [\n",
        "        np.clip(np.abs((true_store[k] - preds_store[k]) / (true_store[k] + 1e-5)) * 100, 0, 200).ravel()\n",
        "        for k in preds_store\n",
        "        if k[1] == date_val and k in true_store\n",
        "    ]\n",
        "    if not arrs_mape:\n",
        "        continue\n",
        "    vmin2, vmax2 = 0, np.max(arrs_mape)\n",
        "    fig, axs = plt.subplots(3, 3, figsize=(12, 12), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
        "    fig.suptitle(f\"MAPE H=1 — {date_val}\", fontsize=16)\n",
        "    for i, b in enumerate(BRANCHES):\n",
        "        for j, m in enumerate(METHODS):\n",
        "            ax = axs[i, j]\n",
        "            ax.set_extent([xmin, xmax, ymin, ymax], ccrs.PlateCarree())\n",
        "            ax.add_geometries(boyaca_gdf.geometry, ccrs.PlateCarree(), edgecolor=\"black\", facecolor=\"none\", linewidth=1)\n",
        "            key = (f\"{m}_{b}\", date_val)\n",
        "            if key in preds_store and key in true_store:\n",
        "                mmap = np.clip(np.abs((true_store[key] - preds_store[key]) / (true_store[key] + 1e-5)) * 100, 0, 200)\n",
        "                pcm2 = ax.pcolormesh(lon, lat, mmap, vmin=vmin2, vmax=vmax2, transform=ccrs.PlateCarree(), cmap=\"Reds\")\n",
        "            ax.set_title(f\"{m}_{b}\")\n",
        "    fig.colorbar(pcm2, ax=axs, orientation=\"horizontal\", fraction=0.05, pad=0.04, label=\"MAPE (%)\")\n",
        "    fig.savefig(IMAGE_DIR / f\"mape_H1_{date_val}.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "# --- Meta-modelo U-Net + ConvLSTM ---\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_meta_dataset(preds_store, true_store, val_dates, METHODS, BRANCHES, ny, nx):\n",
        "    \"\"\"\n",
        "    Prepara X, y para meta-modelo como imágenes con shape:\n",
        "    X: (samples, num_models=9, ny, nx, 1)\n",
        "    y: (samples, output_horizon=3, ny, nx, 1)\n",
        "    \"\"\"\n",
        "    X_meta, y_meta = [], []\n",
        "    for date_val in val_dates:\n",
        "        # recoger predicciones para cada modelo (9 en total)\n",
        "        X_imgs = []\n",
        "        for branch in BRANCHES:\n",
        "            for method in METHODS:\n",
        "                key = (f\"{method}_{branch}\", date_val)\n",
        "                if key in preds_store:\n",
        "                    X_imgs.append(preds_store[key][None, ..., None])  # (1, ny, nx, 1)\n",
        "                else:\n",
        "                    logger.warning(f\"⚠ Falta {key} para meta dataset.\")\n",
        "                    break\n",
        "            else:\n",
        "                continue\n",
        "            break\n",
        "        else:\n",
        "            X_imgs_stacked = np.vstack(X_imgs)  # (9, ny, nx, 1)\n",
        "            y_img = true_store[(f\"{METHODS[0]}_{BRANCHES[0]}\", date_val)][None, ..., None]  # (1, ny, nx, 1)\n",
        "            X_meta.append(X_imgs_stacked)\n",
        "            y_meta.append(y_img)\n",
        "    if not X_meta:\n",
        "        logger.warning(\"⚠ No hay muestras para meta-modelo.\")\n",
        "        return None, None\n",
        "    return np.stack(X_meta), np.stack(y_meta)\n",
        "\n",
        "\n",
        "def build_unet_convlstm_meta(input_shape, output_horizon, ny, nx, channels=1):\n",
        "    \"\"\"\n",
        "    Modelo híbrido U-Net + ConvLSTM para meta-modelo que recibe\n",
        "    secuencia de mapas (num_models, ny, nx, channels)\n",
        "    y predice secuencia (output_horizon, ny, nx, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = Input(shape=input_shape)  # (num_models, ny, nx, channels)\n",
        "\n",
        "    # reshape para concatenar modelos en canales\n",
        "    x = tf.reshape(inputs, (-1, ny, nx, input_shape[0]*channels))\n",
        "\n",
        "    # Encoder convolucional\n",
        "    conv1 = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(2)(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(2)(conv2)\n",
        "\n",
        "    # Bottleneck ConvLSTM: reshape para secuencia temporal\n",
        "    bottleneck = tf.reshape(pool2, (-1, output_horizon, pool2.shape[1], pool2.shape[2], 128))\n",
        "    convlstm = ConvLSTM2D(64, kernel_size=3, padding=\"same\", return_sequences=True)(bottleneck)\n",
        "\n",
        "    # Decoder convolucional con upsampling y skip connections\n",
        "    up1 = UpSampling2D(2)(convlstm[:, -1])\n",
        "    concat1 = Concatenate()([up1, conv2])\n",
        "    conv3 = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(concat1)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(128, 3, activation=\"relu\", padding=\"same\")(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "\n",
        "    up2 = UpSampling2D(2)(conv3)\n",
        "    concat2 = Concatenate()([up2, conv1])\n",
        "    conv4 = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(concat2)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Conv2D(64, 3, activation=\"relu\", padding=\"same\")(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "\n",
        "    outputs = TimeDistributed(Conv2D(1, 1, activation=\"linear\"))(tf.expand_dims(convlstm, axis=2))\n",
        "    outputs = tf.squeeze(outputs, axis=2)  # (batch, time_steps, ny, nx, 1)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    return model\n",
        "\n",
        "# Preparar datos meta-modelo\n",
        "ny, nx = len(lat), len(lon)\n",
        "X_meta, y_meta = prepare_meta_dataset(preds_store, true_store, val_dates, METHODS, BRANCHES, ny, nx)\n",
        "\n",
        "if X_meta is not None and y_meta is not None:\n",
        "    # Separar train/test meta\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X_meta, y_meta, test_size=0.2, random_state=42)\n",
        "\n",
        "    meta_model = build_unet_convlstm_meta(\n",
        "        input_shape=X_meta.shape[1:], output_horizon=OUTPUT_HORIZON, ny=ny, nx=nx, channels=1\n",
        "    )\n",
        "\n",
        "    hist_meta = meta_model.fit(\n",
        "        Xtr,\n",
        "        ytr,\n",
        "        validation_data=(Xte, yte),\n",
        "        epochs=MAX_EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[es_cb, lr_cb],\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    # Guardar meta-modelo\n",
        "    meta_model_path = MODEL_DIR / f\"meta_model_unet_convlstm_{ref}.keras\"\n",
        "    meta_model.save(str(meta_model_path))\n",
        "\n",
        "    # Evaluar meta-modelo\n",
        "    yhat_meta = meta_model.predict(Xte, verbose=0)\n",
        "    yte_flat = yte.reshape(-1, ny * nx)\n",
        "    yhat_flat = yhat_meta.reshape(-1, ny * nx)\n",
        "\n",
        "    rmse_meta, mae_meta, mape_meta, r2_meta = evaluate_metrics(yte_flat, yhat_flat)\n",
        "    logger.info(f\"Meta-modelo U-Net + ConvLSTM RMSE: {rmse_meta:.3f}\")\n",
        "\n",
        "    # Gráficos True vs Predicted para meta-modelo en cada horizonte\n",
        "    for h in range(OUTPUT_HORIZON):\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        y_true_h = yte[:, h, :, :, 0].ravel()\n",
        "        y_pred_h = yhat_meta[:, h, :, :, 0].ravel()\n",
        "        plt.scatter(y_true_h, y_pred_h, alpha=0.3, s=2)\n",
        "        lims = [min(min(y_true_h), min(y_pred_h)), max(max(y_true_h), max(y_pred_h))]\n",
        "        plt.plot(lims, lims, \"k--\")\n",
        "        plt.xlabel(\"True\")\n",
        "        plt.ylabel(\"Predicted\")\n",
        "        plt.title(f\"Meta-model True vs Predicted — Horizon {h+1}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Tabla de métricas para meta-modelo por horizonte\n",
        "    meta_metrics = []\n",
        "    for h in range(OUTPUT_HORIZON):\n",
        "        y_true_h = yte[:, h, :, :, 0].ravel()\n",
        "        y_pred_h = yhat_meta[:, h, :, :, 0].ravel()\n",
        "        rmse, mae, mape, r2 = evaluate_metrics(y_true_h, y_pred_h)\n",
        "        meta_metrics.append(\n",
        "            {\"horizon\": h + 1, \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape, \"R2\": r2}\n",
        "        )\n",
        "    df_meta = pd.DataFrame(meta_metrics)\n",
        "    display(df_meta)\n",
        "    df_meta.to_csv(MODEL_DIR / f\"meta_metrics_unet_convlstm_w{OUTPUT_HORIZON}_ref{ref}.csv\", index=False)\n",
        "\n",
        "else:\n",
        "    logger.warning(\"⚠ No hay muestras para entrenar el meta-modelo.\")\n",
        "\n",
        "logger.info(\"🏁 Proceso completo.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "precipitation_prediction",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
