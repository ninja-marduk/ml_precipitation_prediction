{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/advanced_spatial_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CozjYqgoXqrJ"
      },
      "source": [
        "# ğŸš€ Mejoras Propuestas para Modelos Espaciales de PrecipitaciÃ³n\n",
        "\n",
        "## ğŸ“Š AnÃ¡lisis de Resultados Actuales\n",
        "\n",
        "### Problemas Identificados:\n",
        "\n",
        "1. **RÂ² Negativos**: Varios modelos muestran RÂ² < 0, especialmente en H=2\n",
        "   - ConvLSTM en KCE/PAFC: RÂ² = -0.042 y -0.161\n",
        "   - ConvRNN en KCE: RÂ² = -0.340\n",
        "   - ConvGRU en PAFC: RÂ² = -0.052\n",
        "\n",
        "2. **DegradaciÃ³n en H=3**: RMSE > 100 en muchos casos\n",
        "   - ConvLSTM: RMSE hasta 111.3\n",
        "   - ConvGRU: RMSE hasta 106.9\n",
        "   - ConvRNN: RMSE hasta 87.3\n",
        "\n",
        "3. **Inestabilidad**: Batch size = 4 es muy pequeÃ±o\n",
        "\n",
        "### Mejores Resultados Actuales:\n",
        "- **H=1**: ConvRNN BASIC (RMSE=43.49, RÂ²=0.767)\n",
        "- **H=2**: ConvGRU BASIC (RMSE=32.40, RÂ²=0.401)\n",
        "- **H=3**: ConvRNN KCE (RMSE=71.47, RÂ²=0.611)\n",
        "\n",
        "## ğŸ”§ Mejoras Implementadas\n",
        "\n",
        "### 1. OptimizaciÃ³n de HiperparÃ¡metros\n",
        "\n",
        "| ParÃ¡metro | Valor Original | Valor Mejorado | JustificaciÃ³n |\n",
        "|-----------|----------------|----------------|---------------|\n",
        "| Batch Size | 4 | **16** | Mayor estabilidad en gradientes |\n",
        "| Learning Rate | 1e-3 | **5e-4** | Convergencia mÃ¡s suave |\n",
        "| Epochs | 50 | **100** | MÃ¡s tiempo con early stopping |\n",
        "| Patience | 6 | **10** | Evitar detenciÃ³n prematura |\n",
        "| Dropout | 0 | **0.2** | RegularizaciÃ³n |\n",
        "| L2 Reg | 0 | **1e-5** | Prevenir overfitting |\n",
        "\n",
        "### 2. Arquitecturas Mejoradas\n",
        "\n",
        "#### ConvLSTM con AtenciÃ³n (ConvLSTM_Att)\n",
        "```python\n",
        "- 3 capas ConvLSTM (64â†’32â†’16 filtros)\n",
        "- CBAM (Channel + Spatial Attention)\n",
        "- BatchNorm + Dropout en cada capa\n",
        "- Cabeza multi-escala (1Ã—1, 3Ã—3, 5Ã—5)\n",
        "```\n",
        "\n",
        "#### ConvGRU Residual (ConvGRU_Res)\n",
        "```python\n",
        "- Skip connections desde input\n",
        "- BatchNorm mejorado\n",
        "- 2 bloques ConvGRU (64â†’32 filtros)\n",
        "- ConexiÃ³n residual final\n",
        "```\n",
        "\n",
        "#### Transformer HÃ­brido (Hybrid_Trans)\n",
        "```python\n",
        "- Encoder CNN temporal\n",
        "- Multi-head attention (4 heads)\n",
        "- LSTM para agregaciÃ³n temporal\n",
        "- Decoder espacial\n",
        "```\n",
        "\n",
        "### 3. TÃ©cnicas Avanzadas\n",
        "\n",
        "#### Learning Rate Scheduling\n",
        "- **Warmup**: 5 Ã©pocas iniciales\n",
        "- **Cosine Decay**: ReducciÃ³n suave despuÃ©s del warmup\n",
        "- **ReduceLROnPlateau**: ReducciÃ³n adicional si se estanca\n",
        "\n",
        "#### Data Augmentation\n",
        "- Ruido gaussiano (Ïƒ=0.005)\n",
        "- Preserva coherencia espacial y temporal\n",
        "\n",
        "#### RegularizaciÃ³n\n",
        "- Dropout espacial (0.2)\n",
        "- L2 en todos los pesos\n",
        "- Batch Normalization\n",
        "\n",
        "## ğŸ“ˆ Mejoras Esperadas\n",
        "\n",
        "### Por Horizonte:\n",
        "- **H=1**: RMSE < 40 (mejora ~8%)\n",
        "- **H=2**: RMSE < 30, RÂ² > 0.5 (mejora significativa)\n",
        "- **H=3**: RMSE < 65, RÂ² > 0.65 (mejora ~10%)\n",
        "\n",
        "### Por Modelo:\n",
        "1. **ConvLSTM_Att**: Mejor captura de patrones espaciales relevantes\n",
        "2. **ConvGRU_Res**: Mayor estabilidad y menos degradaciÃ³n temporal\n",
        "3. **Hybrid_Trans**: Mejor modelado de dependencias largas\n",
        "\n",
        "## ğŸš€ PrÃ³ximos Pasos\n",
        "\n",
        "### Corto Plazo:\n",
        "1. Entrenar modelos con configuraciÃ³n mejorada\n",
        "2. Validar mejoras en mÃ©tricas\n",
        "3. AnÃ¡lisis de errores por regiÃ³n\n",
        "\n",
        "### Medio Plazo:\n",
        "1. **Ensemble Methods**: Combinar mejores modelos\n",
        "2. **Multi-Task Learning**: Predecir mÃºltiples variables\n",
        "3. **Physics-Informed Loss**: Incorporar restricciones fÃ­sicas\n",
        "\n",
        "### Largo Plazo:\n",
        "1. **Modelos 3D**: ConvLSTM3D para capturar altura\n",
        "2. **Graph Neural Networks**: Para relaciones espaciales irregulares\n",
        "3. **Uncertainty Quantification**: Intervalos de confianza\n",
        "\n",
        "## ğŸ’» Uso del Script\n",
        "\n",
        "```bash\n",
        "# Entrenar modelos avanzados\n",
        "python models/train_advanced_models.py\n",
        "\n",
        "# Con GPU especÃ­fica\n",
        "CUDA_VISIBLE_DEVICES=0 python models/train_advanced_models.py\n",
        "```\n",
        "\n",
        "## ğŸ“Š Monitoreo\n",
        "\n",
        "Los resultados se guardan en:\n",
        "- `models/output/Advanced_Spatial/advanced_results.csv`\n",
        "- Historiales de entrenamiento por experimento\n",
        "- Modelos guardados en formato .keras\n",
        "\n",
        "## ğŸ” ComparaciÃ³n con Baseline\n",
        "\n",
        "El script genera automÃ¡ticamente comparaciones con los modelos originales, mostrando:\n",
        "- % de mejora en RMSE\n",
        "- EvoluciÃ³n de RÂ² por horizonte\n",
        "- Tabla resumen de mejores modelos\n",
        "\n",
        "## ğŸ“Š AnÃ¡lisis de Resultados y Mejoras Propuestas\n",
        "\n",
        "### Problemas Identificados en los Modelos Originales:\n",
        "1. **RÂ² negativos** en varios casos (especialmente H=2)\n",
        "2. **DegradaciÃ³n severa** en H=3 (RMSE >100)\n",
        "3. **Batch size muy pequeÃ±o** (4) causando inestabilidad\n",
        "4. **Arquitecturas muy simples** (solo 2 capas)\n",
        "\n",
        "### Mejoras Implementadas:\n",
        "\n",
        "#### 1. **HiperparÃ¡metros Optimizados**\n",
        "- Batch size: 4 â†’ 16 (mejor estabilidad)\n",
        "- Learning rate: 1e-3 â†’ 5e-4 (mÃ¡s conservador)\n",
        "- Epochs: 50 â†’ 100 (con early stopping)\n",
        "- RegularizaciÃ³n: Dropout (0.2) + L2 (1e-5)\n",
        "\n",
        "#### 2. **Arquitecturas Mejoradas**\n",
        "- **ConvLSTM con AtenciÃ³n**: CBAM (Channel + Spatial Attention)\n",
        "- **ConvGRU con Skip Connections**: Conexiones residuales\n",
        "- **PredRNN++**: Estado del arte para predicciÃ³n espacio-temporal\n",
        "- **ConvTransformer**: HÃ­brido CNN + Transformer\n",
        "\n",
        "#### 3. **TÃ©cnicas Avanzadas**\n",
        "- Learning rate scheduling (cosine decay con warmup)\n",
        "- Data augmentation (ruido gaussiano)\n",
        "- Multi-scale processing en la cabeza de salida\n",
        "- Batch normalization en todas las capas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiFmcqSEXqrM",
        "outputId": "6c6ed6ab-ecdc-4bde-9447-eef89d6e8a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (2025.3.1)\n",
            "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: ace_tools_open in /usr/local/lib/python3.11/dist-packages (0.1.0)\n",
            "Requirement already satisfied: cartopy in /usr/local/lib/python3.11/dist-packages (0.24.1)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from xarray) (24.2)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.2.2)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.11/dist-packages (from netCDF4) (1.6.4.post1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2025.4.26)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: itables in /usr/local/lib/python3.11/dist-packages (from ace_tools_open) (2.4.2)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (from ace_tools_open) (7.34.0)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.1.1)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.7.1)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas) (0.11.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython->ace_tools_open) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython->ace_tools_open) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->ace_tools_open) (0.2.13)\n",
            "BASE_PATH = /content/drive/MyDrive/ml_precipitation_prediction\n",
            "ğŸ“ BASE_PATH: /content/drive/MyDrive/ml_precipitation_prediction\n",
            "ğŸ“Š Dataset: complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc\n",
            "Mounted at /content/drive\n",
            "âœ… Imports completados\n"
          ]
        }
      ],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ IMPORTS Y CONFIGURACIÃ“N â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import sys, os, gc, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, ConvLSTM2D, LSTM,SimpleRNN, LSTM, GRU, Flatten, Dense, Reshape, RepeatVector,\n",
        "    Lambda, Permute, Layer, TimeDistributed, BatchNormalization, Dropout, Add,\n",
        "    Add, Multiply, Concatenate, GlobalAveragePooling2D, Activation,\n",
        "    LayerNormalization, MultiHeadAttention, MaxPooling2D, Embedding, Conv3D\n",
        ")\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,\n",
        "    CSVLogger, Callback, LearningRateScheduler\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam, AdamW\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import geopandas as gpd\n",
        "import imageio.v2 as imageio\n",
        "from IPython.display import clear_output, display, Image\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "\n",
        "\n",
        "## â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â RutasÂ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "# â–¶ï¸ Path configuration\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "print('BASE_PATH =', BASE_PATH)\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = BASE_PATH / 'data' / 'output'\n",
        "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
        "OUT_ROOT = BASE_PATH / 'models' / 'output' / 'advanced_spatial'\n",
        "OUT_ROOT.mkdir(exist_ok=True)\n",
        "BASE_MODEL_DIR = BASE_PATH / 'models' / 'output' / 'advanced_spatial' / 'base_models'\n",
        "BASE_MODEL_DIR.mkdir(exist_ok=True)\n",
        "MODEL_DIR = OUT_ROOT\n",
        "SHAPE_DIR = BASE_PATH / 'data' / 'input' / 'shapes'\n",
        "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
        "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IMAGE_DIR = MODEL_DIR/'images'\n",
        "IMAGE_DIR.mkdir(exist_ok=True)\n",
        "GIF_DIR = MODEL_DIR / \"gifs\"\n",
        "GIF_DIR.mkdir(exist_ok=True)\n",
        "# Dataset paths\n",
        "FULL_NC = DATA_DIR / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation.nc'\n",
        "FULL_NC_CLEAN = DATA_DIR / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "DEPT_GDF = gpd.read_file(SHAPE_DIR/'MGN_Departamento.shp')\n",
        "\n",
        "print(f\"ğŸ“ BASE_PATH: {BASE_PATH}\")\n",
        "print(f\"ğŸ“Š Dataset: {FULL_NC_CLEAN.name if FULL_NC_CLEAN.exists() else FULL_NC.name}\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIGURACIÃ“N MEJORADA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context('notebook')\n",
        "\n",
        "# GPU config\n",
        "for g in tf.config.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(g, True)\n",
        "\n",
        "print(\"âœ… Imports completados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMyBUMZulFhR",
        "outputId": "e41a4c34-a249-41a8-fe42-647536c8f8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŸ¢ Dataset limpio localizado â†’ complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc\n"
          ]
        }
      ],
      "source": [
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Datasets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "LAG_VARS = ['total_precipitation_lag1',\n",
        "            'total_precipitation_lag2',\n",
        "            'total_precipitation_lag12']\n",
        "\n",
        "if FULL_NC_CLEAN.exists():\n",
        "    print(f\"ğŸŸ¢ Dataset limpio localizado â†’ {FULL_NC_CLEAN.name}\")\n",
        "    ds = xr.open_dataset(FULL_NC_CLEAN)\n",
        "\n",
        "else:\n",
        "    # ============================================================\n",
        "    print(f\"ğŸŸ  Aviso: no se encontrÃ³ el dataset limpio.\\n\")\n",
        "    ds = xr.open_dataset(FULL_NC)\n",
        "    print(\"\\nğŸ“Š  Resumen global de NaNs\")\n",
        "    print(\"â”€\"*55)\n",
        "    for var in LAG_VARS:\n",
        "        arr    = ds[var].values\n",
        "        total  = arr.size\n",
        "        n_nans = int(np.isnan(arr).sum())\n",
        "        print(f\"{var:<28}: {n_nans:>8,} / {total:,}  ({n_nans/total:6.2%})\")\n",
        "\n",
        "    # ============================================================\n",
        "    print(\"\\nğŸ•’  Fechas con NaNs por variable\")\n",
        "    print(\"â”€\"*55)\n",
        "    for var in LAG_VARS:\n",
        "        arr         = ds[var].values\n",
        "        nan_per_ts  = np.isnan(arr).reshape(len(ds.time), -1).sum(axis=1)\n",
        "        if nan_per_ts.sum() == 0:\n",
        "            print(f\"{var}: sin NaNs âœ”ï¸\")\n",
        "            continue\n",
        "\n",
        "        df_nan = (pd\n",
        "                  .DataFrame({\"time\": pd.to_datetime(ds.time.values),\n",
        "                              \"na_cells\": nan_per_ts})\n",
        "                  .query(\"na_cells > 0\"))\n",
        "\n",
        "        # primeras 3 y Ãºltimas 3 fechas con NaNs\n",
        "        head = df_nan.head(3).to_string(index=False)\n",
        "        tail = df_nan.tail(3).to_string(index=False)\n",
        "        last = df_nan[\"time\"].iloc[-1].strftime(\"%Y-%m\")\n",
        "\n",
        "        print(f\"\\n{var}\")\n",
        "        print(head)\n",
        "        if len(df_nan) > 6:\n",
        "            print(\"   â€¦\")\n",
        "        print(tail)\n",
        "        print(f\"   â‡¢  Ãºltima fecha con NaNs: {last}\")\n",
        "\n",
        "    # ============================================================\n",
        "    # Primera fecha en la que las TRES variables estÃ¡n 100 % limpias\n",
        "    # ------------------------------------------------------------\n",
        "    def last_nan_index(var: str) -> int:\n",
        "        \"\"\"Ãndice del Ãºltimo timestamp que contiene al menos un NaN en `var`.\"\"\"\n",
        "        nan_per_ts = np.isnan(ds[var].values).reshape(len(ds.time), -1).sum(axis=1)\n",
        "        idxs       = np.where(nan_per_ts > 0)[0]\n",
        "        return idxs[-1] if len(idxs) else -1\n",
        "\n",
        "    last_nan_any = max(last_nan_index(v) for v in LAG_VARS)\n",
        "    first_clean  = pd.to_datetime(ds.time.values[last_nan_any + 1])\n",
        "\n",
        "    print(\"\\nPrimera fecha 100 % libre de NaNs en TODOS los lags:\",\n",
        "          first_clean.strftime(\"%Y-%m\"))\n",
        "\n",
        "    ds_clean = ds.sel(time=~(ds['time.year'] == 1981))   # descarta TODO 1981\n",
        "\n",
        "    print(\"ğŸ”  Timestamps antes :\", len(ds.time))\n",
        "    print(\"ğŸ”  Timestamps despuÃ©s:\", len(ds_clean.time))\n",
        "\n",
        "    # 3) Guarda nuevo archivo NetCDF\n",
        "    ds_clean.to_netcdf(FULL_NC_CLEAN, mode='w')\n",
        "    print(f\"ğŸ’¾  Dataset sin 1981 guardado en {FULL_NC_CLEAN}\")\n",
        "\n",
        "    # 4) (-- opcional --)  verifica que ya no queden NaNs en los lags\n",
        "    LAG_VARS = ['total_precipitation_lag1',\n",
        "                'total_precipitation_lag2',\n",
        "                'total_precipitation_lag12']\n",
        "\n",
        "    print(\"\\nğŸ“Š  NaNs restantes tras quitar 1981\")\n",
        "    print(\"â”€\"*50)\n",
        "    for var in LAG_VARS:\n",
        "        n_nan = int(np.isnan(ds_clean[var].values).sum())\n",
        "        print(f\"{var:<28}: {n_nan:,} NaNs\")\n",
        "\n",
        "    ds = ds_clean\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWbAk1lBXqrO",
        "outputId": "f8d8b2d2-5dc4-4486-a721-24537a3d22f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â–¶ï¸  [1/3] ConvLSTM-ED â€“ F1 (val=2018)\n",
            "VentanasÂ train: 382 Â· val: 10\n",
            "â©Â ConvLSTM-ED_F1 ya existe â†’Â skip\n",
            "\n",
            "â–¶ï¸  [2/3] ConvLSTM-ED-KCE â€“ F1 (val=2018)\n",
            "VentanasÂ train: 382 Â· val: 10\n"
          ]
        }
      ],
      "source": [
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Shapes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "lat, lon    = len(ds.latitude), len(ds.longitude)\n",
        "cells       = lat * lon\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hyperâ€‘parÃ¡metros globales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "INPUT_WINDOW   = 60\n",
        "HORIZON        = 3\n",
        "TARGET_VAR     = 'total_precipitation'\n",
        "EPOCHS         = 1\n",
        "BATCH_SIZE     = 16           # tamaÃ±o pequeÃ±oÂ â†’Â menor RAMÂ GPU\n",
        "PATIENCE       = 1\n",
        "LR             = 1e-3\n",
        "L2_REG         = 1e-5 # RegularizaciÃ³n L2\n",
        "DROPOUT        = 0.2 # RegularizaciÃ³n\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â Modelo base ConvLSTMÂ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def tile_step_emb(batch_ref, step_emb_tab):\n",
        "    # Forma estÃ¡tica (TensorShape / TensorSpec)\n",
        "    if isinstance(batch_ref, (tf.TensorShape, tf.TensorSpec)):\n",
        "        return tf.TensorShape([batch_ref[0],\n",
        "                               step_emb_tab.shape[0],\n",
        "                               step_emb_tab.shape[1]])\n",
        "    # EjecuciÃ³n\n",
        "    b   = tf.shape(batch_ref)[0]\n",
        "    emb = tf.expand_dims(step_emb_tab, 0)\n",
        "    return tf.tile(emb, [b, 1, 1])\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "custom = {'tile_step_emb': tile_step_emb}\n",
        "\n",
        "def _build_convlstm_ed(\n",
        "        *,\n",
        "        input_window: int,\n",
        "        output_horizon: int,\n",
        "        spatial_height: int,\n",
        "        spatial_width: int,\n",
        "        n_features: int,\n",
        "        n_filters: int = 64,\n",
        "        n_heads: int = 4,\n",
        "        use_attention: bool = True,\n",
        "        use_positional_emb: bool = True,\n",
        "        lr: float = 1e-3\n",
        "    ) -> Model:\n",
        "    \"\"\"\n",
        "    Encoder-Decoder ConvLSTM + GRU.\n",
        "    Si `use_positional_emb` = True aÃ±ade un embedding del paso de salida\n",
        "    que evita que el modelo genere la misma predicciÃ³n en todos los horizontes.\n",
        "    \"\"\"\n",
        "\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Encoder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    enc_inputs = Input(\n",
        "        shape=(input_window, spatial_height, spatial_width, n_features),\n",
        "        name=\"enc_input\"\n",
        "    )\n",
        "\n",
        "    x = ConvLSTM2D(n_filters, (3, 3), padding='same',\n",
        "                   return_sequences=True,  name=\"enc_lstm_1\")(enc_inputs)\n",
        "    x = ConvLSTM2D(n_filters // 2, (3, 3), padding='same',\n",
        "                   return_sequences=False, name=\"enc_lstm_2\")(x)\n",
        "\n",
        "    # â”€â”€ Aplana grilla y repite contexto T_out veces â”€â”€\n",
        "    flat = Flatten(name=\"flatten_spatial\")(x)                 # (B, HÂ·WÂ·C)\n",
        "    ctx  = RepeatVector(output_horizon, name=\"context\")(flat) # (B, T_out, HÂ·WÂ·C)\n",
        "\n",
        "    # â”€â”€ Positional embedding CORREGIDO â”€â”€\n",
        "    if use_positional_emb:\n",
        "        # Crear IDs de pasos como input constante\n",
        "        step_ids_input = Input(shape=(output_horizon,), dtype=tf.int32, name=\"step_ids\")\n",
        "\n",
        "        # Embedding layer\n",
        "        step_emb_layer = Embedding(output_horizon, n_filters, name=\"step_embedding\")\n",
        "        step_emb = step_emb_layer(step_ids_input)  # (B, T_out, D)\n",
        "\n",
        "        # Concatenar con contexto\n",
        "        dec_in = Concatenate(name=\"dec_concat\")([ctx, step_emb])\n",
        "\n",
        "        # Actualizar inputs del modelo\n",
        "        model_inputs = [enc_inputs, step_ids_input]\n",
        "    else:\n",
        "        dec_in = ctx\n",
        "        model_inputs = enc_inputs\n",
        "\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Decoder temporal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    dec = GRU(2 * n_filters, return_sequences=True, name=\"dec_gru\")(dec_in) # (B, T_out, 2Â·F)\n",
        "\n",
        "    # â”€â”€â”€â”€â”€â”€â”€ Attention (opcional) â”€â”€â”€â”€â”€â”€â”€\n",
        "    if use_attention:\n",
        "        attn = MultiHeadAttention(num_heads=n_heads,\n",
        "                                  key_dim=n_filters,\n",
        "                                  dropout=0.1,\n",
        "                                  name=\"mha\")(dec, dec)\n",
        "        dec  = Add(name=\"mha_residual\")([dec, attn])\n",
        "        dec  = LayerNormalization(name=\"mha_norm\")(dec)\n",
        "\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ProyecciÃ³n a grilla â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    proj = TimeDistributed(\n",
        "        Dense(spatial_height * spatial_width, activation='linear'),\n",
        "        name=\"dense_proj\"\n",
        "    )(dec)                                                    # (B, T_out, HÂ·W)\n",
        "\n",
        "    out = Reshape(\n",
        "        (output_horizon, spatial_height, spatial_width, 1),\n",
        "        name=\"reshape_out\"\n",
        "    )(proj)\n",
        "\n",
        "    name = (\"ConvLSTM_ED_Attn_PE\" if use_attention else \"ConvLSTM_ED_PE\") \\\n",
        "           if use_positional_emb else \\\n",
        "           (\"ConvLSTM_ED_Attn\"     if use_attention else \"ConvLSTM_ED\")\n",
        "\n",
        "    model = Model(model_inputs, out, name=name)\n",
        "    model.compile(optimizer=Adam(lr), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Factories ---------------------------------------------------\n",
        "\n",
        "def factory_no_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=False, **kw)\n",
        "\n",
        "def factory_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=True, **kw)\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â MÃ©tricasÂ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "\n",
        "def evaluate(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae  = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-5))) * 100\n",
        "    r2   = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, mape, r2\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Quickâ€‘plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "\n",
        "def quick_plot(ax, data, cmap, title, date_label, vmin=None, vmax=None):\n",
        "    mesh = ax.pcolormesh(ds.longitude, ds.latitude, data, cmap=cmap, shading='nearest', vmin=vmin, vmax=vmax, transform=ccrs.PlateCarree())\n",
        "    ax.coastlines(); ax.add_geometries(DEPT_GDF.geometry, ccrs.PlateCarree(), edgecolor='black', facecolor='none', linewidth=1)\n",
        "    gl = ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
        "    ax.set_title(f\"{title}\\n{date_label}\", pad=12)\n",
        "    return mesh\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Experiments &Â Folds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "# â–¸ Solo mostramos los tres primeros niveles; aÃ±ade los demÃ¡s igual\n",
        "BASE_FEATURES = [\n",
        "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
        "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
        "    'elevation','slope','aspect'\n",
        "]\n",
        "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
        "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
        "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
        "\n",
        "FOLDS = {'F1': {'year': 2018,'active': True}}\n",
        "\n",
        "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
        "    'ConvLSTM-ED': {\n",
        "        'active': True,\n",
        "        'feature_list': BASE_FEATURES,\n",
        "        'builder': factory_attn, #factory_no_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE': {\n",
        "        'active': True,\n",
        "        'feature_list': KCE_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4,\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE-PAFC': {\n",
        "        'active': True,\n",
        "        'feature_list': PAFC_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 96,\n",
        "        'n_heads'  : 6,\n",
        "    },\n",
        "}\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ventanas deslizadas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "\n",
        "def make_windows(mask:np.ndarray, allow_past_context:bool)->tuple[np.ndarray,np.ndarray]:\n",
        "    \"\"\"Genera ventanas **descartando** las que contienen NaNs.  # ğŸ”¸ NEW\"\"\"\n",
        "    seq_X, seq_y = [], []\n",
        "    lim = len(mask) - INPUT_WINDOW - HORIZON + 1\n",
        "    for start in range(lim):\n",
        "        end_w = start + INPUT_WINDOW; end_y = end_w + HORIZON\n",
        "        if allow_past_context:\n",
        "            if not mask[end_w:end_y].all():\n",
        "                continue\n",
        "        else:\n",
        "            if not mask[start:end_y].all():\n",
        "                continue\n",
        "        Xw = Xarr[start:end_w]; yw = yarr[end_w:end_y]\n",
        "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
        "            continue  # ğŸ”¸ NEWÂ â€” descarta ventana con NaNs\n",
        "        seq_X.append(Xw); seq_y.append(yw)\n",
        "    return np.array(seq_X), np.array(seq_y)\n",
        "\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Bucle principal de entrenamiento â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "RESULTS: List[Dict[str, Any]] = []\n",
        "\n",
        "# ğŸ”¸ NEW helper ------------------------------------------------\n",
        "\n",
        "def _impute_nans(a:np.ndarray, per_feature_mean:np.ndarray|None=None, is_target:bool=False)->np.ndarray:\n",
        "    \"\"\"Imputa NaNs restantes (seguridad extra).\"\"\"\n",
        "    if not np.isnan(a).any():\n",
        "        return a\n",
        "    if is_target:\n",
        "        a[np.isnan(a)] = 0.0  # ğŸ”¸ NEW â€“Â 0 para y\n",
        "        return a\n",
        "    if per_feature_mean is None:\n",
        "        raise ValueError('per_feature_mean required for imputing X')\n",
        "    flat = a.reshape(-1, a.shape[-1])\n",
        "    nan_idx = np.isnan(flat)\n",
        "    for f in range(a.shape[-1]):\n",
        "        flat[nan_idx[:,f], f] = per_feature_mean[f]  # ğŸ”¸ NEW\n",
        "    return flat.reshape(a.shape)\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "def run_all_experiments():\n",
        "    times = pd.to_datetime(ds.time.values)\n",
        "    total = sum(e['active'] for e in EXPERIMENTS.values()) * sum(f['active'] for f in FOLDS.values())\n",
        "    cnt   = 0\n",
        "\n",
        "    for exp_name, exp_cfg in EXPERIMENTS.items():\n",
        "        if not exp_cfg['active']:\n",
        "            continue\n",
        "        vars_     = exp_cfg['feature_list']\n",
        "        builder   = exp_cfg['builder']      # fÃ¡brica especÃ­fica\n",
        "        n_filters = exp_cfg.get('n_filters',64)\n",
        "        n_heads   = exp_cfg.get('n_heads',4)\n",
        "\n",
        "        # â”€ Preâ€‘load features por experimento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        global Xarr, yarr\n",
        "        Xarr = ds[vars_].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "        yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
        "        feats = Xarr.shape[-1]\n",
        "\n",
        "        for fold_name, fold_cfg in FOLDS.items():\n",
        "            if not fold_cfg['active']:\n",
        "                continue\n",
        "            cnt += 1\n",
        "            year_val = fold_cfg['year']\n",
        "            print(f\"\\nâ–¶ï¸  [{cnt}/{total}] {exp_name} â€“ {fold_name} (val={year_val})\")\n",
        "\n",
        "            mask_val = times.year == year_val\n",
        "            mask_tr  = ~mask_val\n",
        "            if mask_val.sum() < HORIZON:\n",
        "                print(\"âš ï¸Â AÃ±o sin pasos suficientes â†’Â skip\"); continue\n",
        "\n",
        "            X_tr, y_tr = make_windows(mask_tr,  allow_past_context=False)\n",
        "            X_va, y_va = make_windows(mask_val, allow_past_context=True)\n",
        "            print(f\"VentanasÂ train: {len(X_tr)} Â· val: {len(X_va)}\")\n",
        "            if len(X_tr)==0 or len(X_va)==0:\n",
        "                print(\"âš ï¸Â Sin ventanas vÃ¡lidas â†’Â skip\"); continue\n",
        "\n",
        "            # ğŸ”¸ NEWÂ â€” ImputaciÃ³n de seguridad\n",
        "            feat_mean = np.nanmean(X_tr.reshape(-1,feats),axis=0)\n",
        "            X_tr = _impute_nans(X_tr,feat_mean); X_va=_impute_nans(X_va,feat_mean)\n",
        "            y_tr = _impute_nans(y_tr,is_target=True); y_va=_impute_nans(y_va,is_target=True)\n",
        "\n",
        "            # â”€ Scaling (fit solo en train) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "            sx = StandardScaler().fit(X_tr.reshape(-1, feats))\n",
        "            sy = StandardScaler().fit(y_tr.reshape(-1, 1))\n",
        "            X_tr_sc = sx.transform(X_tr.reshape(-1, feats)).reshape(X_tr.shape)\n",
        "            X_va_sc = sx.transform(X_va.reshape(-1, feats)).reshape(X_va.shape)\n",
        "            y_tr_sc = sy.transform(y_tr.reshape(-1, 1)).reshape(y_tr.shape)[..., None]\n",
        "            y_va_sc = sy.transform(y_va.reshape(-1, 1)).reshape(y_va.shape)[..., None]\n",
        "\n",
        "            # â”€ Build & train model (factory) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "            tag        = f\"{exp_name.replace('+','_')}_{fold_name}\"\n",
        "            model_path = BASE_MODEL_DIR / f\"{tag}.keras\"\n",
        "            if model_path.exists():\n",
        "                print(f\"â©Â {tag} ya existe â†’Â skip\"); continue\n",
        "\n",
        "            model = builder(\n",
        "                input_window=INPUT_WINDOW,\n",
        "                output_horizon=HORIZON,\n",
        "                spatial_height=lat,\n",
        "                spatial_width=lon,\n",
        "                n_features=feats,\n",
        "                n_filters=n_filters,\n",
        "                n_heads=n_heads,\n",
        "                lr=LR\n",
        "            )\n",
        "\n",
        "            # Preparar step_ids para entrenamiento\n",
        "            step_ids_train = np.tile(np.arange(HORIZON), (len(X_tr_sc), 1))\n",
        "            step_ids_val = np.tile(np.arange(HORIZON), (len(X_va_sc), 1))\n",
        "\n",
        "            # Verificar si el modelo usa positional embedding\n",
        "            uses_pe = len(model.inputs) > 1\n",
        "\n",
        "            if uses_pe:\n",
        "                X_train_input = [X_tr_sc, step_ids_train]\n",
        "                X_val_input = [X_va_sc, step_ids_val]\n",
        "            else:\n",
        "                X_train_input = X_tr_sc\n",
        "                X_val_input = X_va_sc\n",
        "\n",
        "            es   = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
        "            hist = model.fit(X_train_input, y_tr_sc, validation_data=(X_val_input, y_va_sc), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=1)\n",
        "\n",
        "            # â”€ EvaluaciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "            if uses_pe:\n",
        "                y_hat_sc = model.predict([X_va_sc, step_ids_val], verbose=0)\n",
        "            else:\n",
        "                y_hat_sc = model.predict(X_va_sc, verbose=0)\n",
        "            y_hat    = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(y_hat_sc.shape)\n",
        "            y_true   = sy.inverse_transform(y_va_sc.reshape(-1,1)).reshape(y_va_sc.shape)\n",
        "\n",
        "            rmse, mae, mape, r2 = evaluate(y_true.ravel(), y_hat.ravel())\n",
        "            RESULTS.append(dict(experiment=exp_name, fold=fold_name, RMSE=rmse, MAE=mae, MAPE=mape, R2=r2, epochs=len(hist.history['loss'])))\n",
        "\n",
        "            # â”€ Guardado artefactos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "            model.save(model_path)\n",
        "            plt.figure(); plt.plot(hist.history['loss'], label='train'); plt.plot(hist.history['val_loss'], label='val'); plt.legend(); plt.title(tag); plt.savefig(IMAGE_DIR/f\"{tag}.png\"); plt.close()\n",
        "\n",
        "            # Verificar que las predicciones varÃ­an entre horizontes\n",
        "            print(f\"VerificaciÃ³n de predicciones para {tag}:\")\n",
        "            for h in range(HORIZON):\n",
        "                pred_h = y_hat[0, h, ..., 0]  # Primera muestra, horizonte h\n",
        "                print(f\"  H{h+1}: min={pred_h.min():.3f}, max={pred_h.max():.3f}, mean={pred_h.mean():.3f}, std={pred_h.std():.3f}\")\n",
        "\n",
        "            # Usar la Ãºltima ventana de validaciÃ³n para mejor visualizaciÃ³n\n",
        "            last_idx = min(len(y_hat)-1, 10)  # Usar una de las Ãºltimas ventanas\n",
        "            _generate_gif(y_true[last_idx], y_hat[last_idx], tag)\n",
        "            print(f\"âœ…Â Guardado {model_path.name}\")\n",
        "\n",
        "    # â”€ MÃ©tricas globales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    df = pd.DataFrame(RESULTS)\n",
        "    out_csv = BASE_MODEL_DIR / \"metrics_experiments_folds.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"\\nğŸ“‘Â Tabla de mÃ©tricas en {out_csv}\")\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Generador de GIF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "\n",
        "def _generate_gif(y_true_sample, y_pred_sample, tag):\n",
        "    pcm_min, pcm_max = 0, np.max(y_pred_sample)\n",
        "    frames = []\n",
        "    for h in range(HORIZON):\n",
        "        pmap = y_pred_sample[h, ..., 0]\n",
        "        fig, ax = plt.subplots(1,1, figsize=(6,5), subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "        mesh = ax.pcolormesh(ds.longitude, ds.latitude, pmap, cmap='Blues', shading='nearest', vmin=pcm_min, vmax=pcm_max, transform=ccrs.PlateCarree())\n",
        "        ax.coastlines(); ax.gridlines(draw_labels=True)\n",
        "        ax.set_title(f\"{tag} â€“ H{h+1}\")\n",
        "        fig.colorbar(mesh, ax=ax, fraction=0.046, pad=0.04)\n",
        "        tmp = GIF_DIR/f\"tmp_{tag}_h{h}.png\"\n",
        "        fig.savefig(tmp, bbox_inches='tight'); plt.close(fig)\n",
        "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
        "    gif_path = GIF_DIR/f\"{tag}.gif\"\n",
        "    imageio.mimsave(gif_path, frames, fps=0.5)\n",
        "    print(f\"ğŸ’¾Â GIF {gif_path.name} listo\")\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â Bucle principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "run_all_experiments()\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "#â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Experiments &Â Folds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "\n",
        "# Factories ---------------------------------------------------\n",
        "\n",
        "def factory_no_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=False, **kw)\n",
        "\n",
        "def factory_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=True, **kw)\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def tile_step_emb(batch_ref, step_emb_tab):\n",
        "    \"\"\"\n",
        "    Replica la tabla de embedding (T_out, D) â†’ (B, T_out, D).\n",
        "\n",
        "    Â· Durante la inferencia de forma, `batch_ref` es TensorShape\n",
        "      â†’ devolvemos TensorShape (None, T_out, D).\n",
        "    Â· En ejecuciÃ³n, `batch_ref` es tensor\n",
        "      â†’ devolvemos tensor (B, T_out, D).\n",
        "\n",
        "    â–¸ `step_emb_tab` SIEMPRE llega desde el cierre de la Lambda original,\n",
        "      asÃ­ que NO lo pongas opcional.\n",
        "    \"\"\"\n",
        "    # â€”â€”â€” 1) Forma estÃ¡tica â€”â€”â€”\n",
        "    if isinstance(batch_ref, (tf.TensorShape, tf.TensorSpec)):\n",
        "        return tf.TensorShape([batch_ref[0],\n",
        "                               step_emb_tab.shape[0],\n",
        "                               step_emb_tab.shape[1]])\n",
        "\n",
        "    # â€”â€”â€” 2) EjecuciÃ³n â€”â€”â€”\n",
        "    b   = tf.shape(batch_ref)[0]\n",
        "    emb = tf.expand_dims(step_emb_tab, 0)    # (1, T_out, D)\n",
        "    return tf.tile(emb, [b, 1, 1])           # (B, T_out, D)\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "custom = {'tile_step_emb': tile_step_emb}\n",
        "\n",
        "# â–¸ Solo mostramos los tres primeros niveles; aÃ±ade los demÃ¡s igual\n",
        "BASE_FEATURES = [\n",
        "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
        "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
        "    'elevation','slope','aspect'\n",
        "]\n",
        "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
        "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
        "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
        "\n",
        "FOLDS = {'F1': {'year': 2018,'active': True}}\n",
        "\n",
        "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
        "    'ConvLSTM-ED': {\n",
        "        'active': True,\n",
        "        'feature_list': BASE_FEATURES,\n",
        "        'builder': factory_attn, #factory_no_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE': {\n",
        "        'active': True,\n",
        "        'feature_list': KCE_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4,\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE-PAFC': {\n",
        "        'active': True,\n",
        "        'feature_list': PAFC_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 96,\n",
        "        'n_heads'  : 6,\n",
        "    },\n",
        "}\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "\n",
        "def quick_plot(ax,data,cmap,title,date_label,vmin=None,vmax=None):\n",
        "    mesh=ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
        "    ax.coastlines(); ax.add_geometries(DEPT_GDF.geometry,ccrs.PlateCarree(),edgecolor='black',facecolor='none',linewidth=1)\n",
        "    gl=ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
        "    ax.set_title(f\"{title}\\n{date_label}\",pad=10); return mesh\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Recuperamos diccionario EXPERIMENTS (del bloque de entrenamiento) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "EXPERIMENTS:Dict[str,Dict[str,Any]] = {\n",
        "    'ConvLSTM-ED':              {'feature_list': \"+\".join(BASE_FEATURES).split(\"+\")},\n",
        "    'ConvLSTM-ED-KCE':          {'feature_list': \"+\".join(KCE_FEATURES).split(\"+\")},\n",
        "    'ConvLSTM-ED-KCE-PAFC':     {'feature_list': \"+\".join(PAFC_FEATURES).split(\"+\")},\n",
        "    #Â otros experimentos\n",
        "}\n",
        "\n",
        "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” EvaluaciÃ³n â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
        "all_metrics=[]; times=pd.to_datetime(ds.time.values)\n",
        "for mpath in sorted(BASE_MODEL_DIR.glob(\"*.keras\")):\n",
        "    tag   = mpath.stem                        # p.ej. ConvLSTM-ED_F1\n",
        "    parts = tag.split(\"_\")\n",
        "    fold  = parts[-1]                         # F1\n",
        "    exp_token = \"_\".join(parts[:-1])\n",
        "    exp_name  = exp_token.replace(\"_\",\"+\")  # vuelve al nombre original con +\n",
        "    if exp_name not in EXPERIMENTS:\n",
        "        print(\"âš ï¸Â Exp no encontrado para\",tag); continue\n",
        "    feats = EXPERIMENTS[exp_name]['feature_list']\n",
        "    print(f\"\\nğŸ” Evaluando {tag} â€¦\")\n",
        "\n",
        "    # â€” ExtracciÃ³n de arrays â€”\n",
        "    Xarr = ds[feats].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "    yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
        "    T,_,_,F = Xarr.shape\n",
        "    Xfull = Xarr; yfull=yarr  # mantenemos (T,H,W,F)\n",
        "\n",
        "    # ventana final (idÃ©ntica lÃ³gica del cuaderno original)\n",
        "    start=T-INPUT_WINDOW-HORIZON; end_w=start+INPUT_WINDOW; end_y=end_w+HORIZON\n",
        "    X_eval = Xfull[start:end_w]                 # (60,H,W,F)\n",
        "    y_eval = yfull[end_w:end_y]                 # (3,H,W)\n",
        "\n",
        "    # â€” Scalers (fit vectorizado) â€”\n",
        "    flat_X = Xfull.reshape(-1, F)      # (TÂ·HÂ·W, F)\n",
        "    flat_y = yfull.reshape(-1, 1)      # (TÂ·HÂ·W, 1)\n",
        "\n",
        "    sx = StandardScaler().fit(flat_X)\n",
        "    sy = StandardScaler().fit(flat_y)\n",
        "\n",
        "    Xe_sc = sx.transform(X_eval.reshape(-1, F)).reshape(1, INPUT_WINDOW, lat, lon, F)\n",
        "    ye_sc = sy.transform(y_eval.reshape(-1, 1)).reshape(1, HORIZON, lat, lon, 1)\n",
        "\n",
        "\n",
        "    model = tf.keras.models.load_model(\n",
        "    mpath,\n",
        "    compile=False,\n",
        "    custom_objects={'tile_step_emb': tile_step_emb}\n",
        "    )\n",
        "\n",
        "    # Verificar si el modelo usa positional embedding\n",
        "    uses_pe = len(model.inputs) > 1\n",
        "\n",
        "    if uses_pe:\n",
        "        step_ids_eval = np.tile(np.arange(HORIZON), (1, 1))\n",
        "        yhat_sc = model.predict([Xe_sc, step_ids_eval], verbose=0)  # (1,3,H,W,1)\n",
        "    else:\n",
        "        yhat_sc = model.predict(Xe_sc, verbose=0)  # (1,3,H,W,1)\n",
        "    print(f\"VerificaciÃ³n de predicciones para {tag}:\")\n",
        "    for h in range(HORIZON):\n",
        "        pred_h = yhat_sc[0, h, ..., 0]  # Primera muestra, horizonte h\n",
        "        print(f\"  H{h+1}: min={pred_h.min():.3f}, max={pred_h.max():.3f}, mean={pred_h.mean():.3f}, std={pred_h.std():.3f}\")\n",
        "    # Verificar si las predicciones son idÃ©nticas\n",
        "    if HORIZON > 1:\n",
        "        diff_h1_h2 = np.abs(yhat_sc[0, 0] - yhat_sc[0, 1]).mean()\n",
        "        print(f\"  Diferencia promedio H1 vs H2: {diff_h1_h2:.6f}\")\n",
        "    yhat   = sy.inverse_transform(yhat_sc.reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
        "    ytrue  = y_eval\n",
        "\n",
        "    # â€” MÃ©tricas por horizonte â€”\n",
        "    for h in range(HORIZON):\n",
        "        yt = ytrue[h].ravel()\n",
        "        yp = yhat[h].ravel()\n",
        "\n",
        "        # ---------- filtro NaN / Â±âˆ ----------\n",
        "        mask = np.isfinite(yt) & np.isfinite(yp)\n",
        "        if mask.sum() == 0:          # ventana vacÃ­a â†’ se ignora\n",
        "            print(f\"   Â· h={h+1}: todos los valores son NaN/Inf â†’ skip\")\n",
        "            continue\n",
        "        yt, yp = yt[mask], yp[mask]\n",
        "        # -------------------------------------\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(yt, yp))\n",
        "        mae  = mean_absolute_error(yt, yp)\n",
        "        mape = np.mean(np.abs((yt - yp) / (yt + 1e-5))) * 100\n",
        "        r2   = r2_score(yt, yp)\n",
        "\n",
        "        all_metrics.append(dict(\n",
        "            model      = tag,\n",
        "            experiment = exp_name,\n",
        "            fold       = fold,\n",
        "            horizon    = h + 1,\n",
        "            RMSE       = rmse,\n",
        "            MAE        = mae,\n",
        "            MAPE       = mape,\n",
        "            R2         = r2\n",
        "        ))\n",
        "\n",
        "    # â€” Figura Real vsÂ Pred vsÂ MAPE â€”\n",
        "    fig,axes=plt.subplots(HORIZON,3,figsize=(14,4*HORIZON),subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "    dates=pd.date_range(times[end_w],periods=HORIZON,freq='MS')\n",
        "    vmin=0; vmax=max(yhat.max(),ytrue.max())\n",
        "    for h in range(HORIZON):\n",
        "        quick_plot(axes[h,0],ytrue[h],'Blues',f\"Real h={h+1}\",dates[h].strftime('%Y-%m'),vmin,vmax)\n",
        "        quick_plot(axes[h,1],yhat [h],'Blues',f\"Pred h={h+1}\",dates[h].strftime('%Y-%m'),vmin,vmax)\n",
        "        err=np.clip(np.abs((ytrue[h]-yhat[h])/(ytrue[h]+1e-5))*100,0,100)\n",
        "        quick_plot(axes[h,2],err,'Reds',f\"MAPE% h={h+1}\",dates[h].strftime('%Y-%m'),0,100)\n",
        "    fig.suptitle(f\"{tag}  â€”Â Eval final ventana\",fontsize=16); fig.tight_layout();\n",
        "    fig.savefig(BASE_MODEL_DIR/f\"fig_{tag}.png\"); plt.close(fig)\n",
        "\n",
        "    # â€” GIF â€”\n",
        "    frames=[]; pcm_min,pcm_max=0,yhat.max()\n",
        "    for h in range(HORIZON):\n",
        "        figg,ax=plt.subplots(1,1,figsize=(6,5),subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "        m=ax.pcolormesh(ds.longitude,ds.latitude,yhat[h],cmap='Blues',shading='nearest',vmin=pcm_min,vmax=pcm_max,transform=ccrs.PlateCarree())\n",
        "        ax.coastlines(); ax.set_title(f\"{tag} â€“ H{h+1}\"); figg.colorbar(m,ax=ax,fraction=0.046,pad=0.04)\n",
        "        tmp=GIF_DIR/f\"tmp_{tag}_{h}.png\"; figg.savefig(tmp,bbox_inches='tight'); plt.close(figg)\n",
        "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
        "    imageio.mimsave(GIF_DIR/f\"{tag}.gif\",frames,fps=0.5)\n",
        "    print(\"ğŸ’¾ GIF\",f\"{tag}.gif\",\"creado\")\n",
        "\n",
        "# â€”â€”â€” Guardar tabla â€”â€”â€”\n",
        "pd.DataFrame(all_metrics).to_csv(BASE_MODEL_DIR/'metrics_eval.csv',index=False)\n",
        "print(\"ğŸ“‘Â MÃ©tricas guardadas en\",BASE_MODEL_DIR/'metrics_eval.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qcvm08VIguq1"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import imageio.v2 as imageio\n",
        "\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, ConvLSTM2D, GRU, Flatten, RepeatVector, Reshape, TimeDistributed,\n",
        "    Dense, MultiHeadAttention, Add, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "## â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â RutasÂ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "# â–¶ï¸ Path configuration\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "print('BASE_PATH =', BASE_PATH)\n",
        "\n",
        "# Dataset paths\n",
        "DATA_DIR = BASE_PATH/'data'/'output'\n",
        "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
        "MODEL_DIR = BASE_PATH/'models'/'output'/'HybridLSTMModels'\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
        "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IMAGE_DIR = MODEL_DIR/'images'\n",
        "IMAGE_DIR.mkdir(exist_ok=True)\n",
        "FULL_NC = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation.nc'\n",
        "FULL_NC_CLEAN = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "dept_gdf = gpd.read_file(MODEL_INPUT_DIR/'MGN_Departamento.shp')\n",
        "\n",
        "BASE_MODEL_DIR = MODEL_DIR\n",
        "GIF_DIR        = MODEL_DIR / \"gifs\"\n",
        "GIF_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Dataset & Shapes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "ds          = xr.open_dataset(FULL_NC)\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "LAG_VARS = ['total_precipitation_lag1',\n",
        "            'total_precipitation_lag2',\n",
        "            'total_precipitation_lag12']\n",
        "\n",
        "# ============================================================\n",
        "print(\"\\nğŸ“Š  Resumen global de NaNs\")\n",
        "print(\"â”€\"*55)\n",
        "for var in LAG_VARS:\n",
        "    arr    = ds[var].values\n",
        "    total  = arr.size\n",
        "    n_nans = int(np.isnan(arr).sum())\n",
        "    print(f\"{var:<28}: {n_nans:>8,} / {total:,}  ({n_nans/total:6.2%})\")\n",
        "\n",
        "# ============================================================\n",
        "print(\"\\nğŸ•’  Fechas con NaNs por variable\")\n",
        "print(\"â”€\"*55)\n",
        "for var in LAG_VARS:\n",
        "    arr         = ds[var].values\n",
        "    nan_per_ts  = np.isnan(arr).reshape(len(ds.time), -1).sum(axis=1)\n",
        "    if nan_per_ts.sum() == 0:\n",
        "        print(f\"{var}: sin NaNs âœ”ï¸\")\n",
        "        continue\n",
        "\n",
        "    df_nan = (pd\n",
        "              .DataFrame({\"time\": pd.to_datetime(ds.time.values),\n",
        "                          \"na_cells\": nan_per_ts})\n",
        "              .query(\"na_cells > 0\"))\n",
        "\n",
        "    # primeras 3 y Ãºltimas 3 fechas con NaNs\n",
        "    head = df_nan.head(3).to_string(index=False)\n",
        "    tail = df_nan.tail(3).to_string(index=False)\n",
        "    last = df_nan[\"time\"].iloc[-1].strftime(\"%Y-%m\")\n",
        "\n",
        "    print(f\"\\n{var}\")\n",
        "    print(head)\n",
        "    if len(df_nan) > 6:\n",
        "        print(\"   â€¦\")\n",
        "    print(tail)\n",
        "    print(f\"   â‡¢  Ãºltima fecha con NaNs: {last}\")\n",
        "\n",
        "# ============================================================\n",
        "# Primera fecha en la que las TRES variables estÃ¡n 100 % limpias\n",
        "# ------------------------------------------------------------\n",
        "def last_nan_index(var: str) -> int:\n",
        "    \"\"\"Ãndice del Ãºltimo timestamp que contiene al menos un NaN en `var`.\"\"\"\n",
        "    nan_per_ts = np.isnan(ds[var].values).reshape(len(ds.time), -1).sum(axis=1)\n",
        "    idxs       = np.where(nan_per_ts > 0)[0]\n",
        "    return idxs[-1] if len(idxs) else -1\n",
        "\n",
        "last_nan_any = max(last_nan_index(v) for v in LAG_VARS)\n",
        "first_clean  = pd.to_datetime(ds.time.values[last_nan_any + 1])\n",
        "\n",
        "print(\"\\nPrimera fecha 100 % libre de NaNs en TODOS los lags:\",\n",
        "      first_clean.strftime(\"%Y-%m\"))\n",
        "\n",
        "ds_clean = ds.sel(time=~(ds['time.year'] == 1981))   # descarta TODO 1981\n",
        "\n",
        "print(\"ğŸ”  Timestamps antes :\", len(ds.time))\n",
        "print(\"ğŸ”  Timestamps despuÃ©s:\", len(ds_clean.time))\n",
        "\n",
        "# 3) Guarda nuevo archivo NetCDF\n",
        "ds_clean.to_netcdf(FULL_NC_CLEAN, mode='w')\n",
        "print(f\"ğŸ’¾  Dataset sin 1981 guardado en {FULL_NC_CLEAN}\")\n",
        "\n",
        "# 4) (-- opcional --)  verifica que ya no queden NaNs en los lags\n",
        "LAG_VARS = ['total_precipitation_lag1',\n",
        "            'total_precipitation_lag2',\n",
        "            'total_precipitation_lag12']\n",
        "\n",
        "print(\"\\nğŸ“Š  NaNs restantes tras quitar 1981\")\n",
        "print(\"â”€\"*50)\n",
        "for var in LAG_VARS:\n",
        "    n_nan = int(np.isnan(ds_clean[var].values).sum())\n",
        "    print(f\"{var:<28}: {n_nan:,} NaNs\")\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import imageio.v2 as imageio\n",
        "\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, ConvLSTM2D, GRU, Flatten, RepeatVector, Reshape,\n",
        "    TimeDistributed, Dense, MultiHeadAttention, Add,\n",
        "    LayerNormalization, Embedding, Concatenate, Lambda\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "## â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â RutasÂ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "# â–¶ï¸ Path configuration\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "from functools import partial\n",
        "print('BASE_PATH =', BASE_PATH)\n",
        "\n",
        "# Dataset paths\n",
        "DATA_DIR = BASE_PATH/'data'/'output'\n",
        "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
        "MODEL_DIR = BASE_PATH/'models'/'output'/'HybridLSTMModels'\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
        "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IMAGE_DIR = MODEL_DIR/'images'\n",
        "IMAGE_DIR.mkdir(exist_ok=True)\n",
        "FULL_NC = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "dept_gdf = gpd.read_file(MODEL_INPUT_DIR/'MGN_Departamento.shp')\n",
        "\n",
        "BASE_MODEL_DIR = MODEL_DIR\n",
        "GIF_DIR        = MODEL_DIR / \"gifs\"\n",
        "GIF_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Dataset & Shapes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "ds          = xr.open_dataset(FULL_NC)\n",
        "lat, lon    = len(ds.latitude), len(ds.longitude)\n",
        "cells       = lat * lon\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hyperâ€‘parÃ¡metros globales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "INPUT_WINDOW   = 60\n",
        "HORIZON        = 3\n",
        "TARGET_VAR     = 'total_precipitation'\n",
        "EPOCHS         = 50\n",
        "BATCH_SIZE     = 16           # tamaÃ±o pequeÃ±oÂ â†’Â menor RAMÂ GPU\n",
        "PATIENCE       = 40\n",
        "LR             = 1e-3\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â Modelo base ConvLSTMÂ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def tile_step_emb(batch_ref, step_emb_tab):\n",
        "    # Forma estÃ¡tica (TensorShape / TensorSpec)\n",
        "    if isinstance(batch_ref, (tf.TensorShape, tf.TensorSpec)):\n",
        "        return tf.TensorShape([batch_ref[0],\n",
        "                               step_emb_tab.shape[0],\n",
        "                               step_emb_tab.shape[1]])\n",
        "    # EjecuciÃ³n\n",
        "    b   = tf.shape(batch_ref)[0]\n",
        "    emb = tf.expand_dims(step_emb_tab, 0)\n",
        "    return tf.tile(emb, [b, 1, 1])\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "custom = {'tile_step_emb': tile_step_emb}\n",
        "\n",
        "def _build_convlstm_ed(\n",
        "        *,\n",
        "        input_window: int,\n",
        "        output_horizon: int,\n",
        "        spatial_height: int,\n",
        "        spatial_width: int,\n",
        "        n_features: int,\n",
        "        n_filters: int = 64,\n",
        "        n_heads: int = 4,\n",
        "        use_attention: bool = True,\n",
        "        use_positional_emb: bool = True,\n",
        "        lr: float = 1e-3\n",
        "    ) -> Model:\n",
        "    \"\"\"\n",
        "    Encoder-Decoder ConvLSTM + GRU.\n",
        "    Si `use_positional_emb` = True aÃ±ade un embedding del paso de salida\n",
        "    que evita que el modelo genere la misma predicciÃ³n en todos los horizontes.\n",
        "    \"\"\"\n",
        "\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Encoder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    enc_inputs = Input(\n",
        "        shape=(input_window, spatial_height, spatial_width, n_features),\n",
        "        name=\"enc_input\"\n",
        "    )\n",
        "\n",
        "    x = ConvLSTM2D(n_filters, (3, 3), padding='same',\n",
        "                   return_sequences=True,  name=\"enc_lstm_1\")(enc_inputs)\n",
        "    x = ConvLSTM2D(n_filters // 2, (3, 3), padding='same',\n",
        "                   return_sequences=False, name=\"enc_lstm_2\")(x)\n",
        "\n",
        "    # â”€â”€ Aplana grilla y repite contexto T_out veces â”€â”€\n",
        "    flat = Flatten(name=\"flatten_spatial\")(x)                 # (B, HÂ·WÂ·C)\n",
        "    ctx  = RepeatVector(output_horizon, name=\"context\")(flat) # (B, T_out, HÂ·WÂ·C)\n",
        "\n",
        "    # â”€â”€ Positional embedding CORREGIDO â”€â”€\n",
        "    if use_positional_emb:\n",
        "        # Crear IDs de pasos como input constante\n",
        "        step_ids_input = Input(shape=(output_horizon,), dtype=tf.int32, name=\"step_ids\")\n",
        "\n",
        "        # Embedding layer\n",
        "        step_emb_layer = Embedding(output_horizon, n_filters, name=\"step_embedding\")\n",
        "        step_emb = step_emb_layer(step_ids_input)  # (B, T_out, D)\n",
        "\n",
        "        # Concatenar con contexto\n",
        "        dec_in = Concatenate(name=\"dec_concat\")([ctx, step_emb])\n",
        "\n",
        "        # Actualizar inputs del modelo\n",
        "        model_inputs = [enc_inputs, step_ids_input]\n",
        "    else:\n",
        "        dec_in = ctx\n",
        "        model_inputs = enc_inputs\n",
        "\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Decoder temporal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    dec = GRU(2 * n_filters, return_sequences=True, name=\"dec_gru\")(dec_in) # (B, T_out, 2Â·F)\n",
        "\n",
        "    # â”€â”€â”€â”€â”€â”€â”€ Attention (opcional) â”€â”€â”€â”€â”€â”€â”€\n",
        "    if use_attention:\n",
        "        attn = MultiHeadAttention(num_heads=n_heads,\n",
        "                                  key_dim=n_filters,\n",
        "                                  dropout=0.1,\n",
        "                                  name=\"mha\")(dec, dec)\n",
        "        dec  = Add(name=\"mha_residual\")([dec, attn])\n",
        "        dec  = LayerNormalization(name=\"mha_norm\")(dec)\n",
        "\n",
        "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ProyecciÃ³n a grilla â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    proj = TimeDistributed(\n",
        "        Dense(spatial_height * spatial_width, activation='linear'),\n",
        "        name=\"dense_proj\"\n",
        "    )(dec)                                                    # (B, T_out, HÂ·W)\n",
        "\n",
        "    out = Reshape(\n",
        "        (output_horizon, spatial_height, spatial_width, 1),\n",
        "        name=\"reshape_out\"\n",
        "    )(proj)\n",
        "\n",
        "    name = (\"ConvLSTM_ED_Attn_PE\" if use_attention else \"ConvLSTM_ED_PE\") \\\n",
        "           if use_positional_emb else \\\n",
        "           (\"ConvLSTM_ED_Attn\"     if use_attention else \"ConvLSTM_ED\")\n",
        "\n",
        "    model = Model(model_inputs, out, name=name)\n",
        "    model.compile(optimizer=Adam(lr), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Factories ---------------------------------------------------\n",
        "\n",
        "def factory_no_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=False, **kw)\n",
        "\n",
        "def factory_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=True, **kw)\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â MÃ©tricasÂ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "\n",
        "def evaluate(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae  = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-5))) * 100\n",
        "    r2   = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, mape, r2\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Quickâ€‘plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "\n",
        "def quick_plot(ax, data, cmap, title, date_label, vmin=None, vmax=None):\n",
        "    mesh = ax.pcolormesh(ds.longitude, ds.latitude, data, cmap=cmap, shading='nearest', vmin=vmin, vmax=vmax, transform=ccrs.PlateCarree())\n",
        "    ax.coastlines(); ax.add_geometries(dept_gdf.geometry, ccrs.PlateCarree(), edgecolor='black', facecolor='none', linewidth=1)\n",
        "    gl = ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
        "    ax.set_title(f\"{title}\\n{date_label}\", pad=12)\n",
        "    return mesh\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Experiments &Â Folds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "# â–¸ Solo mostramos los tres primeros niveles; aÃ±ade los demÃ¡s igual\n",
        "BASE_FEATURES = [\n",
        "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
        "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
        "    'elevation','slope','aspect'\n",
        "]\n",
        "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
        "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
        "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
        "\n",
        "FOLDS = {'F1': {'year': 2018,'active': True}}\n",
        "\n",
        "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
        "    'ConvLSTM-ED': {\n",
        "        'active': True,\n",
        "        'feature_list': BASE_FEATURES,\n",
        "        'builder': factory_attn, #factory_no_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE': {\n",
        "        'active': True,\n",
        "        'feature_list': KCE_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4,\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE-PAFC': {\n",
        "        'active': True,\n",
        "        'feature_list': PAFC_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 96,\n",
        "        'n_heads'  : 6,\n",
        "    },\n",
        "}\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ventanas deslizadas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "\n",
        "def make_windows(mask:np.ndarray, allow_past_context:bool)->tuple[np.ndarray,np.ndarray]:\n",
        "    \"\"\"Genera ventanas **descartando** las que contienen NaNs.  # ğŸ”¸ NEW\"\"\"\n",
        "    seq_X, seq_y = [], []\n",
        "    lim = len(mask) - INPUT_WINDOW - HORIZON + 1\n",
        "    for start in range(lim):\n",
        "        end_w = start + INPUT_WINDOW; end_y = end_w + HORIZON\n",
        "        if allow_past_context:\n",
        "            if not mask[end_w:end_y].all():\n",
        "                continue\n",
        "        else:\n",
        "            if not mask[start:end_y].all():\n",
        "                continue\n",
        "        Xw = Xarr[start:end_w]; yw = yarr[end_w:end_y]\n",
        "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
        "            continue  # ğŸ”¸ NEWÂ â€” descarta ventana con NaNs\n",
        "        seq_X.append(Xw); seq_y.append(yw)\n",
        "    return np.array(seq_X), np.array(seq_y)\n",
        "\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Bucle principal de entrenamiento â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "RESULTS: List[Dict[str, Any]] = []\n",
        "\n",
        "# ğŸ”¸ NEW helper ------------------------------------------------\n",
        "\n",
        "def _impute_nans(a:np.ndarray, per_feature_mean:np.ndarray|None=None, is_target:bool=False)->np.ndarray:\n",
        "    \"\"\"Imputa NaNs restantes (seguridad extra).\"\"\"\n",
        "    if not np.isnan(a).any():\n",
        "        return a\n",
        "    if is_target:\n",
        "        a[np.isnan(a)] = 0.0  # ğŸ”¸ NEW â€“Â 0 para y\n",
        "        return a\n",
        "    if per_feature_mean is None:\n",
        "        raise ValueError('per_feature_mean required for imputing X')\n",
        "    flat = a.reshape(-1, a.shape[-1])\n",
        "    nan_idx = np.isnan(flat)\n",
        "    for f in range(a.shape[-1]):\n",
        "        flat[nan_idx[:,f], f] = per_feature_mean[f]  # ğŸ”¸ NEW\n",
        "    return flat.reshape(a.shape)\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "def run_all_experiments():\n",
        "    times = pd.to_datetime(ds.time.values)\n",
        "    total = sum(e['active'] for e in EXPERIMENTS.values()) * sum(f['active'] for f in FOLDS.values())\n",
        "    cnt   = 0\n",
        "\n",
        "    for exp_name, exp_cfg in EXPERIMENTS.items():\n",
        "        if not exp_cfg['active']:\n",
        "            continue\n",
        "        vars_     = exp_cfg['feature_list']\n",
        "        builder   = exp_cfg['builder']      # fÃ¡brica especÃ­fica\n",
        "        n_filters = exp_cfg.get('n_filters',64)\n",
        "        n_heads   = exp_cfg.get('n_heads',4)\n",
        "\n",
        "        # â”€ Preâ€‘load features por experimento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        global Xarr, yarr\n",
        "        Xarr = ds[vars_].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "        yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
        "        feats = Xarr.shape[-1]\n",
        "\n",
        "        for fold_name, fold_cfg in FOLDS.items():\n",
        "            if not fold_cfg['active']:\n",
        "                continue\n",
        "            cnt += 1\n",
        "            year_val = fold_cfg['year']\n",
        "            print(f\"\\nâ–¶ï¸  [{cnt}/{total}] {exp_name} â€“ {fold_name} (val={year_val})\")\n",
        "\n",
        "            mask_val = times.year == year_val\n",
        "            mask_tr  = ~mask_val\n",
        "            if mask_val.sum() < HORIZON:\n",
        "                print(\"âš ï¸Â AÃ±o sin pasos suficientes â†’Â skip\"); continue\n",
        "\n",
        "            X_tr, y_tr = make_windows(mask_tr,  allow_past_context=False)\n",
        "            X_va, y_va = make_windows(mask_val, allow_past_context=True)\n",
        "            print(f\"VentanasÂ train: {len(X_tr)} Â· val: {len(X_va)}\")\n",
        "            if len(X_tr)==0 or len(X_va)==0:\n",
        "                print(\"âš ï¸Â Sin ventanas vÃ¡lidas â†’Â skip\"); continue\n",
        "\n",
        "            # ğŸ”¸ NEWÂ â€” ImputaciÃ³n de seguridad\n",
        "            feat_mean = np.nanmean(X_tr.reshape(-1,feats),axis=0)\n",
        "            X_tr = _impute_nans(X_tr,feat_mean); X_va=_impute_nans(X_va,feat_mean)\n",
        "            y_tr = _impute_nans(y_tr,is_target=True); y_va=_impute_nans(y_va,is_target=True)\n",
        "\n",
        "            # â”€ Scaling (fit solo en train) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "            sx = StandardScaler().fit(X_tr.reshape(-1, feats))\n",
        "            sy = StandardScaler().fit(y_tr.reshape(-1, 1))\n",
        "            X_tr_sc = sx.transform(X_tr.reshape(-1, feats)).reshape(X_tr.shape)\n",
        "            X_va_sc = sx.transform(X_va.reshape(-1, feats)).reshape(X_va.shape)\n",
        "            y_tr_sc = sy.transform(y_tr.reshape(-1, 1)).reshape(y_tr.shape)[..., None]\n",
        "            y_va_sc = sy.transform(y_va.reshape(-1, 1)).reshape(y_va.shape)[..., None]\n",
        "\n",
        "            # â”€ Build & train model (factory) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "            tag        = f\"{exp_name.replace('+','_')}_{fold_name}\"\n",
        "            model_path = BASE_MODEL_DIR / f\"{tag}.keras\"\n",
        "            if model_path.exists():\n",
        "                print(f\"â©Â {tag} ya existe â†’Â skip\"); continue\n",
        "\n",
        "            model = builder(\n",
        "                input_window=INPUT_WINDOW,\n",
        "                output_horizon=HORIZON,\n",
        "                spatial_height=lat,\n",
        "                spatial_width=lon,\n",
        "                n_features=feats,\n",
        "                n_filters=n_filters,\n",
        "                n_heads=n_heads,\n",
        "                lr=LR\n",
        "            )\n",
        "\n",
        "            # Preparar step_ids para entrenamiento\n",
        "            step_ids_train = np.tile(np.arange(HORIZON), (len(X_tr_sc), 1))\n",
        "            step_ids_val = np.tile(np.arange(HORIZON), (len(X_va_sc), 1))\n",
        "\n",
        "            # Verificar si el modelo usa positional embedding\n",
        "            uses_pe = len(model.inputs) > 1\n",
        "\n",
        "            if uses_pe:\n",
        "                X_train_input = [X_tr_sc, step_ids_train]\n",
        "                X_val_input = [X_va_sc, step_ids_val]\n",
        "            else:\n",
        "                X_train_input = X_tr_sc\n",
        "                X_val_input = X_va_sc\n",
        "\n",
        "            es   = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
        "            hist = model.fit(X_train_input, y_tr_sc, validation_data=(X_val_input, y_va_sc), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=1)\n",
        "\n",
        "            # â”€ EvaluaciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "            if uses_pe:\n",
        "                y_hat_sc = model.predict([X_va_sc, step_ids_val], verbose=0)\n",
        "            else:\n",
        "                y_hat_sc = model.predict(X_va_sc, verbose=0)\n",
        "            y_hat    = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(y_hat_sc.shape)\n",
        "            y_true   = sy.inverse_transform(y_va_sc.reshape(-1,1)).reshape(y_va_sc.shape)\n",
        "\n",
        "            rmse, mae, mape, r2 = evaluate(y_true.ravel(), y_hat.ravel())\n",
        "            RESULTS.append(dict(experiment=exp_name, fold=fold_name, RMSE=rmse, MAE=mae, MAPE=mape, R2=r2, epochs=len(hist.history['loss'])))\n",
        "\n",
        "            # â”€ Guardado artefactos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "            model.save(model_path)\n",
        "            plt.figure(); plt.plot(hist.history['loss'], label='train'); plt.plot(hist.history['val_loss'], label='val'); plt.legend(); plt.title(tag); plt.savefig(IMAGE_DIR/f\"{tag}.png\"); plt.close()\n",
        "\n",
        "            # Verificar que las predicciones varÃ­an entre horizontes\n",
        "            print(f\"VerificaciÃ³n de predicciones para {tag}:\")\n",
        "            for h in range(HORIZON):\n",
        "                pred_h = y_hat[0, h, ..., 0]  # Primera muestra, horizonte h\n",
        "                print(f\"  H{h+1}: min={pred_h.min():.3f}, max={pred_h.max():.3f}, mean={pred_h.mean():.3f}, std={pred_h.std():.3f}\")\n",
        "\n",
        "            # Usar la Ãºltima ventana de validaciÃ³n para mejor visualizaciÃ³n\n",
        "            last_idx = min(len(y_hat)-1, 10)  # Usar una de las Ãºltimas ventanas\n",
        "            _generate_gif(y_true[last_idx], y_hat[last_idx], tag)\n",
        "            print(f\"âœ…Â Guardado {model_path.name}\")\n",
        "\n",
        "    # â”€ MÃ©tricas globales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    df = pd.DataFrame(RESULTS)\n",
        "    out_csv = BASE_MODEL_DIR / \"metrics_experiments_folds.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"\\nğŸ“‘Â Tabla de mÃ©tricas en {out_csv}\")\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Generador de GIF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "\n",
        "def _generate_gif(y_true_sample, y_pred_sample, tag):\n",
        "    pcm_min, pcm_max = 0, np.max(y_pred_sample)\n",
        "    frames = []\n",
        "    for h in range(HORIZON):\n",
        "        pmap = y_pred_sample[h, ..., 0]\n",
        "        fig, ax = plt.subplots(1,1, figsize=(6,5), subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "        mesh = ax.pcolormesh(ds.longitude, ds.latitude, pmap, cmap='Blues', shading='nearest', vmin=pcm_min, vmax=pcm_max, transform=ccrs.PlateCarree())\n",
        "        ax.coastlines(); ax.gridlines(draw_labels=True)\n",
        "        ax.set_title(f\"{tag} â€“ H{h+1}\")\n",
        "        fig.colorbar(mesh, ax=ax, fraction=0.046, pad=0.04)\n",
        "        tmp = GIF_DIR/f\"tmp_{tag}_h{h}.png\"\n",
        "        fig.savefig(tmp, bbox_inches='tight'); plt.close(fig)\n",
        "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
        "    gif_path = GIF_DIR/f\"{tag}.gif\"\n",
        "    imageio.mimsave(gif_path, frames, fps=0.5)\n",
        "    print(f\"ğŸ’¾Â GIF {gif_path.name} listo\")\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â Bucle principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "run_all_experiments()\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "\n",
        "# ğŸ“ˆ **Evaluador para salidas espaciales ConvLSTM**\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, xarray as xr, tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt, geopandas as gpd, imageio.v2 as imageio\n",
        "import sys\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Paths & Constantes â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# â–¶ï¸ Path configuration\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "print('BASE_PATH =', BASE_PATH)\n",
        "\n",
        "# Dataset paths\n",
        "DATA_DIR = BASE_PATH/'data'/'output'\n",
        "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
        "MODEL_DIR = BASE_PATH/'models'/'output'/'HybridLSTMModels'\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
        "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IMAGE_DIR = MODEL_DIR/'images'\n",
        "IMAGE_DIR.mkdir(exist_ok=True)\n",
        "FULL_NC = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "departamentos = gpd.read_file(MODEL_INPUT_DIR/'MGN_Departamento.shp')\n",
        "\n",
        "BASE_MODEL_DIR = MODEL_DIR\n",
        "GIF_DIR        = MODEL_DIR / \"gifs\"\n",
        "GIF_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hyperâ€‘parÃ¡metros globales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "INPUT_WINDOW   = 60\n",
        "HORIZON        = 3\n",
        "TARGET_VAR     = 'total_precipitation'\n",
        "EPOCHS         = 12\n",
        "BATCH_SIZE     = 4           # tamaÃ±o pequeÃ±oÂ â†’Â menor RAMÂ GPU\n",
        "PATIENCE       = 10\n",
        "LR             = 1e-3\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Dataset &Â shapes â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ds = xr.open_dataset(FULL_NC); lat,lon=len(ds.latitude),len(ds.longitude)\n",
        "\n",
        "#â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Experiments &Â Folds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
        "\n",
        "# Factories ---------------------------------------------------\n",
        "\n",
        "def factory_no_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=False, **kw)\n",
        "\n",
        "def factory_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=True, **kw)\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def tile_step_emb(batch_ref, step_emb_tab):\n",
        "    \"\"\"\n",
        "    Replica la tabla de embedding (T_out, D) â†’ (B, T_out, D).\n",
        "\n",
        "    Â· Durante la inferencia de forma, `batch_ref` es TensorShape\n",
        "      â†’ devolvemos TensorShape (None, T_out, D).\n",
        "    Â· En ejecuciÃ³n, `batch_ref` es tensor\n",
        "      â†’ devolvemos tensor (B, T_out, D).\n",
        "\n",
        "    â–¸ `step_emb_tab` SIEMPRE llega desde el cierre de la Lambda original,\n",
        "      asÃ­ que NO lo pongas opcional.\n",
        "    \"\"\"\n",
        "    # â€”â€”â€” 1) Forma estÃ¡tica â€”â€”â€”\n",
        "    if isinstance(batch_ref, (tf.TensorShape, tf.TensorSpec)):\n",
        "        return tf.TensorShape([batch_ref[0],\n",
        "                               step_emb_tab.shape[0],\n",
        "                               step_emb_tab.shape[1]])\n",
        "\n",
        "    # â€”â€”â€” 2) EjecuciÃ³n â€”â€”â€”\n",
        "    b   = tf.shape(batch_ref)[0]\n",
        "    emb = tf.expand_dims(step_emb_tab, 0)    # (1, T_out, D)\n",
        "    return tf.tile(emb, [b, 1, 1])           # (B, T_out, D)\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "custom = {'tile_step_emb': tile_step_emb}\n",
        "\n",
        "# â–¸ Solo mostramos los tres primeros niveles; aÃ±ade los demÃ¡s igual\n",
        "BASE_FEATURES = [\n",
        "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
        "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
        "    'elevation','slope','aspect'\n",
        "]\n",
        "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
        "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
        "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
        "\n",
        "FOLDS = {'F1': {'year': 2018,'active': True}}\n",
        "\n",
        "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
        "    'ConvLSTM-ED': {\n",
        "        'active': True,\n",
        "        'feature_list': BASE_FEATURES,\n",
        "        'builder': factory_attn, #factory_no_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE': {\n",
        "        'active': True,\n",
        "        'feature_list': KCE_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4,\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE-PAFC': {\n",
        "        'active': True,\n",
        "        'feature_list': PAFC_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 96,\n",
        "        'n_heads'  : 6,\n",
        "    },\n",
        "}\n",
        "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
        "\n",
        "\n",
        "def quick_plot(ax,data,cmap,title,date_label,vmin=None,vmax=None):\n",
        "    mesh=ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
        "    ax.coastlines(); ax.add_geometries(departamentos.geometry,ccrs.PlateCarree(),edgecolor='black',facecolor='none',linewidth=1)\n",
        "    gl=ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
        "    ax.set_title(f\"{title}\\n{date_label}\",pad=10); return mesh\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Recuperamos diccionario EXPERIMENTS (del bloque de entrenamiento) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from typing import Dict\n",
        "EXPERIMENTS:Dict[str,Dict[str,Any]] = {\n",
        "    'ConvLSTM-ED':              {'feature_list': \"+\".join(BASE_FEATURES).split(\"+\")},\n",
        "    'ConvLSTM-ED-KCE':          {'feature_list': \"+\".join(KCE_FEATURES).split(\"+\")},\n",
        "    'ConvLSTM-ED-KCE-PAFC':     {'feature_list': \"+\".join(PAFC_FEATURES).split(\"+\")},\n",
        "    #Â otros experimentos\n",
        "}\n",
        "\n",
        "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” EvaluaciÃ³n â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
        "all_metrics=[]; times=pd.to_datetime(ds.time.values)\n",
        "for mpath in sorted(BASE_MODEL_DIR.glob(\"*.keras\")):\n",
        "    tag   = mpath.stem                        # p.ej. ConvLSTM-ED_F1\n",
        "    parts = tag.split(\"_\")\n",
        "    fold  = parts[-1]                         # F1\n",
        "    exp_token = \"_\".join(parts[:-1])\n",
        "    exp_name  = exp_token.replace(\"_\",\"+\")  # vuelve al nombre original con +\n",
        "    if exp_name not in EXPERIMENTS:\n",
        "        print(\"âš ï¸Â Exp no encontrado para\",tag); continue\n",
        "    feats = EXPERIMENTS[exp_name]['feature_list']\n",
        "    print(f\"\\nğŸ” Evaluando {tag} â€¦\")\n",
        "\n",
        "    # â€” ExtracciÃ³n de arrays â€”\n",
        "    Xarr = ds[feats].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "    yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
        "    T,_,_,F = Xarr.shape\n",
        "    Xfull = Xarr; yfull=yarr  # mantenemos (T,H,W,F)\n",
        "\n",
        "    # ventana final (idÃ©ntica lÃ³gica del cuaderno original)\n",
        "    start=T-INPUT_WINDOW-HORIZON; end_w=start+INPUT_WINDOW; end_y=end_w+HORIZON\n",
        "    X_eval = Xfull[start:end_w]                 # (60,H,W,F)\n",
        "    y_eval = yfull[end_w:end_y]                 # (3,H,W)\n",
        "\n",
        "    # â€” Scalers (fit vectorizado) â€”\n",
        "    flat_X = Xfull.reshape(-1, F)      # (TÂ·HÂ·W, F)\n",
        "    flat_y = yfull.reshape(-1, 1)      # (TÂ·HÂ·W, 1)\n",
        "\n",
        "    sx = StandardScaler().fit(flat_X)\n",
        "    sy = StandardScaler().fit(flat_y)\n",
        "\n",
        "    Xe_sc = sx.transform(X_eval.reshape(-1, F)).reshape(1, INPUT_WINDOW, lat, lon, F)\n",
        "    ye_sc = sy.transform(y_eval.reshape(-1, 1)).reshape(1, HORIZON, lat, lon, 1)\n",
        "\n",
        "\n",
        "    model = tf.keras.models.load_model(\n",
        "    mpath,\n",
        "    compile=False,\n",
        "    custom_objects={'tile_step_emb': tile_step_emb}\n",
        "    )\n",
        "\n",
        "    # Verificar si el modelo usa positional embedding\n",
        "    uses_pe = len(model.inputs) > 1\n",
        "\n",
        "    if uses_pe:\n",
        "        step_ids_eval = np.tile(np.arange(HORIZON), (1, 1))\n",
        "        yhat_sc = model.predict([Xe_sc, step_ids_eval], verbose=0)  # (1,3,H,W,1)\n",
        "    else:\n",
        "        yhat_sc = model.predict(Xe_sc, verbose=0)  # (1,3,H,W,1)\n",
        "    print(f\"VerificaciÃ³n de predicciones para {tag}:\")\n",
        "    for h in range(HORIZON):\n",
        "        pred_h = yhat_sc[0, h, ..., 0]  # Primera muestra, horizonte h\n",
        "        print(f\"  H{h+1}: min={pred_h.min():.3f}, max={pred_h.max():.3f}, mean={pred_h.mean():.3f}, std={pred_h.std():.3f}\")\n",
        "    # Verificar si las predicciones son idÃ©nticas\n",
        "    if HORIZON > 1:\n",
        "        diff_h1_h2 = np.abs(yhat_sc[0, 0] - yhat_sc[0, 1]).mean()\n",
        "        print(f\"  Diferencia promedio H1 vs H2: {diff_h1_h2:.6f}\")\n",
        "    yhat   = sy.inverse_transform(yhat_sc.reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
        "    ytrue  = y_eval\n",
        "\n",
        "    # â€” MÃ©tricas por horizonte â€”\n",
        "    for h in range(HORIZON):\n",
        "        yt = ytrue[h].ravel()\n",
        "        yp = yhat[h].ravel()\n",
        "\n",
        "        # ---------- filtro NaN / Â±âˆ ----------\n",
        "        mask = np.isfinite(yt) & np.isfinite(yp)\n",
        "        if mask.sum() == 0:          # ventana vacÃ­a â†’ se ignora\n",
        "            print(f\"   Â· h={h+1}: todos los valores son NaN/Inf â†’ skip\")\n",
        "            continue\n",
        "        yt, yp = yt[mask], yp[mask]\n",
        "        # -------------------------------------\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(yt, yp))\n",
        "        mae  = mean_absolute_error(yt, yp)\n",
        "        mape = np.mean(np.abs((yt - yp) / (yt + 1e-5))) * 100\n",
        "        r2   = r2_score(yt, yp)\n",
        "\n",
        "        all_metrics.append(dict(\n",
        "            model      = tag,\n",
        "            experiment = exp_name,\n",
        "            fold       = fold,\n",
        "            horizon    = h + 1,\n",
        "            RMSE       = rmse,\n",
        "            MAE        = mae,\n",
        "            MAPE       = mape,\n",
        "            R2         = r2\n",
        "        ))\n",
        "\n",
        "    # â€” Figura Real vsÂ Pred vsÂ MAPE â€”\n",
        "    fig,axes=plt.subplots(HORIZON,3,figsize=(14,4*HORIZON),subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "    dates=pd.date_range(times[end_w],periods=HORIZON,freq='MS')\n",
        "    vmin=0; vmax=max(yhat.max(),ytrue.max())\n",
        "    for h in range(HORIZON):\n",
        "        quick_plot(axes[h,0],ytrue[h],'Blues',f\"Real h={h+1}\",dates[h].strftime('%Y-%m'),vmin,vmax)\n",
        "        quick_plot(axes[h,1],yhat [h],'Blues',f\"Pred h={h+1}\",dates[h].strftime('%Y-%m'),vmin,vmax)\n",
        "        err=np.clip(np.abs((ytrue[h]-yhat[h])/(ytrue[h]+1e-5))*100,0,100)\n",
        "        quick_plot(axes[h,2],err,'Reds',f\"MAPE% h={h+1}\",dates[h].strftime('%Y-%m'),0,100)\n",
        "    fig.suptitle(f\"{tag}  â€”Â Eval final ventana\",fontsize=16); fig.tight_layout();\n",
        "    fig.savefig(BASE_MODEL_DIR/f\"fig_{tag}.png\"); plt.close(fig)\n",
        "\n",
        "    # â€” GIF â€”\n",
        "    frames=[]; pcm_min,pcm_max=0,yhat.max()\n",
        "    for h in range(HORIZON):\n",
        "        figg,ax=plt.subplots(1,1,figsize=(6,5),subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "        m=ax.pcolormesh(ds.longitude,ds.latitude,yhat[h],cmap='Blues',shading='nearest',vmin=pcm_min,vmax=pcm_max,transform=ccrs.PlateCarree())\n",
        "        ax.coastlines(); ax.set_title(f\"{tag} â€“ H{h+1}\"); figg.colorbar(m,ax=ax,fraction=0.046,pad=0.04)\n",
        "        tmp=GIF_DIR/f\"tmp_{tag}_{h}.png\"; figg.savefig(tmp,bbox_inches='tight'); plt.close(figg)\n",
        "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
        "    imageio.mimsave(GIF_DIR/f\"{tag}.gif\",frames,fps=0.5)\n",
        "    print(\"ğŸ’¾ GIF\",f\"{tag}.gif\",\"creado\")\n",
        "\n",
        "# â€”â€”â€” Guardar tabla â€”â€”â€”\n",
        "pd.DataFrame(all_metrics).to_csv(BASE_MODEL_DIR/'metrics_eval.csv',index=False)\n",
        "print(\"ğŸ“‘Â MÃ©tricas guardadas en\",BASE_MODEL_DIR/'metrics_eval.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbyj_vPBXqrQ"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CAPAS DE ATENCIÃ“N â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "class SpatialAttention(Layer):\n",
        "    \"\"\"AtenciÃ³n espacial para resaltar regiones importantes\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.conv = Conv2D(1, (7, 7), padding='same', activation='sigmoid')\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Calcular estadÃ­sticas del canal\n",
        "        avg_pool = K.mean(inputs, axis=-1, keepdims=True)\n",
        "        max_pool = K.max(inputs, axis=-1, keepdims=True)\n",
        "        concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
        "\n",
        "        # Generar mapa de atenciÃ³n\n",
        "        attention = self.conv(concat)\n",
        "\n",
        "        return Multiply()([inputs, attention])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class ChannelAttention(Layer):\n",
        "    \"\"\"AtenciÃ³n de canal para ponderar features importantes\"\"\"\n",
        "\n",
        "    def __init__(self, reduction_ratio=8, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "        self.fc1 = Dense(channels // self.reduction_ratio, activation='relu')\n",
        "        self.fc2 = Dense(channels, activation='sigmoid')\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Global pooling\n",
        "        avg_pool = GlobalAveragePooling2D()(inputs)\n",
        "        max_pool = K.max(inputs, axis=[1, 2])\n",
        "\n",
        "        # Shared MLP\n",
        "        avg_out = self.fc2(self.fc1(avg_pool))\n",
        "        max_out = self.fc2(self.fc1(max_pool))\n",
        "\n",
        "        # Combinar\n",
        "        attention = avg_out + max_out\n",
        "        attention = K.expand_dims(K.expand_dims(attention, 1), 1)\n",
        "\n",
        "        return Multiply()([inputs, attention])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'reduction_ratio': self.reduction_ratio\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class CBAM(Layer):\n",
        "    \"\"\"Convolutional Block Attention Module\"\"\"\n",
        "\n",
        "    def __init__(self, reduction_ratio=8, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.channel_attention = ChannelAttention(self.reduction_ratio)\n",
        "        self.spatial_attention = SpatialAttention()\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.channel_attention(inputs)\n",
        "        x = self.spatial_attention(x)\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'reduction_ratio': self.reduction_ratio\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"âœ… Capas de atenciÃ³n implementadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yqAzSOTXqrQ"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CAPAS AVANZADAS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "class ConvGRU2DCell(Layer):\n",
        "    \"\"\"Celda ConvGRU2D mejorada con BatchNorm\"\"\"\n",
        "\n",
        "    def __init__(self, filters, kernel_size, padding='same', activation='tanh',\n",
        "                 recurrent_activation='sigmoid', use_batch_norm=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.padding = padding\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.recurrent_activation = tf.keras.activations.get(recurrent_activation)\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.state_size = (filters,)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        # Kernels\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(*self.kernel_size, input_dim, self.filters * 3),\n",
        "            initializer='glorot_uniform',\n",
        "            regularizer=l1_l2(l1=0, l2=L2_REG),\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(*self.kernel_size, self.filters, self.filters * 3),\n",
        "            initializer='orthogonal',\n",
        "            regularizer=l1_l2(l1=0, l2=L2_REG),\n",
        "            name='recurrent_kernel'\n",
        "        )\n",
        "\n",
        "        self.bias = self.add_weight(\n",
        "            shape=(self.filters * 3,),\n",
        "            initializer='zeros',\n",
        "            name='bias'\n",
        "        )\n",
        "\n",
        "        if self.use_batch_norm:\n",
        "            self.bn_x = BatchNormalization()\n",
        "            self.bn_h = BatchNormalization()\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        h_tm1 = states[0]\n",
        "\n",
        "        # Convoluciones\n",
        "        x_conv = K.conv2d(inputs, self.kernel, padding=self.padding)\n",
        "        h_conv = K.conv2d(h_tm1, self.recurrent_kernel, padding=self.padding)\n",
        "\n",
        "        if self.use_batch_norm:\n",
        "            x_conv = self.bn_x(x_conv, training=training)\n",
        "            h_conv = self.bn_h(h_conv, training=training)\n",
        "\n",
        "        x_z, x_r, x_h = tf.split(x_conv, 3, axis=-1)\n",
        "        h_z, h_r, h_h = tf.split(h_conv, 3, axis=-1)\n",
        "        b_z, b_r, b_h = tf.split(self.bias, 3)\n",
        "\n",
        "        # Gates\n",
        "        z = self.recurrent_activation(x_z + h_z + b_z)\n",
        "        r = self.recurrent_activation(x_r + h_r + b_r)\n",
        "\n",
        "        # Hidden state\n",
        "        h_candidate = self.activation(x_h + r * h_h + b_h)\n",
        "        h = (1 - z) * h_tm1 + z * h_candidate\n",
        "\n",
        "        return h, [h]\n",
        "\n",
        "\n",
        "class ConvGRU2D(Layer):\n",
        "    \"\"\"ConvGRU2D mejorado con soporte para BatchNorm y Dropout\"\"\"\n",
        "\n",
        "    def __init__(self, filters, kernel_size, padding='same', activation='tanh',\n",
        "                 recurrent_activation='sigmoid', return_sequences=False,\n",
        "                 use_batch_norm=True, dropout=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.activation = activation\n",
        "        self.recurrent_activation = recurrent_activation\n",
        "        self.return_sequences = return_sequences\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.cell = ConvGRU2DCell(\n",
        "            filters, kernel_size, padding, activation,\n",
        "            recurrent_activation, use_batch_norm\n",
        "        )\n",
        "\n",
        "        if dropout > 0:\n",
        "            self.dropout_layer = Dropout(dropout)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.cell.build(input_shape[2:])\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        time_steps = tf.shape(inputs)[1]\n",
        "        height = tf.shape(inputs)[2]\n",
        "        width = tf.shape(inputs)[3]\n",
        "\n",
        "        # Estado inicial\n",
        "        initial_state = tf.zeros((batch_size, height, width, self.filters))\n",
        "\n",
        "        # Procesar secuencia\n",
        "        outputs = []\n",
        "        state = initial_state\n",
        "\n",
        "        for t in range(inputs.shape[1]):\n",
        "            output, [state] = self.cell(inputs[:, t], [state], training=training)\n",
        "\n",
        "            if self.dropout > 0:\n",
        "                output = self.dropout_layer(output, training=training)\n",
        "\n",
        "            outputs.append(output)\n",
        "\n",
        "        outputs = tf.stack(outputs, axis=1)\n",
        "\n",
        "        if self.return_sequences:\n",
        "            return outputs\n",
        "        else:\n",
        "            return outputs[:, -1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'filters': self.filters,\n",
        "            'kernel_size': self.kernel_size,\n",
        "            'padding': self.padding,\n",
        "            'activation': self.activation,\n",
        "            'recurrent_activation': self.recurrent_activation,\n",
        "            'return_sequences': self.return_sequences,\n",
        "            'use_batch_norm': self.use_batch_norm,\n",
        "            'dropout': self.dropout\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"âœ… Capas ConvGRU mejoradas implementadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0t1ixIxXqrR"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MODEL BUILDERS AVANZADOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def _advanced_spatial_head(x, use_attention=True):\n",
        "    \"\"\"Cabeza de proyecciÃ³n mejorada con atenciÃ³n opcional\"\"\"\n",
        "\n",
        "    if use_attention:\n",
        "        x = CBAM()(x)\n",
        "\n",
        "    # Multi-scale processing\n",
        "    conv1 = Conv2D(HORIZON, (1, 1), padding='same')(x)\n",
        "    conv3 = Conv2D(HORIZON, (3, 3), padding='same')(x)\n",
        "    conv5 = Conv2D(HORIZON, (5, 5), padding='same')(x)\n",
        "\n",
        "    # Combine multi-scale features\n",
        "    x = Add()([conv1, conv3, conv5])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('linear')(x)\n",
        "\n",
        "    # Reshape to output format usando Permute y Reshape\n",
        "    x = Permute((3, 1, 2))(x)  # De (batch, H, W, HORIZON) a (batch, HORIZON, H, W)\n",
        "    x = Reshape((HORIZON, lat, lon, 1))(x)  # AÃ±adir dimensiÃ³n de canal\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_convlstm_attention(n_feats: int):\n",
        "    \"\"\"ConvLSTM con mecanismo de atenciÃ³n\"\"\"\n",
        "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
        "\n",
        "    # Primera capa con mÃ¡s filtros\n",
        "    x = ConvLSTM2D(64, (3, 3), padding='same', return_sequences=True,\n",
        "                   kernel_regularizer=l1_l2(l1=0, l2=L2_REG))(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(DROPOUT)(x)\n",
        "\n",
        "    # Segunda capa con atenciÃ³n\n",
        "    x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True,\n",
        "                   kernel_regularizer=l1_l2(l1=0, l2=L2_REG))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Aplicar atenciÃ³n temporal\n",
        "    x = TimeDistributed(CBAM())(x)\n",
        "\n",
        "    # Capa final\n",
        "    x = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=False,\n",
        "                   kernel_regularizer=l1_l2(l1=0, l2=L2_REG))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    out = _advanced_spatial_head(x)\n",
        "    return Model(inp, out, name='ConvLSTM_Attention')\n",
        "\n",
        "\n",
        "def build_convgru_residual(n_feats: int):\n",
        "    \"\"\"ConvGRU con skip connections\"\"\"\n",
        "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
        "\n",
        "    # Encoder path\n",
        "    enc1 = ConvGRU2D(64, (3, 3), return_sequences=True,\n",
        "                     use_batch_norm=True, dropout=DROPOUT)(inp)\n",
        "\n",
        "    enc2 = ConvGRU2D(32, (3, 3), return_sequences=True,\n",
        "                     use_batch_norm=True, dropout=DROPOUT)(enc1)\n",
        "\n",
        "    # Bottleneck\n",
        "    bottleneck = ConvGRU2D(16, (3, 3), return_sequences=False,\n",
        "                           use_batch_norm=True)(enc2)\n",
        "\n",
        "    # Skip connection from input - usar solo la Ãºltima timestep\n",
        "    skip = TimeDistributed(Conv2D(16, (1, 1), padding='same'))(inp)\n",
        "    # Usar Lambda para el slicing\n",
        "    skip = Lambda(lambda x: x[:, -1, :, :, :])(skip)  # Take last timestep\n",
        "\n",
        "    # Combine\n",
        "    x = Add()([bottleneck, skip])\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    out = _advanced_spatial_head(x)\n",
        "    return Model(inp, out, name='ConvGRU_Residual')\n",
        "\n",
        "\n",
        "def build_hybrid_transformer(n_feats: int):\n",
        "    \"\"\"Modelo hÃ­brido CNN + Transformer\"\"\"\n",
        "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
        "\n",
        "    # Encoder convolucional\n",
        "    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(inp)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "\n",
        "    # Reducir dimensionalidad espacial\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2), padding='same'))(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # Self-attention temporal\n",
        "    x = MultiHeadAttention(num_heads=4, key_dim=32, dropout=DROPOUT)(x, x)\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    # AgregaciÃ³n temporal con LSTM\n",
        "    x = LSTM(128, return_sequences=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(DROPOUT)(x)\n",
        "\n",
        "    # Decodificador espacial\n",
        "    x = Dense(lat * lon * 16)(x)\n",
        "    x = Reshape((lat, lon, 16))(x)\n",
        "\n",
        "    out = _advanced_spatial_head(x)\n",
        "    return Model(inp, out, name='Hybrid_Transformer')\n",
        "\n",
        "\n",
        "# Diccionario de modelos\n",
        "ADVANCED_MODELS = {\n",
        "    'ConvLSTM_Att': build_convlstm_attention,\n",
        "    'ConvGRU_Res': build_convgru_residual,\n",
        "    'Hybrid_Trans': build_hybrid_transformer\n",
        "}\n",
        "\n",
        "print(\"âœ… Model builders avanzados creados\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66lB5g6BXqrR"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CALLBACKS AVANZADOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "class AdvancedTrainingMonitor(Callback):\n",
        "    \"\"\"Monitor de entrenamiento con visualizaciÃ³n en tiempo real\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, experiment_name, patience=10):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.experiment_name = experiment_name\n",
        "        self.patience = patience\n",
        "        self.history = {'loss': [], 'val_loss': [], 'lr': [], 'epoch': []}\n",
        "        self.wait = 0\n",
        "        self.best_val_loss = np.inf\n",
        "        self.converged = False\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Actualizar historial\n",
        "        self.history['loss'].append(logs.get('loss', 0))\n",
        "        self.history['val_loss'].append(logs.get('val_loss', 0))\n",
        "        self.history['lr'].append(K.get_value(self.model.optimizer.learning_rate))\n",
        "        self.history['epoch'].append(epoch + 1)\n",
        "\n",
        "        # Verificar mejora\n",
        "        current_val_loss = logs.get('val_loss', 0)\n",
        "        if current_val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = current_val_loss\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "\n",
        "        # Verificar convergencia\n",
        "        if len(self.history['val_loss']) > 5:\n",
        "            recent_losses = self.history['val_loss'][-5:]\n",
        "            loss_std = np.std(recent_losses)\n",
        "            loss_mean = np.mean(recent_losses)\n",
        "            if loss_std / loss_mean < 0.01:  # Menos del 1% de variaciÃ³n\n",
        "                self.converged = True\n",
        "\n",
        "        # VisualizaciÃ³n cada 5 Ã©pocas o en la Ãºltima\n",
        "        if (epoch + 1) % 5 == 0 or (epoch + 1) == self.params['epochs']:\n",
        "            self._plot_progress()\n",
        "\n",
        "    def _plot_progress(self):\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "        # Loss curves\n",
        "        ax1 = plt.subplot(141)\n",
        "        ax1.plot(self.history['epoch'], self.history['loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "        ax1.plot(self.history['epoch'], self.history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title(f'{self.model_name} - {self.experiment_name}')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss ratio\n",
        "        ax2 = plt.subplot(142)\n",
        "        if len(self.history['loss']) > 0:\n",
        "            ratio = [v/t if t > 0 else 1 for v, t in zip(self.history['val_loss'], self.history['loss'])]\n",
        "            ax2.plot(self.history['epoch'], ratio, 'g-', linewidth=2)\n",
        "            ax2.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
        "            ax2.fill_between(self.history['epoch'], 1, ratio,\n",
        "                           where=[r > 1 for r in ratio],\n",
        "                           color='red', alpha=0.2, label='Overfitting')\n",
        "            ax2.set_xlabel('Epoch')\n",
        "            ax2.set_ylabel('Val Loss / Train Loss')\n",
        "            ax2.set_title('Overfitting Monitor')\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Tasa de mejora y convergencia\n",
        "        ax3 = plt.subplot(143)\n",
        "        if len(self.history['val_loss']) > 1:\n",
        "            # Calcular tasa de mejora epoch a epoch\n",
        "            improvements = []\n",
        "            for i in range(1, len(self.history['val_loss'])):\n",
        "                prev_loss = self.history['val_loss'][i-1]\n",
        "                curr_loss = self.history['val_loss'][i]\n",
        "                improvement = ((prev_loss - curr_loss) / prev_loss) * 100\n",
        "                improvements.append(improvement)\n",
        "\n",
        "            # Plot de tasa de mejora\n",
        "            ax3.plot(self.history['epoch'][1:], improvements, 'purple', linewidth=2, alpha=0.7)\n",
        "            ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "            ax3.fill_between(self.history['epoch'][1:], 0, improvements,\n",
        "                           where=[imp > 0 for imp in improvements],\n",
        "                           color='green', alpha=0.3, label='Mejora')\n",
        "            ax3.fill_between(self.history['epoch'][1:], 0, improvements,\n",
        "                           where=[imp < 0 for imp in improvements],\n",
        "                           color='red', alpha=0.3, label='Empeoramiento')\n",
        "\n",
        "            # LÃ­nea de tendencia\n",
        "            if len(improvements) > 3:\n",
        "                z = np.polyfit(range(len(improvements)), improvements, 2)\n",
        "                p = np.poly1d(z)\n",
        "                ax3.plot(self.history['epoch'][1:], p(range(len(improvements))),\n",
        "                       'orange', linewidth=2, linestyle='--', label='Tendencia')\n",
        "\n",
        "            ax3.set_xlabel('Epoch')\n",
        "            ax3.set_ylabel('Mejora (%)')\n",
        "            ax3.set_title('Tasa de Mejora y Convergencia')\n",
        "            ax3.legend()\n",
        "            ax3.grid(True, alpha=0.3)\n",
        "\n",
        "            # Indicador de convergencia\n",
        "            if self.converged:\n",
        "                ax3.text(0.02, 0.98, 'âœ“ Convergido', transform=ax3.transAxes,\n",
        "                       va='top', bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n",
        "\n",
        "        # Training stats\n",
        "        ax4 = plt.subplot(144)\n",
        "        ax4.axis('off')\n",
        "        stats_text = f\"\"\"\n",
        "        {self.model_name} - {self.experiment_name}\n",
        "\n",
        "        Ã‰poca: {self.history['epoch'][-1]}/{self.params['epochs']}\n",
        "\n",
        "        Loss actual:\n",
        "        â€¢ Train: {self.history['loss'][-1]:.6f}\n",
        "        â€¢ Val: {self.history['val_loss'][-1]:.6f}\n",
        "\n",
        "        Mejor val loss: {self.best_val_loss:.6f}\n",
        "        Ã‰pocas sin mejora: {self.wait}/{self.patience}\n",
        "\n",
        "        Learning rate: {self.history['lr'][-1]:.2e}\n",
        "\n",
        "        Estado: {'Convergido âœ“' if self.converged else 'Entrenando...'}\n",
        "        \"\"\"\n",
        "        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes,\n",
        "                fontsize=12, verticalalignment='top', fontfamily='monospace')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def create_callbacks(model_name, experiment_name, model_path):\n",
        "    \"\"\"Crea callbacks optimizados para el entrenamiento\"\"\"\n",
        "\n",
        "    # Learning rate scheduler con warmup\n",
        "    def lr_schedule(epoch, lr):\n",
        "        warmup_epochs = 5\n",
        "        if epoch < warmup_epochs:\n",
        "            return LR * (epoch + 1) / warmup_epochs\n",
        "        else:\n",
        "            # Cosine decay despuÃ©s del warmup\n",
        "            progress = (epoch - warmup_epochs) / (EPOCHS - warmup_epochs)\n",
        "            return LR * 0.5 * (1 + np.cos(np.pi * progress))\n",
        "\n",
        "    callbacks = [\n",
        "        # Monitor de entrenamiento avanzado\n",
        "        AdvancedTrainingMonitor(model_name, experiment_name, patience=PATIENCE),\n",
        "\n",
        "        # Early stopping mejorado\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=PATIENCE,\n",
        "            restore_best_weights=True,\n",
        "            verbose=0,\n",
        "            mode='min',\n",
        "            min_delta=1e-4\n",
        "        ),\n",
        "\n",
        "        # Model checkpoint\n",
        "        ModelCheckpoint(\n",
        "            str(model_path),\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode='min',\n",
        "            verbose=0\n",
        "        ),\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        LearningRateScheduler(lr_schedule, verbose=0),\n",
        "\n",
        "        # Reduce LR on plateau como backup\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6,\n",
        "            verbose=0,\n",
        "            mode='min',\n",
        "            min_delta=1e-4\n",
        "        ),\n",
        "\n",
        "        # CSV logger para anÃ¡lisis posterior\n",
        "        CSVLogger(\n",
        "            str(model_path.parent / f\"{model_name}_training.csv\"),\n",
        "            separator=',',\n",
        "            append=False\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evb3tTGPXqrS"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "def windowed_arrays(X:np.ndarray, y:np.ndarray):\n",
        "    \"\"\"Crear ventanas deslizantes para series temporales\"\"\"\n",
        "    seq_X, seq_y = [], []\n",
        "    T = len(X)\n",
        "    for start in range(T-INPUT_WINDOW-HORIZON+1):\n",
        "        end_w = start+INPUT_WINDOW\n",
        "        end_y = end_w+HORIZON\n",
        "        Xw, yw = X[start:end_w], y[end_w:end_y]\n",
        "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
        "            continue\n",
        "        seq_X.append(Xw)\n",
        "        seq_y.append(yw)\n",
        "    return np.asarray(seq_X,dtype=np.float32), np.asarray(seq_y,dtype=np.float32)\n",
        "\n",
        "\n",
        "def quick_plot(ax,data,cmap,title,vmin=None,vmax=None):\n",
        "    \"\"\"Plotear mapa geogrÃ¡fico\"\"\"\n",
        "    try:\n",
        "        import cartopy.crs as ccrs\n",
        "        mesh = ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',\n",
        "                             vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
        "        ax.coastlines()\n",
        "        ax.add_geometries(DEPT_GDF.geometry,ccrs.PlateCarree(),\n",
        "                          edgecolor='black',facecolor='none',linewidth=1)\n",
        "        ax.gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
        "\n",
        "        # Agregar colorbar para cada mapa\n",
        "        fig = ax.figure\n",
        "        cbar = fig.colorbar(mesh, ax=ax, fraction=0.046, pad=0.04)\n",
        "        if cmap == 'Blues':\n",
        "            cbar.set_label('PrecipitaciÃ³n (mm)', rotation=270, labelpad=15)\n",
        "        elif cmap == 'Reds':\n",
        "            cbar.set_label('MAPE (%)', rotation=270, labelpad=15)\n",
        "    except ImportError:\n",
        "        # Si no hay cartopy, hacer un plot simple\n",
        "        mesh = ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',\n",
        "                             vmin=vmin,vmax=vmax)\n",
        "        ax.set_xlabel('Longitude')\n",
        "        ax.set_ylabel('Latitude')\n",
        "\n",
        "        # Agregar colorbar para cada mapa\n",
        "        fig = ax.figure\n",
        "        cbar = fig.colorbar(mesh, ax=ax, fraction=0.046, pad=0.04)\n",
        "        if cmap == 'Blues':\n",
        "            cbar.set_label('PrecipitaciÃ³n (mm)', rotation=270, labelpad=15)\n",
        "        elif cmap == 'Reds':\n",
        "            cbar.set_label('MAPE (%)', rotation=270, labelpad=15)\n",
        "\n",
        "    ax.set_title(title,fontsize=9)\n",
        "    return mesh\n",
        "\n",
        "\n",
        "def save_hyperparameters(exp_path, model_name, hyperparams):\n",
        "    \"\"\"Guarda los hiperparÃ¡metros en un archivo JSON\"\"\"\n",
        "    hp_file = exp_path / f\"{model_name}_hyperparameters.json\"\n",
        "    with open(hp_file, 'w') as f:\n",
        "        json.dump(hyperparams, f, indent=4)\n",
        "    print(f\"   ğŸ’¾ HiperparÃ¡metros guardados en: {hp_file.name}\")\n",
        "\n",
        "\n",
        "def plot_learning_curves(history, exp_path, model_name, show=True):\n",
        "    \"\"\"Genera y guarda las curvas de aprendizaje\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "    axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss (MSE)')\n",
        "    axes[0].set_title(f'{model_name} - Loss Evolution')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # AnÃ¡lisis de Convergencia y Estabilidad\n",
        "    val_losses = history.history['val_loss']\n",
        "    train_losses = history.history['loss']\n",
        "\n",
        "    if len(val_losses) > 1:\n",
        "        # Calcular mÃ©tricas de convergencia\n",
        "        epochs = range(1, len(val_losses) + 1)\n",
        "\n",
        "        # 1. Ratio de overfitting\n",
        "        overfit_ratio = [val_losses[i] / train_losses[i] for i in range(len(val_losses))]\n",
        "\n",
        "        # 2. Estabilidad (desviaciÃ³n estÃ¡ndar mÃ³vil)\n",
        "        window = min(5, len(val_losses)//3)\n",
        "        val_std = pd.Series(val_losses).rolling(window=window).std()\n",
        "\n",
        "        # Crear subplot con dos ejes Y\n",
        "        ax2_left = axes[1]\n",
        "        ax2_right = ax2_left.twinx()\n",
        "\n",
        "        # Plot ratio de overfitting\n",
        "        line1 = ax2_left.plot(epochs, overfit_ratio, 'r-', linewidth=2,\n",
        "                             label='Ratio Val/Train', alpha=0.8)\n",
        "        ax2_left.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
        "        ax2_left.fill_between(epochs, 1.0, overfit_ratio,\n",
        "                            where=[x > 1.0 for x in overfit_ratio],\n",
        "                            color='red', alpha=0.2)\n",
        "        ax2_left.set_xlabel('Epoch')\n",
        "        ax2_left.set_ylabel('Ratio Val Loss / Train Loss', color='red')\n",
        "        ax2_left.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "        # Plot estabilidad\n",
        "        line2 = ax2_right.plot(epochs[window-1:], val_std[window-1:], 'b-',\n",
        "                             linewidth=2, label='Estabilidad', alpha=0.8)\n",
        "        ax2_right.set_ylabel('DesviaciÃ³n EstÃ¡ndar (ventana mÃ³vil)', color='blue')\n",
        "        ax2_right.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "        # TÃ­tulo y leyenda combinada\n",
        "        ax2_left.set_title(f'{model_name} - AnÃ¡lisis de Convergencia')\n",
        "\n",
        "        # Combinar leyendas\n",
        "        lines = line1 + line2\n",
        "        labels = [l.get_label() for l in lines]\n",
        "        ax2_left.legend(lines, labels, loc='upper left')\n",
        "\n",
        "        ax2_left.grid(True, alpha=0.3)\n",
        "\n",
        "        # AÃ±adir zonas de interpretaciÃ³n\n",
        "        if max(overfit_ratio) > 1.5:\n",
        "            ax2_left.text(0.02, 0.98, 'âš ï¸ Alto overfitting detectado',\n",
        "                        transform=ax2_left.transAxes, va='top',\n",
        "                        bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
        "        elif min(val_std[window-1:]) < 0.001:\n",
        "            ax2_left.text(0.02, 0.98, 'âœ“ Entrenamiento estable',\n",
        "                        transform=ax2_left.transAxes, va='top',\n",
        "                        bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n",
        "    else:\n",
        "        axes[1].text(0.5, 0.5, 'Insufficient data for convergence analysis',\n",
        "                    transform=axes[1].transAxes, ha='center', va='center',\n",
        "                    fontsize=12, color='gray')\n",
        "        axes[1].set_title(f'{model_name} - Convergence Analysis')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Guardar\n",
        "    curves_path = exp_path / f\"{model_name}_learning_curves.png\"\n",
        "    plt.savefig(curves_path, dpi=150, bbox_inches='tight')\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "    return curves_path\n",
        "\n",
        "\n",
        "def print_training_summary(history, model_name, exp_name):\n",
        "    \"\"\"Imprime un resumen del entrenamiento\"\"\"\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    final_val_loss = history.history['val_loss'][-1]\n",
        "    best_val_loss = min(history.history['val_loss'])\n",
        "    best_epoch = history.history['val_loss'].index(best_val_loss) + 1\n",
        "\n",
        "    print(f\"\\n   ğŸ“Š Resumen de entrenamiento {model_name} - {exp_name}:\")\n",
        "    print(f\"      â€¢ Ã‰pocas totales: {len(history.history['loss'])}\")\n",
        "    print(f\"      â€¢ Loss final (train): {final_loss:.6f}\")\n",
        "    print(f\"      â€¢ Loss final (val): {final_val_loss:.6f}\")\n",
        "    print(f\"      â€¢ Mejor loss (val): {best_val_loss:.6f} en Ã©poca {best_epoch}\")\n",
        "    if 'lr' in history.history and len(history.history['lr']) > 0:\n",
        "        final_lr = history.history['lr'][-1]\n",
        "        print(f\"      â€¢ Learning rate final: {final_lr:.2e}\")\n",
        "    else:\n",
        "        print(f\"      â€¢ Learning rate final: No disponible\")\n",
        "\n",
        "print(\"âœ… Funciones helper creadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dU6DGcHXqrS"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TRAIN + EVAL LOOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Diccionario para almacenar historiales de entrenamiento\n",
        "all_histories = {}\n",
        "results = []\n",
        "\n",
        "for exp, feat_list in EXPERIMENTS.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ğŸ”¬ EXPERIMENTO: {exp} ({len(feat_list)} features)\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Preparar datos\n",
        "    Xarr = ds[feat_list].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "    yarr = ds['total_precipitation'].values.astype(np.float32)[...,None]\n",
        "    X, y = windowed_arrays(Xarr, yarr)\n",
        "    split = int(0.8*len(X))\n",
        "    val_split = int(0.9*len(X))\n",
        "\n",
        "    # NormalizaciÃ³n\n",
        "    sx = StandardScaler().fit(X[:split].reshape(-1,len(feat_list)))\n",
        "    sy = StandardScaler().fit(y[:split].reshape(-1,1))\n",
        "    X_sc = sx.transform(X.reshape(-1,len(feat_list))).reshape(X.shape)\n",
        "    y_sc = sy.transform(y.reshape(-1,1)).reshape(y.shape)\n",
        "\n",
        "    # Splits\n",
        "    X_tr, X_va, X_te = X_sc[:split], X_sc[split:val_split], X_sc[val_split:]\n",
        "    y_tr, y_va, y_te = y_sc[:split], y_sc[split:val_split], y_sc[val_split:]\n",
        "\n",
        "    print(f\"   Datos: Train={len(X_tr)}, Val={len(X_va)}, Test={len(X_te)}\")\n",
        "\n",
        "    OUT_EXP = OUT_ROOT/exp\n",
        "    OUT_EXP.mkdir(exist_ok=True)\n",
        "\n",
        "    # Crear subdirectorio para mÃ©tricas de entrenamiento\n",
        "    METRICS_DIR = OUT_EXP / 'training_metrics'\n",
        "    METRICS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "    for mdl_name, builder in ADVANCED_MODELS.items():\n",
        "        print(f\"\\n{'â”€'*50}\")\n",
        "        print(f\"ğŸ¤– Modelo: {mdl_name}\")\n",
        "        print(f\"{'â”€'*50}\")\n",
        "\n",
        "        model_path = OUT_EXP/f\"{mdl_name.lower()}_best.keras\"\n",
        "        if model_path.exists():\n",
        "            model_path.unlink()\n",
        "\n",
        "        try:\n",
        "            # Construir modelo\n",
        "            model = builder(n_feats=len(feat_list))\n",
        "\n",
        "            # Definir optimizador con configuraciÃ³n explÃ­cita\n",
        "            optimizer = AdamW(learning_rate=LR, weight_decay=L2_REG)\n",
        "            model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "            # HiperparÃ¡metros\n",
        "            hyperparams = {\n",
        "                'experiment': exp,\n",
        "                'model': mdl_name,\n",
        "                'features': [str(f) for f in feat_list],  # Convertir a strings\n",
        "                'n_features': int(len(feat_list)),\n",
        "                'input_window': int(INPUT_WINDOW),\n",
        "                'horizon': int(HORIZON),\n",
        "                'batch_size': int(BATCH_SIZE),\n",
        "                'initial_lr': float(LR),\n",
        "                'epochs': int(EPOCHS),\n",
        "                'patience': int(PATIENCE),\n",
        "                'dropout': float(DROPOUT),\n",
        "                'l2_reg': float(L2_REG),\n",
        "                'train_samples': int(len(X_tr)),\n",
        "                'val_samples': int(len(X_va)),\n",
        "                'test_samples': int(len(X_te)),\n",
        "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'model_params': int(model.count_params())\n",
        "            }\n",
        "\n",
        "            # Guardar hiperparÃ¡metros\n",
        "            save_hyperparameters(METRICS_DIR, mdl_name, hyperparams)\n",
        "\n",
        "            # Callbacks\n",
        "            callbacks = create_callbacks(mdl_name, exp, model_path)\n",
        "\n",
        "            # Entrenar con verbose=0 para usar nuestro monitor personalizado\n",
        "            print(f\"\\nğŸƒ Iniciando entrenamiento...\")\n",
        "            print(f\"   ğŸ“Š VisualizaciÃ³n en tiempo real activada\")\n",
        "            print(f\"   ğŸ“ˆ ParÃ¡metros del modelo: {model.count_params():,}\")\n",
        "\n",
        "            history = model.fit(\n",
        "                X_tr, y_tr,\n",
        "                validation_data=(X_va, y_va),\n",
        "                epochs=EPOCHS,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                callbacks=callbacks,\n",
        "                verbose=0  # Usar 0 para que solo se muestre nuestro monitor\n",
        "            )\n",
        "\n",
        "            # Guardar historial\n",
        "            all_histories[f\"{exp}_{mdl_name}\"] = history\n",
        "\n",
        "            # Mostrar resumen de entrenamiento\n",
        "            print_training_summary(history, mdl_name, exp)\n",
        "\n",
        "            # Plotear y guardar curvas de aprendizaje\n",
        "            plot_learning_curves(history, METRICS_DIR, mdl_name, show=True)\n",
        "\n",
        "            # Guardar historial como JSON\n",
        "            # Obtener learning rates del monitor de entrenamiento si no estÃ¡n en history\n",
        "            training_monitor = [cb for cb in callbacks if isinstance(cb, AdvancedTrainingMonitor)][0]\n",
        "            lr_values = history.history.get('lr', [])\n",
        "            if not lr_values and hasattr(training_monitor, 'history'):\n",
        "                lr_values = training_monitor.history['lr']\n",
        "\n",
        "            history_dict = {\n",
        "                'loss': [float(x) for x in history.history['loss']],\n",
        "                'val_loss': [float(x) for x in history.history['val_loss']],\n",
        "                'mae': [float(x) for x in history.history.get('mae', [])],\n",
        "                'val_mae': [float(x) for x in history.history.get('val_mae', [])],\n",
        "                'lr': [float(x) for x in lr_values] if lr_values else []\n",
        "            }\n",
        "\n",
        "            with open(METRICS_DIR / f\"{mdl_name}_history.json\", 'w') as f:\n",
        "                json.dump(history_dict, f, indent=4)\n",
        "\n",
        "            # â”€ EvaluaciÃ³n en Test Set â”€\n",
        "            print(f\"\\nğŸ“Š Evaluando en test set...\")\n",
        "            test_loss, test_mae = model.evaluate(X_te, y_te, verbose=0)\n",
        "            print(f\"   Test Loss: {test_loss:.6f}, Test MAE: {test_mae:.6f}\")\n",
        "\n",
        "            # â”€ Predicciones y visualizaciÃ³n â”€\n",
        "            print(f\"\\nğŸ¯ Generando predicciones...\")\n",
        "            # Usar las primeras 5 muestras del test set\n",
        "            sample_indices = min(5, len(X_te))\n",
        "            y_hat_sc = model.predict(X_te[:sample_indices], verbose=0)\n",
        "            y_hat = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(-1,HORIZON,lat,lon)\n",
        "            y_true = sy.inverse_transform(y_te[:sample_indices].reshape(-1,1)).reshape(-1,HORIZON,lat,lon)\n",
        "\n",
        "            # â”€ MÃ©tricas de evaluaciÃ³n por horizonte â”€\n",
        "            for h in range(HORIZON):\n",
        "                rmse = np.sqrt(mean_squared_error(y_true[:,h].ravel(), y_hat[:,h].ravel()))\n",
        "                mae = mean_absolute_error(y_true[:,h].ravel(), y_hat[:,h].ravel())\n",
        "                r2 = r2_score(y_true[:,h].ravel(), y_hat[:,h].ravel())\n",
        "\n",
        "                results.append({\n",
        "                    'Experiment': exp,\n",
        "                    'Model': mdl_name,\n",
        "                    'H': h+1,\n",
        "                    'RMSE': rmse,\n",
        "                    'MAE': mae,\n",
        "                    'R2': r2,\n",
        "                    'Test_Loss': test_loss,\n",
        "                    'Parameters': model.count_params()\n",
        "                })\n",
        "\n",
        "                print(f\"   ğŸ“ˆ H={h+1}: RMSE={rmse:.4f}, MAE={mae:.4f}, RÂ²={r2:.4f}\")\n",
        "\n",
        "            # â”€ Mapas & GIF â”€\n",
        "            print(f\"\\nğŸ¨ Generando visualizaciones...\")\n",
        "            # Usar la primera muestra para visualizaciÃ³n\n",
        "            sample_idx = 0\n",
        "            vmin, vmax = 0, max(y_true[sample_idx].max(), y_hat[sample_idx].max())\n",
        "            frames = []\n",
        "            dates = pd.date_range(ds.time.values[-HORIZON], periods=HORIZON, freq='MS')\n",
        "\n",
        "            for h in range(HORIZON):\n",
        "                err = np.clip(np.abs((y_true[sample_idx,h]-y_hat[sample_idx,h])/(y_true[sample_idx,h]+1e-5))*100, 0, 100)\n",
        "                try:\n",
        "                    import cartopy.crs as ccrs\n",
        "                    fig, axs = plt.subplots(1, 3, figsize=(16, 4.5), subplot_kw={'projection': ccrs.PlateCarree()})\n",
        "                except ImportError:\n",
        "                    fig, axs = plt.subplots(1, 3, figsize=(16, 4.5))\n",
        "\n",
        "                # Real\n",
        "                real_mesh = axs[0].pcolormesh(ds.longitude, ds.latitude, y_true[sample_idx,h],\n",
        "                                            cmap='Blues', shading='nearest', vmin=vmin, vmax=vmax,\n",
        "                                            transform=ccrs.PlateCarree())\n",
        "                axs[0].coastlines()\n",
        "                axs[0].add_geometries(DEPT_GDF.geometry, ccrs.PlateCarree(),\n",
        "                                    edgecolor='black', facecolor='none', linewidth=1)\n",
        "                axs[0].gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
        "                axs[0].set_title(f\"Real h={h+1}\", fontsize=11)\n",
        "                real_cbar = fig.colorbar(real_mesh, ax=axs[0], fraction=0.046, pad=0.04)\n",
        "                real_cbar.set_label('PrecipitaciÃ³n (mm)', rotation=270, labelpad=15)\n",
        "\n",
        "                # PredicciÃ³n\n",
        "                pred_mesh = axs[1].pcolormesh(ds.longitude, ds.latitude, y_hat[sample_idx,h],\n",
        "                                            cmap='Blues', shading='nearest', vmin=vmin, vmax=vmax,\n",
        "                                            transform=ccrs.PlateCarree())\n",
        "                axs[1].coastlines()\n",
        "                axs[1].add_geometries(DEPT_GDF.geometry, ccrs.PlateCarree(),\n",
        "                                     edgecolor='black', facecolor='none', linewidth=1)\n",
        "                axs[1].gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
        "                axs[1].set_title(f\"{mdl_name} h={h+1}\", fontsize=11)\n",
        "                pred_cbar = fig.colorbar(pred_mesh, ax=axs[1], fraction=0.046, pad=0.04)\n",
        "                pred_cbar.set_label('PrecipitaciÃ³n (mm)', rotation=270, labelpad=15)\n",
        "\n",
        "                # Error\n",
        "                err_mesh = axs[2].pcolormesh(ds.longitude, ds.latitude, err,\n",
        "                                           cmap='Reds', shading='nearest', vmin=0, vmax=100,\n",
        "                                           transform=ccrs.PlateCarree())\n",
        "                axs[2].coastlines()\n",
        "                axs[2].add_geometries(DEPT_GDF.geometry, ccrs.PlateCarree(),\n",
        "                                    edgecolor='black', facecolor='none', linewidth=1)\n",
        "                axs[2].gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
        "                axs[2].set_title(f\"MAPE% h={h+1}\", fontsize=11)\n",
        "                err_cbar = fig.colorbar(err_mesh, ax=axs[2], fraction=0.046, pad=0.04)\n",
        "                err_cbar.set_label('MAPE (%)', rotation=270, labelpad=15)\n",
        "\n",
        "                fig.suptitle(f\"{mdl_name} â€“ {exp} â€“ {dates[h].strftime('%Y-%m')}\", fontsize=13)\n",
        "                png = OUT_EXP/f\"{mdl_name}_{h+1}.png\"\n",
        "                fig.tight_layout()\n",
        "                fig.savefig(png, bbox_inches='tight')\n",
        "                plt.close(fig)\n",
        "                frames.append(imageio.imread(png))\n",
        "\n",
        "            imageio.mimsave(OUT_EXP/f\"{mdl_name}.gif\", frames, fps=0.5)\n",
        "            print(f\"   âœ… GIF guardado: {OUT_EXP/f'{mdl_name}.gif'}\")\n",
        "\n",
        "            tf.keras.backend.clear_session()\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âš ï¸ Error en {mdl_name}: {str(e)}\")\n",
        "            print(f\"  â†’ Saltando {mdl_name} para {exp}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CSV FINAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "res_df = pd.DataFrame(results)\n",
        "res_df.to_csv(OUT_ROOT/'metrics_advanced.csv', index=False)\n",
        "print(\"\\nğŸ“‘ Metrics saved â†’\", OUT_ROOT/'metrics_advanced.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXK-acO1XqrT"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ VISUALIZACIÃ“N COMPARATIVA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“Š GENERANDO VISUALIZACIONES COMPARATIVAS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Crear directorio para comparaciones\n",
        "COMP_DIR = OUT_ROOT / 'comparisons'\n",
        "COMP_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# 1. ComparaciÃ³n de mÃ©tricas entre modelos\n",
        "if 'res_df' in locals() and res_df is not None and len(res_df) > 0:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "    # RMSE por modelo y experimento\n",
        "    pivot_rmse = res_df.pivot_table(values='RMSE', index='Model', columns='Experiment', aggfunc='mean')\n",
        "    pivot_rmse.plot(kind='bar', ax=axes[0,0])\n",
        "    axes[0,0].set_title('RMSE Promedio por Modelo y Experimento', fontsize=14, pad=10)\n",
        "    axes[0,0].set_ylabel('RMSE')\n",
        "    axes[0,0].set_xlabel('Modelo')\n",
        "    axes[0,0].legend(title='Experimento', loc='upper left')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # MAE por modelo y experimento\n",
        "    pivot_mae = res_df.pivot_table(values='MAE', index='Model', columns='Experiment', aggfunc='mean')\n",
        "    pivot_mae.plot(kind='bar', ax=axes[0,1])\n",
        "    axes[0,1].set_title('MAE Promedio por Modelo y Experimento', fontsize=14, pad=10)\n",
        "    axes[0,1].set_ylabel('MAE')\n",
        "    axes[0,1].set_xlabel('Modelo')\n",
        "    axes[0,1].legend(title='Experimento', loc='upper left')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # RÂ² por modelo y experimento\n",
        "    pivot_r2 = res_df.pivot_table(values='R2', index='Model', columns='Experiment', aggfunc='mean')\n",
        "    pivot_r2.plot(kind='bar', ax=axes[1,0])\n",
        "    axes[1,0].set_title('RÂ² Promedio por Modelo y Experimento', fontsize=14, pad=10)\n",
        "    axes[1,0].set_ylabel('RÂ²')\n",
        "    axes[1,0].set_xlabel('Modelo')\n",
        "    axes[1,0].legend(title='Experimento', loc='lower right')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # EvoluciÃ³n de mÃ©tricas por horizonte\n",
        "    ax_horizon = axes[1,1]\n",
        "\n",
        "    for model in res_df['Model'].unique():\n",
        "        model_data = res_df[res_df['Model'] == model]\n",
        "        horizon_means = model_data.groupby('H')['RMSE'].mean()\n",
        "        ax_horizon.plot(horizon_means.index, horizon_means.values,\n",
        "                       marker='o', label=model, linewidth=2.5, markersize=8)\n",
        "\n",
        "    ax_horizon.set_xlabel('Horizonte (meses)')\n",
        "    ax_horizon.set_ylabel('RMSE')\n",
        "    ax_horizon.set_title('EvoluciÃ³n de RMSE por Horizonte', fontsize=14, pad=10)\n",
        "    ax_horizon.legend(title='Modelo', loc='best')\n",
        "    ax_horizon.grid(True, alpha=0.3)\n",
        "    ax_horizon.set_xticks([1, 2, 3])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(COMP_DIR / 'metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# 2. Tabla resumen de mejores modelos\n",
        "if 'res_df' in locals() and res_df is not None and len(res_df) > 0:\n",
        "    print(\"\\nğŸ“‹ TABLA RESUMEN - MEJORES MODELOS POR EXPERIMENTO:\")\n",
        "    print(\"â”€\" * 60)\n",
        "\n",
        "    best_models = res_df.groupby('Experiment').apply(\n",
        "        lambda x: x.loc[x['RMSE'].idxmin()]\n",
        "    )[['Model', 'RMSE', 'MAE', 'R2']]\n",
        "\n",
        "    print(best_models.to_string())\n",
        "else:\n",
        "    print(\"\\nâš ï¸ No hay resultados disponibles para mostrar el resumen\")\n",
        "\n",
        "# 3. ComparaciÃ³n con modelos originales si existen\n",
        "old_metrics_path = BASE_PATH / 'models' / 'output' / 'Spatial_CONVRNN' / 'metrics_spatial.csv'\n",
        "if old_metrics_path.exists():\n",
        "    print(\"\\nğŸ“Š COMPARACIÃ“N CON MODELOS ORIGINALES:\")\n",
        "    print(\"â”€\" * 60)\n",
        "\n",
        "    old_df = pd.read_csv(old_metrics_path)\n",
        "\n",
        "    # Calcular mejoras promedio\n",
        "    if 'res_df' in locals() and res_df is not None and len(res_df) > 0:\n",
        "        for exp in EXPERIMENTS.keys():\n",
        "            print(f\"\\n{exp}:\")\n",
        "\n",
        "            # Mejores modelos nuevos\n",
        "            exp_data = res_df[res_df['Experiment'] == exp]\n",
        "            if len(exp_data) > 0:\n",
        "                new_best = exp_data.groupby('Model')['RMSE'].mean().idxmin()\n",
        "                new_rmse = exp_data[exp_data['Model'] == new_best]['RMSE'].mean()\n",
        "\n",
        "                # Mejor modelo original\n",
        "                old_exp_data = old_df[old_df['Experiment'] == exp]\n",
        "                if len(old_exp_data) > 0:\n",
        "                    old_best_rmse = old_exp_data['RMSE'].min()\n",
        "\n",
        "                    improvement = (old_best_rmse - new_rmse) / old_best_rmse * 100\n",
        "\n",
        "                    print(f\"  â€¢ Mejor modelo nuevo: {new_best} (RMSE: {new_rmse:.4f})\")\n",
        "                    print(f\"  â€¢ Mejor RMSE original: {old_best_rmse:.4f}\")\n",
        "                    print(f\"  â€¢ Mejora: {improvement:.2f}%\")\n",
        "                else:\n",
        "                    print(f\"  â€¢ No hay datos originales para comparar\")\n",
        "\n",
        "print(\"\\nâœ… Visualizaciones comparativas completadas!\")\n",
        "print(f\"ğŸ“‚ Resultados guardados en: {COMP_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJcQwNvLXqrT"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ANÃLISIS DETALLADO DE RESULTADOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "if 'res_df' in locals() and res_df is not None and len(res_df) > 0:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ“Š ANÃLISIS DETALLADO DE RESULTADOS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. MÃ©tricas por horizonte de predicciÃ³n\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "    metrics = ['RMSE', 'MAE', 'R2']\n",
        "    titles = ['RMSE por Horizonte', 'MAE por Horizonte', 'RÂ² por Horizonte']\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(res_df['Model'].unique())))\n",
        "\n",
        "    for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
        "        ax = axes[idx]\n",
        "\n",
        "        # Obtener datos pivoteados\n",
        "        data = res_df.groupby(['H', 'Model'])[metric].mean().unstack()\n",
        "\n",
        "        # Plotear cada modelo\n",
        "        for i, model in enumerate(data.columns):\n",
        "            ax.plot(data.index, data[model],\n",
        "                   marker='o',\n",
        "                   label=model,\n",
        "                   color=colors[i],\n",
        "                   linewidth=2.5,\n",
        "                   markersize=8,\n",
        "                   markeredgewidth=2,\n",
        "                   markeredgecolor='white')\n",
        "\n",
        "        ax.set_xlabel('Horizonte (meses)', fontsize=12)\n",
        "        ax.set_ylabel(metric, fontsize=12)\n",
        "        ax.set_title(title, fontsize=14, fontweight='bold', pad=10)\n",
        "        ax.grid(True, alpha=0.3, linestyle='--')\n",
        "        ax.set_xticks(data.index)\n",
        "\n",
        "        # Leyenda solo en el primer grÃ¡fico\n",
        "        if idx == 0:\n",
        "            ax.legend(title='Modelo', loc='best', frameon=True, fancybox=True, shadow=True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(COMP_DIR / 'metrics_evolution_by_horizon.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Tabla visual de mÃ©tricas\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Preparar datos para la tabla\n",
        "    summary_data = []\n",
        "    experiments = res_df['Experiment'].unique()\n",
        "    models = res_df['Model'].unique()\n",
        "\n",
        "    # Headers\n",
        "    headers = ['Experimento', 'Modelo', 'RMSEâ†“', 'MAEâ†“', 'RÂ²â†‘', 'Mejor H', 'ParÃ¡metros']\n",
        "\n",
        "    for exp in experiments:\n",
        "        for model in models:\n",
        "            exp_model_data = res_df[(res_df['Experiment'] == exp) & (res_df['Model'] == model)]\n",
        "            if not exp_model_data.empty:\n",
        "                avg_rmse = exp_model_data['RMSE'].mean()\n",
        "                avg_mae = exp_model_data['MAE'].mean()\n",
        "                avg_r2 = exp_model_data['R2'].mean()\n",
        "                best_h = exp_model_data.loc[exp_model_data['RMSE'].idxmin(), 'H']\n",
        "                params = exp_model_data['Parameters'].iloc[0]\n",
        "\n",
        "                summary_data.append([\n",
        "                    exp, model,\n",
        "                    f'{avg_rmse:.4f}',\n",
        "                    f'{avg_mae:.4f}',\n",
        "                    f'{avg_r2:.4f}',\n",
        "                    f'H={best_h}',\n",
        "                    f'{params:,}'\n",
        "                ])\n",
        "\n",
        "    # Crear tabla\n",
        "    table = ax.table(cellText=summary_data, colLabels=headers,\n",
        "                    cellLoc='center', loc='center')\n",
        "\n",
        "    # Estilizar tabla\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1.2, 2)\n",
        "\n",
        "    # Colorear celdas segÃºn rendimiento\n",
        "    for i in range(len(summary_data)):\n",
        "        # Obtener valores para comparaciÃ³n\n",
        "        rmse_val = float(summary_data[i][2])\n",
        "        mae_val = float(summary_data[i][3])\n",
        "        r2_val = float(summary_data[i][4])\n",
        "\n",
        "        # Encontrar min/max para normalizaciÃ³n\n",
        "        all_rmse = [float(row[2]) for row in summary_data]\n",
        "        all_mae = [float(row[3]) for row in summary_data]\n",
        "        all_r2 = [float(row[4]) for row in summary_data]\n",
        "\n",
        "        # Normalizar y colorear RMSE (menor es mejor)\n",
        "        rmse_norm = (rmse_val - min(all_rmse)) / (max(all_rmse) - min(all_rmse))\n",
        "        rmse_color = plt.cm.RdYlGn(1 - rmse_norm)\n",
        "        table[(i+1, 2)].set_facecolor(rmse_color)\n",
        "\n",
        "        # Normalizar y colorear MAE (menor es mejor)\n",
        "        mae_norm = (mae_val - min(all_mae)) / (max(all_mae) - min(all_mae))\n",
        "        mae_color = plt.cm.RdYlGn(1 - mae_norm)\n",
        "        table[(i+1, 3)].set_facecolor(mae_color)\n",
        "\n",
        "        # Normalizar y colorear RÂ² (mayor es mejor)\n",
        "        r2_norm = (r2_val - min(all_r2)) / (max(all_r2) - min(all_r2))\n",
        "        r2_color = plt.cm.RdYlGn(r2_norm)\n",
        "        table[(i+1, 4)].set_facecolor(r2_color)\n",
        "\n",
        "        # Colorear experimento\n",
        "        exp_colors = {'BASIC': '#e8f4f8', 'KCE': '#f0e8f8', 'PAFC': '#f8e8f0'}\n",
        "        table[(i+1, 0)].set_facecolor(exp_colors.get(summary_data[i][0], 'white'))\n",
        "\n",
        "    # Colorear headers\n",
        "    for j in range(len(headers)):\n",
        "        table[(0, j)].set_facecolor('#4a86e8')\n",
        "        table[(0, j)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "    plt.title('Resumen de MÃ©tricas por Modelo y Experimento\\n(Verde=Mejor, Rojo=Peor)',\n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "    # AÃ±adir leyenda\n",
        "    plt.text(0.5, -0.05, 'â†“ = Menor es mejor, â†‘ = Mayor es mejor',\n",
        "            transform=ax.transAxes, ha='center', fontsize=10, style='italic')\n",
        "\n",
        "    plt.savefig(COMP_DIR / 'metrics_summary_table.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Identificar el mejor modelo global\n",
        "    print(\"\\nğŸ† MEJOR MODELO GLOBAL:\")\n",
        "    print(\"â”€\" * 50)\n",
        "\n",
        "    # Calcular score compuesto (normalizado)\n",
        "    res_df['score'] = (\n",
        "        (1 - (res_df['RMSE'] - res_df['RMSE'].min()) / (res_df['RMSE'].max() - res_df['RMSE'].min())) +\n",
        "        (1 - (res_df['MAE'] - res_df['MAE'].min()) / (res_df['MAE'].max() - res_df['MAE'].min())) +\n",
        "        ((res_df['R2'] - res_df['R2'].min()) / (res_df['R2'].max() - res_df['R2'].min()))\n",
        "    ) / 3\n",
        "\n",
        "    best_overall = res_df.loc[res_df['score'].idxmax()]\n",
        "    print(f\"Modelo: {best_overall['Model']}\")\n",
        "    print(f\"Experimento: {best_overall['Experiment']}\")\n",
        "    print(f\"Horizonte: {best_overall['H']}\")\n",
        "    print(f\"RMSE: {best_overall['RMSE']:.4f}\")\n",
        "    print(f\"MAE: {best_overall['MAE']:.4f}\")\n",
        "    print(f\"RÂ²: {best_overall['R2']:.4f}\")\n",
        "    print(f\"Score compuesto: {best_overall['score']:.4f}\")\n",
        "\n",
        "    # 4. AnÃ¡lisis de mejora por horizonte\n",
        "    print(\"\\nğŸ“ˆ ANÃLISIS DE MEJORA POR HORIZONTE:\")\n",
        "    print(\"â”€\" * 50)\n",
        "\n",
        "    for h in [1, 2, 3]:\n",
        "        h_data = res_df[res_df['H'] == h]\n",
        "        best_h = h_data.loc[h_data['RMSE'].idxmin()]\n",
        "\n",
        "        print(f\"\\nHorizonte {h}:\")\n",
        "        print(f\"  â€¢ Mejor modelo: {best_h['Model']} - {best_h['Experiment']}\")\n",
        "        print(f\"  â€¢ RMSE: {best_h['RMSE']:.4f}\")\n",
        "        print(f\"  â€¢ RÂ²: {best_h['R2']:.4f}\")\n",
        "\n",
        "print(\"\\nâœ… AnÃ¡lisis detallado completado!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdvrmOioXqrT"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MOSTRAR PREDICCIONES MÃS RECIENTES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\nğŸ–¼ï¸ PREDICCIONES MÃS RECIENTES:\")\n",
        "for exp in EXPERIMENTS.keys():\n",
        "    exp_dir = OUT_ROOT / exp\n",
        "    if exp_dir.exists():\n",
        "        print(f\"\\n{exp}:\")\n",
        "        # Mostrar primera imagen de cada modelo\n",
        "        for model in ADVANCED_MODELS.keys():\n",
        "            img_path = exp_dir / f\"{model}_1.png\"\n",
        "            gif_path = exp_dir / f\"{model}.gif\"\n",
        "\n",
        "            if img_path.exists():\n",
        "                from IPython.display import Image, display\n",
        "                print(f\"  {model} - Primera predicciÃ³n (H=1):\")\n",
        "                display(Image(str(img_path), width=800))\n",
        "\n",
        "            if gif_path.exists():\n",
        "                print(f\"  ğŸ“¹ GIF animado disponible: {gif_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ‰ NOTEBOOK COMPLETADO!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nğŸ“Š Resultados guardados en: {OUT_ROOT}\")\n",
        "if 'res_df' in locals() and res_df is not None and len(res_df) > 0:\n",
        "    print(f\"ğŸ“ˆ MÃ©tricas en: {OUT_ROOT/'metrics_advanced.csv'}\")\n",
        "    print(f\"ğŸ–¼ï¸ Visualizaciones en: {COMP_DIR if 'COMP_DIR' in locals() else 'N/A'}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No se generaron mÃ©tricas en esta ejecuciÃ³n\")\n",
        "print(\"\\nğŸ’¡ PrÃ³ximos pasos:\")\n",
        "print(\"   1. Revisar las mÃ©tricas y seleccionar el mejor modelo\")\n",
        "print(\"   2. Hacer fine-tuning de hiperparÃ¡metros si es necesario\")\n",
        "print(\"   3. Entrenar un ensemble con los mejores modelos\")\n",
        "print(\"   4. Evaluar en datos mÃ¡s recientes o diferentes regiones\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}