{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "635dc005",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "635dc005",
        "outputId": "6debef30-7776-447d-d06b-a618849d92db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entorno configurado. Usando ruta base: ..\n"
          ]
        }
      ],
      "source": [
        "# Configuración del entorno (compatible con Colab y local)\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "# Regenerar el código con las condiciones específicas\n",
        "notebook_globals = {\n",
        "    \"USE_CROSS_VALIDATION\": False,\n",
        "    \"ENABLED_MODELS\": ['CNN', 'GRU'],\n",
        "    \"ENABLED_EXPERIMENTS\": ['time+cycles', 'all_features'],\n",
        "    \"ENABLED_HORIZONS\": [3],\n",
        "}\n",
        "\n",
        "# Detectar si estamos en Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Si estamos en Colab, clonar el repositorio\n",
        "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
        "    %cd ml_precipitation_prediction\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools\n",
        "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
        "else:\n",
        "    # Si estamos en local, usar la ruta actual\n",
        "    if '/models' in os.getcwd():\n",
        "        BASE_PATH = Path('..')\n",
        "    else:\n",
        "        BASE_PATH = Path('.')\n",
        "\n",
        "print(f\"Entorno configurado. Usando ruta base: {BASE_PATH}\")\n",
        "\n",
        "# Si BASE_PATH viene como string, lo convertimos\n",
        "BASE_PATH = Path(BASE_PATH)\n",
        "\n",
        "# Ahora puedes concatenar correctamente\n",
        "data_output_dir = BASE_PATH / 'data' / 'output'\n",
        "model_output_dir = BASE_PATH / 'models' / 'output'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7f8ccc04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "7f8ccc04",
        "outputId": "9c975eb1-2e4a-4abb-f5b1-49c266ec88f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Detectando dispositivo disponible...\n",
            "⚠️ No se detectó GPU. Usando CPU.\n",
            "ℹ️ En Colab puedes activar GPU en Entorno de ejecución > Cambiar tipo de entorno de ejecución.\n",
            "📂 Configurando directorios y cargando dataset...\n",
            "✔️ Dataset cargado desde: ../data/output/complete_dataset_with_features_with_clusters_elevation_with_windows.nc\n",
            "✅ Variables requeridas presentes.\n",
            "\n",
            "🚀 Experimento: time+cycles\n",
            "🔧 Modelo: CNN\n",
            "🕒 Horizonte: 3 meses\n",
            "Epoch 1/20\n",
            "\u001b[1m 9233/87557\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:07\u001b[0m 2ms/step - loss: 9113.4707"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 175\u001b[39m\n\u001b[32m    172\u001b[39m     fold += \u001b[32m1\u001b[39m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m history = model.fit(train_ds,\n\u001b[32m    176\u001b[39m                     validation_data=val_ds,\n\u001b[32m    177\u001b[39m                     epochs=\u001b[32m20\u001b[39m,\n\u001b[32m    178\u001b[39m                     verbose=\u001b[32m1\u001b[39m,\n\u001b[32m    179\u001b[39m                     callbacks=[es])\n\u001b[32m    181\u001b[39m y_pred = model.predict(X_val_fold).flatten()\n\u001b[32m    182\u001b[39m y_true = y_val_fold.flatten()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28mself\u001b[39m.train_function(iterator)\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = multi_step_on_iterator(iterator)\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28mself\u001b[39m._call(*args, **kwds)\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = tracing_compilation.call_function(\n\u001b[32m    879\u001b[39m     args, kwds, \u001b[38;5;28mself\u001b[39m._variable_creation_config\n\u001b[32m    880\u001b[39m )\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m function._call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    140\u001b[39m     flat_inputs, captured_inputs=function.captured_inputs\n\u001b[32m    141\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inference_function.call_preflattened(args)\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28mself\u001b[39m.call_flat(*args)\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m._bound_context.call_function(\n\u001b[32m    252\u001b[39m         \u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m    253\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    254\u001b[39m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.function_type.flat_outputs),\n\u001b[32m    255\u001b[39m     )\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1681\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1682\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1683\u001b[39m   outputs = execute.execute(\n\u001b[32m   1684\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1685\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   1686\u001b[39m       inputs=tensor_inputs,\n\u001b[32m   1687\u001b[39m       attrs=attrs,\n\u001b[32m   1688\u001b[39m       ctx=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1689\u001b[39m   )\n\u001b[32m   1690\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1691\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1692\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1693\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1697\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1698\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Versión final optimizada - Entrenamiento modular y controlado por variables locales\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# ==== Variables de control ====\n",
        "USE_CROSS_VALIDATION = True\n",
        "ENABLED_MODELS = ['CNN', 'GRU']\n",
        "ENABLED_EXPERIMENTS = ['time+cycles', 'all_features']\n",
        "ENABLED_HORIZONS = [3]\n",
        "input_window = 96  # 8 años (mensual)\n",
        "\n",
        "# ==== Configuración de entorno ====\n",
        "print(\"🔍 Detectando dispositivo disponible...\")\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "USE_GPU = bool(gpu_devices)\n",
        "\n",
        "if USE_GPU:\n",
        "    print(\"✅ GPU detectada:\", gpu_devices[0].name)\n",
        "    try:\n",
        "        from tensorflow.keras import mixed_precision\n",
        "        mixed_precision.set_global_policy('mixed_float16')\n",
        "        print(\"⚡ Política 'mixed_float16' activada.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ No se pudo activar mixed precision: {e}\")\n",
        "else:\n",
        "    print(\"⚠️ No se detectó GPU. Usando CPU.\")\n",
        "    print(\"ℹ️ En Colab puedes activar GPU en Entorno de ejecución > Cambiar tipo de entorno de ejecución.\")\n",
        "\n",
        "# ==== Funciones auxiliares ====\n",
        "def build_model(model_type, input_shape, output_neurons):\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, LSTM, GRU, Bidirectional, Reshape, Input\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape))\n",
        "    if model_type == 'LSTM':\n",
        "        model.add(LSTM(64))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(64))\n",
        "    elif model_type == 'BLSTM':\n",
        "        model.add(Bidirectional(LSTM(64)))\n",
        "    elif model_type == 'CNN':\n",
        "        model.add(Reshape((*input_shape, 1)))\n",
        "        model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Flatten())\n",
        "    model.add(Dense(output_neurons))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "def evaluate(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-5))) * 100\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, mape, r2\n",
        "\n",
        "def to_dataset(x, y):\n",
        "    return tf.data.Dataset.from_tensor_slices((x, y)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ==== Directorios y Dataset ====\n",
        "print(\"📂 Configurando directorios y cargando dataset...\")\n",
        "try:\n",
        "    model_output_dir = Path(\"output/ST_HybridWaveStack\")\n",
        "    curves_dir = model_output_dir / \"learning_curves\"\n",
        "\n",
        "    if not model_output_dir.exists():\n",
        "        model_output_dir.mkdir(parents=True)\n",
        "    if not curves_dir.exists():\n",
        "        curves_dir.mkdir(parents=True)\n",
        "\n",
        "    file_path = data_output_dir / \"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
        "    ds = xr.open_dataset(file_path)\n",
        "    print(f\"✔️ Dataset cargado desde: {file_path}\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"❌ Error cargando dataset o creando carpetas: {e}\")\n",
        "\n",
        "# ==== Configuración de experimentos ====\n",
        "experiment_settings = {\n",
        "    \"time+cycles\": ['year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos'],\n",
        "    \"time+cycles+lag\": ['year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
        "                        'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag3',\n",
        "                        'total_precipitation_lag4', 'total_precipitation_lag12', 'total_precipitation_lag24', 'total_precipitation_lag36'],\n",
        "    \"time+cycles+lag+elev\": ['year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
        "                             'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag3',\n",
        "                             'total_precipitation_lag4', 'total_precipitation_lag12', 'total_precipitation_lag24', 'total_precipitation_lag36',\n",
        "                             'elevation', 'slope', 'aspect'],\n",
        "    \"all_features\": ['year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
        "                     'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag3',\n",
        "                     'total_precipitation_lag4', 'total_precipitation_lag12', 'total_precipitation_lag24', 'total_precipitation_lag36',\n",
        "                     'elevation', 'slope', 'aspect', 'cluster_elevation']\n",
        "}\n",
        "\n",
        "# ==== Validación de variables ====\n",
        "ds_vars = set(ds.data_vars)\n",
        "for name, vars_list in experiment_settings.items():\n",
        "    missing = [v for v in vars_list if v not in ds_vars]\n",
        "    if missing:\n",
        "        raise ValueError(f\"❌ Faltan variables necesarias para el experimento '{name}': {missing}\")\n",
        "print(\"✅ Variables requeridas presentes.\")\n",
        "\n",
        "results = []\n",
        "\n",
        "# ==== Entrenamiento modular ====\n",
        "for exp_name, variables in experiment_settings.items():\n",
        "    if exp_name not in ENABLED_EXPERIMENTS:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n🚀 Experimento: {exp_name}\")\n",
        "    try:\n",
        "        cluster_idx = variables.index('cluster_elevation') if 'cluster_elevation' in variables else None\n",
        "        subset = ds[variables].to_array().transpose('time', 'latitude', 'longitude', 'variable').values\n",
        "\n",
        "        if cluster_idx is not None:\n",
        "            encoded = LabelEncoder().fit_transform(subset[..., cluster_idx].ravel()).reshape(subset[..., cluster_idx].shape)\n",
        "            subset[..., cluster_idx] = encoded\n",
        "\n",
        "        subset = subset.astype(np.float32)\n",
        "        target = ds['total_precipitation'].values\n",
        "        samples, lat, lon, feats = subset.shape\n",
        "        X = subset.reshape(samples, lat * lon, feats)\n",
        "        y = target.reshape(samples, lat * lon)\n",
        "        mask = ~np.isnan(y)\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "        if X.shape[0] == 0 or y.shape[0] == 0:\n",
        "            print(f\"⚠️ No hay datos válidos tras aplicar la máscara en '{exp_name}'. Saltando experimento.\")\n",
        "            continue\n",
        "\n",
        "        X_seq = []\n",
        "        Y_targets = {h: [] for h in ENABLED_HORIZONS}\n",
        "        for i in range(len(X) - input_window - max(ENABLED_HORIZONS)):\n",
        "            X_seq.append(X[i:i + input_window])\n",
        "            for h in ENABLED_HORIZONS:\n",
        "                Y_targets[h].append(y[i + input_window + h - 1])\n",
        "        X_seq = np.array(X_seq)\n",
        "        Y_targets = {h: np.array(Y_targets[h]) for h in ENABLED_HORIZONS}\n",
        "\n",
        "        if len(X_seq) == 0:\n",
        "            print(f\"⚠️ No se generaron secuencias para '{exp_name}'. Saltando...\")\n",
        "            continue\n",
        "\n",
        "        input_shape = (X_seq.shape[1], X_seq.shape[2])\n",
        "\n",
        "        for model_name in ENABLED_MODELS:\n",
        "            print(f\"🔧 Modelo: {model_name}\")\n",
        "            for h in ENABLED_HORIZONS:\n",
        "                print(f\"🕒 Horizonte: {h} meses\")\n",
        "                kf = KFold(n_splits=3, shuffle=False)\n",
        "                fold = 1\n",
        "                for train_idx, val_idx in kf.split(X_seq):\n",
        "                    model = build_model(model_name, input_shape, 1)\n",
        "                    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "                    X_train_fold, X_val_fold = X_seq[train_idx], X_seq[val_idx]\n",
        "                    y_train_fold, y_val_fold = Y_targets[h][train_idx], Y_targets[h][val_idx]\n",
        "\n",
        "                    train_ds = to_dataset(X_train_fold, y_train_fold)\n",
        "                    val_ds = to_dataset(X_val_fold, y_val_fold)\n",
        "\n",
        "                    model_path = model_output_dir / f\"{exp_name.replace('+','_')}_{model_name}_H{h}_F{fold}.h5\"\n",
        "                    if model_path.exists():\n",
        "                        print(f\"⏩ Modelo ya existe: {model_path.name}. Saltando...\")\n",
        "                        fold += 1\n",
        "                        continue\n",
        "\n",
        "                    history = model.fit(train_ds,\n",
        "                                        validation_data=val_ds,\n",
        "                                        epochs=20,\n",
        "                                        verbose=1,\n",
        "                                        callbacks=[es])\n",
        "\n",
        "                    y_pred = model.predict(X_val_fold).flatten()\n",
        "                    y_true = y_val_fold.flatten()\n",
        "                    rmse, mae, mape, r2 = evaluate(y_true, y_pred)\n",
        "\n",
        "                    results.append({\n",
        "                        'experiment': exp_name,\n",
        "                        'model': model_name,\n",
        "                        'horizon': h,\n",
        "                        'fold': fold,\n",
        "                        'RMSE': rmse,\n",
        "                        'MAE': mae,\n",
        "                        'MAPE': mape,\n",
        "                        'R2': r2,\n",
        "                        'epochs': len(history.history['loss'])\n",
        "                    })\n",
        "\n",
        "                    # Guardar curva\n",
        "                    plt.figure()\n",
        "                    plt.plot(history.history['loss'], label='Train')\n",
        "                    plt.plot(history.history['val_loss'], label='Val')\n",
        "                    plt.title(f'{exp_name} - {model_name} - H{h} - F{fold}')\n",
        "                    plt.xlabel('Epoch')\n",
        "                    plt.ylabel('Loss')\n",
        "                    plt.legend()\n",
        "                    plt.savefig(curves_dir / f\"{exp_name.replace('+','_')}_{model_name}_H{h}_F{fold}.png\")\n",
        "                    plt.close()\n",
        "\n",
        "                    model.save(model_path)\n",
        "                    print(f\"💾 Guardado: {model_path.name}\")\n",
        "                    fold += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error en experimento '{exp_name}': {e}\")\n",
        "\n",
        "# ==== Guardar resultados ====\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"resultados_modelos_cv_8anios_mvp.csv\", index=False)\n",
        "\n",
        "import ace_tools as tools\n",
        "tools.display_dataframe_to_user(name=\"Resultados CV MVP\", dataframe=results_df)\n",
        "print(\"✅ Resultados guardados y mostrados al usuario.\")\n",
        "print(\"🎉 Proceso finalizado con éxito.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "precipitation_prediction",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
