{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd8a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 22:10:06,847 INFO Cargando datasets‚Ä¶\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entorno configurado. Usando ruta base: ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 22:10:08,310 INFO Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-25 22:10:08,310 INFO Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-25 22:10:08,311 INFO Shapes: prec=(530, 61, 65), da_br=(530, 61, 65, 3), da_lags=(530, 61, 65, 7)\n",
      "2025-05-25 22:10:08,311 INFO Armando ventanas deslizantes con procesamiento por chunks‚Ä¶\n",
      "2025-05-25 22:10:08,311 INFO Procesando chunk de ventanas 0 a 49 de 468\n",
      "2025-05-25 22:10:08,310 INFO Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-25 22:10:08,311 INFO Shapes: prec=(530, 61, 65), da_br=(530, 61, 65, 3), da_lags=(530, 61, 65, 7)\n",
      "2025-05-25 22:10:08,311 INFO Armando ventanas deslizantes con procesamiento por chunks‚Ä¶\n",
      "2025-05-25 22:10:08,311 INFO Procesando chunk de ventanas 0 a 49 de 468\n",
      "2025-05-25 22:10:09,424 INFO Procesando chunk de ventanas 50 a 99 de 468\n",
      "2025-05-25 22:10:09,424 INFO Procesando chunk de ventanas 50 a 99 de 468\n",
      "2025-05-25 22:10:10,485 INFO Procesando chunk de ventanas 100 a 149 de 468\n",
      "2025-05-25 22:10:10,485 INFO Procesando chunk de ventanas 100 a 149 de 468\n",
      "2025-05-25 22:10:11,620 INFO Procesando chunk de ventanas 150 a 199 de 468\n",
      "2025-05-25 22:10:11,620 INFO Procesando chunk de ventanas 150 a 199 de 468\n",
      "2025-05-25 22:10:12,857 INFO Procesando chunk de ventanas 200 a 249 de 468\n",
      "2025-05-25 22:10:12,857 INFO Procesando chunk de ventanas 200 a 249 de 468\n",
      "2025-05-25 22:10:13,953 INFO Procesando chunk de ventanas 250 a 299 de 468\n",
      "2025-05-25 22:10:13,953 INFO Procesando chunk de ventanas 250 a 299 de 468\n",
      "2025-05-25 22:10:15,081 INFO Procesando chunk de ventanas 300 a 349 de 468\n",
      "2025-05-25 22:10:15,081 INFO Procesando chunk de ventanas 300 a 349 de 468\n",
      "2025-05-25 22:10:16,159 INFO Procesando chunk de ventanas 350 a 399 de 468\n",
      "2025-05-25 22:10:16,159 INFO Procesando chunk de ventanas 350 a 399 de 468\n",
      "2025-05-25 22:10:17,113 INFO Procesando chunk de ventanas 400 a 449 de 468\n",
      "2025-05-25 22:10:17,113 INFO Procesando chunk de ventanas 400 a 449 de 468\n",
      "2025-05-25 22:10:18,555 INFO Procesando chunk de ventanas 450 a 467 de 468\n",
      "2025-05-25 22:10:18,555 INFO Procesando chunk de ventanas 450 a 467 de 468\n",
      "2025-05-25 22:10:58,002 INFO Ventanas v√°lidas totales: 432\n",
      "2025-05-25 22:10:58,002 INFO Ventanas v√°lidas totales: 432\n",
      "2025-05-25 22:10:58,032 INFO Escalado de features y codificaci√≥n de cluster‚Ä¶\n",
      "2025-05-25 22:10:58,032 INFO Escalado de features y codificaci√≥n de cluster‚Ä¶\n",
      "2025-05-25 22:14:17,500 INFO Forma de matriz de topograf√≠a+cluster: (3965, 5)\n",
      "2025-05-25 22:14:17,500 INFO Forma de matriz de topograf√≠a+cluster: (3965, 5)\n",
      "2025-05-25 22:14:17,519 INFO Split train=302, val=130\n",
      "2025-05-25 22:14:17,531 INFO Entrenando LSTM‚Ä¶\n",
      "2025-05-25 22:14:17,519 INFO Split train=302, val=130\n",
      "2025-05-25 22:14:17,531 INFO Entrenando LSTM‚Ä¶\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 22:29:08,792 INFO Entrenando GRU‚Ä¶\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 22:41:34,828 INFO Entrenando MLP‚Ä¶\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloque√≥ al ejecutar c√≥digo en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el c√≥digo de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqu√≠</a> para obtener m√°s informaci√≥n. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener m√°s detalles."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TopoRain-Net: entrenamiento y evaluaci√≥n de modelos base (LSTM, GRU, MLP, XGB)\n",
    "y meta-modelo MLP multisalida sobre features_fusion_branches + lags + topograf√≠a.\n",
    "Genera m√©tricas, scatter, mapas y tablas (global, por elevaci√≥n, por percentiles).\n",
    "\"\"\"\n",
    "\n",
    "import warnings, logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy            as np\n",
    "import pandas           as pd\n",
    "import xarray           as xr\n",
    "import geopandas        as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs      as ccrs\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics        import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost                import XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models    import Sequential\n",
    "from tensorflow.keras.layers    import Input, Dense, LSTM, GRU, Flatten, Reshape, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import ace_tools_open as tools\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuraci√≥n y rutas\n",
    "# -----------------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuraci√≥n del entorno (compatible con Colab y local)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Detectar si estamos en Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Si estamos en Colab, clonar el repositorio\n",
    "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
    "    %cd ml_precipitation_prediction\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn\n",
    "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
    "else:\n",
    "    # Si estamos en local, usar la ruta actual\n",
    "    if '/models' in os.getcwd():\n",
    "        BASE_PATH = Path('..')\n",
    "    else:\n",
    "        BASE_PATH = Path('.')\n",
    "\n",
    "BASE = Path(BASE_PATH)\n",
    "print(f\"Entorno configurado. Usando ruta base: {BASE}\")\n",
    "\n",
    "FULL_NC      = BASE/\"data\"/\"output\"/\"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
    "FUSION_NC    = BASE/\"models\"/\"output\"/\"features_fusion_branches.nc\"\n",
    "TRAINED_DIR  = BASE/\"models\"/\"output\"/\"trained_models\"\n",
    "TRAINED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INPUT_WINDOW   = 60\n",
    "OUTPUT_HORIZON = 3\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Carga de datos\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Cargando datasets‚Ä¶\")\n",
    "ds_full = xr.open_dataset(FULL_NC)\n",
    "ds_fuse = xr.open_dataset(FUSION_NC)\n",
    "\n",
    "# precipitacion y variables\n",
    "prec    = ds_full[\"total_precipitation\"].values            # (T, ny, nx)\n",
    "lags    = sorted([v for v in ds_full.data_vars if \"_lag\" in v])\n",
    "da_lags = np.stack([ds_full[lag].values for lag in lags], axis=-1)  # (T, ny, nx, n_lags)\n",
    "\n",
    "# ramas fusionadas\n",
    "branches = [\"FUSION_high\", \"FUSION_medium\", \"FUSION_low\"]\n",
    "# Asegur√©monos de que da_br sea un ndarray correcto\n",
    "da_br = np.stack([ds_fuse[branch].values for branch in branches], axis=-1)  # (T, ny, nx, 3)\n",
    "\n",
    "# topograf√≠a y cluster\n",
    "elev    = ds_full[\"elevation\"].values.ravel()               # (cells,)\n",
    "slope   = ds_full[\"slope\"].values.ravel()\n",
    "\n",
    "# Manejar correctamente los valores de cluster (pueden ser texto)\n",
    "cluster_values = ds_full[\"cluster_elevation\"].values.ravel()\n",
    "# Verificar si los valores son strings o num√©ricos\n",
    "if isinstance(cluster_values[0], (str, np.str_)):\n",
    "    # Usar un LabelEncoder para convertir strings a enteros\n",
    "    le = LabelEncoder()\n",
    "    cluster = le.fit_transform(cluster_values)\n",
    "    logger.info(f\"Clusters codificados de texto a n√∫meros: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    # Si ya son num√©ricos, convertir a enteros\n",
    "    cluster = cluster_values.astype(int)\n",
    "\n",
    "# dimensiones\n",
    "lat     = ds_full.latitude.values\n",
    "lon     = ds_full.longitude.values\n",
    "ny, nx  = len(lat), len(lon)\n",
    "cells   = ny*nx\n",
    "T       = prec.shape[0]\n",
    "\n",
    "logger.info(f\"Dimensiones: T={T}, ny={ny}, nx={nx}, cells={cells}\")\n",
    "logger.info(f\"Shapes: prec={prec.shape}, da_br={da_br.shape}, da_lags={da_lags.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Ventanas deslizantes (implementaci√≥n escalable con chunks)\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Armando ventanas deslizantes con procesamiento por chunks‚Ä¶\")\n",
    "\n",
    "# Definir el tama√±o de chunks para procesamiento por lotes\n",
    "CHUNK_SIZE = 50  # Ajustar seg√∫n capacidad de memoria disponible\n",
    "\n",
    "# N√∫mero total de ventanas posibles\n",
    "n_windows = T - INPUT_WINDOW - OUTPUT_HORIZON + 1\n",
    "Xw, Yw = [], []\n",
    "\n",
    "# Procesar por chunks para evitar problemas de memoria\n",
    "for chunk_start in range(0, n_windows, CHUNK_SIZE):\n",
    "    chunk_end = min(chunk_start + CHUNK_SIZE, n_windows)\n",
    "    logger.info(f\"Procesando chunk de ventanas {chunk_start} a {chunk_end-1} de {n_windows}\")\n",
    "    \n",
    "    # Crear ventanas para este chunk\n",
    "    chunk_Xw, chunk_Yw = [], []\n",
    "    \n",
    "    for i in range(chunk_start, chunk_end):\n",
    "        # Stack de features en ventana\n",
    "        # Branches\n",
    "        bwin = da_br[i:i+INPUT_WINDOW].reshape(INPUT_WINDOW, cells, 3)\n",
    "        # Lags\n",
    "        lwin = da_lags[i:i+INPUT_WINDOW].reshape(INPUT_WINDOW, cells, len(lags))\n",
    "        # Concatenar\n",
    "        feat = np.concatenate([bwin, lwin], axis=-1)            # (W, cells, F)\n",
    "        chunk_Xw.append(feat.reshape(INPUT_WINDOW, cells*feat.shape[-1]))\n",
    "        # Targets\n",
    "        tw = [prec[i+INPUT_WINDOW+h].reshape(cells) for h in range(OUTPUT_HORIZON)]\n",
    "        chunk_Yw.append(np.stack(tw,axis=0))                    # (H, cells)\n",
    "    \n",
    "    # Convertir a arrays y aplicar filtro de NaNs dentro del chunk\n",
    "    chunk_X = np.stack(chunk_Xw)                              # (chunk_size, W, cells*F)\n",
    "    chunk_Y = np.stack(chunk_Yw)                              # (chunk_size, H, cells)\n",
    "    \n",
    "    # Filtrar NaNs en este chunk\n",
    "    chunk_mask = (~np.isnan(chunk_X).any(axis=(1,2))) & (~np.isnan(chunk_Y).any(axis=(1,2)))\n",
    "    valid_X = chunk_X[chunk_mask]\n",
    "    valid_Y = chunk_Y[chunk_mask]\n",
    "    \n",
    "    # A√±adir los datos v√°lidos de este chunk a las listas principales\n",
    "    if len(valid_X) > 0:\n",
    "        Xw.append(valid_X)\n",
    "        Yw.append(valid_Y)\n",
    "    \n",
    "    # Limpiar memoria expl√≠citamente\n",
    "    del chunk_Xw, chunk_Yw, chunk_X, chunk_Y, valid_X, valid_Y\n",
    "    if 'gc' in sys.modules:\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "# Concatenar todos los chunks\n",
    "X = np.vstack(Xw) if Xw else np.array([])  # (N, W, cells*F)\n",
    "Y = np.vstack(Yw) if Yw else np.array([])  # (N, H, cells)\n",
    "N = len(X)\n",
    "\n",
    "logger.info(f\"Ventanas v√°lidas totales: {N}\")\n",
    "\n",
    "# Opcional: Guardar en disco para futuros usos\n",
    "# np.save(BASE/\"models\"/\"output\"/\"ventanas_X.npy\", X)\n",
    "# np.save(BASE/\"models\"/\"output\"/\"ventanas_Y.npy\", Y)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Escalado + one-hot de cluster\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Escalado de features y codificaci√≥n de cluster‚Ä¶\")\n",
    "# escalar X\n",
    "scX = StandardScaler()\n",
    "Xf  = scX.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "\n",
    "# preparar topograf√≠a+cluster (repite por celda)\n",
    "try:\n",
    "    # Para versiones m√°s recientes de scikit-learn\n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "except TypeError:\n",
    "    # Para versiones anteriores de scikit-learn\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    \n",
    "c_ohe  = ohe.fit_transform(cluster.reshape(-1,1))         # (cells, n_clusters)\n",
    "topo   = np.hstack([elev.reshape(-1,1), slope.reshape(-1,1), c_ohe])  # (cells, 2+n_clusters)\n",
    "logger.info(f\"Forma de matriz de topograf√≠a+cluster: {topo.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Train/val split\n",
    "# -----------------------------------------------------------------------------\n",
    "split   = int(0.7*N)\n",
    "X_tr    = Xf[:split];    X_va = Xf[split:]\n",
    "Y_tr    = Y[:split];     Y_va = Y[split:]\n",
    "logger.info(f\"Split train={len(X_tr)}, val={len(X_va)}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Entrenamiento de modelos base\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_ts_model(kind):\n",
    "    m = Sequential([ Input(shape=(INPUT_WINDOW,X_tr.shape[-1])) ])\n",
    "    if kind==\"LSTM\": m.add(LSTM(64))\n",
    "    elif kind==\"GRU\": m.add(GRU(64))\n",
    "    elif kind==\"MLP\":\n",
    "        m.add(Flatten())\n",
    "        m.add(Dense(128,activation=\"relu\"))\n",
    "    m.add(Dense(OUTPUT_HORIZON*cells))\n",
    "    m.add(Reshape((OUTPUT_HORIZON,cells)))\n",
    "    m.compile(\"adam\",\"mse\")\n",
    "    return m\n",
    "\n",
    "BASES   = [\"LSTM\",\"GRU\",\"MLP\",\"XGB\"]\n",
    "pred_va = {}\n",
    "pred_fc = {}\n",
    "\n",
    "# TensorFlow models\n",
    "for b in [\"LSTM\",\"GRU\",\"MLP\"]:\n",
    "    logger.info(f\"Entrenando {b}‚Ä¶\")\n",
    "    m = build_ts_model(b)\n",
    "    m.fit(X_tr, Y_tr,\n",
    "          validation_data=(X_va,Y_va),\n",
    "          epochs=100, batch_size=16,\n",
    "          callbacks=[EarlyStopping(\"val_loss\",patience=10,restore_best_weights=True)],\n",
    "          verbose=0)\n",
    "    pred_va[b] = m.predict(X_va)             # (Nv,H,cells)\n",
    "    pred_fc[b] = m.predict(Xf[-1:])[0]       # (H,cells)\n",
    "\n",
    "# XGBoost por horizonte\n",
    "for h in range(OUTPUT_HORIZON):\n",
    "    logger.info(f\"Entrenando XGB H={h+1}‚Ä¶\")\n",
    "    xgb = XGBRegressor(n_estimators=100, max_depth=5, verbosity=0)\n",
    "    # entrenar\n",
    "    xgb.fit(X_tr.reshape(-1,X_tr.shape[-1]), Y_tr[:,h].ravel())\n",
    "    # preds\n",
    "    pv = xgb.predict(X_va.reshape(-1,X_va.shape[-1])).reshape(-1,cells)\n",
    "    fc = xgb.predict(Xf[-1:].reshape(-1,Xf.shape[-1])).ravel()\n",
    "    pred_va.setdefault(\"XGB\",[]).append(pv)\n",
    "    pred_fc.setdefault(\"XGB\",[]).append(fc)\n",
    "\n",
    "# apilar XGB preds ‚Üí (Nv,H,cells)\n",
    "pred_va[\"XGB\"] = np.stack(pred_va[\"XGB\"],axis=1)\n",
    "pred_fc[\"XGB\"] = np.stack(pred_fc[\"XGB\"],axis=0)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Evaluaci√≥n Base-models\n",
    "# -----------------------------------------------------------------------------\n",
    "rows=[]\n",
    "for b in BASES:\n",
    "    pv = pred_va[b]\n",
    "    for h in range(OUTPUT_HORIZON):\n",
    "        yt = Y_va[:,h,:].ravel()\n",
    "        yp = pv[:,h,:].ravel()\n",
    "        rows.append({\n",
    "            \"model\":b, \"horizon\":h+1,\n",
    "            \"RMSE\":np.sqrt(mean_squared_error(yt,yp)),\n",
    "            \"MAE\": mean_absolute_error(yt,yp),\n",
    "            \"MAPE\":np.mean(np.abs((yt-yp)/(yt+1e-5)))*100,\n",
    "            \"R2\":  r2_score(yt,yp)\n",
    "        })\n",
    "df_base = pd.DataFrame(rows)\n",
    "tools.display_dataframe_to_user(\"Base_models_metrics\", df_base)\n",
    "\n",
    "# scatter y mapas Base-models\n",
    "grid_lon,grid_lat = np.meshgrid(lon,lat)\n",
    "for b in BASES:\n",
    "    pv = pred_va[b]\n",
    "    for h in range(OUTPUT_HORIZON):\n",
    "        # scatter\n",
    "        yt,yp = Y_va[:,h,:].ravel(), pv[:,h,:].ravel()\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.scatter(yt,yp,s=2,alpha=0.3)\n",
    "        mn,mx = yt.min(),yp.max()\n",
    "        plt.plot([mn,mx],[mn,mx],'k--')\n",
    "        plt.title(f\"{b} True vs Pred H={h+1}\")\n",
    "        plt.show()\n",
    "        # mapas H\n",
    "        pm = yp.reshape(-1,cells).reshape(-1,ny,nx)[0]\n",
    "        tm = Y_va[:,h,:].reshape(-1,cells).reshape(-1,ny,nx)[0]\n",
    "        err= np.abs((tm-pm)/(tm+1e-5))*100\n",
    "        fig,ax = plt.subplots(1,2,figsize=(10,4),\n",
    "                              subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
    "        ax[0].pcolormesh(grid_lon,grid_lat,pm,transform=ccrs.PlateCarree(),cmap=\"Blues\")\n",
    "        ax[0].set_title(f\"{b} Pred H={h+1}\")\n",
    "        ax[1].pcolormesh(grid_lon,grid_lat,err,transform=ccrs.PlateCarree(),\n",
    "                         cmap=\"Reds\",vmin=0,vmax=100)\n",
    "        ax[1].set_title(f\"{b} MAPE% H={h+1}\")\n",
    "        plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Preparar datos para Meta-modelo\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Armando caracter√≠sticas para meta-modelo‚Ä¶\")\n",
    "X_meta_va = []\n",
    "for i in range(len(X_va)):\n",
    "    feats = []\n",
    "    for b in BASES:\n",
    "        feats.append(pred_va[b][i])                 # (H,cells)\n",
    "    stack = np.vstack(feats)                       # (B*H, cells)\n",
    "    vec   = stack.T.flatten()                      # (cells*B*H,)\n",
    "    topo_flat = topo.flatten()                     # (cells*topo_dim,)\n",
    "    X_meta_va.append(np.hstack([vec, topo_flat]))\n",
    "X_meta_va = np.stack(X_meta_va)                    # (Nv, ...)\n",
    "\n",
    "# forecast meta\n",
    "stack_fc = []\n",
    "for b in BASES:\n",
    "    stack_fc.append(pred_fc[b])                     # (H,cells)\n",
    "stack_fc = np.vstack(stack_fc)                      # (B*H, cells)\n",
    "Xm_fc   = np.hstack([stack_fc.T.flatten(), topo.flatten()])[None,:]  # (1,...)\n",
    "\n",
    "# target meta\n",
    "Y_meta_va = Y_va.reshape(len(X_va), -1)            # (Nv, H*cells)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8) Entrenamiento Meta-modelo MLP\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Entrenando meta-modelo‚Ä¶\")\n",
    "meta = Sequential([\n",
    "    Input(shape=(X_meta_va.shape[-1],)),\n",
    "    Dense(256,activation=\"relu\"), Dropout(0.4),\n",
    "    Dense(128,activation=\"relu\"),\n",
    "    Dense(Y_meta_va.shape[-1])\n",
    "])\n",
    "meta.compile(\"adam\",\"mse\")\n",
    "meta.fit(X_meta_va, Y_meta_va,\n",
    "         validation_split=0.2,\n",
    "         epochs=200, batch_size=32,\n",
    "         callbacks=[EarlyStopping(\"val_loss\",patience=20,restore_best_weights=True)],\n",
    "         verbose=0)\n",
    "\n",
    "# preds meta\n",
    "P_meta_va = meta.predict(X_meta_va)               # (Nv, H*cells)\n",
    "P_meta_fc = meta.predict(Xm_fc)                   # (1, H*cells)\n",
    "\n",
    "Y_meta_va = P_meta_va.reshape(-1,OUTPUT_HORIZON,cells)\n",
    "Y_meta_fc = P_meta_fc.reshape(OUTPUT_HORIZON,cells)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9) Evaluaci√≥n Meta-modelo\n",
    "# -----------------------------------------------------------------------------\n",
    "rows=[]\n",
    "for h in range(OUTPUT_HORIZON):\n",
    "    yt = Y_va[:,h,:].ravel(); yp = Y_meta_va[:,h,:].ravel()\n",
    "    rows.append({\n",
    "        \"horizon\":h+1,\n",
    "        \"RMSE\":np.sqrt(mean_squared_error(yt,yp)),\n",
    "        \"MAE\": mean_absolute_error(yt,yp),\n",
    "        \"MAPE\":np.mean(np.abs((yt-yp)/(yt+1e-5)))*100,\n",
    "        \"R2\":  r2_score(yt,yp)\n",
    "    })\n",
    "df_meta = pd.DataFrame(rows)\n",
    "tools.display_dataframe_to_user(\"Meta_model_metrics\", df_meta)\n",
    "\n",
    "# scatter y mapas Meta\n",
    "for h in range(OUTPUT_HORIZON):\n",
    "    yt,yp = Y_va[:,h,:].ravel(), Y_meta_va[:,h,:].ravel()\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.scatter(yt,yp,s=2,alpha=0.3)\n",
    "    mn,mx = yt.min(),yp.max()\n",
    "    plt.plot([mn,mx],[mn,mx],'k--')\n",
    "    plt.title(f\"Meta True vs Pred H={h+1}\"); plt.show()\n",
    "\n",
    "    pm = yp.reshape(-1,cells).reshape(-1,ny,nx)[0]\n",
    "    tm = Y_va[:,h,:].reshape(-1,cells).reshape(-1,ny,nx)[0]\n",
    "    err= np.abs((tm-pm)/(tm+1e-5))*100\n",
    "    fig,ax = plt.subplots(1,2,figsize=(10,4),\n",
    "                          subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
    "    ax[0].pcolormesh(grid_lon,grid_lat,pm,transform=ccrs.PlateCarree(),cmap=\"Blues\")\n",
    "    ax[0].set_title(f\"Meta Pred H={h+1}\")\n",
    "    ax[1].pcolormesh(grid_lon,grid_lat,err,transform=ccrs.PlateCarree(),\n",
    "                     cmap=\"Reds\",vmin=0,vmax=100)\n",
    "    ax[1].set_title(f\"Meta MAPE% H={h+1}\")\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 10) M√©tricas desagregadas por elevaci√≥n y percentiles\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Calculando m√©tricas por elevaci√≥n y percentiles‚Ä¶\")\n",
    "mask_low  = elev < 200\n",
    "mask_mid  = (elev>=200)&(elev<=1000)\n",
    "mask_high = elev>1000\n",
    "\n",
    "elev_rows = []\n",
    "pct_rows  = []\n",
    "for h in range(OUTPUT_HORIZON):\n",
    "    yt = Y_va[:,h,:].ravel(); yp = Y_meta_va[:,h,:].ravel()\n",
    "    # elevaci√≥n\n",
    "    for name, m in zip([\"<200m\",\"200-1000m\",\">1000m\"], [mask_low,mask_mid,mask_high]):\n",
    "        yt_m,yp_m = yt[m], yp[m]\n",
    "        elev_rows.append({\n",
    "            \"horizon\":h+1, \"region\":name,\n",
    "            \"RMSE\":np.sqrt(mean_squared_error(yt_m,yp_m)),\n",
    "            \"MAE\": mean_absolute_error(yt_m,yp_m),\n",
    "            \"MAPE\":np.mean(np.abs((yt_m-yp_m)/(yt_m+1e-5)))*100,\n",
    "            \"R2\":  r2_score(yt_m,yp_m)\n",
    "        })\n",
    "    # percentiles\n",
    "    edges = [0,25,50,75,100]\n",
    "    pcts  = np.percentile(yt, edges)\n",
    "    for i in range(4):\n",
    "        lo,hi = pcts[i],pcts[i+1]\n",
    "        idx   = (yt>=lo)&(yt<hi)\n",
    "        yt_p, yp_p = yt[idx], yp[idx]\n",
    "        pct_rows.append({\n",
    "            \"horizon\":h+1,\n",
    "            \"pct_range\":f\"{edges[i]}-{edges[i+1]}%\",\n",
    "            \"RMSE\":np.sqrt(mean_squared_error(yt_p,yp_p)),\n",
    "            \"MAE\": mean_absolute_error(yt_p,yp_p),\n",
    "            \"MAPE\":np.mean(np.abs((yt_p-yp_p)/(yt_p+1e-5)))*100,\n",
    "            \"R2\":  r2_score(yt_p,yp_p)\n",
    "        })\n",
    "\n",
    "df_elev = pd.DataFrame(elev_rows)\n",
    "df_pct  = pd.DataFrame(pct_rows)\n",
    "tools.display_dataframe_to_user(\"Meta_by_elevation\", df_elev)\n",
    "tools.display_dataframe_to_user(\"Meta_by_percentile\", df_pct)\n",
    "\n",
    "logger.info(\"üéâ Proceso completado con √©xito.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
