{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/base_models_gnn_convlstm_stacking_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab-badge"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/main/models/base_models_gnn_convlstm_stacking_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# V5 GNN-ConvLSTM Stacking: Meta-Learner Ensemble for Spatiotemporal Precipitation Prediction\n",
    "\n",
    "## Novel Hybrid Architecture - Dual-Branch Stacking with Interpretable Meta-Learner\n",
    "\n",
    "**Version:** 5.0  \n",
    "**Date:** January 2026  \n",
    "**Author:** Manuel Perez  \n",
    "**Institution:** UPTC - Doctoral Thesis in Engineering  \n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "V5 combines the complementary strengths of V2 (ConvLSTM) and V4 (GNN-TAT) through a stacking ensemble:\n",
    "\n",
    "| Component | Description | Innovation |\n",
    "|-----------|-------------|------------|\n",
    "| **Branch 1: ConvLSTM** | Euclidean spatial patterns (BASIC features) | Local spatiotemporal extraction |\n",
    "| **Branch 2: GNN-TAT** | Non-Euclidean topographic relations (KCE features) | Graph-based orographic modeling |\n",
    "| **Grid-Graph Fusion** | Cross-attention between representations | Novel grid-graph alignment |\n",
    "| **Meta-Learner** | Interpretable weighted fusion | Context-dependent branch weighting |\n",
    "\n",
    "### Target Performance (from spec.md)\n",
    "\n",
    "| Metric | V4 Baseline | V5 Target | V5 Excellent |\n",
    "|--------|-------------|-----------|---------------|\n",
    "| R^2 (H1-H6) | 0.628 | > 0.65 | > 0.70 |\n",
    "| RMSE (mm) | 92.12 | < 85 | < 80 |\n",
    "| Parameters | 98K | < 200K | < 180K |\n",
    "\n",
    "### Innovation Status\n",
    "**NOVEL CONTRIBUTION:** No existing Q1 publications combine GNN and ConvLSTM in stacking ensemble for precipitation prediction (verified January 2026)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-1"
   },
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "setup-environment",
    "outputId": "2f41b2ea-4e80-4f42-ae9e-20ab0af5715f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Google Colab: True\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Installing PyG wheels from: https://data.pyg.org/whl/torch-2.9.0+cu126.html\n",
      "Output directory: /content/drive/MyDrive/ml_precipitation_prediction/models/output/V5_GNN_ConvLSTM_Stacking\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 1.1: ENVIRONMENT DETECTION AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Install required packages (match PyTorch/CUDA build)\n",
    "    import torch\n",
    "\n",
    "    torch_version = torch.__version__.split('+')[0]\n",
    "    cuda_version = torch.version.cuda\n",
    "    if cuda_version:\n",
    "        cuda_tag = f\"cu{cuda_version.replace('.', '')}\"\n",
    "    else:\n",
    "        cuda_tag = \"cpu\"\n",
    "    pyg_url = f\"https://data.pyg.org/whl/torch-{torch_version}+{cuda_tag}.html\"\n",
    "    print(f\"Installing PyG wheels from: {pyg_url}\")\n",
    "    !pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f {pyg_url}\n",
    "    !pip install -q netCDF4 xarray dask h5netcdf\n",
    "\n",
    "    # Set base paths for Colab\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    DRIVE_DATA_FILE = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "    LOCAL_DATA_FILE = Path('/content/complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc')\n",
    "    OUTPUT_ROOT = BASE_PATH / 'models' / 'output' / 'V5_GNN_ConvLSTM_Stacking'\n",
    "\n",
    "    # Copy dataset to local for faster access\n",
    "    if not LOCAL_DATA_FILE.exists():\n",
    "        !cp \"{DRIVE_DATA_FILE}\" \"{LOCAL_DATA_FILE}\"\n",
    "        print(\"Dataset copied to local storage for faster access\")\n",
    "    DATA_FILE = LOCAL_DATA_FILE\n",
    "else:\n",
    "    # Local paths\n",
    "    BASE_PATH = Path('d:/github.com/ninja-marduk/ml_precipitation_prediction')\n",
    "    DATA_FILE = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "    OUTPUT_ROOT = BASE_PATH / 'models' / 'output' / 'V5_GNN_ConvLSTM_Stacking'\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "imports",
    "outputId": "fd8d44b1-6703-46b3-af32-d482481a62cf"
   },
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1402773363.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Set seeds for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mSEED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_initialization_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mcb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mdefault_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nSearch for `cudaErrorAssert' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 1.2: IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "import gc\n",
    "import copy\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "except ImportError:\n",
    "    gpd = None\n",
    "\n",
    "try:\n",
    "    import imageio.v2 as imageio\n",
    "except ImportError:\n",
    "    imageio = None\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed: int) -> bool:\n",
    "    np.random.seed(seed)\n",
    "    torch._C._manual_seed(seed)\n",
    "    cuda_ok = False\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            cuda_ok = True\n",
    "        except Exception as exc:\n",
    "            print(f\"[WARN] CUDA seed failed: {exc}\")\n",
    "    return cuda_ok\n",
    "\n",
    "CUDA_READY = seed_everything(SEED)\n",
    "USE_CUDA = torch.cuda.is_available() and CUDA_READY\n",
    "\n",
    "if USE_CUDA:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if USE_CUDA:\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "elif torch.cuda.is_available() and not CUDA_READY:\n",
    "    print(\"[WARN] CUDA is in a bad state. Restart the runtime or force CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-2"
   },
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 2: V5 CONFIGURATION (from spec.md Section 3.3)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class V5Config:\n",
    "    \"\"\"Complete V5 GNN-ConvLSTM Stacking configuration.\"\"\"\n",
    "\n",
    "    # Data configuration\n",
    "    input_window: int = 60      # Input months\n",
    "    horizon: int = 12           # Prediction months\n",
    "    train_val_split: float = 0.8\n",
    "\n",
    "    # Light mode for testing\n",
    "    light_mode: bool = False     # Set to False for full grid\n",
    "    light_grid_size: int = 5    # Grid subset size when light_mode=True\n",
    "\n",
    "    # Enabled horizons for experiments\n",
    "    enabled_horizons: List[int] = field(default_factory=lambda: [12])\n",
    "\n",
    "    # Branch 1: ConvLSTM configuration (V2 Enhanced)\n",
    "    convlstm_filters: List[int] = field(default_factory=lambda: [32, 16])\n",
    "    convlstm_kernel_size: int = 3\n",
    "    convlstm_attention: bool = True\n",
    "    convlstm_bidirectional: bool = True\n",
    "    convlstm_residual: bool = True\n",
    "    convlstm_output_dim: int = 64\n",
    "\n",
    "    # Branch 2: GNN-TAT configuration (V4)\n",
    "    gnn_type: str = 'GAT'           # GAT, SAGE, or GCN\n",
    "    gnn_hidden_dim: int = 64\n",
    "    gnn_num_layers: int = 2\n",
    "    gnn_num_heads: int = 4          # For GAT\n",
    "    gnn_temporal_heads: int = 4\n",
    "    gnn_lstm_hidden: int = 64\n",
    "    gnn_lstm_layers: int = 2\n",
    "    gnn_output_dim: int = 64\n",
    "    gnn_dropout: float = 0.1\n",
    "\n",
    "    # Graph construction\n",
    "    edge_threshold: float = 0.3\n",
    "    max_neighbors: int = 8\n",
    "    use_distance_edges: bool = True\n",
    "    use_elevation_edges: bool = True\n",
    "    use_correlation_edges: bool = True\n",
    "    distance_scale_km: float = 10.0\n",
    "    elevation_weight: float = 0.3\n",
    "    correlation_weight: float = 0.5\n",
    "    elevation_scale_m: float = 500.0\n",
    "    min_edge_weight: float = 0.01\n",
    "\n",
    "    # Grid-Graph Fusion\n",
    "    fusion_type: str = 'auto'       # cross_attention, gated, or auto\n",
    "    fusion_heads: int = 4\n",
    "    fusion_hidden_dim: int = 64\n",
    "    fusion_dropout: float = 0.1\n",
    "    fusion_max_nodes: int = 1024\n",
    "\n",
    "    # Meta-Learner\n",
    "    meta_hidden_dim: int = 128\n",
    "    meta_dropout: float = 0.1\n",
    "    meta_use_context_features: bool = True\n",
    "    meta_context_features: List[str] = field(\n",
    "        default_factory=lambda: ['mean_elevation', 'elevation_cluster', 'temporal_regime']\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 200\n",
    "    batch_size: int = 4\n",
    "    learning_rate: float = 0.0005\n",
    "    weight_decay: float = 1e-4\n",
    "    patience: int = 60\n",
    "    gradient_clip: float = 1.0\n",
    "    # Branch weight regularization\n",
    "    weight_floor: float = 0.1\n",
    "    weight_reg_lambda: float = 0.02\n",
    "\n",
    "    # Model sizing\n",
    "    compact_model: bool = True\n",
    "\n",
    "\n",
    "    # Export/visualization alignment\n",
    "    export_predictions: bool = True\n",
    "    generate_map_plots: bool = True\n",
    "    map_cycle_all_val_windows: bool = False\n",
    "    map_sample_index: int = -1\n",
    "    map_export_dpi: int = 300\n",
    "    map_gif_duration: float = 1.5\n",
    "    plot_graph_diagnostics: bool = True\n",
    "\n",
    "    # Quality checks / gates\n",
    "    max_bias_pct: float = 10.0\n",
    "    max_scale_ratio: float = 50.0\n",
    "    max_negative_frac: float = 0.001\n",
    "    min_branch_weight: float = 0.05\n",
    "    param_budget: int = 200000\n",
    "    enforce_quality_gates: bool = False\n",
    "    allow_missing_features: bool = False\n",
    "    allow_overlap_windows: bool = False\n",
    "    correlation_train_only: bool = True\n",
    "\n",
    "# Initialize config\n",
    "CONFIG = V5Config()\n",
    "\n",
    "# Run-mode overrides\n",
    "RUN_FULL_DATASET = True  # Set False for quick smoke tests\n",
    "if RUN_FULL_DATASET:\n",
    "    CONFIG.light_mode = False\n",
    "    CONFIG.enabled_horizons = [12]\n",
    "    CONFIG.horizon = 12\n",
    "\n",
    "if CONFIG.compact_model:\n",
    "    CONFIG.convlstm_filters = [16, 8]\n",
    "    CONFIG.convlstm_output_dim = 32\n",
    "    CONFIG.gnn_hidden_dim = 32\n",
    "    CONFIG.gnn_num_heads = 2\n",
    "    CONFIG.gnn_temporal_heads = 2\n",
    "    CONFIG.gnn_lstm_hidden = 32\n",
    "    CONFIG.gnn_output_dim = 32\n",
    "    CONFIG.fusion_hidden_dim = 32\n",
    "    CONFIG.fusion_heads = 2\n",
    "    CONFIG.meta_hidden_dim = 64\n",
    "\n",
    "print(\"V5 Configuration initialized:\")\n",
    "print(f\"  - Light mode: {CONFIG.light_mode}\")\n",
    "print(f\"  - GNN type: {CONFIG.gnn_type}\")\n",
    "print(f\"  - Enabled horizons: {CONFIG.enabled_horizons}\")\n",
    "print(f\"  - Epochs: {CONFIG.epochs}\")\n",
    "print(f\"  - Fusion type: {CONFIG.fusion_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-3"
   },
   "source": [
    "## 3. Data Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data-loading"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 3: DATA LOADING AND FEATURE EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "# Feature set definitions\n",
    "FEATURE_SETS = {\n",
    "    'BASIC': [\n",
    "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "        'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "        'elevation', 'slope', 'aspect'\n",
    "    ],\n",
    "    'KCE': [\n",
    "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "        'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "        'elevation', 'slope', 'aspect',\n",
    "        'elev_high', 'elev_med', 'elev_low'\n",
    "    ],\n",
    "    'PAFC': [\n",
    "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "        'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "        'elevation', 'slope', 'aspect',\n",
    "        'elev_high', 'elev_med', 'elev_low',\n",
    "        'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag12'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def _resolve_dim(ds: xr.Dataset, candidates: Tuple[str, ...]) -> str:\n",
    "    for name in candidates:\n",
    "        if name in ds.dims:\n",
    "            return name\n",
    "    raise ValueError(f\"Missing required dimension. Expected one of: {candidates}\")\n",
    "\n",
    "\n",
    "def _required_features(feature_sets: Dict[str, List[str]]) -> List[str]:\n",
    "    required = set()\n",
    "    for name in feature_sets['BASIC'] + feature_sets['KCE']:\n",
    "        if name.startswith('elev_'):\n",
    "            continue\n",
    "        required.add(name)\n",
    "    required.add('total_precipitation')\n",
    "    return sorted(required)\n",
    "\n",
    "\n",
    "def validate_dataset(ds: xr.Dataset, config: V5Config, feature_sets: Dict[str, List[str]]) -> Tuple[str, str]:\n",
    "    if 'time' not in ds.dims:\n",
    "        raise ValueError(\"Dataset missing required 'time' dimension\")\n",
    "\n",
    "    lat_dim = _resolve_dim(ds, ('latitude', 'lat'))\n",
    "    lon_dim = _resolve_dim(ds, ('longitude', 'lon'))\n",
    "\n",
    "    missing = [name for name in _required_features(feature_sets)\n",
    "               if name not in ds.data_vars and name not in ds.coords]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required variables: {missing}\")\n",
    "\n",
    "    n_time = int(ds.dims['time'])\n",
    "    if n_time < config.input_window + config.horizon:\n",
    "        raise ValueError(\n",
    "            f\"Not enough timesteps: {n_time} < {config.input_window + config.horizon}\"\n",
    "        )\n",
    "\n",
    "    return lat_dim, lon_dim\n",
    "\n",
    "\n",
    "def load_dataset(data_path: Path, config: V5Config) -> xr.Dataset:\n",
    "    \"\"\"Load and validate the NetCDF dataset.\"\"\"\n",
    "    print(f\"Loading dataset from: {data_path}\")\n",
    "    ds = xr.open_dataset(data_path)\n",
    "\n",
    "    lat_dim, lon_dim = validate_dataset(ds, config, FEATURE_SETS)\n",
    "\n",
    "    # Print dataset info\n",
    "    print(f\"\\nDataset dimensions:\")\n",
    "    for dim, size in ds.dims.items():\n",
    "        print(f\"  - {dim}: {size}\")\n",
    "\n",
    "    print(f\"\\nAvailable variables: {list(ds.data_vars)}\")\n",
    "\n",
    "    # Apply light mode if enabled\n",
    "    if config.light_mode:\n",
    "        lat_slice = slice(0, config.light_grid_size)\n",
    "        lon_slice = slice(0, config.light_grid_size)\n",
    "        ds = ds.isel({lat_dim: lat_slice, lon_dim: lon_slice})\n",
    "        print(f\"\\nLight mode enabled: using {config.light_grid_size}x{config.light_grid_size} grid\")\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_elevation_clusters(ds: xr.Dataset, n_clusters: int = 3) -> xr.Dataset:\n",
    "    \"\"\"Add elevation cluster features (KCE) to dataset.\"\"\"\n",
    "    elevation = ds['elevation'].values\n",
    "    elev_dims = ds['elevation'].dims\n",
    "\n",
    "    # Handle 3D elevation (time, lat, lon) by taking first timestep\n",
    "    if elevation.ndim == 3:\n",
    "        print(\"Handling 3D elevation: using first timestep for static clustering\")\n",
    "        elevation = elevation[0]\n",
    "        elev_dims = elev_dims[-2:]\n",
    "\n",
    "    valid_mask = ~np.isnan(elevation)\n",
    "\n",
    "    # Flatten for clustering\n",
    "    elev_flat = elevation[valid_mask].reshape(-1, 1)\n",
    "\n",
    "    # K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=10)\n",
    "    labels = np.full(elevation.shape, -1)\n",
    "    labels[valid_mask] = kmeans.fit_predict(elev_flat)\n",
    "\n",
    "    # Create one-hot encoded features\n",
    "    for i, name in enumerate(['elev_low', 'elev_med', 'elev_high']):\n",
    "        cluster_data = np.zeros_like(elevation)\n",
    "        cluster_data[labels == i] = 1.0\n",
    "        ds[name] = xr.DataArray(\n",
    "            data=cluster_data,\n",
    "            dims=elev_dims,\n",
    "            attrs={'description': f'Elevation cluster {name}'}\n",
    "        )\n",
    "\n",
    "    print(\"Added elevation clusters: elev_low, elev_med, elev_high\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def extract_features(ds: xr.Dataset, feature_names: List[str], config: V5Config) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"Extract features from dataset into numpy array.\"\"\"\n",
    "    features = []\n",
    "    missing = []\n",
    "\n",
    "    for name in feature_names:\n",
    "        if name in ds.data_vars:\n",
    "            data = ds[name].values\n",
    "            # Handle different dimensions\n",
    "            if data.ndim == 2:  # (lat, lon) - static features\n",
    "                # Broadcast to (time, lat, lon)\n",
    "                data = np.broadcast_to(data, (ds.dims['time'], *data.shape))\n",
    "            features.append(data)\n",
    "        else:\n",
    "            missing.append(name)\n",
    "\n",
    "    if missing:\n",
    "        msg = f\"Missing features: {missing}\"\n",
    "        if config.allow_missing_features:\n",
    "            print(f\"Warning: {msg}\")\n",
    "        else:\n",
    "            raise ValueError(msg)\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(\"No features extracted; check dataset and feature list\")\n",
    "\n",
    "    # Stack features: (time, lat, lon, n_features)\n",
    "    features = np.stack(features, axis=-1)\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    print(f\"Extracted features shape: {features.shape}\")\n",
    "\n",
    "    return features.astype(np.float32), missing\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(DATA_FILE, CONFIG)\n",
    "LAT_DIM = _resolve_dim(ds, ('latitude', 'lat'))\n",
    "LON_DIM = _resolve_dim(ds, ('longitude', 'lon'))\n",
    "ds = create_elevation_clusters(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-4"
   },
   "source": [
    "## 4. Graph Construction for GNN Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "graph-construction"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 4: GRAPH CONSTRUCTION\n",
    "# =============================================================================\n",
    "\n",
    "class SpatialGraphBuilder:\n",
    "    \"\"\"Build spatial graph for GNN branch based on geographic and topographic similarity.\"\"\"\n",
    "\n",
    "    def __init__(self, config: V5Config):\n",
    "        self.config = config\n",
    "\n",
    "    @staticmethod\n",
    "    def _safe_correlation(ts_a: np.ndarray, ts_b: np.ndarray) -> float:\n",
    "        \"\"\"Compute correlation robustly (returns 0.0 for invalid cases).\"\"\"\n",
    "        mask = np.isfinite(ts_a) & np.isfinite(ts_b)\n",
    "        if mask.sum() < 2:\n",
    "            return 0.0\n",
    "        a = ts_a[mask]\n",
    "        b = ts_b[mask]\n",
    "        a = a - a.mean()\n",
    "        b = b - b.mean()\n",
    "        denom = np.sqrt(np.sum(a * a)) * np.sqrt(np.sum(b * b))\n",
    "        if denom < 1e-6:\n",
    "            return 0.0\n",
    "        corr = float(np.sum(a * b) / denom)\n",
    "        if not np.isfinite(corr):\n",
    "            return 0.0\n",
    "        return corr\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_coord(ds: xr.Dataset, names: List[str]) -> np.ndarray:\n",
    "        for name in names:\n",
    "            if name in ds.coords:\n",
    "                return ds.coords[name].values\n",
    "            if name in ds.variables:\n",
    "                return ds[name].values\n",
    "        raise KeyError(f\"Missing coordinate; tried {names}\")\n",
    "\n",
    "    def build_graph(self, ds: xr.Dataset, train_time_idx: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Build graph structure from dataset.\n",
    "\n",
    "        Returns:\n",
    "            edge_index: (2, num_edges) tensor of edge indices\n",
    "            edge_weight: (num_edges,) tensor of edge weights\n",
    "        \"\"\"\n",
    "        lat = self._get_coord(ds, ['lat', 'latitude'])\n",
    "        lon = self._get_coord(ds, ['lon', 'longitude'])\n",
    "        elevation = ds['elevation'].values\n",
    "\n",
    "        if elevation.ndim == 3:\n",
    "            elevation = elevation[0]\n",
    "\n",
    "        n_lat, n_lon = len(lat), len(lon)\n",
    "        n_nodes = n_lat * n_lon\n",
    "\n",
    "        print(f\"Building graph for {n_lat}x{n_lon} = {n_nodes} nodes\")\n",
    "\n",
    "        # Create node positions in km for distance scaling\n",
    "        lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')\n",
    "        mean_lat = float(np.mean(lat))\n",
    "        km_per_deg_lat = 111.32\n",
    "        km_per_deg_lon = 111.32 * np.cos(np.deg2rad(mean_lat))\n",
    "        positions = np.stack(\n",
    "            [lat_grid.flatten() * km_per_deg_lat, lon_grid.flatten() * km_per_deg_lon],\n",
    "            axis=1\n",
    "        )\n",
    "        elev_flat = elevation.flatten()\n",
    "\n",
    "        self.node_positions = positions\n",
    "        self.node_elevations = elev_flat\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "        # Precompute precipitation time series for correlation edges\n",
    "        precip_flat = None\n",
    "        if self.config.use_correlation_edges:\n",
    "            precip = ds['total_precipitation'].values.astype(np.float32)\n",
    "            if train_time_idx is not None:\n",
    "                train_time_idx = max(2, min(train_time_idx, precip.shape[0]))\n",
    "                precip = precip[:train_time_idx]\n",
    "                print(f\"Correlation edges using first {train_time_idx} timesteps\")\n",
    "            precip_flat = precip.reshape(precip.shape[0], n_nodes)\n",
    "\n",
    "        edges = []\n",
    "        weights = []\n",
    "\n",
    "        distance_coeff = 1.0\n",
    "        if self.config.use_elevation_edges or self.config.use_correlation_edges:\n",
    "            distance_coeff = max(\n",
    "                0.0,\n",
    "                1.0 - self.config.elevation_weight - self.config.correlation_weight\n",
    "            )\n",
    "\n",
    "        for i in range(n_nodes):\n",
    "            # Calculate distances to all other nodes\n",
    "            distances = np.sqrt(np.sum((positions - positions[i])**2, axis=1))\n",
    "\n",
    "            # Get k nearest neighbors (excluding self)\n",
    "            distances[i] = np.inf\n",
    "            nearest_idx = np.argsort(distances)[:self.config.max_neighbors]\n",
    "            ts_i = precip_flat[:, i] if precip_flat is not None else None\n",
    "\n",
    "            for j in nearest_idx:\n",
    "                if distances[j] == np.inf:\n",
    "                    continue\n",
    "\n",
    "                component_weights = []\n",
    "\n",
    "                if self.config.use_distance_edges:\n",
    "                    dist_weight = np.exp(-distances[j] / self.config.distance_scale_km)\n",
    "                    component_weights.append((dist_weight, distance_coeff))\n",
    "\n",
    "                if self.config.use_elevation_edges:\n",
    "                    if not np.isnan(elev_flat[i]) and not np.isnan(elev_flat[j]):\n",
    "                        elev_diff = np.abs(elev_flat[i] - elev_flat[j])\n",
    "                        elev_weight = np.exp(-elev_diff / self.config.elevation_scale_m)\n",
    "                    else:\n",
    "                        elev_weight = 0.5\n",
    "                    component_weights.append((elev_weight, self.config.elevation_weight))\n",
    "\n",
    "                if self.config.use_correlation_edges:\n",
    "                    if precip_flat is not None:\n",
    "                        corr = self._safe_correlation(ts_i, precip_flat[:, j])\n",
    "                    else:\n",
    "                        corr = 0.0\n",
    "                    corr_weight = max(0.0, corr)\n",
    "                    component_weights.append((corr_weight, self.config.correlation_weight))\n",
    "\n",
    "                if not component_weights:\n",
    "                    continue\n",
    "\n",
    "                coeff_sum = sum(weight for _, weight in component_weights)\n",
    "                if coeff_sum <= 0:\n",
    "                    coeff_sum = len(component_weights)\n",
    "                    combined_weight = sum(val for val, _ in component_weights) / coeff_sum\n",
    "                else:\n",
    "                    combined_weight = sum(val * weight for val, weight in component_weights) / coeff_sum\n",
    "\n",
    "                threshold = max(self.config.edge_threshold, self.config.min_edge_weight)\n",
    "                if combined_weight >= threshold:\n",
    "                    edges.append([i, j])\n",
    "                    weights.append(combined_weight)\n",
    "\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        edge_weight = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "        print(f\"Graph built: {n_nodes} nodes, {edge_index.shape[1]} edges\")\n",
    "        print(f\"Average edges per node: {edge_index.shape[1] / n_nodes:.1f}\")\n",
    "\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "# Build graph\n",
    "graph_builder = SpatialGraphBuilder(CONFIG)\n",
    "train_time_idx = None\n",
    "if CONFIG.use_correlation_edges and CONFIG.correlation_train_only:\n",
    "    train_time_idx = int(ds.dims['time'] * CONFIG.train_val_split)\n",
    "edge_index, edge_weight = graph_builder.build_graph(ds, train_time_idx=train_time_idx)\n",
    "if edge_index.numel() == 0:\n",
    "    raise ValueError(\"Graph construction produced zero edges\")\n",
    "print(f\"Edge index shape: {edge_index.shape}\")\n",
    "print(f\"Edge weight shape: {edge_weight.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lmX0x0oqvlL"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 4.1: GRAPH DIAGNOSTICS (V4-COMPATIBLE)\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_graph(graph_builder: SpatialGraphBuilder, edge_index: np.ndarray,\n",
    "                   edge_weight: np.ndarray, title: str = 'Spatial Graph'):\n",
    "    if edge_weight.size == 0:\n",
    "        print('Graph visualization skipped: empty edge list')\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    ax1 = axes[0]\n",
    "    pos = graph_builder.node_positions\n",
    "    n_edges_to_draw = min(500, len(edge_weight))\n",
    "    edge_indices = np.random.choice(len(edge_weight), n_edges_to_draw, replace=False)\n",
    "    for idx in edge_indices:\n",
    "        i, j = edge_index[0, idx], edge_index[1, idx]\n",
    "        ax1.plot([pos[i, 1], pos[j, 1]], [pos[i, 0], pos[j, 0]], 'b-', alpha=0.2, linewidth=0.5)\n",
    "\n",
    "    scatter = ax1.scatter(pos[:, 1], pos[:, 0], c=graph_builder.node_elevations,\n",
    "                         cmap='terrain', s=80, edgecolors='black', linewidth=0.4)\n",
    "    plt.colorbar(scatter, ax=ax1, label='Elevation (m)')\n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('Graph Structure (colored by elevation)')\n",
    "\n",
    "    ax2 = axes[1]\n",
    "    ax2.hist(edge_weight, bins=50, color='steelblue', edgecolor='black')\n",
    "    ax2.set_xlabel('Edge Weight')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Edge Weight Distribution')\n",
    "    ax2.axvline(np.mean(edge_weight), color='red', linestyle='--', label=f'Mean: {np.mean(edge_weight):.3f}')\n",
    "    ax2.legend()\n",
    "\n",
    "    ax3 = axes[2]\n",
    "    degrees = np.bincount(edge_index[0], minlength=graph_builder.n_nodes)\n",
    "    ax3.hist(degrees, bins=20, color='forestgreen', edgecolor='black')\n",
    "    ax3.set_xlabel('Node Degree')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Node Degree Distribution')\n",
    "    ax3.axvline(np.mean(degrees), color='red', linestyle='--', label=f'Mean: {np.mean(degrees):.1f}')\n",
    "    ax3.legend()\n",
    "\n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_path = OUTPUT_ROOT / 'graph_visualization_v5.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Graph visualization saved to: {fig_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_graph_edges(graph_builder: SpatialGraphBuilder, edge_index: np.ndarray,\n",
    "                        edge_weight: np.ndarray, output_dir: Path):\n",
    "    if edge_weight.size == 0:\n",
    "        print('Graph edge analysis skipped: empty edge list')\n",
    "        return\n",
    "\n",
    "    max_nodes_for_adj = 6000\n",
    "    if graph_builder.n_nodes > max_nodes_for_adj:\n",
    "        print(f\"Skipping adjacency matrix plot: {graph_builder.n_nodes} nodes\")\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
    "        axes = [ax]\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    if len(axes) > 1:\n",
    "        ax1 = axes[0]\n",
    "        adj_matrix = np.zeros((graph_builder.n_nodes, graph_builder.n_nodes), dtype=np.float32)\n",
    "        for idx in range(len(edge_weight)):\n",
    "            i, j = edge_index[0, idx], edge_index[1, idx]\n",
    "            adj_matrix[i, j] = edge_weight[idx]\n",
    "        im1 = ax1.imshow(adj_matrix, cmap='YlOrRd', aspect='auto')\n",
    "        plt.colorbar(im1, ax=ax1, label='Edge Weight')\n",
    "        ax1.set_xlabel('Node Index')\n",
    "        ax1.set_ylabel('Node Index')\n",
    "        ax1.set_title('Adjacency Matrix (Edge Weights)')\n",
    "        ax2 = axes[1]\n",
    "    else:\n",
    "        ax2 = axes[0]\n",
    "\n",
    "    threshold = np.percentile(edge_weight, 90)\n",
    "    strong_edges = edge_weight > threshold\n",
    "    pos = graph_builder.node_positions\n",
    "\n",
    "    scatter = ax2.scatter(pos[:, 1], pos[:, 0], c=graph_builder.node_elevations,\n",
    "                         cmap='terrain', s=80, edgecolors='black', linewidth=0.4, zorder=2)\n",
    "    plt.colorbar(scatter, ax=ax2, label='Elevation (m)')\n",
    "\n",
    "    for idx in np.where(strong_edges)[0]:\n",
    "        i, j = edge_index[0, idx], edge_index[1, idx]\n",
    "        ax2.plot([pos[i, 1], pos[j, 1]], [pos[i, 0], pos[j, 0]],\n",
    "                'r-', alpha=0.6, linewidth=edge_weight[idx] * 2, zorder=1)\n",
    "\n",
    "    ax2.set_xlabel('Longitude')\n",
    "    ax2.set_ylabel('Latitude')\n",
    "    ax2.set_title(f'Top 10% Strongest Edges (n={int(strong_edges.sum())})')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_path = output_dir / 'graph_edge_analysis_v5.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Edge analysis saved to: {fig_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    print('Graph Statistics:')\n",
    "    print(f\"  Total nodes: {graph_builder.n_nodes}\")\n",
    "    print(f\"  Total edges: {len(edge_weight)}\")\n",
    "    print(f\"  Average edge weight: {np.mean(edge_weight):.4f}\")\n",
    "    print(f\"  Max edge weight: {np.max(edge_weight):.4f}\")\n",
    "    print(f\"  Strong edges (top 10%): {int(strong_edges.sum())}\")\n",
    "\n",
    "\n",
    "if getattr(CONFIG, 'plot_graph_diagnostics', True):\n",
    "    edge_index_np = edge_index.cpu().numpy()\n",
    "    edge_weight_np = edge_weight.cpu().numpy()\n",
    "    try:\n",
    "        visualize_graph(\n",
    "            graph_builder, edge_index_np, edge_weight_np,\n",
    "            title=f\"V5 Spatial Graph ({int(ds.dims[LAT_DIM])}x{int(ds.dims[LON_DIM])} grid)\"\n",
    "        )\n",
    "    except Exception as exc:\n",
    "        print(f\"Graph visualization skipped: {exc}\")\n",
    "    try:\n",
    "        analyze_graph_edges(graph_builder, edge_index_np, edge_weight_np, OUTPUT_ROOT)\n",
    "    except Exception as exc:\n",
    "        print(f\"Graph edge analysis skipped: {exc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-5"
   },
   "source": [
    "## 5. Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocessing"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 5: DATA PREPROCESSING AND DATASET CLASS\n",
    "# =============================================================================\n",
    "\n",
    "def create_temporal_windows(\n",
    "    features: np.ndarray,\n",
    "    target: np.ndarray,\n",
    "    input_window: int,\n",
    "    horizon: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create temporal windows for sequence-to-sequence prediction.\n",
    "\n",
    "    Note: This is memory-heavy for full grids and kept for debugging only.\n",
    "    \"\"\"\n",
    "    n_time = features.shape[0]\n",
    "    n_samples = n_time - input_window - horizon + 1\n",
    "\n",
    "    if n_samples <= 0:\n",
    "        raise ValueError(f\"Not enough timesteps: {n_time} < {input_window + horizon}\")\n",
    "\n",
    "    X_list = []\n",
    "    Y_list = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        X_list.append(features[i:i+input_window])\n",
    "        Y_list.append(target[i+input_window:i+input_window+horizon])\n",
    "\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    Y = np.stack(Y_list, axis=0)\n",
    "\n",
    "    print(f\"Created {n_samples} samples\")\n",
    "    print(f\"  X shape: {X.shape}\")\n",
    "    print(f\"  Y shape: {Y.shape}\")\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "class V5DualBranchDataset(Dataset):\n",
    "    \"\"\"Dataset for V5 dual-branch model (ConvLSTM + GNN).\n",
    "\n",
    "    Provides grid and graph representations with on-the-fly windowing\n",
    "    to avoid materializing huge tensors for full-grid training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features_basic: torch.Tensor,  # (time, lat, lon, features)\n",
    "        features_kce: torch.Tensor,    # (time, lat, lon, features)\n",
    "        target: torch.Tensor,          # (time, lat, lon)\n",
    "        input_window: int,\n",
    "        horizon: int,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_weight: torch.Tensor,\n",
    "        start_idx: int,\n",
    "        end_idx: int\n",
    "    ):\n",
    "        self.features_basic = features_basic\n",
    "        self.features_kce = features_kce\n",
    "        self.target = target\n",
    "        self.input_window = input_window\n",
    "        self.horizon = horizon\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_weight = edge_weight\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx\n",
    "\n",
    "        if end_idx <= start_idx:\n",
    "            raise ValueError(f\"Invalid window range: {start_idx} to {end_idx}\")\n",
    "\n",
    "        # Store grid dimensions for later use\n",
    "        self.n_lat = features_basic.shape[1]\n",
    "        self.n_lon = features_basic.shape[2]\n",
    "        self.n_nodes = self.n_lat * self.n_lon\n",
    "\n",
    "        max_start = target.shape[0] - input_window - horizon\n",
    "        if max_start < 0:\n",
    "            raise ValueError(\n",
    "                f\"Not enough timesteps: {target.shape[0]} < {input_window + horizon}\"\n",
    "            )\n",
    "        if self.start_idx < 0 or self.end_idx - 1 > max_start:\n",
    "            raise ValueError(\n",
    "                f\"Window range out of bounds: {self.start_idx}..{self.end_idx - 1} > {max_start}\"\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end_idx - self.start_idx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.start_idx + idx\n",
    "        x_basic = self.features_basic[i:i+self.input_window]\n",
    "        x_kce = self.features_kce[i:i+self.input_window]\n",
    "        y = self.target[i+self.input_window:i+self.input_window+self.horizon]\n",
    "        return {\n",
    "            'x_grid': x_basic,\n",
    "            'x_graph': x_kce.reshape(\n",
    "                self.input_window, -1, x_kce.shape[-1]\n",
    "            ),\n",
    "            'y': y,\n",
    "            'edge_index': self.edge_index,\n",
    "            'edge_weight': self.edge_weight\n",
    "        }\n",
    "\n",
    "def prepare_data(\n",
    "    ds: xr.Dataset,\n",
    "    config: V5Config,\n",
    "    edge_index: torch.Tensor,\n",
    "    edge_weight: torch.Tensor,\n",
    "    feature_set_basic: str = 'BASIC',\n",
    "    feature_set_kce: str = 'KCE'\n",
    "):\n",
    "    \"\"\"Prepare data for training.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Preparing data for V5 dual-branch model\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Extract features for both branches\n",
    "    print(f\"\\nExtracting {feature_set_basic} features (ConvLSTM branch):\")\n",
    "    features_basic, missing_basic = extract_features(ds, FEATURE_SETS[feature_set_basic], config)\n",
    "\n",
    "    print(f\"\\nExtracting {feature_set_kce} features (GNN branch):\")\n",
    "    features_kce, missing_kce = extract_features(ds, FEATURE_SETS[feature_set_kce], config)\n",
    "\n",
    "    # Target variable\n",
    "    target = ds['total_precipitation'].values.astype(np.float32)\n",
    "    print(f\"\\nTarget shape: {target.shape}\")\n",
    "\n",
    "    # Handle NaN values\n",
    "    features_basic = np.nan_to_num(features_basic, nan=0.0)\n",
    "    features_kce = np.nan_to_num(features_kce, nan=0.0)\n",
    "    target = np.nan_to_num(target, nan=0.0)\n",
    "\n",
    "    # Window counts (avoid materializing all windows)\n",
    "    n_time = features_basic.shape[0]\n",
    "    max_start = n_time - config.input_window - config.horizon\n",
    "    if max_start < 0:\n",
    "        raise ValueError(f\"Not enough timesteps: {n_time} < {config.input_window + config.horizon}\")\n",
    "    n_samples = max_start + 1\n",
    "\n",
    "    print(\"\\nComputed temporal windows:\")\n",
    "    print(f\"  Samples: {n_samples}\")\n",
    "    print(f\"  Input window: {config.input_window}\")\n",
    "    print(f\"  Horizon: {config.horizon}\")\n",
    "\n",
    "    # Train/val split by time BEFORE windowing (avoid leakage)\n",
    "    min_split = config.input_window + config.horizon\n",
    "    max_split = max_start\n",
    "    split_mode = 'time'\n",
    "    split_time_idx = int(n_time * config.train_val_split)\n",
    "\n",
    "    if min_split > max_split:\n",
    "        msg = (\n",
    "            f\"Not enough timesteps for leakage-free split: n_time={n_time}, \"\n",
    "            f\"need >= {2 * (config.input_window + config.horizon)}\"\n",
    "        )\n",
    "        if config.allow_overlap_windows:\n",
    "            print(f\"Warning: {msg}. Falling back to overlap-prone split.\")\n",
    "            split_mode = 'window'\n",
    "            n_train = int(n_samples * config.train_val_split)\n",
    "            n_val = n_samples - n_train\n",
    "            train_last_start = n_train - 1\n",
    "            val_start_idx = n_train\n",
    "            val_last_start = max_start\n",
    "            split_time_idx = val_start_idx\n",
    "        elif config.enforce_quality_gates:\n",
    "            raise ValueError(msg)\n",
    "        else:\n",
    "            raise ValueError(msg)\n",
    "    else:\n",
    "        if split_time_idx < min_split:\n",
    "            print(f\"Warning: split_time_idx={split_time_idx} < min_split={min_split}; clipping.\")\n",
    "            split_time_idx = min_split\n",
    "        if split_time_idx > max_split:\n",
    "            print(f\"Warning: split_time_idx={split_time_idx} > max_split={max_split}; clipping.\")\n",
    "            split_time_idx = max_split\n",
    "\n",
    "        train_last_start = split_time_idx - config.input_window - config.horizon\n",
    "        val_start_idx = split_time_idx\n",
    "        val_last_start = max_start\n",
    "        n_train = train_last_start + 1\n",
    "        n_val = val_last_start - val_start_idx + 1\n",
    "\n",
    "    if n_train <= 0 or n_val <= 0:\n",
    "        raise ValueError(f\"Invalid train/val split: {n_train}/{n_val}\")\n",
    "\n",
    "    print(\"\\nSplit configuration:\")\n",
    "    print(f\"  Split mode: {split_mode}\")\n",
    "    print(f\"  Split index: {split_time_idx}\")\n",
    "    print(f\"  Train window starts: 0..{train_last_start} ({n_train})\")\n",
    "    print(f\"  Val window starts: {val_start_idx}..{val_last_start} ({n_val})\")\n",
    "    gap = val_start_idx - (train_last_start + 1)\n",
    "    print(f\"  Gap windows: {gap}\")\n",
    "\n",
    "    # Overlap leakage check\n",
    "    last_train_end = train_last_start + config.input_window + config.horizon - 1\n",
    "    first_val_start = val_start_idx\n",
    "    overlap_leakage = last_train_end >= first_val_start\n",
    "    if overlap_leakage:\n",
    "        msg = \"Train/val windows overlap. Consider splitting before windowing.\"\n",
    "        if config.allow_overlap_windows:\n",
    "            print(f\"Warning: {msg}\")\n",
    "        elif config.enforce_quality_gates:\n",
    "            raise ValueError(msg)\n",
    "        else:\n",
    "            print(f\"Warning: {msg}\")\n",
    "\n",
    "    # Normalize using training input range only (avoid leakage)\n",
    "    print(\"\\nNormalizing features using training split only:\")\n",
    "    train_input_end = train_last_start + config.input_window - 1\n",
    "    train_slice = slice(0, train_input_end + 1)\n",
    "\n",
    "    basic_mean = features_basic[train_slice].mean(axis=(0, 1, 2), keepdims=True)\n",
    "    basic_std = features_basic[train_slice].std(axis=(0, 1, 2), keepdims=True)\n",
    "    basic_std = np.where(basic_std > 1e-6, basic_std, 1.0)\n",
    "\n",
    "    kce_mean = features_kce[train_slice].mean(axis=(0, 1, 2), keepdims=True)\n",
    "    kce_std = features_kce[train_slice].std(axis=(0, 1, 2), keepdims=True)\n",
    "    kce_std = np.where(kce_std > 1e-6, kce_std, 1.0)\n",
    "\n",
    "    features_basic = ((features_basic - basic_mean) / basic_std).astype(np.float32)\n",
    "    features_kce = ((features_kce - kce_mean) / kce_std).astype(np.float32)\n",
    "\n",
    "    # Convert to torch once to avoid repeated conversion in __getitem__\n",
    "    features_basic_t = torch.from_numpy(features_basic)\n",
    "    features_kce_t = torch.from_numpy(features_kce)\n",
    "    target_t = torch.from_numpy(target)\n",
    "\n",
    "    train_dataset = V5DualBranchDataset(\n",
    "        features_basic_t, features_kce_t, target_t,\n",
    "        config.input_window, config.horizon,\n",
    "        edge_index, edge_weight,\n",
    "        start_idx=0,\n",
    "        end_idx=n_train\n",
    "    )\n",
    "    val_dataset = V5DualBranchDataset(\n",
    "        features_basic_t, features_kce_t, target_t,\n",
    "        config.input_window, config.horizon,\n",
    "        edge_index, edge_weight,\n",
    "        start_idx=val_start_idx,\n",
    "        end_idx=val_last_start + 1\n",
    "    )\n",
    "\n",
    "    data_report = {\n",
    "        'n_samples': int(n_samples),\n",
    "        'n_train': int(n_train),\n",
    "        'n_val': int(n_val),\n",
    "        'n_time': int(n_time),\n",
    "        'feature_set_basic': feature_set_basic,\n",
    "        'feature_set_kce': feature_set_kce,\n",
    "        'split_mode': split_mode,\n",
    "        'split_time_idx': int(split_time_idx),\n",
    "        'train_last_start': int(train_last_start),\n",
    "        'val_start_idx': int(val_start_idx),\n",
    "        'val_end_idx': int(val_last_start),\n",
    "        'train_input_end': int(train_input_end),\n",
    "        'overlap_leakage': bool(overlap_leakage),\n",
    "        'missing_features_basic': missing_basic,\n",
    "        'missing_features_kce': missing_kce\n",
    "    }\n",
    "\n",
    "    print(f\"\\nDataset split:\")\n",
    "    print(f\"  Training samples: {len(train_dataset)}\")\n",
    "    print(f\"  Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "    return train_dataset, val_dataset, features_basic.shape[-1], features_kce.shape[-1], data_report\n",
    "\n",
    "# Prepare data\n",
    "DEFAULT_FEATURE_SET_BASIC = 'BASIC'\n",
    "DEFAULT_FEATURE_SET_KCE = 'KCE'\n",
    "\n",
    "train_dataset, val_dataset, n_features_basic, n_features_kce, data_report = prepare_data(\n",
    "    ds, CONFIG, edge_index, edge_weight,\n",
    "    feature_set_basic=DEFAULT_FEATURE_SET_BASIC,\n",
    "    feature_set_kce=DEFAULT_FEATURE_SET_KCE\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nData loaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6"
   },
   "source": [
    "## 6. V5 Model Architecture\n",
    "\n",
    "### 6.1 Branch 1: ConvLSTM (Euclidean Spatial Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "convlstm-branch"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6.1: CONVLSTM BRANCH (V2 Enhanced Architecture)\n",
    "# =============================================================================\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"Single ConvLSTM cell with spatial convolutions.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, kernel_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_dim + hidden_dim,\n",
    "            out_channels=4 * hidden_dim,  # i, f, o, g gates\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]):\n",
    "        h, c = state\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        gates = self.conv(combined)\n",
    "\n",
    "        i, f, o, g = torch.split(gates, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "\n",
    "        c_new = f * c + i * g\n",
    "        h_new = o * torch.tanh(c_new)\n",
    "\n",
    "        return h_new, c_new\n",
    "\n",
    "    def init_hidden(self, batch_size: int, height: int, width: int, device: torch.device):\n",
    "        return (\n",
    "            torch.zeros(batch_size, self.hidden_dim, height, width, device=device),\n",
    "            torch.zeros(batch_size, self.hidden_dim, height, width, device=device)\n",
    "        )\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial attention mechanism for ConvLSTM output.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        attention = torch.sigmoid(self.conv(x))\n",
    "        return x * attention\n",
    "\n",
    "class ConvLSTMBranch(nn.Module):\n",
    "    \"\"\"Branch 1: ConvLSTM encoder for Euclidean spatial patterns.\n",
    "\n",
    "    Based on V2 Enhanced architecture with attention and residual connections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: V5Config, n_features: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Input projection\n",
    "        self.input_proj = nn.Conv2d(\n",
    "            n_features, config.convlstm_filters[0],\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "        # ConvLSTM layers\n",
    "        self.convlstm_cells_fw = nn.ModuleList()\n",
    "        self.convlstm_cells_bw = nn.ModuleList() if config.convlstm_bidirectional else None\n",
    "        in_dim = config.convlstm_filters[0]\n",
    "        for out_dim in config.convlstm_filters:\n",
    "            self.convlstm_cells_fw.append(\n",
    "                ConvLSTMCell(in_dim, out_dim, config.convlstm_kernel_size)\n",
    "            )\n",
    "            if config.convlstm_bidirectional:\n",
    "                self.convlstm_cells_bw.append(\n",
    "                    ConvLSTMCell(in_dim, out_dim, config.convlstm_kernel_size)\n",
    "                )\n",
    "            in_dim = out_dim\n",
    "\n",
    "        # Spatial attention\n",
    "        attn_in_dim = config.convlstm_filters[-1] * (2 if config.convlstm_bidirectional else 1)\n",
    "        if config.convlstm_attention:\n",
    "            self.attention = SpatialAttention(attn_in_dim)\n",
    "        else:\n",
    "            self.attention = None\n",
    "\n",
    "        # Output projection to match GNN branch\n",
    "        self.output_proj = nn.Conv2d(\n",
    "            config.convlstm_filters[-1] * (2 if config.convlstm_bidirectional else 1),\n",
    "            config.convlstm_output_dim,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "        # Residual connection\n",
    "        if config.convlstm_residual:\n",
    "            self.residual_proj = nn.Conv2d(n_features, config.convlstm_output_dim, kernel_size=1)\n",
    "        else:\n",
    "            self.residual_proj = None\n",
    "\n",
    "    def _run_convlstm(self, x: torch.Tensor, cells: nn.ModuleList) -> torch.Tensor:\n",
    "        \"\"\"Run ConvLSTM stack and return last hidden state.\"\"\"\n",
    "        batch_size, seq_len, _, h, w = x.shape\n",
    "        layer_input = x\n",
    "        for layer_idx, cell in enumerate(cells):\n",
    "            h_state, c_state = cell.init_hidden(batch_size, h, w, x.device)\n",
    "            outputs = []\n",
    "            for t in range(seq_len):\n",
    "                x_t = layer_input[:, t]\n",
    "                if layer_idx == 0:\n",
    "                    x_t = self.input_proj(x_t)\n",
    "                h_state, c_state = cell(x_t, (h_state, c_state))\n",
    "                outputs.append(h_state)\n",
    "            layer_input = torch.stack(outputs, dim=1)\n",
    "        return h_state\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, lat, lon, features)\n",
    "\n",
    "        Returns:\n",
    "            output: (batch, lat, lon, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, h, w, _ = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # Reshape for Conv2d: (batch, seq, features, h, w)\n",
    "        x_reshaped = x.permute(0, 1, 4, 2, 3)\n",
    "\n",
    "        # Store residual\n",
    "        if self.residual_proj is not None:\n",
    "            residual = self.residual_proj(x_reshaped[:, -1])  # Use last timestep\n",
    "\n",
    "        # Forward direction\n",
    "        h_forward = self._run_convlstm(x_reshaped, self.convlstm_cells_fw)\n",
    "\n",
    "        if self.config.convlstm_bidirectional:\n",
    "            # Backward direction\n",
    "            x_backward = x_reshaped.flip(1)\n",
    "            h_backward = self._run_convlstm(x_backward, self.convlstm_cells_bw)\n",
    "            output = torch.cat([h_forward, h_backward], dim=1)\n",
    "        else:\n",
    "            output = h_forward\n",
    "\n",
    "        # Apply attention\n",
    "        if self.attention is not None:\n",
    "            output = self.attention(output)\n",
    "\n",
    "        # Project to output dimension\n",
    "        output = self.output_proj(output)  # (batch, output_dim, h, w)\n",
    "\n",
    "        # Add residual\n",
    "        if self.residual_proj is not None:\n",
    "            output = output + residual\n",
    "\n",
    "        # Reshape to (batch, h, w, output_dim)\n",
    "        output = output.permute(0, 2, 3, 1)\n",
    "\n",
    "        return output\n",
    "\n",
    "print(\"ConvLSTM Branch defined successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6-2"
   },
   "source": [
    "### 6.2 Branch 2: GNN-TAT (Non-Euclidean Spatial Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnn-branch"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6.2: GNN-TAT BRANCH (V4 Architecture)\n",
    "# =============================================================================\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"Multi-head temporal self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"x: (batch, seq_len, hidden_dim)\"\"\"\n",
    "        attended, _ = self.attention(x, x, x)\n",
    "        x = self.norm(x + self.dropout(attended))\n",
    "        return x\n",
    "\n",
    "class SpatialGNNEncoder(nn.Module):\n",
    "    \"\"\"GNN encoder for non-Euclidean spatial patterns.\"\"\"\n",
    "\n",
    "    def __init__(self, config: V5Config, n_features: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(n_features, config.gnn_hidden_dim)\n",
    "\n",
    "        # GNN layers\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        for i in range(config.gnn_num_layers):\n",
    "            if config.gnn_type == 'GAT':\n",
    "                self.gnn_layers.append(\n",
    "                    GATConv(\n",
    "                        config.gnn_hidden_dim,\n",
    "                        config.gnn_hidden_dim // config.gnn_num_heads,\n",
    "                        heads=config.gnn_num_heads,\n",
    "                        dropout=config.gnn_dropout,\n",
    "                        concat=True\n",
    "                    )\n",
    "                )\n",
    "            elif config.gnn_type == 'SAGE':\n",
    "                self.gnn_layers.append(\n",
    "                    SAGEConv(config.gnn_hidden_dim, config.gnn_hidden_dim)\n",
    "                )\n",
    "            else:  # GCN\n",
    "                self.gnn_layers.append(\n",
    "                    GCNConv(config.gnn_hidden_dim, config.gnn_hidden_dim)\n",
    "                )\n",
    "\n",
    "        self.dropout = nn.Dropout(config.gnn_dropout)\n",
    "        self.norm = nn.LayerNorm(config.gnn_hidden_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_weight: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (n_nodes, n_features)\n",
    "            edge_index: (2, n_edges)\n",
    "            edge_weight: (n_edges,)\n",
    "\n",
    "        Returns:\n",
    "            x: (n_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        for gnn in self.gnn_layers:\n",
    "            if self.config.gnn_type == 'GCN' and edge_weight is not None:\n",
    "                x_new = gnn(x, edge_index, edge_weight)\n",
    "            else:\n",
    "                x_new = gnn(x, edge_index)\n",
    "            x_new = F.relu(x_new)\n",
    "            x_new = self.dropout(x_new)\n",
    "            x = self.norm(x + x_new)  # Residual\n",
    "\n",
    "        return x\n",
    "\n",
    "class GNNTATBranch(nn.Module):\n",
    "    \"\"\"Branch 2: GNN-TAT encoder for non-Euclidean spatial patterns.\n",
    "\n",
    "    Based on V4 architecture: GNN + Temporal Attention + LSTM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: V5Config, n_features: int, n_nodes: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "        # Spatial GNN encoder\n",
    "        self.spatial_encoder = SpatialGNNEncoder(config, n_features)\n",
    "\n",
    "        # Temporal attention\n",
    "        self.temporal_attention = TemporalAttention(\n",
    "            config.gnn_hidden_dim,\n",
    "            config.gnn_temporal_heads,\n",
    "            config.gnn_dropout\n",
    "        )\n",
    "\n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=config.gnn_hidden_dim,\n",
    "            hidden_size=config.gnn_lstm_hidden,\n",
    "            num_layers=config.gnn_lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=config.gnn_dropout if config.gnn_lstm_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(\n",
    "            config.gnn_lstm_hidden * 2,  # Bidirectional\n",
    "            config.gnn_output_dim\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_weight: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, n_nodes, n_features)\n",
    "            edge_index: (2, n_edges)\n",
    "            edge_weight: (n_edges,)\n",
    "\n",
    "        Returns:\n",
    "            output: (batch, n_nodes, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, n_nodes, _ = x.shape\n",
    "\n",
    "        # Process each timestep through GNN\n",
    "        spatial_outputs = []\n",
    "        for t in range(seq_len):\n",
    "            # Process all nodes for this timestep\n",
    "            x_t = x[:, t]  # (batch, n_nodes, features)\n",
    "\n",
    "            # Flatten batch and process\n",
    "            batch_outputs = []\n",
    "            for b in range(batch_size):\n",
    "                h = self.spatial_encoder(x_t[b], edge_index, edge_weight)\n",
    "                batch_outputs.append(h)\n",
    "\n",
    "            spatial_out = torch.stack(batch_outputs, dim=0)  # (batch, n_nodes, hidden)\n",
    "            spatial_outputs.append(spatial_out)\n",
    "\n",
    "        # Stack temporal: (batch, seq_len, n_nodes, hidden)\n",
    "        spatial_outputs = torch.stack(spatial_outputs, dim=1)\n",
    "\n",
    "        # Apply temporal attention per node\n",
    "        # Reshape: (batch*n_nodes, seq_len, hidden)\n",
    "        x_temporal = spatial_outputs.permute(0, 2, 1, 3).reshape(\n",
    "            batch_size * n_nodes, seq_len, -1\n",
    "        )\n",
    "        x_temporal = self.temporal_attention(x_temporal)\n",
    "\n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(x_temporal)  # (batch*n_nodes, seq_len, hidden*2)\n",
    "\n",
    "        # Take last timestep\n",
    "        last_hidden = lstm_out[:, -1]  # (batch*n_nodes, hidden*2)\n",
    "\n",
    "        # Project to output dimension\n",
    "        output = self.output_proj(last_hidden)  # (batch*n_nodes, output_dim)\n",
    "\n",
    "        # Reshape back: (batch, n_nodes, output_dim)\n",
    "        output = output.reshape(batch_size, n_nodes, -1)\n",
    "\n",
    "        return output\n",
    "\n",
    "print(\"GNN-TAT Branch defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6-3"
   },
   "source": [
    "### 6.3 Grid-Graph Fusion Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fusion-module"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6.3: GRID-GRAPH FUSION MODULE (NOVEL INNOVATION)\n",
    "# =============================================================================\n",
    "\n",
    "class GridGraphFusion(nn.Module):\n",
    "    \"\"\"Novel Grid-Graph Fusion via Cross-Attention.\n",
    "\n",
    "    Bridges Euclidean (ConvLSTM grid) and Non-Euclidean (GNN graph) representations.\n",
    "    This is a key innovation in the V5 architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: V5Config, n_lat: int, n_lon: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_lat = n_lat\n",
    "        self.n_lon = n_lon\n",
    "        self.n_nodes = n_lat * n_lon\n",
    "        self._warned_large = False\n",
    "\n",
    "        hidden_dim = config.fusion_hidden_dim\n",
    "\n",
    "        # Project both branches to same dimension\n",
    "        self.grid_proj = nn.Linear(config.convlstm_output_dim, hidden_dim)\n",
    "        self.graph_proj = nn.Linear(config.gnn_output_dim, hidden_dim)\n",
    "\n",
    "        # Cross-attention: grid attends to graph\n",
    "        self.grid_to_graph_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=config.fusion_heads,\n",
    "            dropout=config.fusion_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Cross-attention: graph attends to grid\n",
    "        self.graph_to_grid_attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=config.fusion_heads,\n",
    "            dropout=config.fusion_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Gated fusion (lighter fallback)\n",
    "        self.gate_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.fusion_dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Layer norms\n",
    "        self.norm_grid = nn.LayerNorm(hidden_dim)\n",
    "        self.norm_graph = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Final fusion\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.fusion_dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def _select_fusion_mode(self) -> str:\n",
    "        if self.config.fusion_type == 'auto':\n",
    "            if self.n_nodes > self.config.fusion_max_nodes:\n",
    "                return 'gated'\n",
    "            return 'cross_attention'\n",
    "        return self.config.fusion_type\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        grid_features: torch.Tensor,   # From ConvLSTM: (batch, lat, lon, dim)\n",
    "        graph_features: torch.Tensor   # From GNN: (batch, n_nodes, dim)\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Fuse grid and graph representations through cross-attention.\n",
    "\n",
    "        Returns:\n",
    "            fused_grid: (batch, lat, lon, hidden_dim)\n",
    "            fused_graph: (batch, n_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size = grid_features.shape[0]\n",
    "\n",
    "        # Flatten grid to sequence: (batch, n_nodes, dim)\n",
    "        grid_flat = grid_features.reshape(batch_size, self.n_nodes, -1)\n",
    "\n",
    "        # Project to common dimension\n",
    "        grid_proj = self.grid_proj(grid_flat)\n",
    "        graph_proj = self.graph_proj(graph_features)\n",
    "\n",
    "        mode = self._select_fusion_mode()\n",
    "        if mode == 'cross_attention':\n",
    "            if self.n_nodes > self.config.fusion_max_nodes and not self._warned_large:\n",
    "                print(\"Warning: cross-attention on large grids may OOM. Consider fusion_type='gated'.\")\n",
    "                self._warned_large = True\n",
    "\n",
    "            # Cross-attention: grid attends to graph\n",
    "            grid_attended, _ = self.grid_to_graph_attn(\n",
    "                query=grid_proj,\n",
    "                key=graph_proj,\n",
    "                value=graph_proj\n",
    "            )\n",
    "            grid_fused = self.norm_grid(grid_proj + grid_attended)\n",
    "\n",
    "            # Cross-attention: graph attends to grid\n",
    "            graph_attended, _ = self.graph_to_grid_attn(\n",
    "                query=graph_proj,\n",
    "                key=grid_proj,\n",
    "                value=grid_proj\n",
    "            )\n",
    "            graph_fused = self.norm_graph(graph_proj + graph_attended)\n",
    "        elif mode == 'gated':\n",
    "            gate_input = torch.cat([grid_proj, graph_proj], dim=-1)\n",
    "            gate = self.gate_mlp(gate_input)\n",
    "            grid_fused = self.norm_grid(gate * grid_proj)\n",
    "            graph_fused = self.norm_graph((1.0 - gate) * graph_proj)\n",
    "        else:\n",
    "            grid_fused = self.norm_grid(grid_proj)\n",
    "            graph_fused = self.norm_graph(graph_proj)\n",
    "\n",
    "        # Combine fused features\n",
    "        combined = torch.cat([grid_fused, graph_fused], dim=-1)\n",
    "        fused = self.fusion_mlp(combined)\n",
    "\n",
    "        # Reshape grid back to spatial\n",
    "        fused_grid = fused.reshape(batch_size, self.n_lat, self.n_lon, -1)\n",
    "        fused_graph = fused  # Keep as (batch, n_nodes, hidden_dim)\n",
    "\n",
    "        return fused_grid, fused_graph\n",
    "\n",
    "print(\"Grid-Graph Fusion Module defined successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6-4"
   },
   "source": [
    "### 6.4 Meta-Learner (Interpretable Weighted Fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meta-learner"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6.4: META-LEARNER (INTERPRETABLE BRANCH WEIGHTING)\n",
    "# =============================================================================\n",
    "\n",
    "class MetaLearner(nn.Module):\n",
    "    \"\"\"Interpretable Meta-Learner for branch weighting.\n",
    "\n",
    "    Learns context-dependent weights for ConvLSTM and GNN predictions:\n",
    "        output = w1(context) * ConvLSTM + w2(context) * GNN\n",
    "\n",
    "    Enables analysis of which branch contributes more for different contexts\n",
    "    (elevation regimes, temporal patterns, etc.).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: V5Config, n_lat: int, n_lon: int, horizon: int, context_dim: int = 0):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_lat = n_lat\n",
    "        self.n_lon = n_lon\n",
    "        self.n_nodes = n_lat * n_lon\n",
    "        self.horizon = horizon\n",
    "        self.context_dim = context_dim\n",
    "\n",
    "        hidden_dim = config.fusion_hidden_dim\n",
    "        meta_dim = config.meta_hidden_dim\n",
    "        self.weight_floor = min(max(config.weight_floor, 0.0), 0.49)\n",
    "\n",
    "        # Prediction heads for each branch\n",
    "        self.convlstm_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, meta_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.meta_dropout),\n",
    "            nn.Linear(meta_dim, horizon)\n",
    "        )\n",
    "\n",
    "        self.gnn_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, meta_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.meta_dropout),\n",
    "            nn.Linear(meta_dim, horizon)\n",
    "        )\n",
    "\n",
    "        # Context-dependent weight network\n",
    "        # Input: concatenated fused features (+ optional context)\n",
    "        self.weight_network = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 + context_dim, meta_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.meta_dropout),\n",
    "            nn.Linear(meta_dim, 2),  # 2 weights: w_convlstm, w_gnn\n",
    "            nn.Softmax(dim=-1)       # Ensure weights sum to 1\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        fused_grid: torch.Tensor,    # (batch, lat, lon, hidden_dim)\n",
    "        fused_graph: torch.Tensor,   # (batch, n_nodes, hidden_dim)\n",
    "        context_features: Optional[torch.Tensor] = None  # (batch, n_nodes, context_dim)\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Generate predictions with interpretable branch weighting.\n",
    "\n",
    "        Returns:\n",
    "            predictions: (batch, horizon, lat, lon)\n",
    "            weights: (batch, n_nodes, 2) - weights for each branch per location\n",
    "        \"\"\"\n",
    "        batch_size = fused_grid.shape[0]\n",
    "\n",
    "        # Flatten grid to match graph\n",
    "        grid_flat = fused_grid.reshape(batch_size, self.n_nodes, -1)\n",
    "\n",
    "        # Generate predictions from each branch\n",
    "        pred_convlstm = self.convlstm_head(grid_flat)   # (batch, n_nodes, horizon)\n",
    "        pred_gnn = self.gnn_head(fused_graph)          # (batch, n_nodes, horizon)\n",
    "\n",
    "        # Compute context-dependent weights\n",
    "        # Context is the concatenation of both fused representations\n",
    "        if context_features is not None:\n",
    "            context = torch.cat([grid_flat, fused_graph, context_features], dim=-1)\n",
    "        else:\n",
    "            context = torch.cat([grid_flat, fused_graph], dim=-1)\n",
    "        weights = self.weight_network(context)  # (batch, n_nodes, 2)\n",
    "        if self.weight_floor > 0.0:\n",
    "            weights = torch.clamp(weights, min=self.weight_floor, max=1.0 - self.weight_floor)\n",
    "            weights = weights / weights.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # Weighted combination\n",
    "        w_convlstm = weights[..., 0:1]  # (batch, n_nodes, 1)\n",
    "        w_gnn = weights[..., 1:2]       # (batch, n_nodes, 1)\n",
    "\n",
    "        predictions = w_convlstm * pred_convlstm + w_gnn * pred_gnn\n",
    "\n",
    "        # Reshape to spatial grid: (batch, horizon, lat, lon)\n",
    "        predictions = predictions.reshape(batch_size, self.n_lat, self.n_lon, self.horizon)\n",
    "        predictions = predictions.permute(0, 3, 1, 2)  # (batch, horizon, lat, lon)\n",
    "\n",
    "        return predictions, weights\n",
    "\n",
    "print(\"Meta-Learner defined successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6-5"
   },
   "source": [
    "### 6.5 Complete V5 Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5-model"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6.5: COMPLETE V5 STACKING MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class V5StackingModel(nn.Module):\n",
    "    \"\"\"V5 GNN-ConvLSTM Stacking Ensemble.\n",
    "\n",
    "    Novel hybrid architecture combining:\n",
    "    - Branch 1: ConvLSTM for Euclidean spatial patterns (BASIC features)\n",
    "    - Branch 2: GNN-TAT for non-Euclidean topographic relations (KCE features)\n",
    "    - Grid-Graph Fusion via cross-attention\n",
    "    - Interpretable Meta-Learner for weighted combination\n",
    "\n",
    "    Innovation: First work to combine GNN and ConvLSTM in stacking ensemble\n",
    "    for precipitation prediction (verified January 2026).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: V5Config,\n",
    "        n_features_basic: int,\n",
    "        n_features_kce: int,\n",
    "        n_lat: int,\n",
    "        n_lon: int,\n",
    "        horizon: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_lat = n_lat\n",
    "        self.n_lon = n_lon\n",
    "        self.n_nodes = n_lat * n_lon\n",
    "        self.horizon = horizon\n",
    "        self.use_context_features = config.meta_use_context_features\n",
    "\n",
    "        # Context feature indices (from KCE)\n",
    "        self.context_indices = {}\n",
    "        self.context_dim = 0\n",
    "        if self.use_context_features:\n",
    "            try:\n",
    "                kce_features = FEATURE_SETS['KCE']\n",
    "                self.context_indices = {\n",
    "                    'elevation': kce_features.index('elevation'),\n",
    "                    'elev_low': kce_features.index('elev_low'),\n",
    "                    'elev_med': kce_features.index('elev_med'),\n",
    "                    'elev_high': kce_features.index('elev_high'),\n",
    "                    'month_sin': kce_features.index('month_sin'),\n",
    "                    'month_cos': kce_features.index('month_cos')\n",
    "                }\n",
    "                # mean_elevation (1) + elevation_cluster (3) + temporal_regime (2)\n",
    "                self.context_dim = 6\n",
    "            except Exception as exc:\n",
    "                print(f\"Warning: context features unavailable ({exc}); disabling meta context.\")\n",
    "                self.use_context_features = False\n",
    "                self.context_dim = 0\n",
    "\n",
    "        # Branch 1: ConvLSTM (Euclidean)\n",
    "        self.convlstm_branch = ConvLSTMBranch(config, n_features_basic)\n",
    "\n",
    "        # Branch 2: GNN-TAT (Non-Euclidean)\n",
    "        self.gnn_branch = GNNTATBranch(config, n_features_kce, self.n_nodes)\n",
    "\n",
    "        # Grid-Graph Fusion\n",
    "        self.fusion = GridGraphFusion(config, n_lat, n_lon)\n",
    "\n",
    "        # Meta-Learner\n",
    "        self.meta_learner = MetaLearner(config, n_lat, n_lon, horizon, context_dim=self.context_dim)\n",
    "\n",
    "    def _extract_context(self, x_graph: torch.Tensor) -> Optional[torch.Tensor]:\n",
    "        \"\"\"Extract per-node context features for the meta-learner.\"\"\"\n",
    "        if not self.use_context_features:\n",
    "            return None\n",
    "\n",
    "        idx = self.context_indices\n",
    "\n",
    "        # (batch, seq, n_nodes, features)\n",
    "        elevation = x_graph[:, -1, :, idx['elevation']].unsqueeze(-1)\n",
    "        elev_clusters = x_graph[:, -1, :, [idx['elev_low'], idx['elev_med'], idx['elev_high']]]\n",
    "\n",
    "        # Temporal regime from mean seasonal encoding\n",
    "        month_sin = x_graph[:, :, :, idx['month_sin']].mean(dim=1)\n",
    "        month_cos = x_graph[:, :, :, idx['month_cos']].mean(dim=1)\n",
    "        temporal = torch.stack([month_sin, month_cos], dim=-1)\n",
    "\n",
    "        context = torch.cat([elevation, elev_clusters, temporal], dim=-1)\n",
    "        return context\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_grid: torch.Tensor,       # (batch, seq, lat, lon, features_basic)\n",
    "        x_graph: torch.Tensor,      # (batch, seq, n_nodes, features_kce)\n",
    "        edge_index: torch.Tensor,   # (2, n_edges)\n",
    "        edge_weight: torch.Tensor   # (n_edges,)\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through V5 stacking model.\n",
    "\n",
    "        Returns:\n",
    "            predictions: (batch, horizon, lat, lon) - precipitation predictions\n",
    "            weights: (batch, n_nodes, 2) - branch weights for interpretability\n",
    "        \"\"\"\n",
    "        # Branch 1: ConvLSTM processing\n",
    "        convlstm_out = self.convlstm_branch(x_grid)  # (batch, lat, lon, dim)\n",
    "\n",
    "        # Branch 2: GNN-TAT processing\n",
    "        gnn_out = self.gnn_branch(x_graph, edge_index, edge_weight)  # (batch, n_nodes, dim)\n",
    "\n",
    "        # Grid-Graph Fusion\n",
    "        fused_grid, fused_graph = self.fusion(convlstm_out, gnn_out)\n",
    "\n",
    "        # Meta-Learner for final predictions\n",
    "        context = self._extract_context(x_graph)\n",
    "        predictions, weights = self.meta_learner(fused_grid, fused_graph, context)\n",
    "\n",
    "        return predictions, weights\n",
    "\n",
    "    def count_parameters(self) -> Dict[str, int]:\n",
    "        \"\"\"Count parameters per component.\"\"\"\n",
    "        counts = {\n",
    "            'convlstm_branch': sum(p.numel() for p in self.convlstm_branch.parameters()),\n",
    "            'gnn_branch': sum(p.numel() for p in self.gnn_branch.parameters()),\n",
    "            'fusion': sum(p.numel() for p in self.fusion.parameters()),\n",
    "            'meta_learner': sum(p.numel() for p in self.meta_learner.parameters())\n",
    "        }\n",
    "        counts['total'] = sum(counts.values())\n",
    "        return counts\n",
    "\n",
    "# Test model instantiation\n",
    "# Use dataset dimensions directly (ds is already sliced if light_mode is True)\n",
    "# Updated to use correct dimension names 'latitude' and 'longitude'\n",
    "n_lat = ds.dims[LAT_DIM]\n",
    "n_lon = ds.dims[LON_DIM]\n",
    "\n",
    "model = V5StackingModel(\n",
    "    config=CONFIG,\n",
    "    n_features_basic=n_features_basic,\n",
    "    n_features_kce=n_features_kce,\n",
    "    n_lat=n_lat,\n",
    "    n_lon=n_lon,\n",
    "    horizon=CONFIG.horizon\n",
    ").to(device)\n",
    "\n",
    "# Print parameter counts\n",
    "param_counts = model.count_parameters()\n",
    "print(\"\\nV5 Model Parameter Counts:\")\n",
    "print(\"=\"*40)\n",
    "for name, count in param_counts.items():\n",
    "    print(f\"  {name}: {count:,}\")\n",
    "print(f\"\\nTotal: {param_counts['total']:,} parameters\")\n",
    "print(f\"Target: < {CONFIG.param_budget:,} parameters\")\n",
    "if param_counts['total'] > CONFIG.param_budget:\n",
    "    print(\"WARNING: parameter budget exceeded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-7"
   },
   "source": [
    "## 7. Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trainer"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 7: TRAINING INFRASTRUCTURE\n",
    "# =============================================================================\n",
    "\n",
    "class V5Trainer:\n",
    "    \"\"\"Training infrastructure for V5 model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: V5StackingModel,\n",
    "        config: V5Config,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "\n",
    "        # Loss function\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "\n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=20,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "\n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_mae': [],\n",
    "            'val_mae': [],\n",
    "            'lr': [],\n",
    "            'branch_weights': [],  # Track meta-learner weights\n",
    "            'train_weight_reg': []\n",
    "        }\n",
    "\n",
    "        # Early stopping\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.best_epoch = 0\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def train_epoch(self) -> Tuple[float, float, float]:\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_mae = 0.0\n",
    "        total_weight_reg = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in self.train_loader:\n",
    "            x_grid = batch['x_grid'].to(self.device)\n",
    "            x_graph = batch['x_graph'].to(self.device)\n",
    "            y = batch['y'].to(self.device)\n",
    "            edge_index = batch['edge_index'][0].to(self.device)\n",
    "            edge_weight = batch['edge_weight'][0].to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            predictions, weights = self.model(x_grid, x_graph, edge_index, edge_weight)\n",
    "\n",
    "            # Loss computation\n",
    "            loss = self.criterion(predictions, y)\n",
    "\n",
    "            weight_reg = torch.tensor(0.0, device=self.device)\n",
    "            if self.config.weight_reg_lambda > 0:\n",
    "                weight_reg = torch.mean((weights[..., 0] - 0.5) ** 2)\n",
    "                loss = loss + self.config.weight_reg_lambda * weight_reg\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                self.model.parameters(),\n",
    "                self.config.gradient_clip\n",
    "            )\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            total_loss += loss.item()\n",
    "            total_mae += torch.mean(torch.abs(predictions - y)).item()\n",
    "            total_weight_reg += weight_reg.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        return (\n",
    "            total_loss / n_batches,\n",
    "            total_mae / n_batches,\n",
    "            total_weight_reg / n_batches\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self) -> Tuple[float, float, np.ndarray]:\n",
    "        \"\"\"Validate model.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_mae = 0.0\n",
    "        n_batches = 0\n",
    "        all_weights = []\n",
    "\n",
    "        for batch in self.val_loader:\n",
    "            x_grid = batch['x_grid'].to(self.device)\n",
    "            x_graph = batch['x_graph'].to(self.device)\n",
    "            y = batch['y'].to(self.device)\n",
    "            edge_index = batch['edge_index'][0].to(self.device)\n",
    "            edge_weight = batch['edge_weight'][0].to(self.device)\n",
    "\n",
    "            predictions, weights = self.model(x_grid, x_graph, edge_index, edge_weight)\n",
    "\n",
    "            loss = self.criterion(predictions, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_mae += torch.mean(torch.abs(predictions - y)).item()\n",
    "            all_weights.append(weights.cpu().numpy())\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_weights = np.concatenate(all_weights, axis=0).mean(axis=(0, 1))\n",
    "\n",
    "        return total_loss / n_batches, total_mae / n_batches, avg_weights\n",
    "\n",
    "    def train(self, output_dir: Path = None) -> Dict:\n",
    "        \"\"\"Full training loop.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Starting V5 Training\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        for epoch in range(self.config.epochs):\n",
    "            # Train\n",
    "            train_loss, train_mae, train_weight_reg = self.train_epoch()\n",
    "\n",
    "            # Validate\n",
    "            val_loss, val_mae, weights = self.validate()\n",
    "\n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_loss)\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "            # Record history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_mae'].append(train_mae)\n",
    "            self.history['val_mae'].append(val_mae)\n",
    "            self.history['lr'].append(current_lr)\n",
    "            self.history['branch_weights'].append(weights.tolist())\n",
    "            self.history['train_weight_reg'].append(train_weight_reg)\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.best_epoch = epoch\n",
    "                self.patience_counter = 0\n",
    "                self.best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "                # Save checkpoint\n",
    "                if output_dir:\n",
    "                    checkpoint_path = output_dir / 'v5_stacking_best.pt'\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'val_loss': val_loss,\n",
    "                        'config': asdict(self.config)\n",
    "                    }, checkpoint_path)\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "\n",
    "            # Logging\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                w_conv, w_gnn = weights\n",
    "                log_msg = (\n",
    "                    f\"Epoch {epoch+1:3d}/{self.config.epochs} | \"\n",
    "                    f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "                    f\"LR: {current_lr:.2e} | W_conv: {w_conv:.2f} W_gnn: {w_gnn:.2f}\"\n",
    "                )\n",
    "                if self.config.weight_reg_lambda > 0:\n",
    "                    log_msg += f\" | WReg: {train_weight_reg:.4f}\"\n",
    "                print(log_msg)\n",
    "\n",
    "            # Early stopping\n",
    "            if self.patience_counter >= self.config.patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # Restore best model\n",
    "        if self.best_model_state:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "\n",
    "        elapsed = datetime.now() - start_time\n",
    "\n",
    "        print(f\"\\nTraining completed in {elapsed}\")\n",
    "        print(f\"Best validation loss: {self.best_val_loss:.4f} at epoch {self.best_epoch+1}\")\n",
    "\n",
    "        return self.history\n",
    "\n",
    "print(\"V5 Trainer defined successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics-evaluation"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 7.2: METRICS EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(\n",
    "    model: V5StackingModel,\n",
    "    data_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    return_arrays: bool = False\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate model and compute metrics per horizon.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_weights = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        x_grid = batch['x_grid'].to(device)\n",
    "        x_graph = batch['x_graph'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        edge_index = batch['edge_index'][0].to(device)\n",
    "        edge_weight = batch['edge_weight'][0].to(device)\n",
    "\n",
    "        predictions, weights = model(x_grid, x_graph, edge_index, edge_weight)\n",
    "\n",
    "        all_preds.append(predictions.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "        all_weights.append(weights.cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(all_preds, axis=0).astype(np.float32)\n",
    "    targets = np.concatenate(all_targets, axis=0).astype(np.float32)\n",
    "    weights = np.concatenate(all_weights, axis=0)\n",
    "\n",
    "    # Global quality stats\n",
    "    pred_finite = np.isfinite(preds)\n",
    "    target_finite = np.isfinite(targets)\n",
    "    neg_pred_frac = float(np.mean(preds < 0))\n",
    "    nan_pred_frac = float(np.mean(~pred_finite))\n",
    "    nan_target_frac = float(np.mean(~target_finite))\n",
    "\n",
    "    # Compute metrics per horizon\n",
    "    metrics = {}\n",
    "    n_horizons = preds.shape[1]\n",
    "\n",
    "    for h in range(n_horizons):\n",
    "        pred_h = preds[:, h].flatten()\n",
    "        target_h = targets[:, h].flatten()\n",
    "\n",
    "        # Remove NaN values\n",
    "        valid_mask = np.isfinite(pred_h) & np.isfinite(target_h)\n",
    "        pred_h = pred_h[valid_mask]\n",
    "        target_h = target_h[valid_mask]\n",
    "\n",
    "        if len(pred_h) == 0:\n",
    "            continue\n",
    "\n",
    "        # RMSE\n",
    "        rmse = np.sqrt(np.mean((pred_h - target_h) ** 2))\n",
    "\n",
    "        # MAE\n",
    "        mae = np.mean(np.abs(pred_h - target_h))\n",
    "\n",
    "        # R2\n",
    "        ss_res = np.sum((target_h - pred_h) ** 2)\n",
    "        ss_tot = np.sum((target_h - np.mean(target_h)) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
    "\n",
    "        # Bias\n",
    "        mean_true = np.mean(target_h)\n",
    "        mean_pred = np.mean(pred_h)\n",
    "        total_true = float(np.sum(target_h))\n",
    "        total_pred = float(np.sum(pred_h))\n",
    "        bias_mm = mean_pred - mean_true\n",
    "        bias_pct = 100 * bias_mm / mean_true if mean_true != 0 else 0.0\n",
    "        scale_ratio = abs(mean_pred) / max(abs(mean_true), 1e-6)\n",
    "\n",
    "        metrics[f'H{h+1}'] = {\n",
    "            'RMSE': float(rmse),\n",
    "            'MAE': float(mae),\n",
    "            'R^2': float(r2),\n",
    "            'Mean_True_mm': float(mean_true),\n",
    "            'Mean_Pred_mm': float(mean_pred),\n",
    "            'mean_bias_mm': float(bias_mm),\n",
    "            'mean_bias_pct': float(bias_pct),\n",
    "            'scale_ratio': float(scale_ratio),\n",
    "            'TotalPrecipitation': float(total_true),\n",
    "            'TotalPrecipitation_Pred': float(total_pred)\n",
    "        }\n",
    "\n",
    "    # Average branch weights\n",
    "    avg_weights = weights.mean(axis=(0, 1))\n",
    "    metrics['branch_weights'] = {\n",
    "        'w_convlstm': float(avg_weights[0]),\n",
    "        'w_gnn': float(avg_weights[1])\n",
    "    }\n",
    "\n",
    "    metrics['quality'] = {\n",
    "        'neg_pred_frac': neg_pred_frac,\n",
    "        'nan_pred_frac': nan_pred_frac,\n",
    "        'nan_target_frac': nan_target_frac\n",
    "    }\n",
    "\n",
    "    if return_arrays:\n",
    "        return metrics, preds, targets\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def run_quality_checks(\n",
    "    metrics: Dict,\n",
    "    config: V5Config,\n",
    "    param_counts: Dict[str, int],\n",
    "    data_report: Dict = None\n",
    ") -> Dict:\n",
    "    issues = []\n",
    "    max_abs_bias = 0.0\n",
    "    max_scale_ratio = 0.0\n",
    "\n",
    "    for h_key, h_metrics in metrics.items():\n",
    "        if not h_key.startswith('H'):\n",
    "            continue\n",
    "        max_abs_bias = max(max_abs_bias, abs(h_metrics.get('mean_bias_pct', 0.0)))\n",
    "        max_scale_ratio = max(max_scale_ratio, h_metrics.get('scale_ratio', 0.0))\n",
    "\n",
    "    neg_pred_frac = metrics.get('quality', {}).get('neg_pred_frac', 0.0)\n",
    "    nan_pred_frac = metrics.get('quality', {}).get('nan_pred_frac', 0.0)\n",
    "    nan_target_frac = metrics.get('quality', {}).get('nan_target_frac', 0.0)\n",
    "\n",
    "    if max_abs_bias > config.max_bias_pct:\n",
    "        issues.append('bias_pct')\n",
    "    if max_scale_ratio > config.max_scale_ratio:\n",
    "        issues.append('scale_ratio')\n",
    "    if neg_pred_frac > config.max_negative_frac:\n",
    "        issues.append('neg_pred')\n",
    "    if nan_pred_frac > 0 or nan_target_frac > 0:\n",
    "        issues.append('nan_values')\n",
    "\n",
    "    w_conv = metrics.get('branch_weights', {}).get('w_convlstm', 0.5)\n",
    "    if w_conv < config.min_branch_weight or w_conv > (1.0 - config.min_branch_weight):\n",
    "        issues.append('branch_collapse')\n",
    "\n",
    "    param_budget_ok = param_counts.get('total', 0) <= config.param_budget\n",
    "    if not param_budget_ok:\n",
    "        issues.append('param_budget')\n",
    "\n",
    "    overlap_leakage = False\n",
    "    missing_features = []\n",
    "    if data_report:\n",
    "        overlap_leakage = bool(data_report.get('overlap_leakage', False))\n",
    "        missing_features = data_report.get('missing_features_basic', []) + data_report.get('missing_features_kce', [])\n",
    "    if overlap_leakage:\n",
    "        issues.append('overlap_windows')\n",
    "    if missing_features:\n",
    "        issues.append('missing_features')\n",
    "\n",
    "    return {\n",
    "        'issues': issues,\n",
    "        'max_abs_bias_pct': float(max_abs_bias),\n",
    "        'max_scale_ratio': float(max_scale_ratio),\n",
    "        'neg_pred_frac': float(neg_pred_frac),\n",
    "        'nan_pred_frac': float(nan_pred_frac),\n",
    "        'nan_target_frac': float(nan_target_frac),\n",
    "        'branch_weight_convlstm': float(w_conv),\n",
    "        'param_budget_ok': bool(param_budget_ok),\n",
    "        'overlap_leakage': bool(overlap_leakage),\n",
    "        'missing_features': missing_features\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def _forecast_dates(ds: xr.Dataset, val_indices: List[int], input_window: int, horizon: int) -> List[List[str]]:\n",
    "    if 'time' not in ds.coords and 'time' not in ds.dims:\n",
    "        return [[] for _ in val_indices]\n",
    "    times = pd.to_datetime(ds['time'].values)\n",
    "    dates = []\n",
    "    for start in val_indices:\n",
    "        base = start + input_window\n",
    "        horizon_dates = []\n",
    "        for h in range(horizon):\n",
    "            idx = base + h\n",
    "            if idx < len(times):\n",
    "                horizon_dates.append(pd.Timestamp(times[idx]).strftime('%Y-%m'))\n",
    "        dates.append(horizon_dates)\n",
    "    return dates\n",
    "\n",
    "\n",
    "def export_predictions_for_maps(\n",
    "    preds: np.ndarray,\n",
    "    targets: np.ndarray,\n",
    "    output_root: Path,\n",
    "    horizon: int,\n",
    "    exp_name: str,\n",
    "    model_name: str,\n",
    "    config: V5Config,\n",
    "    ds: xr.Dataset,\n",
    "    data_report: Dict\n",
    ") -> Path:\n",
    "    # Export predictions for map generation (V2/V3 compatible format).\n",
    "    map_out_root = output_root / 'map_exports'\n",
    "    export_dir = map_out_root / f'H{horizon}' / exp_name / model_name\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    np.save(export_dir / 'predictions.npy', preds.astype(np.float32))\n",
    "    np.save(export_dir / 'targets.npy', targets.astype(np.float32))\n",
    "\n",
    "    val_count = preds.shape[0]\n",
    "    if data_report:\n",
    "        val_start_idx = int(data_report.get('val_start_idx', data_report.get('n_train', 0)))\n",
    "        val_end_idx = int(data_report.get('val_end_idx', val_start_idx + val_count - 1))\n",
    "        if (val_end_idx - val_start_idx + 1) != val_count:\n",
    "            val_end_idx = val_start_idx + val_count - 1\n",
    "    else:\n",
    "        val_start_idx = 0\n",
    "        val_end_idx = val_start_idx + val_count - 1\n",
    "\n",
    "    val_indices = list(range(val_start_idx, val_end_idx + 1))\n",
    "    forecast_dates = _forecast_dates(ds, val_indices, config.input_window, horizon)\n",
    "\n",
    "    metadata = {\n",
    "        'exp': exp_name,\n",
    "        'model': model_name,\n",
    "        'horizon': horizon,\n",
    "        'input_window': config.input_window,\n",
    "        'train_val_split': config.train_val_split,\n",
    "        'feature_set_basic': data_report.get('feature_set_basic') if data_report else None,\n",
    "        'feature_set_kce': data_report.get('feature_set_kce') if data_report else None,\n",
    "        'split_mode': data_report.get('split_mode', 'window') if data_report else 'window',\n",
    "        'split_time_idx': int(data_report.get('split_time_idx', val_start_idx)) if data_report else int(val_start_idx),\n",
    "        'val_start_idx': int(val_start_idx),\n",
    "        'val_end_idx': int(val_end_idx),\n",
    "        'val_indices': val_indices,\n",
    "        'forecast_dates': forecast_dates,\n",
    "        'shape': list(preds.shape),\n",
    "        'generated_at': datetime.now().isoformat(),\n",
    "        'rmse_mean': float(np.sqrt(np.mean((preds - targets) ** 2))),\n",
    "        'framework': 'V5 GNN-ConvLSTM Stacking'\n",
    "    }\n",
    "    (export_dir / 'metadata.json').write_text(json.dumps(metadata, indent=2))\n",
    "\n",
    "    print(f\"Exported predictions to: {export_dir}\")\n",
    "    return export_dir\n",
    "\n",
    "print(\"Metrics evaluation functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-8"
   },
   "source": [
    "## 8. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "main-training"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 8: MAIN TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    {'name': 'BASIC_BASIC', 'basic_set': 'BASIC', 'gnn_set': 'BASIC'},\n",
    "    {'name': 'KCE_KCE', 'basic_set': 'KCE', 'gnn_set': 'KCE'},\n",
    "    {'name': 'PAFC_PAFC', 'basic_set': 'PAFC', 'gnn_set': 'PAFC'},\n",
    "]\n",
    "RUN_MIXED_ABLATION = True\n",
    "if RUN_MIXED_ABLATION:\n",
    "    EXPERIMENTS.append({'name': 'BASIC_KCE', 'basic_set': 'BASIC', 'gnn_set': 'KCE'})\n",
    "\n",
    "# Meta-learner stabilization (step 2)\n",
    "WEIGHT_FLOOR = 0.1\n",
    "WEIGHT_REG_LAMBDA = 0.02\n",
    "\n",
    "\n",
    "def run_v5_experiments(\n",
    "    ds: xr.Dataset,\n",
    "    config: V5Config,\n",
    "    edge_index: torch.Tensor,\n",
    "    edge_weight: torch.Tensor,\n",
    "    output_root: Path,\n",
    "    experiments: List[Dict]\n",
    "):\n",
    "    \"\"\"Run V5 experiments across multiple horizons.\"\"\"\n",
    "\n",
    "    all_metrics = []\n",
    "    experiment_state = {\n",
    "        'config': asdict(config),\n",
    "        'experiments': experiments,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'results': {}\n",
    "    }\n",
    "\n",
    "    for exp in experiments:\n",
    "        exp_name = exp['name']\n",
    "        feature_set_basic = exp['basic_set']\n",
    "        feature_set_kce = exp['gnn_set']\n",
    "        exp_weight_floor = exp.get('weight_floor', WEIGHT_FLOOR)\n",
    "        exp_weight_reg = exp.get('weight_reg_lambda', WEIGHT_REG_LAMBDA)\n",
    "\n",
    "        experiment_state['results'][exp_name] = {}\n",
    "\n",
    "        for horizon in config.enabled_horizons:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(f\"TRAINING V5 STACKING MODEL - {exp_name} - HORIZON H{horizon}\")\n",
    "            print(\"=\"*70)\n",
    "\n",
    "            # Update horizon in config\n",
    "            current_config = V5Config(**{\n",
    "                **asdict(config),\n",
    "                'horizon': horizon,\n",
    "                'weight_floor': exp_weight_floor,\n",
    "                'weight_reg_lambda': exp_weight_reg\n",
    "            })\n",
    "\n",
    "            # Prepare data\n",
    "            train_dataset, val_dataset, n_basic, n_kce, data_report = prepare_data(\n",
    "                ds, current_config, edge_index, edge_weight,\n",
    "                feature_set_basic=feature_set_basic,\n",
    "                feature_set_kce=feature_set_kce\n",
    "            )\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=current_config.batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=current_config.batch_size, shuffle=False)\n",
    "\n",
    "            # Get grid dimensions\n",
    "            n_lat = train_dataset.n_lat\n",
    "            n_lon = train_dataset.n_lon\n",
    "\n",
    "            # Initialize model\n",
    "            model = V5StackingModel(\n",
    "                config=current_config,\n",
    "                n_features_basic=n_basic,\n",
    "                n_features_kce=n_kce,\n",
    "                n_lat=n_lat,\n",
    "                n_lon=n_lon,\n",
    "                horizon=horizon\n",
    "            ).to(device)\n",
    "\n",
    "            param_counts = model.count_parameters()\n",
    "            if param_counts['total'] > current_config.param_budget:\n",
    "                print(\"Warning: parameter budget exceeded\")\n",
    "\n",
    "            # Create output directory for this horizon\n",
    "            horizon_dir = output_root / f'h{horizon}' / exp_name / 'training_metrics'\n",
    "            horizon_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Train\n",
    "            trainer = V5Trainer(model, current_config, train_loader, val_loader, device)\n",
    "            history = trainer.train(horizon_dir)\n",
    "\n",
    "            # Evaluate\n",
    "            return_arrays = bool(current_config.export_predictions or current_config.generate_map_plots)\n",
    "            if return_arrays:\n",
    "                metrics, preds, targets = evaluate_model(model, val_loader, device, return_arrays=True)\n",
    "            else:\n",
    "                metrics = evaluate_model(model, val_loader, device)\n",
    "                preds, targets = None, None\n",
    "            quality = run_quality_checks(metrics, current_config, param_counts, data_report)\n",
    "            metrics['quality_checks'] = quality\n",
    "\n",
    "            if quality['issues']:\n",
    "                print(f\"Warning: quality checks flagged {quality['issues']}\")\n",
    "                if current_config.enforce_quality_gates:\n",
    "                    raise ValueError(f\"Quality gates failed: {quality['issues']}\")\n",
    "\n",
    "            map_export_dir = None\n",
    "            if current_config.export_predictions and preds is not None:\n",
    "                map_export_dir = export_predictions_for_maps(\n",
    "                    preds, targets, output_root, horizon,\n",
    "                    exp_name=exp_name, model_name='V5_STACKING',\n",
    "                    config=current_config, ds=ds, data_report=data_report\n",
    "                )\n",
    "\n",
    "            # Save training history\n",
    "            history_data = {\n",
    "                'model_name': 'V5_STACKING',\n",
    "                'experiment': exp_name,\n",
    "                'feature_set_basic': feature_set_basic,\n",
    "                'feature_set_kce': feature_set_kce,\n",
    "                'weight_floor': exp_weight_floor,\n",
    "                'weight_reg_lambda': exp_weight_reg,\n",
    "                'horizon': horizon,\n",
    "                'best_epoch': trainer.best_epoch,\n",
    "                'best_val_loss': trainer.best_val_loss,\n",
    "                'final_train_loss': history['train_loss'][-1],\n",
    "                'final_val_loss': history['val_loss'][-1],\n",
    "                'total_epochs': len(history['train_loss']),\n",
    "                'parameters': param_counts['total'],\n",
    "                'branch_weights_final': history['branch_weights'][-1] if history['branch_weights'] else [0.5, 0.5],\n",
    "                'data_report': data_report,\n",
    "                'quality_checks': quality,\n",
    "                'map_export_dir': str(map_export_dir) if map_export_dir else None\n",
    "            }\n",
    "\n",
    "            with open(horizon_dir / 'v5_stacking_history.json', 'w') as f:\n",
    "                json.dump(history_data, f, indent=2)\n",
    "\n",
    "            # Save training log CSV\n",
    "            log_df = pd.DataFrame({\n",
    "                'epoch': range(1, len(history['train_loss']) + 1),\n",
    "                'train_loss': history['train_loss'],\n",
    "                'val_loss': history['val_loss'],\n",
    "                'train_mae': history['train_mae'],\n",
    "                'val_mae': history['val_mae'],\n",
    "                'lr': history['lr'],\n",
    "                'train_weight_reg': history.get('train_weight_reg', [])\n",
    "            })\n",
    "            log_df.to_csv(horizon_dir / f'v5_stacking_training_log_h{horizon}.csv', index=False)\n",
    "\n",
    "            # Collect metrics for CSV\n",
    "            for h_key, h_metrics in metrics.items():\n",
    "                if h_key.startswith('H'):\n",
    "                    h_num = int(h_key[1:])\n",
    "                    row = {\n",
    "                        'TotalHorizon': horizon,\n",
    "                        'Experiment': exp_name,\n",
    "                        'Feature_Set_Basic': feature_set_basic,\n",
    "                        'Feature_Set_GNN': feature_set_kce,\n",
    "                        'Model': 'V5_STACKING',\n",
    "                        'H': h_num,\n",
    "                        **h_metrics\n",
    "                    }\n",
    "                    all_metrics.append(row)\n",
    "\n",
    "            # Store results\n",
    "            experiment_state['results'][exp_name][f'H{horizon}'] = {\n",
    "                'metrics': metrics,\n",
    "                'history': history_data,\n",
    "                'param_counts': param_counts,\n",
    "                'data_report': data_report,\n",
    "                'quality_checks': quality,\n",
    "                'map_export_dir': str(map_export_dir) if map_export_dir else None\n",
    "            }\n",
    "\n",
    "            # Print summary\n",
    "            print(f\"\\n{exp_name} H{horizon} Results Summary:\")\n",
    "            for h_key, h_metrics in metrics.items():\n",
    "                if h_key.startswith('H'):\n",
    "                    print(f\"  {h_key}: RMSE={h_metrics['RMSE']:.2f}mm, \"\n",
    "                          f\"MAE={h_metrics['MAE']:.2f}mm, R2={h_metrics['R^2']:.4f}\")\n",
    "            print(f\"  Branch weights: ConvLSTM={metrics['branch_weights']['w_convlstm']:.2%}, \"\n",
    "                  f\"GNN={metrics['branch_weights']['w_gnn']:.2%}\")\n",
    "            if quality['issues']:\n",
    "                print(f\"  Quality issues: {', '.join(quality['issues'])}\")\n",
    "\n",
    "            # Cleanup\n",
    "            del model, trainer\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Save consolidated metrics CSV\n",
    "    if all_metrics:\n",
    "        metrics_df = pd.DataFrame(all_metrics)\n",
    "        metrics_df.to_csv(output_root / 'metrics_spatial_v5_all_horizons.csv', index=False)\n",
    "        print(f\"\\nConsolidated metrics saved to: {output_root / 'metrics_spatial_v5_all_horizons.csv'}\")\n",
    "\n",
    "    # Save experiment state\n",
    "    with open(output_root / 'experiment_state_v5.json', 'w') as f:\n",
    "        json.dump(experiment_state, f, indent=2, default=str)\n",
    "\n",
    "    return experiment_state, metrics_df if all_metrics else None\n",
    "\n",
    "print(\"Main training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-training"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 8.2: EXECUTE TRAINING (ROBUST MODE)\n",
    "# =============================================================================\n",
    "\n",
    "# Monkey-patch V5Trainer with robust error checking to diagnose/prevent CUDA asserts\n",
    "def robust_train_epoch(self) -> Tuple[float, float, float]:\n",
    "    \"\"\"Train for one epoch with added safety checks on CPU first.\"\"\"\n",
    "    self.model.train()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_weight_reg = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for i, batch in enumerate(self.train_loader):\n",
    "        # Keep data on CPU initially for checks\n",
    "        x_grid_cpu = batch['x_grid']\n",
    "        x_graph_cpu = batch['x_graph']\n",
    "        y_cpu = batch['y']\n",
    "        edge_index_cpu = batch['edge_index'][0]\n",
    "        edge_weight_cpu = batch['edge_weight'][0]\n",
    "\n",
    "        # Safety Check 1: Validate Graph Indices (CPU)\n",
    "        num_nodes = x_graph_cpu.size(2)\n",
    "        if edge_index_cpu.numel() > 0:\n",
    "            max_idx = edge_index_cpu.max().item()\n",
    "            min_idx = edge_index_cpu.min().item()\n",
    "            if max_idx >= num_nodes or min_idx < 0:\n",
    "                raise ValueError(f\"Graph index out of bounds on CPU check: range=[{min_idx}, {max_idx}], num_nodes={num_nodes}\")\n",
    "\n",
    "        # Safety Check 2: Validate Inputs (CPU)\n",
    "        if torch.isnan(x_grid_cpu).any() or torch.isnan(x_graph_cpu).any():\n",
    "             raise ValueError(\"NaNs detected in input features (CPU check)\")\n",
    "        if torch.isnan(y_cpu).any():\n",
    "             raise ValueError(\"NaNs detected in targets (CPU check)\")\n",
    "\n",
    "        # Move to Device\n",
    "        x_grid = x_grid_cpu.to(self.device)\n",
    "        x_graph = x_graph_cpu.to(self.device)\n",
    "        y = y_cpu.to(self.device)\n",
    "        edge_index = edge_index_cpu.to(self.device)\n",
    "        edge_weight = edge_weight_cpu.to(self.device)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions, weights = self.model(x_grid, x_graph, edge_index, edge_weight)\n",
    "\n",
    "        # Safety Check 3: Validate Outputs (Device)\n",
    "        if torch.isnan(predictions).any():\n",
    "            raise ValueError(f\"NaNs produced by model in batch {i}\")\n",
    "\n",
    "        # Loss computation\n",
    "        loss = self.criterion(predictions, y)\n",
    "\n",
    "        weight_reg = torch.tensor(0.0, device=self.device)\n",
    "        if self.config.weight_reg_lambda > 0:\n",
    "            weight_reg = torch.mean((weights[..., 0] - 0.5) ** 2)\n",
    "            loss = loss + self.config.weight_reg_lambda * weight_reg\n",
    "\n",
    "        # Backward pass\n",
    "        if torch.isnan(loss):\n",
    "             raise ValueError(f\"NaN loss computed in batch {i}\")\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            self.model.parameters(),\n",
    "            self.config.gradient_clip\n",
    "        )\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        total_mae += torch.mean(torch.abs(predictions - y)).item()\n",
    "        total_weight_reg += weight_reg.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return (\n",
    "        total_loss / n_batches,\n",
    "        total_mae / n_batches,\n",
    "        total_weight_reg / n_batches\n",
    "    )\n",
    "\n",
    "# Apply the patch\n",
    "V5Trainer.train_epoch = robust_train_epoch\n",
    "print(\"Applied robust training loop with CPU-side pre-checks.\")\n",
    "\n",
    "# Run experiments\n",
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"#\" + \" \"*20 + \"V5 GNN-ConvLSTM STACKING\" + \" \"*20 + \"#\")\n",
    "print(\"#\"*70)\n",
    "\n",
    "if CONFIG.light_mode:\n",
    "    print(\"\\n>>> LIGHT MODE ENABLED - Using reduced grid for testing <<<\")\n",
    "    print(f\">>> Grid size: {CONFIG.light_grid_size}x{CONFIG.light_grid_size} <<<\\n\")\n",
    "\n",
    "# Enable anomaly detection for better tracebacks\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "try:\n",
    "    experiment_state, metrics_df = run_v5_experiments(\n",
    "        ds=ds,\n",
    "        config=CONFIG,\n",
    "        edge_index=edge_index,\n",
    "        edge_weight=edge_weight,\n",
    "        output_root=OUTPUT_ROOT,\n",
    "        experiments=EXPERIMENTS\n",
    "    )\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"V5 TRAINING COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nCRITICAL TRAINING ERROR: {e}\")\n",
    "    print(\"Tip: If you see 'device-side assert triggered', try Restarting the Runtime (Runtime -> Restart session) to clear the CUDA state.\")\n",
    "    raise e\n",
    "finally:\n",
    "    torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-9"
   },
   "source": [
    "## 9. Results Export and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualization"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 9: RESULTS VISUALIZATION (700 DPI)\n",
    "# =============================================================================\n",
    "\n",
    "def plot_v5_results(metrics_df: pd.DataFrame, experiment_state: Dict, output_dir: Path):\n",
    "    \"\"\"Generate V5 results visualizations at 700 DPI.\"\"\"\n",
    "\n",
    "    if metrics_df is None or len(metrics_df) == 0:\n",
    "        print(\"No metrics to plot\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "    # 1. RMSE by Horizon\n",
    "    ax1 = axes[0, 0]\n",
    "    horizons = sorted(metrics_df['H'].unique())\n",
    "    rmse_by_h = metrics_df.groupby('H')['RMSE'].mean()\n",
    "    ax1.bar(rmse_by_h.index, rmse_by_h.values, color='steelblue', edgecolor='black')\n",
    "    ax1.set_xlabel('Prediction Horizon (months)', fontsize=12)\n",
    "    ax1.set_ylabel('RMSE (mm)', fontsize=12)\n",
    "    ax1.set_title('V5 Stacking: RMSE by Forecast Horizon', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Add target line\n",
    "    ax1.axhline(y=85, color='red', linestyle='--', label='Target: 85mm')\n",
    "    ax1.legend()\n",
    "\n",
    "    # 2. R^2 by Horizon\n",
    "    ax2 = axes[0, 1]\n",
    "    r2_by_h = metrics_df.groupby('H')['R^2'].mean()\n",
    "    ax2.bar(r2_by_h.index, r2_by_h.values, color='forestgreen', edgecolor='black')\n",
    "    ax2.set_xlabel('Prediction Horizon (months)', fontsize=12)\n",
    "    ax2.set_ylabel('R^2', fontsize=12)\n",
    "    ax2.set_title('V5 Stacking: R^2 by Forecast Horizon', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # Add target line\n",
    "    ax2.axhline(y=0.65, color='red', linestyle='--', label='Target: 0.65')\n",
    "    ax2.legend()\n",
    "\n",
    "    # 3. Bias by Horizon\n",
    "    ax3 = axes[1, 0]\n",
    "    bias_by_h = metrics_df.groupby('H')['mean_bias_mm'].mean()\n",
    "    colors = ['red' if b < 0 else 'blue' for b in bias_by_h.values]\n",
    "    ax3.bar(bias_by_h.index, bias_by_h.values, color=colors, edgecolor='black')\n",
    "    ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax3.set_xlabel('Prediction Horizon (months)', fontsize=12)\n",
    "    ax3.set_ylabel('Mean Bias (mm)', fontsize=12)\n",
    "    ax3.set_title('V5 Stacking: Bias by Forecast Horizon', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # 4. Branch Weights Evolution\n",
    "    ax4 = axes[1, 1]\n",
    "\n",
    "    labels = []\n",
    "    w_convlstm = []\n",
    "    w_gnn = []\n",
    "\n",
    "    for exp_name, exp_results in experiment_state.get('results', {}).items():\n",
    "        for h_key, results in exp_results.items():\n",
    "            if 'metrics' in results and 'branch_weights' in results['metrics']:\n",
    "                labels.append(f\"{exp_name}-{h_key}\")\n",
    "                w_convlstm.append(results['metrics']['branch_weights']['w_convlstm'])\n",
    "                w_gnn.append(results['metrics']['branch_weights']['w_gnn'])\n",
    "\n",
    "    if not labels:\n",
    "        ax4.text(0.5, 0.5, 'No branch weights found', ha='center', va='center')\n",
    "        ax4.axis('off')\n",
    "    else:\n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "\n",
    "        ax4.bar(x - width/2, w_convlstm, width, label='ConvLSTM', color='coral', edgecolor='black')\n",
    "        ax4.bar(x + width/2, w_gnn, width, label='GNN-TAT', color='teal', edgecolor='black')\n",
    "\n",
    "        ax4.set_xlabel('Experiment-Horizon', fontsize=12)\n",
    "        ax4.set_ylabel('Branch Weight', fontsize=12)\n",
    "        ax4.set_title('V5 Meta-Learner: Branch Weight Distribution', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xticks(x)\n",
    "        ax4.set_xticklabels(labels, rotation=30, ha='right')\n",
    "        ax4.legend()\n",
    "        ax4.grid(axis='y', alpha=0.3)\n",
    "        ax4.set_ylim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save at 700 DPI\n",
    "    fig_path = output_dir / 'v5_results_summary.png'\n",
    "    fig.savefig(fig_path, dpi=700, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nVisualization saved to: {fig_path} (700 DPI)\")\n",
    "\n",
    "# Generate visualizations\n",
    "plot_v5_results(metrics_df, experiment_state, OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGvybhtiqvlU"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 9.1: EVOLUTION BY HORIZON (V2/V3 STYLE)\n",
    "# =============================================================================\n",
    "\n",
    "COMP_DIR = OUTPUT_ROOT / 'comparisons'\n",
    "COMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_v5_evolution_by_horizon(metrics_df: pd.DataFrame, output_dir: Path):\n",
    "    if metrics_df is None or metrics_df.empty:\n",
    "        print('No metrics to plot for evolution panels')\n",
    "        return\n",
    "\n",
    "    metrics = ['RMSE', 'MAE', 'R^2']\n",
    "    has_tp = 'TotalPrecipitation' in metrics_df.columns and 'TotalPrecipitation_Pred' in metrics_df.columns\n",
    "    if has_tp:\n",
    "        metrics.append('TotalPrecipitation')\n",
    "\n",
    "    for total_h in sorted(metrics_df['TotalHorizon'].unique()):\n",
    "        sub_df = metrics_df[metrics_df['TotalHorizon'] == total_h].copy()\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(metrics), figsize=(6 * len(metrics), 6))\n",
    "        if len(metrics) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for metric, ax in zip(metrics, axes):\n",
    "            if metric != 'TotalPrecipitation':\n",
    "                data = sub_df.groupby(['H', 'Model'])[metric].mean().unstack()\n",
    "                data.plot(ax=ax, marker='o')\n",
    "                ax.set_ylabel(metric)\n",
    "            else:\n",
    "                data_true = sub_df.groupby(['H', 'Model'])['TotalPrecipitation'].mean().unstack()\n",
    "                data_pred = sub_df.groupby(['H', 'Model'])['TotalPrecipitation_Pred'].mean().unstack()\n",
    "                for model in data_true.columns:\n",
    "                    ax.plot(data_true.index, data_true[model], marker='o', label=f'{model} True')\n",
    "                    ax.plot(data_pred.index, data_pred[model], marker='s', linestyle='--', label=f'{model} Pred')\n",
    "                ax.set_ylabel('Total Precipitation (mm)')\n",
    "\n",
    "            ax.set_xlabel('Horizon (months)')\n",
    "            ax.set_title(f'{metric} by Horizon')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend(fontsize=8)\n",
    "\n",
    "        fig.suptitle(f'V5 Evolution by Horizon (H={total_h})', fontsize=14, fontweight='bold')\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.92])\n",
    "        fig_path = output_dir / f'v5_evolution_h{total_h}.png'\n",
    "        fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f'Evolution plot saved to: {fig_path}')\n",
    "\n",
    "\n",
    "if metrics_df is not None and len(metrics_df) > 0:\n",
    "    plot_v5_evolution_by_horizon(metrics_df, COMP_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ayt6nPwQqvlU"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 9.2: METRICS TABLE (V2/V3 STYLE)\n",
    "# =============================================================================\n",
    "\n",
    "COMP_DIR = OUTPUT_ROOT / 'comparisons'\n",
    "COMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_v5_metrics_table(metrics_df: pd.DataFrame, output_dir: Path):\n",
    "    if metrics_df is None or metrics_df.empty:\n",
    "        print('No metrics available to build summary table')\n",
    "        return\n",
    "\n",
    "    has_tp = 'TotalPrecipitation' in metrics_df.columns and 'TotalPrecipitation_Pred' in metrics_df.columns\n",
    "\n",
    "    summary_data = []\n",
    "    experiments = metrics_df['Experiment'].unique()\n",
    "    models = metrics_df['Model'].unique()\n",
    "    headers = ['Experiment', 'Model', 'RMSE', 'MAE', 'R^2', 'Total Pcp (True)', 'Total Pcp (Pred)', 'Best H']\n",
    "\n",
    "    for exp in experiments:\n",
    "        for model in models:\n",
    "            sub = metrics_df[(metrics_df['Experiment'] == exp) & (metrics_df['Model'] == model)]\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            avg_rmse = sub['RMSE'].mean()\n",
    "            avg_mae = sub['MAE'].mean()\n",
    "            avg_r2 = sub['R^2'].mean()\n",
    "            if has_tp:\n",
    "                avg_tp_t = sub['TotalPrecipitation'].mean()\n",
    "                avg_tp_p = sub['TotalPrecipitation_Pred'].mean()\n",
    "            else:\n",
    "                avg_tp_t = float('nan')\n",
    "                avg_tp_p = float('nan')\n",
    "            best_h = sub.loc[sub['RMSE'].idxmin(), 'H']\n",
    "            summary_data.append([\n",
    "                exp, model,\n",
    "                f'{avg_rmse:.4f}', f'{avg_mae:.4f}', f'{avg_r2:.4f}',\n",
    "                f'{avg_tp_t:.1f}' if has_tp else 'n/a',\n",
    "                f'{avg_tp_p:.1f}' if has_tp else 'n/a',\n",
    "                f'H={best_h}'\n",
    "            ])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(17, 10))\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=summary_data, colLabels=headers, cellLoc='center', loc='center', bbox=[0, 0, 1, 0.85])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.15, 1.8)\n",
    "\n",
    "    for j in range(len(headers)):\n",
    "        table[(0, j)].set_facecolor('#4a86e8')\n",
    "        table[(0, j)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "    fig.suptitle('Metrics Summary by Model and Experiment', fontsize=16, fontweight='bold', y=0.95)\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    fig_path = output_dir / 'v5_metrics_summary_table.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight', pad_inches=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    if summary_data:\n",
    "        print('Metrics summary table saved to:', fig_path)\n",
    "\n",
    "\n",
    "if metrics_df is not None and len(metrics_df) > 0:\n",
    "    plot_v5_metrics_table(metrics_df, COMP_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hewtd5BRqvlU"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 9.3: MODEL COMPARISON (V4 STYLE)\n",
    "# =============================================================================\n",
    "\n",
    "COMP_DIR = OUTPUT_ROOT / 'comparisons'\n",
    "COMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def plot_model_comparison_v5(res_df: pd.DataFrame, output_dir: Path):\n",
    "    if res_df is None or res_df.empty:\n",
    "        print('No results to plot for model comparison')\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    ax1 = axes[0, 0]\n",
    "    pivot_rmse = res_df.groupby(['Model', 'Experiment'])['RMSE'].mean().unstack()\n",
    "    pivot_rmse.plot(kind='bar', ax=ax1, width=0.8)\n",
    "    ax1.set_ylabel('RMSE (mm)')\n",
    "    ax1.set_title('RMSE by Model and Feature Set')\n",
    "    ax1.legend(title='Feature Set')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    ax2 = axes[0, 1]\n",
    "    pivot_r2 = res_df.groupby(['Model', 'Experiment'])['R^2'].mean().unstack()\n",
    "    pivot_r2.plot(kind='bar', ax=ax2, width=0.8)\n",
    "    ax2.set_ylabel('R^2')\n",
    "    ax2.set_title('R^2 by Model and Feature Set')\n",
    "    ax2.legend(title='Feature Set')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.axhline(y=0.628, color='red', linestyle='--', label='V4 Baseline (0.628)')\n",
    "\n",
    "    ax3 = axes[1, 0]\n",
    "    for model in res_df['Model'].unique():\n",
    "        model_data = res_df[res_df['Model'] == model]\n",
    "        for exp in model_data['Experiment'].unique():\n",
    "            exp_data = model_data[model_data['Experiment'] == exp]\n",
    "            ax3.plot(exp_data['H'], exp_data['RMSE'], marker='o', label=f\"{model} ({exp})\")\n",
    "    ax3.set_xlabel('Horizon (H)')\n",
    "    ax3.set_ylabel('RMSE (mm)')\n",
    "    ax3.set_title('RMSE Degradation by Horizon')\n",
    "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    ax4 = axes[1, 1]\n",
    "    for model in res_df['Model'].unique():\n",
    "        model_bias = res_df[res_df['Model'] == model]['mean_bias_mm']\n",
    "        ax4.hist(model_bias, bins=20, alpha=0.5, label=model)\n",
    "    ax4.axvline(x=0, color='red', linestyle='--')\n",
    "    ax4.set_xlabel('Mean Bias (mm)')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Bias Distribution by Model')\n",
    "    ax4.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_path = output_dir / 'model_comparison_v5_stacking.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Comparison plot saved to: {fig_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if metrics_df is not None and len(metrics_df) > 0:\n",
    "    plot_model_comparison_v5(metrics_df, COMP_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V7vCTj5ZqvlV"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 9.4: MAP EXPORTS (V2/V3 COMPATIBLE)\n",
    "# =============================================================================\n",
    "\n",
    "SHAPE_DIR = BASE_PATH / 'data' / 'input' / 'shapes'\n",
    "\n",
    "\n",
    "def _load_boyaca_shape(shape_dir: Path):\n",
    "    if gpd is None:\n",
    "        print('geopandas not available; overlay disabled')\n",
    "        return None, None\n",
    "    if not shape_dir.exists():\n",
    "        print(f\"[WARN] Shape directory not found: {shape_dir}\")\n",
    "        return None, None\n",
    "\n",
    "    shp_path = shape_dir / 'MGN_Departamento.shp'\n",
    "    try:\n",
    "        dept_gdf = gpd.read_file(shp_path)\n",
    "    except Exception as exc:\n",
    "        print(f\"[WARN] Failed to read shapefile {shp_path}: {exc}\")\n",
    "        return None, None\n",
    "\n",
    "    if dept_gdf is None or dept_gdf.empty:\n",
    "        print(f\"[WARN] Shapefile is empty: {shp_path}\")\n",
    "        return dept_gdf, None\n",
    "\n",
    "    col_candidates = [c for c in dept_gdf.columns if c.lower() in {\n",
    "        'nombre_dpt', 'nombre_dept', 'nombre', 'departamen', 'dpto'\n",
    "    }]\n",
    "    boyaca = None\n",
    "    if col_candidates:\n",
    "        name_col = col_candidates[0]\n",
    "        boyaca = dept_gdf[dept_gdf[name_col].astype(str).str.upper().str.contains('BOYACA', na=False)].copy()\n",
    "    elif len(dept_gdf) == 1:\n",
    "        boyaca = dept_gdf.copy()\n",
    "        print('[INFO] No name column found; assuming single feature is Boyaca')\n",
    "    else:\n",
    "        print(f\"[WARN] Could not identify Boyaca geometry; columns present: {list(dept_gdf.columns)}\")\n",
    "\n",
    "    if boyaca is not None and boyaca.empty:\n",
    "        print('[WARN] Boyaca geometry not found in shapefile; overlay disabled')\n",
    "        boyaca = None\n",
    "\n",
    "    return dept_gdf, boyaca\n",
    "\n",
    "\n",
    "DEPT_GDF, BOYACA = _load_boyaca_shape(SHAPE_DIR)\n",
    "\n",
    "MAP_OUT_ROOT = OUTPUT_ROOT / 'map_exports'\n",
    "PLOT_HORIZONS = CONFIG.enabled_horizons or [CONFIG.horizon]\n",
    "PLOT_EXPERIMENTS = [exp['name'] for exp in EXPERIMENTS] if 'EXPERIMENTS' in globals() else ['BASIC_KCE']\n",
    "PLOT_MODELS = ['V5_STACKING']\n",
    "MAP_SAMPLE_INDEX = CONFIG.map_sample_index\n",
    "CYCLE_ALL_VAL_WINDOWS = CONFIG.map_cycle_all_val_windows\n",
    "EXPORT_DPI = CONFIG.map_export_dpi\n",
    "GIF_FRAME_DURATION = CONFIG.map_gif_duration\n",
    "MAP_MAX_WINDOWS = None\n",
    "\n",
    "lats = ds[LAT_DIM].values\n",
    "lons = ds[LON_DIM].values\n",
    "EXTENT = [float(lons.min()), float(lons.max()), float(lats.min()), float(lats.max())]\n",
    "\n",
    "\n",
    "def _load_exports(exp_name: str, model_name: str, horizon: int):\n",
    "    export_dir = MAP_OUT_ROOT / f'H{horizon}' / exp_name / model_name\n",
    "    pred_f = export_dir / 'predictions.npy'\n",
    "    targ_f = export_dir / 'targets.npy'\n",
    "    meta_f = export_dir / 'metadata.json'\n",
    "    if not (pred_f.exists() and targ_f.exists() and meta_f.exists()):\n",
    "        return None, None, None, None\n",
    "    y_pred = np.load(pred_f)\n",
    "    y_true = np.load(targ_f)\n",
    "    meta = json.loads(meta_f.read_text())\n",
    "    if y_pred.ndim == 5:\n",
    "        y_pred = y_pred[..., 0]\n",
    "    if y_true.ndim == 5:\n",
    "        y_true = y_true[..., 0]\n",
    "    return y_pred, y_true, meta, export_dir\n",
    "\n",
    "\n",
    "def _plot_triplet(ax, data, title, cmap, vmin=None, vmax=None):\n",
    "    im = ax.imshow(data, origin='lower', extent=EXTENT, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    return im\n",
    "\n",
    "\n",
    "def plot_maps_v5():\n",
    "    if not MAP_OUT_ROOT.exists():\n",
    "        print(f\"Map export root not found: {MAP_OUT_ROOT}\")\n",
    "        return\n",
    "    if imageio is None:\n",
    "        print('imageio not available; GIF export disabled')\n",
    "\n",
    "    for horizon in PLOT_HORIZONS:\n",
    "        for exp_name in PLOT_EXPERIMENTS:\n",
    "            for model_name in PLOT_MODELS:\n",
    "                y_pred, y_true, meta, export_dir = _load_exports(exp_name, model_name, horizon)\n",
    "                if y_pred is None:\n",
    "                    print(f\"[SKIP] {exp_name} - {model_name} - H{horizon}: exports not found\")\n",
    "                    continue\n",
    "\n",
    "                if CYCLE_ALL_VAL_WINDOWS:\n",
    "                    indices_to_plot = range(len(y_true))\n",
    "                else:\n",
    "                    idx = MAP_SAMPLE_INDEX if MAP_SAMPLE_INDEX >= 0 else (len(y_true) - 1)\n",
    "                    idx = max(min(idx, len(y_true) - 1), 0)\n",
    "                    indices_to_plot = [idx]\n",
    "\n",
    "                if MAP_MAX_WINDOWS is not None:\n",
    "                    indices_to_plot = list(indices_to_plot)[:MAP_MAX_WINDOWS]\n",
    "\n",
    "                forecast_dates = meta.get('forecast_dates', [])\n",
    "                if not forecast_dates and meta.get('val_indices'):\n",
    "                    forecast_dates = _forecast_dates(ds, meta['val_indices'], CONFIG.input_window, horizon)\n",
    "\n",
    "                frame_count = 0\n",
    "                gif_path = export_dir / f'{model_name}_H{horizon}.gif'\n",
    "                writer = None\n",
    "\n",
    "                for idx in indices_to_plot:\n",
    "                    sample_dates = forecast_dates[idx] if idx < len(forecast_dates) else []\n",
    "                    for h in range(min(horizon, len(sample_dates) or horizon)):\n",
    "                        real = y_true[idx, h]\n",
    "                        pred = y_pred[idx, h]\n",
    "                        err = np.abs(pred - real) / (np.abs(real) + 1e-6) * 100.0\n",
    "\n",
    "                        fig, axes = plt.subplots(1, 3, figsize=(14, 4.5), constrained_layout=True)\n",
    "                        date_label = sample_dates[h] if h < len(sample_dates) else f'H{h + 1}'\n",
    "                        im0 = _plot_triplet(axes[0], real, f'Real {date_label}', 'Blues')\n",
    "                        im1 = _plot_triplet(axes[1], pred, f'{model_name} H{h + 1} {date_label}', 'Blues')\n",
    "                        im2 = _plot_triplet(axes[2], np.clip(err, 0, 100), f'MAPE% {model_name} H{h + 1} {date_label}', 'Reds', vmin=0, vmax=100)\n",
    "                        if BOYACA is not None:\n",
    "                            try:\n",
    "                                BOYACA.boundary.plot(ax=axes[0], color='k', linewidth=0.8)\n",
    "                                BOYACA.boundary.plot(ax=axes[1], color='k', linewidth=0.8)\n",
    "                                BOYACA.boundary.plot(ax=axes[2], color='k', linewidth=0.8)\n",
    "                            except Exception as exc:\n",
    "                                print(f\"[WARN] shapefile overlay failed: {exc}\")\n",
    "\n",
    "                        cbar0 = fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "                        cbar0.set_label('Precipitation (mm)', rotation=270, labelpad=12)\n",
    "                        cbar1 = fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "                        cbar1.set_label('Precipitation (mm)', rotation=270, labelpad=12)\n",
    "                        cbar2 = fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "                        cbar2.set_label('MAPE (%)', rotation=270, labelpad=12)\n",
    "\n",
    "                        title = f'{exp_name} | {model_name} | H{horizon} | {date_label}'\n",
    "                        fig.suptitle(title, fontsize=12)\n",
    "\n",
    "                        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        png_path = export_dir / f'{model_name}_H{horizon}_{date_label}.png'\n",
    "                        fig.savefig(png_path, dpi=EXPORT_DPI)\n",
    "                        plt.close(fig)\n",
    "\n",
    "                        if imageio is not None:\n",
    "                            frame = imageio.imread(png_path)\n",
    "                            if writer is None:\n",
    "                                writer = imageio.get_writer(gif_path, mode='I', duration=GIF_FRAME_DURATION, loop=0)\n",
    "                            writer.append_data(frame)\n",
    "                            del frame\n",
    "\n",
    "                        frame_count += 1\n",
    "                        del real, pred, err\n",
    "\n",
    "                if writer is not None:\n",
    "                    writer.close()\n",
    "\n",
    "                print(f\"[OK] {exp_name} - {model_name} - H{horizon}: saved {frame_count} frames to {export_dir}\")\n",
    "\n",
    "\n",
    "if getattr(CONFIG, 'generate_map_plots', True):\n",
    "    plot_maps_v5()\n",
    "else:\n",
    "    print('Map plot generation disabled')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-curves"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 9.2: TRAINING CURVES VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def plot_training_curves(experiment_state: Dict, output_dir: Path):\n",
    "    \"\"\"Plot training curves for all horizons.\"\"\"\n",
    "\n",
    "    entries = []\n",
    "    for exp_name, exp_results in experiment_state.get('results', {}).items():\n",
    "        for h_key, results in exp_results.items():\n",
    "            entries.append((exp_name, h_key, results))\n",
    "\n",
    "    if not entries:\n",
    "        print(\"No training results to plot\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    colors = ['steelblue', 'coral', 'forestgreen', 'purple']\n",
    "\n",
    "    for idx, (exp_name, h_key, results) in enumerate(entries[:4]):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        # Load training log\n",
    "        h_num = h_key.replace('H', '')\n",
    "        log_path = output_dir / f'h{h_num}' / exp_name / 'training_metrics' / f'v5_stacking_training_log_h{h_num}.csv'\n",
    "\n",
    "        if log_path.exists():\n",
    "            log_df = pd.read_csv(log_path)\n",
    "\n",
    "            ax.plot(log_df['epoch'], log_df['train_loss'], label='Train Loss', color=colors[idx], alpha=0.8)\n",
    "            ax.plot(log_df['epoch'], log_df['val_loss'], label='Val Loss', color=colors[idx], linestyle='--', alpha=0.8)\n",
    "\n",
    "            # Mark best epoch\n",
    "            best_epoch = results['history']['best_epoch'] + 1\n",
    "            best_val = results['history']['best_val_loss']\n",
    "            ax.scatter([best_epoch], [best_val], color='red', s=100, zorder=5, marker='*', label=f'Best: {best_val:.4f}')\n",
    "\n",
    "        ax.set_xlabel('Epoch', fontsize=10)\n",
    "        ax.set_ylabel('Loss', fontsize=10)\n",
    "        ax.set_title(f'Training Curves - {exp_name} {h_key}', fontsize=12, fontweight='bold')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save at 700 DPI\n",
    "    fig_path = output_dir / 'v5_training_curves.png'\n",
    "    fig.savefig(fig_path, dpi=700, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Training curves saved to: {fig_path} (700 DPI)\")\n",
    "\n",
    "# Plot training curves\n",
    "plot_training_curves(experiment_state, OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-10"
   },
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 10: SUMMARY AND RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"V5 GNN-ConvLSTM STACKING - EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Print configuration\n",
    "print(\"\\n[Configuration]\")\n",
    "print(f\"  Light Mode: {CONFIG.light_mode}\")\n",
    "if CONFIG.light_mode:\n",
    "    print(f\"  Grid Size: {CONFIG.light_grid_size}x{CONFIG.light_grid_size}\")\n",
    "print(f\"  GNN Type: {CONFIG.gnn_type}\")\n",
    "print(f\"  Horizons: {CONFIG.enabled_horizons}\")\n",
    "print(f\"  Epochs: {CONFIG.epochs}\")\n",
    "print(f\"  Batch Size: {CONFIG.batch_size}\")\n",
    "if 'EXPERIMENTS' in globals():\n",
    "    exp_names = [exp['name'] for exp in EXPERIMENTS]\n",
    "    print(f\"  Experiments: {', '.join(exp_names)}\")\n",
    "if 'data_report' in globals() and data_report:\n",
    "    print(f\"  Split Mode: {data_report.get('split_mode', 'unknown')}\")\n",
    "    print(f\"  Split Index: {data_report.get('split_time_idx', 'unknown')}\")\n",
    "\n",
    "# Print results summary\n",
    "if metrics_df is not None and len(metrics_df) > 0:\n",
    "    print(\"\\n[Results Summary]\")\n",
    "    print(\"-\"*70)\n",
    "    print(metrics_df[['TotalHorizon', 'H', 'RMSE', 'MAE', 'R^2', 'mean_bias_mm']].to_string(index=False))\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    # Overall averages\n",
    "    print(\"\\n[Overall Averages]\")\n",
    "    print(f\"  RMSE: {metrics_df['RMSE'].mean():.2f} mm (Target: < 85 mm)\")\n",
    "    print(f\"  MAE: {metrics_df['MAE'].mean():.2f} mm\")\n",
    "    print(f\"  R2: {metrics_df['R^2'].mean():.4f} (Target: > 0.65)\")\n",
    "    print(f\"  Bias: {metrics_df['mean_bias_mm'].mean():.2f} mm\")\n",
    "\n",
    "# Comparability caveats\n",
    "print(\"\\n[Comparability Caveats]\")\n",
    "if metrics_df is not None and len(metrics_df) > 0 and 'Mean_True_mm' in metrics_df.columns:\n",
    "    mean_true = metrics_df['Mean_True_mm'].mean()\n",
    "    print(f\"  Mean True (V5): {mean_true:.2f} mm\")\n",
    "print(\"  Apples-to-apples runs: BASIC_BASIC, KCE_KCE, PAFC_PAFC\")\n",
    "if 'RUN_MIXED_ABLATION' in globals() and RUN_MIXED_ABLATION:\n",
    "    print(\"  Mixed feature set BASIC_KCE is treated as an ablation\")\n",
    "else:\n",
    "    print(\"  Mixed feature sets are excluded unless RUN_MIXED_ABLATION=True\")\n",
    "print(\"  Keep dataset slice, mask, scaling, and split identical across versions\")\n",
    "print(\"  Report mean_true_mm and scale_ratio per version\")\n",
    "\n",
    "# Print branch weights\n",
    "print(\"\\n[Branch Weights (Meta-Learner)]\")\n",
    "for exp_name, exp_results in experiment_state.get('results', {}).items():\n",
    "    for h_key, results in exp_results.items():\n",
    "        if 'metrics' in results and 'branch_weights' in results['metrics']:\n",
    "            weights = results['metrics']['branch_weights']\n",
    "            print(f\"  {exp_name} {h_key}: ConvLSTM={weights['w_convlstm']:.1%}, GNN={weights['w_gnn']:.1%}\")\n",
    "\n",
    "# Quality checks\n",
    "print(\"\\n[Quality Checks]\")\n",
    "for exp_name, exp_results in experiment_state.get('results', {}).items():\n",
    "    for h_key, results in exp_results.items():\n",
    "        qc = results.get('quality_checks', {})\n",
    "        issues = qc.get('issues', [])\n",
    "        status = 'OK' if not issues else f\"WARN: {', '.join(issues)}\"\n",
    "        print(f\"  {exp_name} {h_key}: {status}\")\n",
    "\n",
    "# Output files\n",
    "print(\"\\n[Output Files]\")\n",
    "print(f\"  Output Directory: {OUTPUT_ROOT}\")\n",
    "for path in OUTPUT_ROOT.glob('*'):\n",
    "    if path.is_file():\n",
    "        print(f\"  - {path.name}\")\n",
    "\n",
    "# Next steps\n",
    "print(\"\\n[Next Steps]\")\n",
    "if CONFIG.light_mode:\n",
    "    print(\"  1. Set light_mode=False for full grid validation (61x65)\")\n",
    "    print(\"  2. Run full experiments on Colab GPU\")\n",
    "print(\"  3. Compare results with V4 baseline (R2=0.628, RMSE=92.12mm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 10.2: CLEANUP\n",
    "# =============================================================================\n",
    "\n",
    "# Memory cleanup\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU Memory freed. Current usage: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
    "\n",
    "print(\"\\nNotebook execution completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
