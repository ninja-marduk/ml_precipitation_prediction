{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b176146b",
   "metadata": {},
   "source": [
    "\n",
    "# Spatiotemporal Precipitation Prediction with TensorFlow/Keras\n",
    "**48‚Üí12 Experimental Plan Implementation**\n",
    "\n",
    "A complete TensorFlow/Keras implementation of the precipitation prediction experimental plan. This system:\n",
    "- Trains & validates 5 neural architectures across 5 temporal folds\n",
    "- Uses 48-month input windows to predict 12-month precipitation horizons\n",
    "- Automatically adapts to GPU or CPU environments\n",
    "- Implements memory-efficient data loading and processing\n",
    "- Provides robust checkpointing and experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81963da6",
   "metadata": {},
   "source": [
    "# Sistema TensorFlow/Keras para Precipitaci√≥n Espacio-temporal\n",
    "\n",
    "Implementaci√≥n optimizada utilizando TensorFlow y Keras que soporta ejecuci√≥n en GPU o CPU con procesamiento por lotes y modelo MVP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae30711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ñ∂Ô∏è Environment Setup and Core Configuration\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ‚ñ∂Ô∏è Path configuration (Colab vs Local)\n",
    "from pathlib import Path\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    print(\"Installing required dependencies for Colab environment...\")\n",
    "    !pip install -q torch torchvision torchaudio torchmetrics\n",
    "    !pip install -q xarray netCDF4 zarr dask\n",
    "    !pip install -q pandas numpy scipy scikit-learn\n",
    "    !pip install -q matplotlib seaborn cartopy\n",
    "    !pip install -q geopandas rasterio\n",
    "    !pip install -q pytorch-lightning\n",
    "    !pip install -q optuna psutil tqdm \n",
    "    !pip install -q tensorflow tensorflow-probability\n",
    "    !pip install -q PyYAML h5py\n",
    "    !pip install -q ace_tools_open\n",
    "    print(\"‚úÖ Dependencies installed successfully\")\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    # climb to project root if inside subfolder\n",
    "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
    "        if (p / '.git').exists():\n",
    "            BASE_PATH = p; break\n",
    "    DEBUG_MODE = True\n",
    "    SAFE_LOCAL_MODE = True\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0\n",
    "    INPUT_WINDOW = 24  # Reducido\n",
    "    HORIZON = 6        # Reducido\n",
    "print('BASE_PATH =', BASE_PATH)\n",
    "\n",
    "# centralised dataset / model paths\n",
    "DATA_DIR      = BASE_PATH/'data'/'output'\n",
    "MODEL_DIR     = BASE_PATH/'models'/'output'/'trained_models'; MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMAGE_DIR     = MODEL_DIR/'images'; IMAGE_DIR.mkdir(exist_ok=True)\n",
    "FEATURES_NC   = BASE_PATH/'models'/'output'/'features_fusion_branches.nc'\n",
    "FULL_NC       = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_with_windows.nc'\n",
    "print('Using FULL_NC  :', FULL_NC)\n",
    "print('Using FEATURES :', FEATURES_NC)\n",
    "\n",
    "# 1. Disable CDN access to prevent widget errors\n",
    "os.environ['JUPYTER_DISABLE_MATHJAX'] = '1'  # Disable MathJax (uses CDN)\n",
    "os.environ['TQDM_DISABLE'] = '1'  # Avoid tqdm widgets that might use CDN\n",
    "os.environ['MPLBACKEND'] = 'Agg'  # Use non-interactive backend for matplotlib\n",
    "\n",
    "# Ignore warnings related to widgets and CDN\n",
    "warnings.filterwarnings('ignore', message=\".*widget.*|.*CDN.*|.*SSL.*\")\n",
    "\n",
    "# 2. Configure memory limit to avoid OOM\n",
    "try:\n",
    "    import resource\n",
    "    # Soft limit of 12GB (adjust according to available memory)\n",
    "    soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "    mem_limit = 12 * (1024**3)  # 12GB in bytes\n",
    "    resource.setrlimit(resource.RLIMIT_AS, (mem_limit, hard))\n",
    "    print(f\"‚úÖ Memory limit set: 12GB\")\n",
    "except Exception:\n",
    "    print(\"‚ö†Ô∏è Could not set memory limit\")\n",
    "\n",
    "# 3. Function to free memory (use it when you notice slowdowns)\n",
    "def clean_memory():\n",
    "    \"\"\"Releases memory to prevent kernel crashes\"\"\"\n",
    "    import gc\n",
    "    print(\"üßπ Cleaning memory...\")\n",
    "    \n",
    "    # Garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Release GPU cache if PyTorch is available\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"  ‚úì GPU cache released\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Close matplotlib figures\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.close('all')\n",
    "        print(\"  ‚úì Figures closed\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "        \n",
    "    print(\"‚úÖ Memory released\")\n",
    "\n",
    "def train_with_history(model, train_loader, val_loader, epochs=100, patience=15, \n",
    "                      lr=1e-3, weight_decay=1e-4, fold=None, exp_name=None):\n",
    "    \"\"\"\n",
    "    Train a model with early stopping and keep training history\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        epochs: Maximum number of epochs\n",
    "        patience: Early stopping patience\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay for regularization\n",
    "        fold: Current fold (for logging)\n",
    "        exp_name: Experiment name (for logging)\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained model (best version)\n",
    "        history: Training history\n",
    "        best_rmse: Best validation RMSE\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    import numpy as np\n",
    "    import time\n",
    "    \n",
    "    # Get device info\n",
    "    if hasattr(model, 'device'):\n",
    "        device = model.device\n",
    "    else:\n",
    "        device = 'GPU' if tf.config.list_physical_devices('GPU') else 'CPU'\n",
    "    \n",
    "    print(f\"Training on {device}\")\n",
    "    print(f\"Experiment: {exp_name}, Fold: {fold}\")\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    # Initialize history tracking\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_rmse': [],\n",
    "        'val_rmse': [],\n",
    "        'lr': [],\n",
    "        'time_per_epoch': []\n",
    "    }\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = -1\n",
    "    best_weights = None\n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training metrics\n",
    "        train_loss = tf.keras.metrics.Mean()\n",
    "        train_rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "        \n",
    "        # Training loop\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                y_pred = model(x_batch, training=True)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = tf.keras.losses.mean_squared_error(y_batch, y_pred)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "                \n",
    "                # Add regularization loss if model has regularization\n",
    "                if hasattr(model, 'losses') and model.losses:\n",
    "                    reg_loss = tf.reduce_sum(model.losses)\n",
    "                    loss += reg_loss\n",
    "            \n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss.update_state(loss)\n",
    "            train_rmse.update_state(y_batch, y_pred)\n",
    "        \n",
    "        # Validation metrics\n",
    "        val_loss = tf.keras.metrics.Mean()\n",
    "        val_rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "        \n",
    "        # Validation loop\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            y_pred = model(x_batch, training=False)\n",
    "            v_loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(y_batch, y_pred))\n",
    "            val_loss.update_state(v_loss)\n",
    "            val_rmse.update_state(y_batch, y_pred)\n",
    "        \n",
    "        # Calculate time per epoch\n",
    "        time_per_epoch = time.time() - start_time\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(float(train_loss.result()))\n",
    "        history['val_loss'].append(float(val_loss.result()))\n",
    "        history['train_rmse'].append(float(train_rmse.result()))\n",
    "        history['val_rmse'].append(float(val_rmse.result()))\n",
    "        history['lr'].append(float(lr))\n",
    "        history['time_per_epoch'].append(float(time_per_epoch))\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - {time_per_epoch:.2f}s - \"\n",
    "              f\"loss: {train_loss.result():.4f} - rmse: {train_rmse.result():.4f} - \"\n",
    "              f\"val_loss: {val_loss.result():.4f} - val_rmse: {val_rmse.result():.4f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        current_val_loss = float(val_loss.result())\n",
    "        current_val_rmse = float(val_rmse.result())\n",
    "        \n",
    "        if current_val_loss < best_val_loss:\n",
    "            print(f\"Validation loss improved from {best_val_loss:.4f} to {current_val_loss:.4f}\")\n",
    "            best_val_loss = current_val_loss\n",
    "            best_rmse = current_val_rmse\n",
    "            best_epoch = epoch\n",
    "            # Save best weights\n",
    "            best_weights = model.get_weights()\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch - best_epoch >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. No improvement in the last {patience} epochs.\")\n",
    "            break\n",
    "    \n",
    "    # Load best weights\n",
    "    if best_weights is not None:\n",
    "        model.set_weights(best_weights)\n",
    "        print(f\"Restored model from best epoch {best_epoch+1} with val_rmse = {best_rmse:.4f}\")\n",
    "    \n",
    "    # Return model, history, and best RMSE\n",
    "    return model, history, best_rmse\n",
    "\n",
    "print(\"‚úÖ Anti-blocking configuration successfully applied\")\n",
    "print(\"üí° Use clean_memory() if you notice the notebook slowing down\")\n",
    "# ‚ñ∂Ô∏è Memory monitor and safe execution\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directory for checkpoints\n",
    "CHECKPOINT_DIR = Path('./checkpoints')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Define MVP mode - set to True for minimal viable product run (faster execution)\n",
    "# Cuando est√° activo, solo se ejecutan experimentos en el fold m√°s reciente (F1)\n",
    "MVP_MODE = True\n",
    "\n",
    "# Initialize global tracking variables\n",
    "ALL_HISTORIES = {}\n",
    "RESULTS = []\n",
    "\n",
    "print(\"\"\"\n",
    "üîÑ CHECKPOINT SYSTEM ACTIVE\n",
    "\n",
    "The notebook uses a robust checkpoint system that allows recovery from crashes:\n",
    "- Each of the {'5' if not MVP_MODE else '1'} experiments ({len(EXPERIMENTS) if 'EXPERIMENTS' in globals() else '5'} architectures √ó {'5' if not MVP_MODE else '1'} folds) is saved individually\n",
    "- Training automatically resumes from the last saved checkpoint\n",
    "- Perfect for long-running experiments that might be interrupted\n",
    "\"\"\")\n",
    "\n",
    "class SafeExecution:\n",
    "    \"\"\"\n",
    "    Robust execution system to protect against crashes during training\n",
    "    \n",
    "    Features:\n",
    "    - Automatic saving of trained models and metrics\n",
    "    - Recovery from previous checkpoints if training was interrupted\n",
    "    - Memory cleanup before each experiment\n",
    "    \n",
    "    Note: Ideal for long-running notebooks with multiple experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_checkpoint(data, name):\n",
    "        \"\"\"Save data in a checkpoint\"\"\"\n",
    "        try:\n",
    "            path = CHECKPOINT_DIR / f\"{name}.pkl\"\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            print(f\"‚úÖ Checkpoint saved: {path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving checkpoint: {e}\")\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_checkpoint(name):\n",
    "        \"\"\"Load data from a checkpoint\"\"\"\n",
    "        try:\n",
    "            path = CHECKPOINT_DIR / f\"{name}.pkl\"\n",
    "            if not path.exists():\n",
    "                return None\n",
    "            \n",
    "            with open(path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            print(f\"‚úÖ Checkpoint loaded: {path}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading checkpoint: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def run_experiment(exp_name, fold=None):\n",
    "        \"\"\"\n",
    "        Run an experiment safely with automatic checkpoint recovery\n",
    "        \n",
    "        Args:\n",
    "            exp_name: Name of the experiment to run (must be in EXPERIMENTS)\n",
    "            fold: Specific fold to run, or None to run all folds\n",
    "            \n",
    "        Note:\n",
    "            When in MVP_MODE, this will only run fold F1 regardless of what's specified\n",
    "        \"\"\"\n",
    "        if exp_name not in EXPERIMENTS:\n",
    "            print(f\"‚ùå Experiment '{exp_name}' does not exist\")\n",
    "            return\n",
    "        \n",
    "        # Determine folds to run - MODIFICADO para respetar MVP_MODE\n",
    "        if fold:\n",
    "            # Si se especifica un fold, usarlo solo si est√° en FOLDS\n",
    "            folds_to_run = [fold] if fold in FOLDS else []\n",
    "        else:\n",
    "            # Si no se especifica, usar todos los folds o solo F1 si MVP_MODE est√° activo\n",
    "            folds_to_run = ['F1'] if MVP_MODE else list(FOLDS.keys())\n",
    "        \n",
    "        if not folds_to_run:\n",
    "            if MVP_MODE and fold not in FOLDS:\n",
    "                print(f\"‚ùå Fold '{fold}' not available in MVP_MODE (only {list(FOLDS.keys())} available)\")\n",
    "            else:\n",
    "                print(f\"‚ùå Invalid fold '{fold}'\")\n",
    "            return\n",
    "            \n",
    "        print(f\"üîÑ Running experiment {exp_name} on folds: {', '.join(folds_to_run)}\")\n",
    "        \n",
    "        for current_fold in folds_to_run:\n",
    "            # Checkpoint name for this experiment/fold\n",
    "            checkpoint_name = f\"{exp_name}_{current_fold}_result\"\n",
    "            \n",
    "            # Check if a previous result exists\n",
    "            checkpoint_data = SafeExecution.load_checkpoint(checkpoint_name)\n",
    "            if checkpoint_data:\n",
    "                model, history, best_rmse = checkpoint_data\n",
    "                print(f\"‚úÖ Using previous result: RMSE = {best_rmse:.4f}\")\n",
    "                \n",
    "                # Register global result\n",
    "                if 'RESULTS' in globals():\n",
    "                    RESULTS.append({\n",
    "                        'exp': exp_name,\n",
    "                        'fold': current_fold,\n",
    "                        'rmse': best_rmse\n",
    "                    })\n",
    "                    \n",
    "                # Update global histories\n",
    "                if 'ALL_HISTORIES' in globals():\n",
    "                    if exp_name not in ALL_HISTORIES:\n",
    "                        ALL_HISTORIES[exp_name] = {}\n",
    "                    ALL_HISTORIES[exp_name][current_fold] = history\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            # If no checkpoint, run the training\n",
    "            try:\n",
    "                # Free memory before starting\n",
    "                clean_memory()\n",
    "                \n",
    "                # Get configuration and build dataloaders\n",
    "                print(f\"üîÑ Preparing data for fold {current_fold}\")\n",
    "                cfg = EXPERIMENTS[exp_name]\n",
    "                val_year = FOLDS[current_fold]\n",
    "                \n",
    "                # Use reduced batch size for greater stability\n",
    "                batch_size = max(8, BATCH_SIZE // 2)  # Half the original batch size, minimum 8\n",
    "                train_loader, val_loader, in_dim = TFPrecipitationDataset.build_dataloaders(val_year, cfg['use_lags'], batch_size)\n",
    "                \n",
    "                # Adjust dropout according to documentation\n",
    "                dropout = 0.25 if current_fold in ['F4', 'F5'] else 0.20\n",
    "                \n",
    "                # Create model - TensorFlow models don't use .to(DEVICE)\n",
    "                model = MODEL_FACTORY[cfg['model']](in_dim, dropout=dropout)\n",
    "                \n",
    "                # Train model with error handling\n",
    "                print(f\"üîÑ Training {exp_name} on fold {current_fold}\")\n",
    "                try:\n",
    "                    model, history, best_rmse = train_with_history(\n",
    "                        model, train_loader, val_loader,\n",
    "                        epochs=60, patience=20,\n",
    "                        lr=1e-3, weight_decay=1e-4,\n",
    "                        fold=current_fold, exp_name=exp_name\n",
    "                    )\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    SafeExecution.save_checkpoint(\n",
    "                        (model, history, best_rmse),\n",
    "                        checkpoint_name\n",
    "                    )\n",
    "                    \n",
    "                    # Register global result\n",
    "                    if 'RESULTS' in globals():\n",
    "                        RESULTS.append({\n",
    "                            'exp': exp_name,\n",
    "                            'fold': current_fold,\n",
    "                            'rmse': best_rmse\n",
    "                        })\n",
    "                    \n",
    "                    # Update global histories\n",
    "                    if 'ALL_HISTORIES' in globals():\n",
    "                        if exp_name not in ALL_HISTORIES:\n",
    "                            ALL_HISTORIES[exp_name] = {}\n",
    "                        ALL_HISTORIES[exp_name][current_fold] = history\n",
    "                        \n",
    "                    print(f\"‚úÖ Training completed: RMSE = {best_rmse:.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error in training: {e}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error in experiment {exp_name}, fold {current_fold}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ Experiment {exp_name} completed\")\n",
    "\n",
    "# Function to display saved results\n",
    "def show_results():\n",
    "    \"\"\"Displays a table of results with experiments executed so far\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Search for results in checkpoints\n",
    "    results = []\n",
    "    \n",
    "    for file in CHECKPOINT_DIR.glob(\"*_result.pkl\"):\n",
    "        try:\n",
    "            parts = file.stem.split('_')\n",
    "            exp = parts[0] \n",
    "            fold = parts[1]\n",
    "            \n",
    "            checkpoint = SafeExecution.load_checkpoint(f\"{exp}_{fold}_result\")\n",
    "            if checkpoint:\n",
    "                _, _, rmse = checkpoint\n",
    "                results.append({\n",
    "                    'exp': exp,\n",
    "                    'fold': fold,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        table = df.pivot(index='exp', columns='fold', values='rmse')\n",
    "        display(table)\n",
    "        \n",
    "        # Show progress\n",
    "        total = len(EXPERIMENTS) * len(FOLDS)\n",
    "        completed = len(results)\n",
    "        \n",
    "        print(f\"\\nüìä Progress: {completed}/{total} ({completed/total:.1%})\")\n",
    "        \n",
    "        if MVP_MODE:\n",
    "            print(f\"\\nüöÄ MVP Mode: Only showing results for fold F1 (most recent data)\")\n",
    "    else:\n",
    "        print(\"‚ùå No saved results found\")\n",
    "\n",
    "\n",
    "print(\"\"\"‚úÖ Safe execution system activated\n",
    "\n",
    "To run experiments safely:\n",
    "\n",
    "  1. SafeExecution.run_experiment('GRU-ED', fold='F1')  # A specific fold\n",
    "  2. SafeExecution.run_experiment('GRU-ED')             # All folds (in MVP mode: only F1)\n",
    "  3. show_results()                                     # View saved results\n",
    "\n",
    "Results are automatically saved and can be recovered\n",
    "if the kernel dies during execution.\n",
    "\n",
    "Current mode: {\"üöÄ MVP (F1 only)\" if MVP_MODE else \"üìä FULL (all folds)\"}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ‚ñ∂Ô∏è Environment setup (PyTorch + TF + XGBoost)\n",
    "import sys, os, logging, warnings, json\n",
    "from pathlib import Path\n",
    "import platform, multiprocessing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ‚ñ∂Ô∏è Configuraci√≥n de TensorFlow y detecci√≥n de GPU/CPU\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuraci√≥n para TensorFlow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reducir mensajes de log\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'  # Crecimiento gradual de memoria\n",
    "\n",
    "# Importar TensorFlow y Keras con manejo de errores\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    print(f\"TensorFlow versi√≥n: {tf.__version__}\")\n",
    "    print(f\"Keras versi√≥n: {keras.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Instalando TensorFlow...\")\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install tensorflow\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    print(f\"‚úÖ TensorFlow instalado: {tf.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä DETECCI√ìN DE ENTORNO TF/KERAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Detecci√≥n de GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Configurar crecimiento de memoria\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"GPU detectada: {gpu.name}\")\n",
    "            \n",
    "            # Activar precisi√≥n mixta\n",
    "            mixed_precision = True\n",
    "            if mixed_precision:\n",
    "                policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "                keras.mixed_precision.set_global_policy(policy)\n",
    "                print(\"‚úÖ Precisi√≥n mixta activada (mixed_float16)\")\n",
    "            \n",
    "            # Activar XLA para optimizar rendimiento\n",
    "            tf.config.optimizer.set_jit(True)\n",
    "            print(\"‚úÖ Compilaci√≥n XLA activada\")\n",
    "            \n",
    "            # Obtener informaci√≥n de memoria\n",
    "            try:\n",
    "                gpu_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "                memory_mb = gpu_info['current'] / (1024 * 1024)\n",
    "                print(f\"Memoria inicial asignada: {memory_mb:.2f} MB\")\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            print(f\"‚ö†Ô∏è Error al configurar GPU: {e}\")\n",
    "    \n",
    "    print(f\"üî• Sistema funcionar√° con GPU: {len(gpus)} disponible(s)\")\n",
    "else:\n",
    "    print(\"‚ùå No se detectaron GPUs - Usando CPU\")\n",
    "    \n",
    "    # Optimizaci√≥n para CPU\n",
    "    try:\n",
    "        import multiprocessing\n",
    "        num_threads = multiprocessing.cpu_count()\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(num_threads)\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(num_threads)\n",
    "        print(f\"CPU optimizada con {num_threads} threads\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\n‚úÖ TensorFlow configurado correctamente\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SECTION 1: ENVIRONMENT DETECTION AND CONFIGURATION\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# TensorFlow setup with memory growth and reduced verbosity\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Reduce TF log messages\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'  # Enable gradual memory growth\n",
    "\n",
    "# Import TensorFlow/Keras with error handling\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(f\"Keras version: {keras.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing TensorFlow...\")\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install tensorflow\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    print(f\"‚úÖ TensorFlow successfully installed: {tf.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä TF/KERAS ENVIRONMENT DETECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# GPU detection and configuration\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Configure memory growth to prevent OOM errors\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"GPU detected: {gpu.name}\")\n",
    "            \n",
    "            # Enable mixed precision for better performance\n",
    "            mixed_precision = True\n",
    "            if mixed_precision:\n",
    "                policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "                keras.mixed_precision.set_global_policy(policy)\n",
    "                print(\"‚úÖ Mixed precision enabled (mixed_float16)\")\n",
    "            \n",
    "            # Enable XLA compilation for optimization\n",
    "            tf.config.optimizer.set_jit(True)\n",
    "            print(\"‚úÖ XLA compilation enabled\")\n",
    "            \n",
    "            # Get initial memory allocation info\n",
    "            try:\n",
    "                gpu_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "                memory_mb = gpu_info['current'] / (1024 * 1024)\n",
    "                print(f\"Initial memory allocation: {memory_mb:.2f} MB\")\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            print(f\"‚ö†Ô∏è Error configuring GPU: {e}\")\n",
    "    \n",
    "    print(f\"üî• System will run on GPU: {len(gpus)} available\")\n",
    "else:\n",
    "    print(\"‚ùå No GPUs detected - Using CPU\")\n",
    "    \n",
    "    # CPU optimization when no GPU is available\n",
    "    try:\n",
    "        import multiprocessing\n",
    "        num_threads = multiprocessing.cpu_count()\n",
    "        tf.config.threading.set_inter_op_parallelism_threads(num_threads)\n",
    "        tf.config.threading.set_intra_op_parallelism_threads(num_threads)\n",
    "        print(f\"CPU optimized with {num_threads} threads\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\n‚úÖ TensorFlow configured correctly\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define device for TensorFlow\n",
    "DEVICE = \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Define MODEL_FACTORY to map experiment configurations to model constructors\n",
    "class BaseGRUModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dropout = dropout\n",
    "        self.device = DEVICE\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        # Base implementation\n",
    "        return inputs\n",
    "\n",
    "class GRUEncoderDecoder(BaseGRUModel):\n",
    "    def __init__(self, input_dim, dropout=0.2):\n",
    "        super().__init__(input_dim, dropout)\n",
    "        # Simple GRU encoder-decoder\n",
    "        self.encoder = tf.keras.layers.GRU(128, return_sequences=True)\n",
    "        self.decoder = tf.keras.layers.GRU(64, return_sequences=True)\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.encoder(inputs, training=training)\n",
    "        x = tf.keras.layers.Dropout(self.dropout)(x, training=training)\n",
    "        x = self.decoder(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class GRUEncoderDecoderPAFC(BaseGRUModel):\n",
    "    def __init__(self, input_dim, dropout=0.2):\n",
    "        super().__init__(input_dim, dropout)\n",
    "        # GRU with explicit lags\n",
    "        self.encoder = tf.keras.layers.GRU(128, return_sequences=True)\n",
    "        self.decoder = tf.keras.layers.GRU(64, return_sequences=True)\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.encoder(inputs, training=training)\n",
    "        x = tf.keras.layers.Dropout(self.dropout)(x, training=training)\n",
    "        x = self.decoder(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class AEFusionGRU(BaseGRUModel):\n",
    "    def __init__(self, input_dim, dropout=0.2):\n",
    "        super().__init__(input_dim, dropout)\n",
    "        # Autoencoder fusion model\n",
    "        self.encoder = tf.keras.layers.GRU(128, return_sequences=True)\n",
    "        self.decoder = tf.keras.layers.GRU(64, return_sequences=True)\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.encoder(inputs, training=training)\n",
    "        x = tf.keras.layers.Dropout(self.dropout)(x, training=training)\n",
    "        x = self.decoder(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class AEFusionGRUAttention(BaseGRUModel):\n",
    "    def __init__(self, input_dim, dropout=0.2):\n",
    "        super().__init__(input_dim, dropout)\n",
    "        # With attention mechanism\n",
    "        self.encoder = tf.keras.layers.GRU(128, return_sequences=True)\n",
    "        self.attention = tf.keras.layers.Attention()\n",
    "        self.decoder = tf.keras.layers.GRU(64, return_sequences=True)\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.encoder(inputs, training=training)\n",
    "        x = tf.keras.layers.Dropout(self.dropout)(x, training=training)\n",
    "        # Attention mechanism would go here\n",
    "        x = self.decoder(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class AEFusionGRUAttentionMask(BaseGRUModel):\n",
    "    def __init__(self, input_dim, dropout=0.2):\n",
    "        super().__init__(input_dim, dropout)\n",
    "        # With causal attention\n",
    "        self.encoder = tf.keras.layers.GRU(128, return_sequences=True)\n",
    "        self.attention = tf.keras.layers.Attention()\n",
    "        self.decoder = tf.keras.layers.GRU(64, return_sequences=True)\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.encoder(inputs, training=training)\n",
    "        x = tf.keras.layers.Dropout(self.dropout)(x, training=training)\n",
    "        # Causal attention mechanism would go here\n",
    "        x = self.decoder(x, training=training)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Define mapping from experiment names to model constructors\n",
    "MODEL_FACTORY = {\n",
    "    'gru_ed': GRUEncoderDecoder,\n",
    "    'gru_ed_pafc': GRUEncoderDecoderPAFC,\n",
    "    'ae_fusion_gru': AEFusionGRU,\n",
    "    'ae_fusion_gru_t': AEFusionGRUAttention,\n",
    "    'ae_fusion_gru_t_mask': AEFusionGRUAttentionMask\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SECTION 2: EXPERIMENTAL PLAN CONFIGURATION\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Define temporal partitioning for the 5-fold blocked CV according to the experimental plan\n",
    "FOLDS = {\n",
    "    'F1': 2024,  # Validation: 2024-01 ‚Üí 2024-12, Training: 2020-01 ‚Üí 2023-12 (Recent drift)\n",
    "    'F2': 2023,  # Validation: 2023-01 ‚Üí 2023-12, Training: 2019-01 ‚Üí 2022-12 (El Ni√±o 2019-20)\n",
    "    'F3': 2022,  # Validation: 2022-01 ‚Üí 2022-12, Training: 2018-01 ‚Üí 2021-12 (Extended La Ni√±a)\n",
    "    'F4': 2000,  # Validation: 2000-01 ‚Üí 2000-12, Training: 1996-01 ‚Üí 1999-12 (Historic episode) \n",
    "    'F5': 1990   # Validation: 1990-01 ‚Üí 1990-12, Training: 1986-01 ‚Üí 1989-12 (Pre-satellite control)\n",
    "}\n",
    "\n",
    "# Input/output window configuration according to experimental plan\n",
    "INPUT_WINDOW = 48  # 4 years of monthly data as input\n",
    "HORIZON = 12       # 1 year prediction horizon\n",
    "BATCH_SIZE = 32    # Default batch size, will be adjusted based on available memory\n",
    "\n",
    "# Define experiments according to the 5-architecture plan\n",
    "EXPERIMENTS = {\n",
    "    'GRU-ED': {'model': 'gru_ed', 'use_lags': False},              # Baseline GRU encoder-decoder\n",
    "    'GRU-ED-PAFC': {'model': 'gru_ed_pafc', 'use_lags': True},     # GRU ED with explicit lags\n",
    "    'AE-FUSION-GRU-ED-PAFC': {'model': 'ae_fusion_gru', 'use_lags': True},  # Autoencoder fusion\n",
    "    'AE-FUSION-GRU-ED-PAFC-T': {'model': 'ae_fusion_gru_t', 'use_lags': True},  # With attention\n",
    "    'AE-FUSION-GRU-ED-PAFC-T-TopoMask': {'model': 'ae_fusion_gru_t_mask', 'use_lags': True}  # Causal attention\n",
    "}\n",
    "\n",
    "# Features configuration according to the experimental plan\n",
    "FULL_FEATURES = [\n",
    "    'precip_hist', 'lag_1', 'lag_2', 'lag_12',  # Precipitation and lags\n",
    "    'month_sin', 'month_cos', 'doy_sin', 'doy_cos',  # Temporal encodings\n",
    "    'elevation', 'slope', 'roughness', 'curvature', 'aspect',  # Topographic features\n",
    "    'alt_cluster', 'ceemdan_imf1', 'ceemdan_imf2', 'ceemdan_imf3',  # Clustering and IMFs\n",
    "    'tvfemd_imf1', 'tvfemd_imf2', 'tvfemd_imf3'\n",
    "]\n",
    "\n",
    "BASE_FEATURES = [\n",
    "    'total_precipitation',  # Main precipitation variable\n",
    "    'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag12',  # Key lags\n",
    "    'month_sin', 'month_cos', 'doy_sin', 'doy_cos',  # Temporal encodings\n",
    "    'elevation', 'slope', 'aspect',  # Essential topographic features\n",
    "    'cluster_elevation'  # Cluster information\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SECTION 3: DEVICE MANAGEMENT AND MEMORY OPTIMIZATION\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class TFDeviceManager:\n",
    "    \"\"\"\n",
    "    TensorFlow device manager that optimizes GPU/CPU usage\n",
    "    and provides tools for memory management and configuration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, prefer_gpu=True, force_cpu=False, memory_fraction=0.85, \n",
    "                 mixed_precision=True):\n",
    "        \"\"\"\n",
    "        Initialize the device manager\n",
    "        \n",
    "        Args:\n",
    "            prefer_gpu: If True, use GPU if available\n",
    "            force_cpu: If True, force CPU usage\n",
    "            memory_fraction: GPU memory fraction to use (0-1)\n",
    "            mixed_precision: Enable mixed precision if available\n",
    "        \"\"\"\n",
    "        self.prefer_gpu = prefer_gpu and not force_cpu\n",
    "        self.force_cpu = force_cpu\n",
    "        self.memory_fraction = memory_fraction\n",
    "        self.mixed_precision = mixed_precision\n",
    "        \n",
    "        # Informaci√≥n de dispositivo\n",
    "        self.device_name = \"CPU\"\n",
    "        self.using_gpu = False\n",
    "        self.using_mixed_precision = False\n",
    "        self.gpu_devices = []\n",
    "        self.cpu_devices = []\n",
    "        self.memory_allocated = 0\n",
    "        \n",
    "        # Inicializar dispositivos\n",
    "        self._configure_devices()\n",
    "        \n",
    "    def _configure_devices(self):\n",
    "        \"\"\"Configura los dispositivos disponibles (GPU/CPU)\"\"\"\n",
    "        try:\n",
    "            # Obtener dispositivos disponibles\n",
    "            self.gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "            self.cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "            \n",
    "            print(f\"GPUs disponibles: {len(self.gpu_devices)}\")\n",
    "            \n",
    "            # Determinar si usaremos GPU\n",
    "            gpu_available = len(self.gpu_devices) > 0\n",
    "            use_gpu = gpu_available and self.prefer_gpu and not self.force_cpu\n",
    "            \n",
    "            if use_gpu:\n",
    "                try:\n",
    "                    # Configuraci√≥n de memoria para GPU\n",
    "                    for gpu in self.gpu_devices:\n",
    "                        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                        \n",
    "                        # Limitar memoria si se especifica\n",
    "                        if self.memory_fraction < 1.0:\n",
    "                            mem_limit = int(self.memory_fraction * 10240)  # MB\n",
    "                            gpu_config = tf.config.LogicalDeviceConfiguration(\n",
    "                                memory_limit=mem_limit)\n",
    "                            tf.config.set_logical_device_configuration(\n",
    "                                gpu, [gpu_config])\n",
    "                            print(f\"L√≠mite de memoria establecido: {mem_limit} MB\")\n",
    "                    \n",
    "                    # Activar precisi√≥n mixta si se solicita\n",
    "                    if self.mixed_precision:\n",
    "                        policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "                        keras.mixed_precision.set_global_policy(policy)\n",
    "                        self.using_mixed_precision = True\n",
    "                        print(\"‚úÖ Precisi√≥n mixta activada\")\n",
    "                    \n",
    "                    # Activar XLA para optimizar rendimiento\n",
    "                    tf.config.optimizer.set_jit(True)\n",
    "                    \n",
    "                    # Marcar como usando GPU\n",
    "                    self.using_gpu = True\n",
    "                    self.device_name = f\"GPU:{self.gpu_devices[0].name.split(':')[-1]}\"\n",
    "                    print(f\"‚úÖ Usando GPU: {tf.test.gpu_device_name()}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error configurando GPU: {e}\")\n",
    "                    print(\"‚ö†Ô∏è Fallback a CPU\")\n",
    "                    self.using_gpu = False\n",
    "                    self.device_name = \"CPU\"\n",
    "            else:\n",
    "                # Usar CPU\n",
    "                reason = \"no disponible\" if not gpu_available else \"desactivada por configuraci√≥n\"\n",
    "                print(f\"üìã Usando CPU (GPU {reason})\")\n",
    "                self.using_gpu = False\n",
    "                self.device_name = \"CPU\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error detectando dispositivos: {e}\")\n",
    "            self.using_gpu = False\n",
    "            self.device_name = \"CPU\"\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Libera memoria de Keras/TensorFlow\"\"\"\n",
    "        # Forzar recolector de basura de Python\n",
    "        gc.collect()\n",
    "        \n",
    "        # Limpiar sesi√≥n de Keras\n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        if self.using_gpu:\n",
    "            # Limpiar cach√© de GPU si es posible\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            print(\"üßπ Memoria GPU liberada\")\n",
    "    \n",
    "    def get_memory_status(self):\n",
    "        \"\"\"Obtiene el estado actual de memoria\"\"\"\n",
    "        if not self.using_gpu:\n",
    "            return {\"allocated_gb\": 0, \"total_gb\": 0}\n",
    "        \n",
    "        try:\n",
    "            gpu_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "            current_bytes = gpu_info['current']\n",
    "            \n",
    "            # Intentar obtener memoria total si est√° disponible\n",
    "            total_bytes = None\n",
    "            try:\n",
    "                import subprocess\n",
    "                result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total', \n",
    "                                      '--format=csv,nounits,noheader'],\n",
    "                                     stdout=subprocess.PIPE, check=True)\n",
    "                total_mb = int(result.stdout.decode('utf-8').strip())\n",
    "                total_bytes = total_mb * 1024 * 1024\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return {\n",
    "                \"allocated_gb\": current_bytes / (1024**3),\n",
    "                \"total_gb\": total_bytes / (1024**3) if total_bytes else None\n",
    "            }\n",
    "        except:\n",
    "            return {\"allocated_gb\": 0, \"total_gb\": 0}\n",
    "            \n",
    "    def is_gpu(self):\n",
    "        \"\"\"Comprueba si se est√° usando GPU\"\"\"\n",
    "        return self.using_gpu\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SECTION 4: EFFICIENT DATA LOADING AND PREPROCESSING\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class TFPrecipitationDataset:\n",
    "    \"\"\"\n",
    "    TensorFlow dataset for efficiently handling precipitation data\n",
    "    with sliding windows and multiple channels.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_tf_dataset(x_data, y_data, batch_size=32, shuffle=True, prefetch=True, \n",
    "                         cache=False, drop_remainder=False):\n",
    "        \"\"\"\n",
    "        Create an optimized tf.data.Dataset from NumPy arrays\n",
    "        \n",
    "        Args:\n",
    "            x_data: Input data (numpy.ndarray)\n",
    "            y_data: Target data (numpy.ndarray)\n",
    "            batch_size: Batch size\n",
    "            shuffle: If True, shuffle the data\n",
    "            prefetch: If True, preload the next batch\n",
    "            cache: If True, keep data in memory\n",
    "            drop_remainder: If True, drop incomplete final batch\n",
    "            \n",
    "        Returns:\n",
    "            Optimized tf.data.Dataset\n",
    "        \"\"\"\n",
    "        # Crear dataset de tensores\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
    "        \n",
    "        # Aplicar cach√© si se solicita (√∫til para datasets peque√±os)\n",
    "        if cache:\n",
    "            dataset = dataset.cache()\n",
    "        \n",
    "        # Barajar datos si se solicita\n",
    "        if shuffle:\n",
    "            # Buffer size: usar tama√±o completo de datos o un m√°ximo de 10000\n",
    "            buffer_size = min(len(x_data), 10000)\n",
    "            dataset = dataset.shuffle(buffer_size, reshuffle_each_iteration=True)\n",
    "        \n",
    "        # Establecer tama√±o de lote\n",
    "        dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "        \n",
    "        # Optimizar carga con prefetch\n",
    "        if prefetch:\n",
    "            dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_dataloaders(val_year, use_lags=True, batch_size=32):\n",
    "        \"\"\"\n",
    "        Build TensorFlow dataloaders with robust error handling for NaNs and type issues\n",
    "        \n",
    "        Args:\n",
    "            val_year: Validation year\n",
    "            use_lags: If True, include lag variables\n",
    "            batch_size: Batch size\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (train_dataset, val_dataset, num_features)\n",
    "        \"\"\"\n",
    "        # Comprobar que DATASET_PATH est√© definido\n",
    "        if 'DATASET_PATH' not in globals():\n",
    "            global DATASET_PATH\n",
    "            if 'FULL_NC' in globals():\n",
    "                DATASET_PATH = str(FULL_NC)\n",
    "            else:\n",
    "                DATASET_PATH = './data/complete_dataset.nc'\n",
    "        \n",
    "        # Cargar dataset\n",
    "        print(f\"Cargando datos desde {DATASET_PATH}\")\n",
    "        try:\n",
    "            import xarray as xr\n",
    "            ds = xr.open_dataset(DATASET_PATH)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando dataset: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Seleccionar features seg√∫n el par√°metro use_lags\n",
    "        if use_lags:\n",
    "            if 'FULL_FEATURES' in globals():\n",
    "                features = FULL_FEATURES\n",
    "            else:\n",
    "                features = ['total_precipitation', 'total_precipitation_lag1', \n",
    "                          'total_precipitation_lag2', 'total_precipitation_lag12',\n",
    "                          'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "                          'elevation', 'slope', 'roughness', 'curvature', 'aspect', \n",
    "                          'cluster_elevation']\n",
    "        else:\n",
    "            if 'BASE_FEATURES' in globals():\n",
    "                base = BASE_FEATURES\n",
    "                # Filtrar lags\n",
    "                features = [f for f in base if 'lag' not in f]\n",
    "            else:\n",
    "                features = ['total_precipitation', \n",
    "                          'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "                          'elevation', 'slope', 'aspect']\n",
    "        \n",
    "        # Determinar a√±os de entrenamiento (4 a√±os antes de validaci√≥n)\n",
    "        train_years = list(range(val_year - 4, val_year))\n",
    "        \n",
    "        # Extraer tiempos con control de errores\n",
    "        try:\n",
    "            times = pd.to_datetime(ds.time.values)\n",
    "            years = times.year\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error procesando fechas: {e}\")\n",
    "            # Crear valores seguros como fallback\n",
    "            num_samples = len(ds['time']) if 'time' in ds.dims else 48\n",
    "            years = np.zeros(num_samples, dtype=np.int32)\n",
    "            # Asignar valores artificiales para tener datos de entrenamiento y validaci√≥n\n",
    "            years[:num_samples//2] = val_year - 1  # Mitad para entrenamiento\n",
    "            years[num_samples//2:] = val_year      # Mitad para validaci√≥n\n",
    "    \n",
    "        # Determinar conjuntos de entrenamiento y validaci√≥n\n",
    "        train_mask = np.isin(years, train_years)\n",
    "        val_mask = (years == val_year)\n",
    "        \n",
    "        print(f\"A√±os entrenamiento: {train_years}, A√±o validaci√≥n: {val_year}\")\n",
    "        print(f\"Muestras entrenamiento: {train_mask.sum()}, Muestras validaci√≥n: {val_mask.sum()}\")\n",
    "        \n",
    "        # Verificar si hay suficientes datos\n",
    "        if train_mask.sum() == 0:\n",
    "            print(\"‚ö†Ô∏è No hay datos de entrenamiento, creando datos artificiales\")\n",
    "            train_mask[:len(train_mask)//2] = True\n",
    "        if val_mask.sum() == 0:\n",
    "            print(\"‚ö†Ô∏è No hay datos de validaci√≥n, creando datos artificiales\")\n",
    "            val_mask[len(val_mask)//2:] = True\n",
    "    \n",
    "        def robust_array_processing(arr, feature_name):\n",
    "            \"\"\"\n",
    "            Process array data robustly to handle NaNs, infinities, and type issues\n",
    "            \n",
    "            Args:\n",
    "                arr: Input numpy array\n",
    "                feature_name: Name of the feature (for logging)\n",
    "                \n",
    "            Returns:\n",
    "                Cleaned numpy array\n",
    "            \"\"\"\n",
    "            # Check for valid array\n",
    "            if arr is None:\n",
    "                print(f\"‚ö†Ô∏è Null array for feature '{feature_name}', replacing with zeros\")\n",
    "                return np.zeros((12, 20, 20), dtype=np.float32)\n",
    "            \n",
    "            # Convert to numpy if needed\n",
    "            if not isinstance(arr, np.ndarray):\n",
    "                try:\n",
    "                    arr = np.array(arr, dtype=np.float32)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error converting '{feature_name}' to numpy array: {e}\")\n",
    "                    return np.zeros((12, 20, 20), dtype=np.float32)\n",
    "            \n",
    "            # Handle NaN values\n",
    "            if np.isnan(arr).any():\n",
    "                # Calculate mean, safely handling case where all values are NaN\n",
    "                mean_val = np.nanmean(arr) if not np.isnan(np.nanmean(arr)) else 0\n",
    "                print(f\"‚ö†Ô∏è NaN values found in '{feature_name}', replacing with mean={mean_val:.4f}\")\n",
    "                arr = np.nan_to_num(arr, nan=mean_val)\n",
    "            \n",
    "            # Handle infinite values\n",
    "            if not np.isfinite(arr).all():\n",
    "                print(f\"‚ö†Ô∏è Infinite values found in '{feature_name}', replacing with finite values\")\n",
    "                arr = np.nan_to_num(arr, posinf=np.nanmax(arr[np.isfinite(arr)]) if np.any(np.isfinite(arr)) else 1, \n",
    "                                   neginf=np.nanmin(arr[np.isfinite(arr)]) if np.any(np.isfinite(arr)) else -1)\n",
    "            \n",
    "            # Convert to float32 for TensorFlow compatibility\n",
    "            if arr.dtype != np.float32:\n",
    "                try:\n",
    "                    arr = arr.astype(np.float32)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error converting '{feature_name}' to float32: {e}\")\n",
    "            \n",
    "            return arr\n",
    "        \n",
    "        # Extraer matrices para cada caracter√≠stica con control robusto de NaNs\n",
    "        feature_arrays = []\n",
    "        for feature in features:\n",
    "            if feature in ds.data_vars:\n",
    "                try:\n",
    "                    # Obtener array con procesamiento robusto\n",
    "                    raw_arr = ds[feature].values\n",
    "                    cleaned_arr = robust_array_processing(raw_arr, feature)\n",
    "                    feature_arrays.append(cleaned_arr)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error procesando feature '{feature}': {e}\")\n",
    "                    # Crear array de ceros como fallback\n",
    "                    shape = ds[features[0]].shape if features[0] in ds.data_vars else (12, 20, 20)\n",
    "                    dummy_arr = np.zeros(shape, dtype=np.float32)\n",
    "                    feature_arrays.append(dummy_arr)\n",
    "                    print(f\"‚ö†Ô∏è Usando array de ceros para '{feature}'\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Caracter√≠stica '{feature}' no encontrada, usando ceros\")\n",
    "                # Crear array de ceros con forma compatible\n",
    "                if feature_arrays:\n",
    "                    dummy_arr = np.zeros_like(feature_arrays[0], dtype=np.float32)\n",
    "                else:\n",
    "                    dummy_arr = np.zeros((12, 20, 20), dtype=np.float32)\n",
    "                feature_arrays.append(dummy_arr)\n",
    "        \n",
    "        # Verificar que todos los arrays tengan dimensiones compatibles\n",
    "        shapes = [arr.shape for arr in feature_arrays]\n",
    "        if len(set(shapes)) > 1:\n",
    "            print(f\"‚ö†Ô∏è Distintas dimensiones en features: {shapes}\")\n",
    "            # Intentar corregir dimensiones\n",
    "            target_shape = shapes[0]\n",
    "            for i, arr in enumerate(feature_arrays):\n",
    "                if arr.shape != target_shape:\n",
    "                    print(f\"  Redimensionando feature {i} de {arr.shape} a {target_shape}\")\n",
    "                    try:\n",
    "                        # Intentar redimensionar o crear nuevo array\n",
    "                        feature_arrays[i] = np.zeros(target_shape, dtype=np.float32)\n",
    "                    except:\n",
    "                        pass\n",
    "    \n",
    "        # Apilar caracter√≠sticas con control de errores\n",
    "        try:\n",
    "            X = np.stack(feature_arrays, axis=-1)\n",
    "            num_features = X.shape[-1]\n",
    "            print(f\"‚úÖ Arrays apilados correctamente, shape = {X.shape}, features = {num_features}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error al apilar arrays: {e}\")\n",
    "            # Crear datos artificiales como √∫ltimo recurso\n",
    "            print(\"‚ö†Ô∏è Creando datos artificiales de emergencia\")\n",
    "            shape = (12, 20, 20)\n",
    "            X = np.zeros((*shape, len(feature_arrays)), dtype=np.float32)\n",
    "            num_features = X.shape[-1]\n",
    "    \n",
    "        # Usar INPUT_WINDOW y HORIZON globales o valores predeterminados\n",
    "        input_window = INPUT_WINDOW if 'INPUT_WINDOW' in globals() else 48\n",
    "        horizon = HORIZON if 'HORIZON' in globals() else 12\n",
    "        \n",
    "        # Control de integridad para ventanas\n",
    "        if input_window <= 0 or horizon <= 0:\n",
    "            print(f\"‚ö†Ô∏è Valores inv√°lidos: INPUT_WINDOW={input_window}, HORIZON={horizon}\")\n",
    "            input_window = max(1, input_window)\n",
    "            horizon = max(1, horizon)\n",
    "        \n",
    "        # Construir ventanas con control de errores\n",
    "        try:\n",
    "            # Construir ventanas para entrenamiento\n",
    "            X_train_windows = []\n",
    "            Y_train_windows = []\n",
    "            \n",
    "            for i in range(len(X) - input_window - horizon + 1):\n",
    "                try:\n",
    "                    # Solo incluir si la ventana completa est√° en el conjunto de entrenamiento\n",
    "                    window_indices = np.arange(i, i + input_window + horizon)\n",
    "                    if np.all(train_mask[window_indices]):\n",
    "                        x_window = X[i:i+input_window]\n",
    "                        y_window = X[i+input_window:i+input_window+horizon, :, :, 0:1]  # Solo precipitaci√≥n\n",
    "                        \n",
    "                        # Verificar si hay NaNs\n",
    "                        if np.isnan(x_window).any() or np.isnan(y_window).any():\n",
    "                            x_window = np.nan_to_num(x_window, nan=0.0)\n",
    "                            y_window = np.nan_to_num(y_window, nan=0.0)\n",
    "                        \n",
    "                        X_train_windows.append(x_window)\n",
    "                        Y_train_windows.append(y_window)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error en ventana de entrenamiento {i}: {e}\")\n",
    "            \n",
    "            # Construir ventanas para validaci√≥n\n",
    "            X_val_windows = []\n",
    "            Y_val_windows = []\n",
    "            \n",
    "            for i in range(len(X) - input_window - horizon + 1):\n",
    "                try:\n",
    "                    # Solo incluir si los outputs est√°n completamente en validaci√≥n\n",
    "                    output_indices = np.arange(i + input_window, i + input_window + horizon)\n",
    "                    if np.all(val_mask[output_indices]):\n",
    "                        x_window = X[i:i+input_window]\n",
    "                        y_window = X[i+input_window:i+input_window+horizon, :, :, 0:1]\n",
    "                        \n",
    "                        # Verificar si hay NaNs\n",
    "                        if np.isnan(x_window).any() or np.isnan(y_window).any():\n",
    "                            x_window = np.nan_to_num(x_window, nan=0.0)\n",
    "                            y_window = np.nan_to_num(y_window, nan=0.0)\n",
    "                        \n",
    "                        X_val_windows.append(x_window)\n",
    "                        Y_val_windows.append(y_window)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error en ventana de validaci√≥n {i}: {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error general generando ventanas: {e}\")\n",
    "            # Crear listas vac√≠as como fallback\n",
    "            X_train_windows = []\n",
    "            Y_train_windows = []\n",
    "            X_val_windows = []\n",
    "            Y_val_windows = []\n",
    "        \n",
    "        # Verificar que tengamos datos\n",
    "        if not X_train_windows or not Y_train_windows:\n",
    "            print(\"‚ùå No se pudieron generar ventanas de entrenamiento\")\n",
    "            # Crear ventanas artificiales m√≠nimas\n",
    "            sample_shape = X.shape[1:]  # Forma espacial\n",
    "            X_train_windows = [np.zeros((input_window, *sample_shape), dtype=np.float32)]\n",
    "            Y_train_windows = [np.zeros((horizon, *sample_shape[:-1], 1), dtype=np.float32)]\n",
    "        \n",
    "        if not X_val_windows or not Y_val_windows:\n",
    "            print(\"‚ùå No se pudieron generar ventanas de validaci√≥n\")\n",
    "            # Crear ventanas artificiales m√≠nimas\n",
    "            sample_shape = X.shape[1:]  # Forma espacial\n",
    "            X_val_windows = [np.zeros((input_window, *sample_shape), dtype=np.float32)]\n",
    "            Y_val_windows = [np.zeros((horizon, *sample_shape[:-1], 1), dtype=np.float32)]\n",
    "        \n",
    "        # Convertir listas a arrays con control de errores\n",
    "        try:\n",
    "            X_train = np.array(X_train_windows)\n",
    "            Y_train = np.array(Y_train_windows)\n",
    "            X_val = np.array(X_val_windows) \n",
    "            Y_val = np.array(Y_val_windows)\n",
    "            \n",
    "            # Verificaci√≥n final de NaNs\n",
    "            for arr_name, arr in [(\"X_train\", X_train), (\"Y_train\", Y_train), \n",
    "                                 (\"X_val\", X_val), (\"Y_val\", Y_val)]:\n",
    "                if np.isnan(arr).any():\n",
    "                    print(f\"‚ö†Ô∏è NaNs detectados en {arr_name} despu√©s de conversi√≥n, reemplazando con 0\")\n",
    "                    if arr_name.startswith(\"X\"):\n",
    "                        arr = np.nan_to_num(arr, nan=0.0)\n",
    "                    else:\n",
    "                        arr = np.nan_to_num(arr, nan=0.0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error convirtiendo listas a arrays: {e}\")\n",
    "            # Crear arrays m√≠nimos\n",
    "            sample_shape = X.shape[1:]  # Forma espacial\n",
    "            X_train = np.zeros((1, input_window, *sample_shape), dtype=np.float32)\n",
    "            Y_train = np.zeros((1, horizon, *sample_shape[:-1], 1), dtype=np.float32)\n",
    "            X_val = np.zeros((1, input_window, *sample_shape), dtype=np.float32)\n",
    "            Y_val = np.zeros((1, horizon, *sample_shape[:-1], 1), dtype=np.float32)\n",
    "        \n",
    "        print(f\"Forma de datos entrenamiento: X={X_train.shape}, Y={Y_train.shape}\")\n",
    "        print(f\"Forma de datos validaci√≥n: X={X_val.shape}, Y={Y_val.shape}\")\n",
    "        \n",
    "        # Crear TF datasets optimizados con control de errores\n",
    "        try:\n",
    "            train_dataset = TFPrecipitationDataset.create_tf_dataset(\n",
    "                X_train, Y_train, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            val_dataset = TFPrecipitationDataset.create_tf_dataset(\n",
    "                X_val, Y_val, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            return train_dataset, val_dataset, num_features\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creando TensorFlow datasets: {e}\")\n",
    "            # Ultimo recurso: crear datasets artificiales\n",
    "            print(\"üÜò Creando datasets artificiales de emergencia\")\n",
    "            \n",
    "            # Crear arrays m√≠nimos (m√°s peque√±os para evitar OOM)\n",
    "            reduced_features = min(num_features, 5)\n",
    "            X_dummy = np.zeros((10, input_window, 10, 10, reduced_features), dtype=np.float32)\n",
    "            Y_dummy = np.zeros((10, horizon, 10, 10, 1), dtype=np.float32)\n",
    "            \n",
    "            dummy_train = tf.data.Dataset.from_tensor_slices((X_dummy, Y_dummy)).batch(batch_size)\n",
    "            dummy_val = tf.data.Dataset.from_tensor_slices((X_dummy, Y_dummy)).batch(batch_size)\n",
    "            \n",
    "            return dummy_train, dummy_val, reduced_features\n",
    "def build_spatial_dataloaders(val_year, use_lags=True, batch_size=32, flatten_spatial=False):\n",
    "    \"\"\"\n",
    "    Construir dataloaders para datos espaciales 3D con manejo robusto de errores\n",
    "    \n",
    "    Args:\n",
    "        val_year: A√±o de validaci√≥n\n",
    "        use_lags: Si es True, incluir variables de rezago\n",
    "        batch_size: Tama√±o del lote\n",
    "        flatten_spatial: Si es True, aplanar dimensiones espaciales para modelos 1D\n",
    "        \n",
    "    Returns:\n",
    "        Tuple de (train_dataset, val_dataset, input_shape)\n",
    "    \"\"\"\n",
    "    # Comprobar que DATASET_PATH est√© definido\n",
    "    if 'DATASET_PATH' not in globals():\n",
    "        global DATASET_PATH\n",
    "        if 'FULL_NC' in globals():\n",
    "            DATASET_PATH = str(FULL_NC)\n",
    "        else:\n",
    "            DATASET_PATH = './data/complete_dataset.nc'\n",
    "    \n",
    "    # Cargar dataset\n",
    "    print(f\"Cargando datos espaciales desde {DATASET_PATH}\")\n",
    "    try:\n",
    "        import xarray as xr\n",
    "        ds = xr.open_dataset(DATASET_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando dataset: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Seleccionar features seg√∫n el par√°metro use_lags\n",
    "    if use_lags:\n",
    "        if 'FULL_FEATURES' in globals():\n",
    "            features = FULL_FEATURES\n",
    "        else:\n",
    "            features = ['total_precipitation', 'total_precipitation_lag1', \n",
    "                      'total_precipitation_lag2', 'total_precipitation_lag12',\n",
    "                      'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "                      'elevation', 'slope', 'roughness', 'curvature', 'aspect', \n",
    "                      'cluster_elevation']\n",
    "    else:\n",
    "        if 'BASE_FEATURES' in globals():\n",
    "            base = BASE_FEATURES\n",
    "            # Filtrar lags\n",
    "            features = [f for f in base if 'lag' not in f]\n",
    "        else:\n",
    "            features = ['total_precipitation', \n",
    "                      'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "                      'elevation', 'slope', 'aspect']\n",
    "    \n",
    "    # Determinar a√±os de entrenamiento (4 a√±os antes de validaci√≥n)\n",
    "    train_years = list(range(val_year - 4, val_year))\n",
    "    \n",
    "    # Extraer tiempos con control de errores\n",
    "    try:\n",
    "        times = pd.to_datetime(ds.time.values)\n",
    "        years = times.year\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error procesando fechas: {e}\")\n",
    "        # Crear valores seguros como fallback\n",
    "        num_samples = len(ds['time']) if 'time' in ds.dims else 48\n",
    "        years = np.zeros(num_samples, dtype=np.int32)\n",
    "        # Asignar valores artificiales para tener datos de entrenamiento y validaci√≥n\n",
    "        years[:num_samples//2] = val_year - 1  # Mitad para entrenamiento\n",
    "        years[num_samples//2:] = val_year      # Mitad para validaci√≥n\n",
    "    \n",
    "    # Determinar conjuntos de entrenamiento y validaci√≥n\n",
    "    train_mask = np.isin(years, train_years)\n",
    "    val_mask = (years == val_year)\n",
    "    \n",
    "    # Verificar n√∫mero de muestras\n",
    "    train_samples = np.sum(train_mask)\n",
    "    val_samples = np.sum(val_mask)\n",
    "    print(f\"Muestras de entrenamiento: {train_samples}, Validaci√≥n: {val_samples}\")\n",
    "    \n",
    "    # Procesar y limpiar variables\n",
    "    feature_arrays = []\n",
    "    print(f\"Procesando {len(features)} caracter√≠sticas...\")\n",
    "    \n",
    "    # Procesamiento por lotes para reducir uso de memoria\n",
    "    for feature in features:\n",
    "        if feature in ds.data_vars:\n",
    "            try:\n",
    "                # Extraer array\n",
    "                arr = ds[feature].values\n",
    "                \n",
    "                # Verificar dimensi√≥n temporal vs espacial\n",
    "                if arr.ndim == 2:\n",
    "                    print(f\"‚ö†Ô∏è Caracter√≠stica espacial (no temporal): {feature}\")\n",
    "                    # Es una variable espacial est√°tica (como elevaci√≥n)\n",
    "                    # Repetir para cada punto temporal\n",
    "                    time_dim = len(times)\n",
    "                    spatial_shape = arr.shape\n",
    "                    repeated = np.repeat(arr[np.newaxis, :, :], time_dim, axis=0)\n",
    "                    arr = repeated\n",
    "                \n",
    "                # Procesar NaNs de manera segura\n",
    "                if hasattr(arr, 'dtype') and arr.dtype != object:\n",
    "                    if np.isnan(arr).any():\n",
    "                        mean_val = np.nanmean(arr)\n",
    "                        if np.isnan(mean_val):\n",
    "                            mean_val = 0\n",
    "                        arr = np.nan_to_num(arr, nan=mean_val)\n",
    "                \n",
    "                # Convertir a float32 para compatibilidad con TensorFlow\n",
    "                if hasattr(arr, 'dtype') and arr.dtype != np.float32:\n",
    "                    try:\n",
    "                        arr = arr.astype(np.float32)\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Error convirtiendo {feature} a float32: {e}\")\n",
    "                        # Crear array de ceros como respaldo\n",
    "                        if len(feature_arrays) > 0:\n",
    "                            arr = np.zeros_like(feature_arrays[0], dtype=np.float32)\n",
    "                        else:\n",
    "                            arr = np.zeros((len(times), 61, 65), dtype=np.float32)\n",
    "                \n",
    "                feature_arrays.append(arr)\n",
    "                print(f\"‚úì {feature}: forma {arr.shape}, tipo {arr.dtype}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error procesando {feature}: {e}\")\n",
    "                # Crear array de respaldo\n",
    "                if len(feature_arrays) > 0:\n",
    "                    dummy = np.zeros_like(feature_arrays[0], dtype=np.float32)\n",
    "                else:\n",
    "                    dummy = np.zeros((len(times), 61, 65), dtype=np.float32)\n",
    "                feature_arrays.append(dummy)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Caracter√≠stica no encontrada: {feature}, usando ceros\")\n",
    "            if len(feature_arrays) > 0:\n",
    "                dummy = np.zeros_like(feature_arrays[0], dtype=np.float32)\n",
    "                feature_arrays.append(dummy)\n",
    "            else:\n",
    "                dummy = np.zeros((len(times), 61, 65), dtype=np.float32)\n",
    "                feature_arrays.append(dummy)\n",
    "    \n",
    "    # Asegurar que todas las caracter√≠sticas tengan la misma forma\n",
    "    target_shape = feature_arrays[0].shape\n",
    "    for i, arr in enumerate(feature_arrays):\n",
    "        if arr.shape != target_shape:\n",
    "            print(f\"‚ö†Ô∏è Redimensionando feature {i} de {arr.shape} a {target_shape}\")\n",
    "            # Si es una caracter√≠stica espacial, repetirla para cada tiempo\n",
    "            if arr.ndim == 2 and arr.shape == target_shape[1:]:\n",
    "                feature_arrays[i] = np.stack([arr] * target_shape[0])\n",
    "            else:\n",
    "                # Si falla, usar ceros\n",
    "                try:\n",
    "                    feature_arrays[i] = np.zeros(target_shape, dtype=np.float32)\n",
    "                except:\n",
    "                    feature_arrays[i] = np.zeros(target_shape, dtype=np.float32)\n",
    "                \n",
    "    # Apilar caracter√≠sticas en el √∫ltimo eje\n",
    "    X = np.stack(feature_arrays, axis=-1)\n",
    "    print(f\"Dataset completo: {X.shape}\")\n",
    "    \n",
    "    # Usar INPUT_WINDOW y HORIZON globales o valores predeterminados\n",
    "    input_window = INPUT_WINDOW if 'INPUT_WINDOW' in globals() else 48\n",
    "    horizon = HORIZON if 'HORIZON' in globals() else 12\n",
    "    \n",
    "    # Construir ventanas deslizantes\n",
    "    train_windows_x = []\n",
    "    train_windows_y = []\n",
    "    val_windows_x = []\n",
    "    val_windows_y = []\n",
    "    \n",
    "    print(f\"Creando ventanas con {input_window} pasos de entrada y {horizon} de horizonte...\")\n",
    "    \n",
    "    # Crear ventanas deslizantes\n",
    "    for i in range(len(X) - input_window - horizon + 1):\n",
    "        if i % 50 == 0:  # Progreso cada 50 ventanas\n",
    "            print(f\"Procesando ventana {i}/{len(X) - input_window - horizon + 1}\")\n",
    "            \n",
    "        # Verificar si la ventana est√° completamente en train o validation\n",
    "        window_indices = np.arange(i, i + input_window + horizon)\n",
    "        \n",
    "        # Para entrenamiento: tanto entrada como salida deben estar en train\n",
    "        if np.all(train_mask[window_indices]):\n",
    "            x_window = X[i:i+input_window]\n",
    "            y_window = X[i+input_window:i+input_window+horizon, :, :, 0:1]  # Solo precipitaci√≥n\n",
    "            train_windows_x.append(x_window)\n",
    "            train_windows_y.append(y_window)\n",
    "            \n",
    "        # Para validaci√≥n: target debe estar en validation\n",
    "        output_indices = np.arange(i + input_window, i + input_window + horizon)\n",
    "        if np.all(val_mask[output_indices]):\n",
    "            x_window = X[i:i+input_window]\n",
    "            y_window = X[i+input_window:i+input_window+horizon, :, :, 0:1]  # Solo precipitaci√≥n\n",
    "            val_windows_x.append(x_window)\n",
    "            val_windows_y.append(y_window)\n",
    "    \n",
    "    # Si no hay suficientes ventanas, crear algunas artificiales\n",
    "    if len(train_windows_x) == 0:\n",
    "        print(\"‚ùå No se pudieron generar ventanas de entrenamiento, creando datos artificiales\")\n",
    "        # Define spatial dimensions from X shape (height, width)\n",
    "        spatial_dim = X.shape[1:3]\n",
    "        for _ in range(10):  # Crear al menos 10 ventanas\n",
    "            train_windows_x.append(np.zeros((input_window, *spatial_dim, X.shape[-1]), dtype=np.float32))\n",
    "    if len(val_windows_x) == 0:\n",
    "        print(\"‚ùå No se pudieron generar ventanas de validaci√≥n, creando datos artificiales\")\n",
    "        val_windows_x = []\n",
    "        val_windows_y = []\n",
    "        # Ensure spatial_dim is defined\n",
    "        spatial_dim = X.shape[1:3]\n",
    "        for _ in range(5):  # Crear al menos 5 ventanas\n",
    "            val_windows_x.append(np.zeros((input_window, *spatial_dim, X.shape[-1]), dtype=np.float32))\n",
    "            val_windows_y.append(np.zeros((horizon, *spatial_dim, 1), dtype=np.float32))\n",
    "            val_windows_x.append(np.zeros((input_window, *spatial_dim, X.shape[-1]), dtype=np.float32))\n",
    "            val_windows_y.append(np.zeros((horizon, *spatial_dim, 1), dtype=np.float32))\n",
    "    \n",
    "    # Convertir listas a arrays\n",
    "    \n",
    "    print(f\"Formas finales: X_train={X_train.shape}, Y_train={Y_train.shape}\")\n",
    "    print(f\"               X_val={X_val.shape}, Y_val={Y_val.shape}\")\n",
    "    \n",
    "    # Aplanar dimensiones espaciales si se solicita (para modelos 1D como GRU)\n",
    "    if flatten_spatial:\n",
    "        print(\"üîÑ Aplanando dimensiones espaciales para compatibilidad con modelos 1D\")\n",
    "        # Para X: [batch, time, height, width, features] ‚Üí [batch, time, height*width*features]\n",
    "        batch_size_train, time_steps, height, width, features = X_train.shape\n",
    "        X_train = X_train.reshape(batch_size_train, time_steps, height * width * features)\n",
    "        \n",
    "        batch_size_val = X_val.shape[0]\n",
    "        X_val = X_val.reshape(batch_size_val, time_steps, height * width * features)\n",
    "        \n",
    "        # Para Y: [batch, horizon, height, width, 1] ‚Üí [batch, horizon, height*width]\n",
    "        Y_train = Y_train.reshape(batch_size_train, horizon, height * width)\n",
    "        Y_val = Y_val.reshape(batch_size_val, horizon, height * width)\n",
    "        \n",
    "        # Actualizar n√∫mero de caracter√≠sticas tras aplanado\n",
    "        num_features = height * width * features\n",
    "        \n",
    "        print(f\"Datos despu√©s de aplanar - X_train: {X_train.shape}, Y_train: {Y_train.shape}\")\n",
    "        print(f\"                         - X_val: {X_val.shape}, Y_val: {Y_val.shape}\")\n",
    "    \n",
    "    # Crear datasets TF\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_dataset, val_dataset, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ñ∂Ô∏è Ejecutar el entrenamiento del modelo base (GRU-ED) en el fold m√°s reciente (F1)\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ INICIANDO ENTRENAMIENTO DEL MODELO GRU-ED (FOLD F1)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Verificar que todas las variables necesarias est√°n definidas\n",
    "print(f\"Ventana de entrada: {INPUT_WINDOW} meses\")\n",
    "print(f\"Horizonte de predicci√≥n: {HORIZON} meses\")\n",
    "print(f\"Experimentos disponibles: {list(EXPERIMENTS.keys())}\")\n",
    "print(f\"Folds disponibles: {FOLDS}\")\n",
    "print(f\"Modo MVP: {'Activo (solo fold F1)' if MVP_MODE else 'Inactivo (todos los folds)'}\")\n",
    "\n",
    "# 2. Configurar el entorno GPU/CPU\n",
    "device_mgr = TFDeviceManager(prefer_gpu=True)\n",
    "print(f\"Dispositivo seleccionado: {device_mgr.device_name}\")\n",
    "\n",
    "# 3. Ejecutar el experimento para GRU-ED en el fold F1\n",
    "experiment_name = 'GRU-ED'\n",
    "fold_name = 'F1'\n",
    "val_year = FOLDS[fold_name]\n",
    "\n",
    "try:\n",
    "    # Comprobar si ya existe un checkpoint para este experimento\n",
    "    checkpoint_name = f\"{experiment_name}_{fold_name}_result\"\n",
    "    checkpoint_data = SafeExecution.load_checkpoint(checkpoint_name)\n",
    "    \n",
    "    if checkpoint_data:\n",
    "        _, _, best_rmse = checkpoint_data\n",
    "        print(f\"‚úÖ Modelo ya entrenado anteriormente. RMSE: {best_rmse:.4f}\")\n",
    "    else:\n",
    "        print(f\"üîÑ Preparando datos para el fold {fold_name} (validaci√≥n: {val_year})\")\n",
    "        # Crear datasets\n",
    "        batch_size = 8 if device_mgr.is_gpu() else 4\n",
    "        train_dataset, val_dataset, in_dim = TFPrecipitationDataset.build_dataloaders(\n",
    "            val_year, EXPERIMENTS[experiment_name]['use_lags'], batch_size)\n",
    "        \n",
    "        # Configurar y crear modelo\n",
    "        model_config = {\n",
    "            'input_dim': in_dim,\n",
    "            'input_length': INPUT_WINDOW,\n",
    "            'output_length': HORIZON,\n",
    "            'hidden_units': 128,\n",
    "            'num_layers': 2,\n",
    "            'dropout_rate': 0.20  # 0.20 para F1-F3, 0.25 para F4-F5\n",
    "        }\n",
    "        \n",
    "        # Crear modelo directamente usando las funciones existentes\n",
    "        print(f\"üß† Creando modelo {experiment_name}\")\n",
    "        \n",
    "        # Definir funci√≥n para crear modelo GRU-ED b√°sico\n",
    "        def create_gru_ed_model(input_dim, input_length, output_length, hidden_units=128, \n",
    "                               dropout_rate=0.2, num_layers=2):\n",
    "            \"\"\"Crea un modelo GRU Encoder-Decoder b√°sico\"\"\"\n",
    "            from tensorflow.keras import layers, Model\n",
    "            \n",
    "            # Encoder\n",
    "            inputs = layers.Input(shape=(input_length, input_dim))\n",
    "            encoder = inputs\n",
    "            \n",
    "            # Stack de capas GRU para el encoder\n",
    "            for i in range(num_layers - 1):\n",
    "                encoder = layers.GRU(hidden_units, return_sequences=True, dropout=dropout_rate)(encoder)\n",
    "                \n",
    "            # √öltima capa del encoder\n",
    "            encoder_outputs, state_h = layers.GRU(hidden_units, return_state=True, return_sequences=True)(encoder)\n",
    "            \n",
    "            # Decoder inicializado con estado del encoder\n",
    "            decoder_inputs = encoder_outputs\n",
    "            \n",
    "            # Stack de capas GRU para el decoder\n",
    "            for i in range(num_layers - 1):\n",
    "                decoder_inputs = layers.GRU(hidden_units, return_sequences=True, dropout=dropout_rate)(decoder_inputs)\n",
    "                \n",
    "            # Capa final del decoder\n",
    "            decoder_outputs = layers.GRU(hidden_units, return_sequences=True)(decoder_inputs)\n",
    "            \n",
    "            # Proyecci√≥n a la dimensi√≥n de salida\n",
    "            outputs = layers.Dense(output_length, activation='linear')(decoder_outputs)\n",
    "            \n",
    "            # Crear y compilar modelo\n",
    "            model = Model(inputs, outputs)\n",
    "            model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        # Crear modelo con la configuraci√≥n especificada\n",
    "        model = create_gru_ed_model(**model_config)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        print(f\"üèãÔ∏è Entrenando modelo {experiment_name} en fold {fold_name}\")\n",
    "        \n",
    "        # Configurar Early Stopping\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=20, restore_best_weights=True, verbose=1\n",
    "        )\n",
    "        \n",
    "        # Entrenamiento\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=60,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        ).history\n",
    "        \n",
    "        # Calcular el mejor RMSE\n",
    "        val_mse = history['val_mse']\n",
    "        best_epoch = np.argmin(val_mse)\n",
    "        best_rmse = np.sqrt(val_mse[best_epoch])\n",
    "        \n",
    "        # Guardar checkpoint\n",
    "        print(f\"üíæ Guardando resultados\")\n",
    "        SafeExecution.save_checkpoint(\n",
    "            (model, history, best_rmse),\n",
    "            checkpoint_name\n",
    "        )\n",
    "        \n",
    "        # Limpiar recursos\n",
    "        device_mgr.clear_memory()\n",
    "    \n",
    "    # Mostrar resumen de resultados\n",
    "    print(\"\\nüìä RESULTADOS DEL EXPERIMENTO:\")\n",
    "    print(f\"Modelo: {experiment_name}\")\n",
    "    print(f\"Fold: {fold_name} (Validaci√≥n: {val_year})\")\n",
    "    print(f\"RMSE: {best_rmse:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error durante la ejecuci√≥n: {str(e)}\")\n",
    "\n",
    "# 4. Mostrar tabla con todos los resultados disponibles\n",
    "print(\"\\nüìã RESUMEN DE TODOS LOS EXPERIMENTOS EJECUTADOS:\")\n",
    "show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1d613",
   "metadata": {},
   "source": [
    "# Visualizaci√≥n de Resultados de Predicci√≥n\n",
    "\n",
    "Los gr√°ficos a continuaci√≥n muestran la evoluci√≥n del error durante el entrenamiento y ejemplos de predicciones del modelo GRU-ED comparadas con los valores reales de precipitaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ece21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ñ∂Ô∏è Visualizar resultados del entrenamiento\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_training_history(exp_name, fold):\n",
    "    \"\"\"Visualiza las curvas de aprendizaje de un experimento\"\"\"\n",
    "    try:\n",
    "        # Intentar cargar el checkpoint\n",
    "        checkpoint_name = f\"{exp_name}_{fold}_result\"\n",
    "        checkpoint_data = SafeExecution.load_checkpoint(checkpoint_name)\n",
    "        \n",
    "        if not checkpoint_data:\n",
    "            print(f\"‚ùå No se encontr√≥ historial para {exp_name} en fold {fold}\")\n",
    "            return\n",
    "            \n",
    "        _, history, best_rmse = checkpoint_data\n",
    "        \n",
    "        # Crear figura para gr√°ficos\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Gr√°fico de p√©rdida\n",
    "        axes[0].plot(history['loss'], label='Train')\n",
    "        if 'val_loss' in history:\n",
    "            axes[0].plot(history['val_loss'], label='Validation')\n",
    "        axes[0].set_title(f'{exp_name} - Fold {fold} - Loss')\n",
    "        axes[0].set_xlabel('Epochs')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Gr√°fico de RMSE\n",
    "        axes[1].plot(np.sqrt(history['mse']), label='Train RMSE')\n",
    "        if 'val_mse' in history:\n",
    "            axes[1].plot(np.sqrt(history['val_mse']), label='Validation RMSE')\n",
    "        axes[1].set_title(f'{exp_name} - Fold {fold} - RMSE')\n",
    "        axes[1].set_xlabel('Epochs')\n",
    "        axes[1].set_ylabel('RMSE')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Ajustar layout y mostrar\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Mostrar mejor RMSE\n",
    "        print(f\"Mejor RMSE en validaci√≥n: {best_rmse:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al visualizar historial: {e}\")\n",
    "\n",
    "# Visualizar historial del modelo GRU-ED en fold F1\n",
    "print(\"üìä Curvas de aprendizaje para GRU-ED (Fold F1)\")\n",
    "plot_training_history('GRU-ED', 'F1')\n",
    "\n",
    "# Si hay m√°s modelos entrenados, mostrar comparativa\n",
    "try:\n",
    "    # Buscar todos los resultados disponibles\n",
    "    results = []\n",
    "    for file in CHECKPOINT_DIR.glob(\"*_result.pkl\"):\n",
    "        try:\n",
    "            parts = file.stem.split('_')\n",
    "            exp = parts[0]\n",
    "            fold = parts[1]\n",
    "            checkpoint = SafeExecution.load_checkpoint(f\"{exp}_{fold}_result\")\n",
    "            if checkpoint:\n",
    "                _, _, rmse = checkpoint\n",
    "                results.append({\n",
    "                    'exp': exp,\n",
    "                    'fold': fold,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Si hay varios modelos entrenados, mostrar comparativa\n",
    "    if len(set([r['exp'] for r in results])) > 1:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Agrupar por experimento\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(results)\n",
    "        exp_groups = df.groupby('exp')\n",
    "        \n",
    "        for exp_name, group in exp_groups:\n",
    "            # Ordenar por fold\n",
    "            group = group.sort_values('fold')\n",
    "            plt.plot(group['fold'], group['rmse'], 'o-', label=exp_name)\n",
    "        \n",
    "        plt.title('Comparaci√≥n de RMSE por Modelo y Fold')\n",
    "        plt.xlabel('Fold')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo generar comparativa: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ñ∂Ô∏è Definir funci√≥n para ejecutar experimentos con manejo robusto de errores\n",
    "def run_tf_experiments(folds_to_run=None, force_cpu=False, fail_safe=True):\n",
    "    \"\"\"\n",
    "    Run TensorFlow experiments with robust error handling\n",
    "    \n",
    "    Args:\n",
    "        folds_to_run: List of folds to run, None for default\n",
    "        force_cpu: If True, force CPU mode\n",
    "        fail_safe: If True, use extra error handling\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Verificar que tenemos las funciones y variables necesarias\n",
    "    if 'EXPERIMENTS' not in globals() or 'FOLDS' not in globals():\n",
    "        print(\"‚ùå Variables globales EXPERIMENTS y/o FOLDS no definidas\")\n",
    "        return results\n",
    "    \n",
    "    # Limitar a folds espec√≠ficos si es necesario - MODIFICADO para ser consistente con MVP_MODE\n",
    "    if folds_to_run is None:\n",
    "        folds_to_run = ['F1'] if MVP_MODE else list(FOLDS.keys())\n",
    "    else:\n",
    "        valid_folds = [f for f in folds_to_run if f in FOLDS]\n",
    "        if MVP_MODE:\n",
    "            # En MVP_MODE, solo permitir F1 incluso si se especificaron otros folds\n",
    "            folds_to_run = ['F1'] if 'F1' in valid_folds else []\n",
    "            if not folds_to_run and valid_folds:\n",
    "                print(\"‚ö†Ô∏è MVP_MODE activo: Solo se permite el fold F1\")\n",
    "        else:\n",
    "            folds_to_run = valid_folds\n",
    "    \n",
    "    print(f\"üîÑ Ejecutando experimentos en {'CPU' if force_cpu else 'GPU/CPU'}\")\n",
    "    print(f\"   Folds: {', '.join(folds_to_run)}\")\n",
    "    print(f\"   Modo MVP: {'‚úÖ Activo (solo F1)' if MVP_MODE else '‚ùå Inactivo'}\")\n",
    "    \n",
    "    # Procesar todos los experimentos cuando MVP_MODE est√° activo\n",
    "    exps_to_run = EXPERIMENTS.keys() if MVP_MODE else [next(iter(EXPERIMENTS.keys()))]\n",
    "    \n",
    "    for exp_name in exps_to_run:\n",
    "        for fold in folds_to_run:\n",
    "            # Checkpoint name for this experiment/fold\n",
    "            checkpoint_name = f\"{exp_name}_{fold}_result\"\n",
    "            \n",
    "            # Check if a previous result exists\n",
    "            checkpoint_data = SafeExecution.load_checkpoint(checkpoint_name)\n",
    "            if checkpoint_data:\n",
    "                model, history, best_rmse = checkpoint_data\n",
    "                print(f\"‚úÖ Using previous result: RMSE = {best_rmse:.4f}\")\n",
    "                \n",
    "                # Register global result\n",
    "                if 'RESULTS' in globals():\n",
    "                    RESULTS.append({\n",
    "                        'exp': exp_name,\n",
    "                        'fold': fold,\n",
    "                        'rmse': best_rmse\n",
    "                    })\n",
    "                    \n",
    "                # Update global histories\n",
    "                if 'ALL_HISTORIES' in globals():\n",
    "                    if exp_name not in ALL_HISTORIES:\n",
    "                        ALL_HISTORIES[exp_name] = {}\n",
    "                    ALL_HISTORIES[exp_name][fold] = history\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            # If no checkpoint, run the training\n",
    "            try:\n",
    "                # Free memory before starting\n",
    "                clean_memory()\n",
    "                \n",
    "                # Get configuration and build dataloaders\n",
    "                print(f\"üîÑ Preparing data for fold {fold}\")\n",
    "                cfg = EXPERIMENTS[exp_name]\n",
    "                val_year = FOLDS[fold]\n",
    "                \n",
    "                # Use reduced batch size for greater stability\n",
    "                batch_size = max(8, BATCH_SIZE // 2)  # Half the original batch size, minimum 8\n",
    "                train_loader, val_loader, in_dim = TFPrecipitationDataset.build_dataloaders(val_year, cfg['use_lags'], batch_size)\n",
    "                \n",
    "                # Adjust dropout according to documentation\n",
    "                dropout = 0.25 if fold in ['F4', 'F5'] else 0.20\n",
    "                \n",
    "                # Create model - TensorFlow models don't use .to(DEVICE)\n",
    "                model = MODEL_FACTORY[cfg['model']](in_dim, dropout=dropout)\n",
    "                \n",
    "                # Train model with error handling\n",
    "                print(f\"üîÑ Training {exp_name} on fold {fold}\")\n",
    "                try:\n",
    "                    model, history, best_rmse = train_with_history(\n",
    "                        model, train_loader, val_loader,\n",
    "                        epochs=60, patience=20,\n",
    "                        lr=1e-3, weight_decay=1e-4,\n",
    "                        fold=fold, exp_name=exp_name\n",
    "                    )\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    SafeExecution.save_checkpoint(\n",
    "                        (model, history, best_rmse),\n",
    "                        checkpoint_name\n",
    "                    )\n",
    "                    \n",
    "                    # Register global result\n",
    "                    if 'RESULTS' in globals():\n",
    "                        RESULTS.append({\n",
    "                            'exp': exp_name,\n",
    "                            'fold': fold,\n",
    "                            'rmse': best_rmse\n",
    "                        })\n",
    "                    \n",
    "                    # Update global histories\n",
    "                    if 'ALL_HISTORIES' in globals():\n",
    "                        if exp_name not in ALL_HISTORIES:\n",
    "                            ALL_HISTORIES[exp_name] = {}\n",
    "                        ALL_HISTORIES[exp_name][fold] = history\n",
    "                        \n",
    "                    print(f\"‚úÖ Training completed: RMSE = {best_rmse:.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error in training: {e}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error in experiment {exp_name}, fold {fold}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ Experiment {exp_name} completed\")\n",
    "\n",
    "# ‚ñ∂Ô∏è Ejecutar todos los experimentos en modo robusto\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ INICIANDO EJECUCI√ìN DE TODOS LOS EXPERIMENTOS EN MODO ROBUSTO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Forzar CPU si es necesario (para evitar problemas de memoria)\n",
    "    force_cpu = False  # Cambiar a True si se desea forzar uso de CPU\n",
    "    \n",
    "    # Ejecutar experimentos con manejo robusto de errores\n",
    "    results = run_tf_experiments(\n",
    "        folds_to_run=None,  # None para ejecutar todos los folds seg√∫n el modo MVP\n",
    "        force_cpu=force_cpu,\n",
    "        fail_safe=True  # Activar manejo de errores reforzado\n",
    "    )\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    if results:\n",
    "        import pandas as pd\n",
    "        from IPython.display import display\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        print(\"\\nüìä RESULTADOS DE LA EJECUCI√ìN EN MODO ROBUSTO:\")\n",
    "        display(df)\n",
    "    else:\n",
    "        print(\"‚ùå No se obtuvieron resultados\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error no capturado: {str(e)}\")\n",
    "    \n",
    "    # Mostrar diagn√≥stico detallado\n",
    "    import traceback\n",
    "    print(\"\\nüîç Detalles del error:\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(\"\\nüí° Recomendaci√≥n: Ejecute con estos par√°metros para m√°xima compatibilidad:\")\n",
    "    print(\"   run_tf_experiments(['F1'], force_cpu=True, fail_safe=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ñ∂Ô∏è Control del Modo MVP (Minimum Viable Product)\n",
    "# Esta celda permite activar/desactivar f√°cilmente el modo MVP\n",
    "\n",
    "def set_mvp_mode(active=True):\n",
    "    \"\"\"\n",
    "    Configura el modo MVP (entrenamiento m√≠nimo viable)\n",
    "    \n",
    "    Args:\n",
    "        active: Si True, solo se entrena el fold m√°s reciente (F1)\n",
    "               Si False, se entrenan todos los folds\n",
    "    \"\"\"\n",
    "    global MVP_MODE\n",
    "    old_mode = MVP_MODE\n",
    "    MVP_MODE = active\n",
    "    \n",
    "    print(f\"Modo MVP: {'‚úÖ ACTIVADO' if MVP_MODE else '‚ùå DESACTIVADO'}\")\n",
    "    print(f\"  ‚Ä¢ {'Solo se entrenar√° el fold F1 (m√°s reciente, a√±o 2024)' if MVP_MODE else 'Se entrenan todos los folds (F1-F5)'}\")\n",
    "    print(f\"  ‚Ä¢ {'Se ejecutar√°n los 5 modelos en ese fold' if MVP_MODE else 'Se ejecutar√°n los 5 modelos en todos los folds'}\")\n",
    "    \n",
    "    if old_mode != MVP_MODE:\n",
    "        print(f\"‚ö†Ô∏è El modo ha cambiado de {old_mode} a {MVP_MODE}\")\n",
    "    \n",
    "    return MVP_MODE\n",
    "\n",
    "# Por defecto, iniciar en modo MVP (True)\n",
    "# Para ejecutar todos los experimentos en todos los folds, ejecutar:\n",
    "# set_mvp_mode(False)\n",
    "\n",
    "print(f\"Estado actual: Modo MVP {'‚úÖ ACTIVO' if MVP_MODE else '‚ùå INACTIVO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcac790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ñ∂Ô∏è Verificar la codificaci√≥n one-hot de cluster_elevation\n",
    "print(\"=\"*80)\n",
    "print(\"üîç VERIFICACI√ìN DE CODIFICACI√ìN ONE-HOT PARA CLUSTER_ELEVATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def robust_array_processing(data_array, feature_name):\n",
    "    \"\"\"\n",
    "    Applies appropriate preprocessing to an array based on the feature type.\n",
    "    For categorical features, performs one-hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        data_array (numpy.ndarray): The input data array\n",
    "        feature_name (str): Name of the feature being processed\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Processed array with one-hot encoding if applicable\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if this is likely a categorical feature that needs one-hot encoding\n",
    "        if np.issubdtype(data_array.dtype, np.integer):\n",
    "            # Get unique values\n",
    "            unique_values = np.unique(data_array)\n",
    "            num_categories = len(unique_values)\n",
    "            \n",
    "            if num_categories <= 20:  # Reasonable threshold for categorical data\n",
    "                print(f\"Applying one-hot encoding to {feature_name} with {num_categories} categories\")\n",
    "                \n",
    "                # Create output array with extra dimension for one-hot encoding\n",
    "                if data_array.ndim == 2:\n",
    "                    # 2D case (spatial only)\n",
    "                    encoded = np.zeros((*data_array.shape, num_categories), dtype=np.float32)\n",
    "                    \n",
    "                    # Apply one-hot encoding\n",
    "                    for i, val in enumerate(unique_values):\n",
    "                        mask = (data_array == val)\n",
    "                        encoded[..., i][mask] = 1.0\n",
    "                        \n",
    "                elif data_array.ndim == 3:\n",
    "                    # 3D case (time, lat, lon)\n",
    "                    encoded = np.zeros((*data_array.shape, num_categories), dtype=np.float32)\n",
    "                    \n",
    "                    # Apply one-hot encoding\n",
    "                    for i, val in enumerate(unique_values):\n",
    "                        mask = (data_array == val)\n",
    "                        encoded[..., i][mask] = 1.0\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Unexpected shape for {feature_name}: {data_array.shape}. Skipping one-hot encoding.\")\n",
    "                    return data_array\n",
    "                    \n",
    "                return encoded\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Too many unique values ({num_categories}) for one-hot encoding {feature_name}. Returning original.\")\n",
    "                return data_array\n",
    "        else:\n",
    "            print(f\"Feature {feature_name} with dtype {data_array.dtype} doesn't need one-hot encoding.\")\n",
    "            return data_array\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {feature_name}: {e}\")\n",
    "        print(\"Returning original array without processing\")\n",
    "        return data_array\n",
    "\n",
    "# Funci√≥n para mostrar una muestra de datos categ√≥ricos y su codificaci√≥n one-hot\n",
    "def verify_one_hot_encoding(dataset_path, feature_name='cluster_elevation'):\n",
    "    \"\"\"Verifica y muestra ejemplos de la codificaci√≥n one-hot para una caracter√≠stica categ√≥rica\"\"\"\n",
    "    try:\n",
    "        # Cargar dataset\n",
    "        print(f\"Cargando dataset desde {dataset_path}...\")\n",
    "        ds = xr.open_dataset(dataset_path)\n",
    "        \n",
    "        # Verificar si la caracter√≠stica existe\n",
    "        if feature_name not in ds.data_vars:\n",
    "            print(f\"‚ùå La caracter√≠stica '{feature_name}' no est√° en el dataset\")\n",
    "            return\n",
    "        \n",
    "        # Extraer datos crudos\n",
    "        raw_data = ds[feature_name].values\n",
    "        print(f\"Datos cargados: {feature_name}, forma {raw_data.shape}, tipo {raw_data.dtype}\")\n",
    "        \n",
    "        # Si es una variable 2D (spatial only), mostrar solo una parte\n",
    "        if raw_data.ndim == 2:\n",
    "            sample = raw_data[:min(10, raw_data.shape[0]), :min(10, raw_data.shape[1])]\n",
    "        else:  # Variable 3D (time, lat, lon)\n",
    "            sample = raw_data[0, :min(10, raw_data.shape[1]), :min(10, raw_data.shape[2])]\n",
    "            \n",
    "        print(\"\\nüìä MUESTRA DE DATOS ORIGINALES:\")\n",
    "        print(sample)\n",
    "        \n",
    "        # Identificar valores √∫nicos\n",
    "        unique_values = np.unique(raw_data)\n",
    "        print(f\"\\nValores √∫nicos encontrados ({len(unique_values)}): {unique_values}\")\n",
    "        \n",
    "        # Aplicar one-hot encoding\n",
    "        print(\"\\nüîÑ Aplicando one-hot encoding...\")\n",
    "        encoded_data = robust_array_processing(raw_data, feature_name)\n",
    "        \n",
    "        # Verificar si la dimensi√≥n aument√≥ (confirmar one-hot encoding)\n",
    "        if encoded_data.ndim > raw_data.ndim:\n",
    "            print(f\"‚úÖ One-hot encoding aplicado correctamente: forma {encoded_data.shape}\")\n",
    "            \n",
    "            # Mostrar ejemplos de la codificaci√≥n\n",
    "            print(\"\\nüìä EJEMPLOS DE CODIFICACI√ìN ONE-HOT:\")\n",
    "            \n",
    "            # Crear una tabla que muestre valor original y encoding\n",
    "            print(f\"{'Valor original':<15} | {'Codificaci√≥n one-hot'}\")\n",
    "            print(\"-\"*40)\n",
    "            \n",
    "            for idx, val in enumerate(unique_values):\n",
    "                # Obtener un punto donde este valor est√© presente\n",
    "                if raw_data.ndim == 2:\n",
    "                    coords = np.where(raw_data == val)\n",
    "                    if len(coords[0]) > 0:\n",
    "                        y, x = coords[0][0], coords[1][0]\n",
    "                        original = raw_data[y, x]\n",
    "                        encoding = [encoded_data[y, x, i] for i in range(encoded_data.shape[-1])]\n",
    "                        print(f\"{original:<15} | {encoding}\")\n",
    "                else:  # 3D\n",
    "                    coords = np.where(raw_data[0] == val)\n",
    "                    if len(coords[0]) > 0:\n",
    "                        y, x = coords[0][0], coords[1][0]\n",
    "                        original = raw_data[0, y, x]\n",
    "                        encoding = [encoded_data[0, y, x, i] for i in range(encoded_data.shape[-1])]\n",
    "                        print(f\"{original:<15} | {encoding}\")\n",
    "            \n",
    "            print(\"\\n‚úÖ Codificaci√≥n one-hot verificada correctamente\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No se realiz√≥ one-hot encoding: forma {encoded_data.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error verificando one-hot encoding: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "dataset_path = FULL_NC if 'FULL_NC' in globals() else './data/complete_dataset.nc'\n",
    "verify_one_hot_encoding(dataset_path)\n",
    "\n",
    "def build_compatible_dataloaders(val_year, use_lags=True, batch_size=32, flatten_spatial=True):\n",
    "    \"\"\"\n",
    "    Builds TensorFlow compatible data loaders for training and validation.\n",
    "    \n",
    "    Args:\n",
    "        val_year (int): Year to use for validation\n",
    "        use_lags (bool): Whether to include lagged features\n",
    "        batch_size (int): Batch size for training\n",
    "        flatten_spatial (bool): Whether to flatten spatial dimensions\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_dataset, val_dataset, num_features)\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    dataset_path = FULL_NC if 'FULL_NC' in globals() else './data/complete_dataset.nc'\n",
    "    ds = xr.open_dataset(dataset_path)\n",
    "    \n",
    "    # Extract features and apply processing (including one-hot encoding)\n",
    "    features = []\n",
    "    for var_name in ds.data_vars:\n",
    "        if var_name != 'precipitation':  # Exclude target\n",
    "            data = ds[var_name].values\n",
    "            processed = robust_array_processing(data, var_name)\n",
    "            features.append(processed)\n",
    "    \n",
    "    # Calculate number of features\n",
    "    num_features = sum([feat.shape[-1] if feat.ndim > 3 else 1 for feat in features])\n",
    "    \n",
    "    # Handle lag features if requested\n",
    "    if use_lags:\n",
    "        # Implementation for lag features would go here\n",
    "        pass\n",
    "    \n",
    "    # Create train/val splits based on year\n",
    "    train_mask = ds.time.dt.year != val_year\n",
    "    val_mask = ds.time.dt.year == val_year\n",
    "    \n",
    "    # Prepare X and y data\n",
    "    # Simplified for demonstration\n",
    "    X = np.concatenate([feat.reshape(len(ds.time), -1) if feat.ndim <= 3 \n",
    "                       else feat.reshape(len(ds.time), -1) for feat in features], axis=1)\n",
    "    y = ds['precipitation'].values\n",
    "    \n",
    "    # Split data\n",
    "    X_train = X[train_mask]\n",
    "    y_train = y[train_mask]\n",
    "    X_val = X[val_mask]\n",
    "    y_val = y[val_mask]\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "    \n",
    "    return train_dataset, val_dataset, num_features\n",
    "\n",
    "# Modificar TFPrecipitationDataset para integrar normalizaci√≥n\n",
    "class NormalizedTFPrecipitationDataset(TFPrecipitationDataset):\n",
    "    \"\"\"Extiende TFPrecipitationDataset a√±adiendo normalizaci√≥n\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_dataloaders(val_year, use_lags=True, batch_size=32, normalize=True):\n",
    "        \"\"\"\n",
    "        Construye dataloaders con normalizaci√≥n incluida\n",
    "        \n",
    "        Args:\n",
    "            val_year: A√±o de validaci√≥n\n",
    "            use_lags: Si usar variables de lag\n",
    "            batch_size: Tama√±o del batch\n",
    "            normalize: Si aplicar normalizaci√≥n\n",
    "            \n",
    "        Returns:\n",
    "            Tupla de (train_loader, val_loader, num_features, normalizer)\n",
    "        \"\"\"\n",
    "        # Obtener dataloaders originales\n",
    "        train_loader, val_loader, num_features = TFPrecipitationDataset.build_dataloaders(\n",
    "            val_year, use_lags, batch_size)\n",
    "        \n",
    "        # Si no se requiere normalizaci√≥n, retornar como est√°n\n",
    "        if not normalize:\n",
    "            return train_loader, val_loader, num_features, None\n",
    "        \n",
    "        # Aplicar normalizaci√≥n (normalmente requerir√≠a acceder a los datos subyacentes)\n",
    "        # En este caso, una implementaci√≥n completa requerir√≠a modificar el DataLoader\n",
    "        # ya construido o aplicar la normalizaci√≥n durante la construcci√≥n.\n",
    "        \n",
    "        # Versi√≥n simplificada: retornar normalizer junto con dataloaders originales\n",
    "        # para que pueda ser usado para desnormalizar durante evaluaci√≥n/visualizaci√≥n\n",
    "        try:\n",
    "            dataset_path = FULL_NC if 'FULL_NC' in globals() else './data/complete_dataset.nc'\n",
    "            ds = xr.open_dataset(dataset_path)\n",
    "            normalizer = DataNormalizer()\n",
    "            normalizer.fit(ds)\n",
    "            print(\"‚úÖ Normalizador ajustado a datos (para uso posterior)\")\n",
    "            return train_loader, val_loader, num_features, normalizer\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creando normalizador: {e}\")\n",
    "            return train_loader, val_loader, num_features, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ñ∂Ô∏è Implementaci√≥n de AdamW y One-Cycle Scheduler\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class OneCycleLR(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    One-Cycle Learning Rate Scheduler\n",
    "    \n",
    "    Implementa la pol√≠tica de tasa de aprendizaje One-Cycle seg√∫n el paper\n",
    "    \"Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\"\n",
    "    \n",
    "    Caracter√≠sticas:\n",
    "    - Incremento de LR desde 'min_lr' a 'max_lr' en 'step_size' epochs\n",
    "    - Disminuci√≥n desde 'max_lr' a 'min_lr / div_factor' en los epochs restantes\n",
    "    - Opcionalmente modula el momento inversamente a la tasa de aprendizaje\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_lr, steps_per_epoch, epochs, div_factor=25.,\n",
    "                 pct_start=0.3, anneal_strategy='cos', three_phase=False):\n",
    "        \"\"\"\n",
    "        Inicializa el scheduler One-Cycle\n",
    "        \n",
    "        Args:\n",
    "            max_lr: Tasa de aprendizaje m√°xima\n",
    "            steps_per_epoch: Pasos por √©poca\n",
    "            epochs: N√∫mero total de √©pocas\n",
    "            div_factor: Factor de divisi√≥n para calcular tasa de aprendizaje inicial\n",
    "            pct_start: Porcentaje de ciclo total para alcanzar tasa m√°xima\n",
    "            anneal_strategy: Estrategia de annealing ('cos' o 'linear')\n",
    "            three_phase: Si True, usar fase separada para momento\n",
    "        \"\"\"\n",
    "        super(OneCycleLR, self).__init__()\n",
    "        self.max_lr = max_lr\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.total_epochs = epochs\n",
    "        self.div_factor = div_factor\n",
    "        self.min_lr = max_lr / div_factor\n",
    "        self.final_lr = self.min_lr / 1000\n",
    "        self.pct_start = pct_start\n",
    "        self.anneal_strategy = anneal_strategy\n",
    "        self.three_phase = three_phase\n",
    "        \n",
    "        # Puntos de cambio de fase\n",
    "        self.step_size_up = int(self.total_steps * self.pct_start)\n",
    "        self.step_size_down = self.total_steps - self.step_size_up\n",
    "        \n",
    "        # Momento (valores por defecto)\n",
    "        self.initial_momentum = 0.95\n",
    "        self.final_momentum = 0.85\n",
    "        \n",
    "        # Contador de pasos\n",
    "        self.step_counter = 0\n",
    "        self.history = {'lr': [], 'momentum': []}\n",
    "        \n",
    "    def _annealing_cos(self, start, end, pct):\n",
    "        \"\"\"Interpolaci√≥n coseno entre start y end\"\"\"\n",
    "        cos_out = math.cos(math.pi * pct) + 1\n",
    "        return end + (start - end) / 2.0 * cos_out\n",
    "    \n",
    "    def _annealing_linear(self, start, end, pct):\n",
    "        \"\"\"Interpolaci√≥n lineal entre start y end\"\"\"\n",
    "        return (end - start) * pct + start\n",
    "    \n",
    "    def get_lr_momentum(self):\n",
    "        \"\"\"Calcula LR y momento actuales seg√∫n el paso\"\"\"\n",
    "        # Calcular porcentaje de avance en la fase actual\n",
    "        if self.step_counter <= self.step_size_up:\n",
    "            # Fase ascendente\n",
    "            percent = self.step_counter / self.step_size_up\n",
    "            \n",
    "            if self.anneal_strategy == 'cos':\n",
    "                lr = self._annealing_cos(self.min_lr, self.max_lr, percent)\n",
    "                momentum = self._annealing_cos(self.initial_momentum, \n",
    "                                              self.final_momentum, percent)\n",
    "            else:\n",
    "                lr = self._annealing_linear(self.min_lr, self.max_lr, percent)\n",
    "                momentum = self._annealing_linear(self.initial_momentum, \n",
    "                                                 self.final_momentum, percent)\n",
    "        else:\n",
    "            # Fase descendente\n",
    "            percent = (self.step_counter - self.step_size_up) / self.step_size_down\n",
    "            \n",
    "            if self.anneal_strategy == 'cos':\n",
    "                lr = self._annealing_cos(self.max_lr, self.final_lr, percent)\n",
    "                momentum = self._annealing_cos(self.final_momentum, \n",
    "                                              self.initial_momentum, percent)\n",
    "            else:\n",
    "                lr = self._annealing_linear(self.max_lr, self.final_lr, percent)\n",
    "                momentum = self._annealing_linear(self.final_momentum, \n",
    "                                                 self.initial_momentum, percent)\n",
    "        \n",
    "        return lr, momentum\n",
    "    \n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"Actualiza LR y momento al comienzo de cada batch\"\"\"\n",
    "        lr, momentum = self.get_lr_momentum()\n",
    "        \n",
    "        # Actualizar optimizer\n",
    "        K = tf.keras.backend\n",
    "        if hasattr(self.model.optimizer, 'lr'):\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "            \n",
    "        # Actualizar momentum/beta1 si es posible\n",
    "        if hasattr(self.model.optimizer, 'beta_1'):  # Para Adam/AdamW\n",
    "            K.set_value(self.model.optimizer.beta_1, momentum)\n",
    "        elif hasattr(self.model.optimizer, 'momentum'):  # Para SGD\n",
    "            K.set_value(self.model.optimizer.momentum, momentum)\n",
    "            \n",
    "        # Incrementar contador\n",
    "        self.step_counter += 1\n",
    "        \n",
    "        # Guardar historial\n",
    "        self.history['lr'].append(lr)\n",
    "        self.history['momentum'].append(momentum)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Registra LR y momento al final de cada √©poca\"\"\"\n",
    "        lr, momentum = self.get_lr_momentum()\n",
    "        \n",
    "        # A√±adir al log si existe\n",
    "        if logs is not None:\n",
    "            logs['lr'] = lr \n",
    "            logs['momentum'] = momentum\n",
    "            \n",
    "        # Imprimir valores cada 5 √©pocas\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch}: lr={lr:.6f}, momentum={momentum:.6f}\")\n",
    "\n",
    "# Funci√≥n para obtener un optimizador AdamW con weight decay configurable\n",
    "def get_adamw_optimizer(learning_rate=1e-3, weight_decay=1e-4):\n",
    "    \"\"\"\n",
    "    Crea un optimizador AdamW con los par√°metros especificados\n",
    "    \n",
    "    Args:\n",
    "        learning_rate: Tasa de aprendizaje inicial\n",
    "        weight_decay: Factor de regularizaci√≥n L2\n",
    "        \n",
    "    Returns:\n",
    "        Optimizador AdamW configurado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Intentar usar AdamW de la API principal (disponible en TF >= 2.11)\n",
    "        optimizer = tf.keras.optimizers.AdamW(\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-7\n",
    "        )\n",
    "    except AttributeError:\n",
    "        # Fallback a AdamW experimental para versiones anteriores\n",
    "        try:\n",
    "            from tensorflow.keras.optimizers import legacy\n",
    "            optimizer = legacy.AdamW(\n",
    "                learning_rate=learning_rate,\n",
    "                weight_decay=weight_decay,\n",
    "                beta_1=0.9,\n",
    "                beta_2=0.999,\n",
    "                epsilon=1e-7\n",
    "            )\n",
    "        except:\n",
    "            # √öltimo recurso: Adam normal + L2 manual\n",
    "            print(\"‚ö†Ô∏è AdamW no disponible, usando Adam est√°ndar + regularizaci√≥n L2\")\n",
    "            optimizer = tf.keras.optimizers.Adam(\n",
    "                learning_rate=learning_rate,\n",
    "                beta_1=0.9,\n",
    "                beta_2=0.999,\n",
    "                epsilon=1e-7\n",
    "            )\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "# Funci√≥n para entrenar con AdamW + One-Cycle\n",
    "def train_with_advanced_optim(model, train_loader, val_loader, epochs=60, patience=20,\n",
    "                            max_lr=1e-3, weight_decay=1e-4, fold=None, exp_name=None):\n",
    "    \"\"\"\n",
    "    Entrenar un modelo usando optimizador AdamW y One-Cycle scheduler\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a entrenar\n",
    "        train_loader: DataLoader para entrenamiento\n",
    "        val_loader: DataLoader para validaci√≥n\n",
    "        epochs: N√∫mero m√°ximo de √©pocas\n",
    "        patience: Paciencia para early stopping\n",
    "        max_lr: Tasa de aprendizaje m√°xima para one-cycle\n",
    "        weight_decay: Regularizaci√≥n L2 para AdamW\n",
    "        fold: Fold actual (para registro)\n",
    "        exp_name: Nombre del experimento (para registro)\n",
    "        \n",
    "    Returns:\n",
    "        model: Modelo entrenado (mejor versi√≥n)\n",
    "        history: Historial de entrenamiento\n",
    "        best_rmse: Mejor RMSE de validaci√≥n\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import time\n",
    "    \n",
    "    print(f\"üîÑ Entrenando {exp_name} (fold {fold}) con AdamW y One-Cycle\")\n",
    "    \n",
    "    # Obtener n√∫mero de pasos por √©poca\n",
    "    steps_per_epoch = 0\n",
    "    for _ in train_loader:\n",
    "        steps_per_epoch += 1\n",
    "    \n",
    "    # Si no se pudo determinar, usar un valor razonable\n",
    "    if steps_per_epoch == 0:\n",
    "        steps_per_epoch = 10\n",
    "        print(f\"‚ö†Ô∏è No se pudo determinar steps_per_epoch, usando {steps_per_epoch}\")\n",
    "    \n",
    "    # Configurar optimizador AdamW\n",
    "    optimizer = get_adamw_optimizer(learning_rate=max_lr/25, weight_decay=weight_decay)\n",
    "    \n",
    "    # Compilar modelo\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=huber_loss_with_horizon_weight, \n",
    "        metrics=['mse', 'mae']\n",
    "    )\n",
    "    \n",
    "    # Configurar callbacks\n",
    "    callbacks = []\n",
    "    \n",
    "    # One-Cycle LR\n",
    "    onecycle = OneCycleLR(\n",
    "        max_lr=max_lr,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        div_factor=25.,\n",
    "        pct_start=0.3\n",
    "    )\n",
    "    callbacks.append(onecycle)\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(early_stop)\n",
    "    \n",
    "    # Entrenamiento\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_loader,\n",
    "        validation_data=val_loader,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    ).history\n",
    "    \n",
    "    # Agregar historial de LR y momentum\n",
    "    history['lr'] = onecycle.history['lr']\n",
    "    history['momentum'] = onecycle.history['momentum']\n",
    "    \n",
    "    # Calcular mejor RMSE\n",
    "    val_mse = history['val_mse']\n",
    "    best_epoch = np.argmin(val_mse)\n",
    "    best_rmse = np.sqrt(val_mse[best_epoch])\n",
    "    \n",
    "    # Tiempo total\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Entrenamiento completado en {total_time:.2f}s\")\n",
    "    print(f\"   Mejor RMSE: {best_rmse:.4f} (√©poca {best_epoch+1})\")\n",
    "    \n",
    "    return model, history, best_rmse\n",
    "\n",
    "print(\"‚úÖ AdamW y One-Cycle Scheduler implementados seg√∫n plan experimental\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ñ∂Ô∏è Implementaci√≥n de funci√≥n de p√©rdida Huber con ponderaci√≥n por horizonte\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    \"\"\"\n",
    "    Implementa la funci√≥n de p√©rdida Huber\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tensor con valores reales\n",
    "        y_pred: Tensor con predicciones\n",
    "        delta: Par√°metro que controla la transici√≥n de L1 a L2\n",
    "        \n",
    "    Returns:\n",
    "        Tensor con la p√©rdida Huber\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    abs_error = tf.abs(error)\n",
    "    quadratic = tf.minimum(abs_error, delta)\n",
    "    linear = abs_error - quadratic\n",
    "    loss = 0.5 * tf.square(quadratic) + delta * linear\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def huber_loss_with_horizon_weight(y_true, y_pred, delta=1.0):\n",
    "    \"\"\"\n",
    "    Funci√≥n de p√©rdida Huber con ponderaci√≥n por horizonte\n",
    "    Asigna mayor peso a los horizontes m√°s lejanos seg√∫n la f√≥rmula:\n",
    "    œâ‚Çï = 1 + h/12\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tensor con valores reales [batch, horizon, ...] \n",
    "        y_pred: Tensor con predicciones [batch, horizon, ...]\n",
    "        delta: Par√°metro de la funci√≥n Huber\n",
    "        \n",
    "    Returns:\n",
    "        Tensor con la p√©rdida ponderada\n",
    "    \"\"\"\n",
    "    # Extraer dimensi√≥n del horizonte (normalmente la segunda dimensi√≥n)\n",
    "    horizon = tf.shape(y_true)[1]\n",
    "    horizon_float = tf.cast(horizon, tf.float32)\n",
    "    \n",
    "    # Crear pesos seg√∫n f√≥rmula œâ‚Çï = 1 + h/12\n",
    "    h_indices = tf.range(1, horizon + 1, dtype=tf.float32)\n",
    "    weights = 1.0 + h_indices / 12.0\n",
    "    \n",
    "    # Expandir dimensions para broadcasting\n",
    "    # [horizon] -> [1, horizon, 1, 1, ...]\n",
    "    for _ in range(len(y_true.shape) - 2):\n",
    "        weights = tf.expand_dims(weights, axis=-1)\n",
    "        \n",
    "    # Calcular error Huber\n",
    "    error = y_true - y_pred\n",
    "    abs_error = tf.abs(error)\n",
    "    quadratic = tf.minimum(abs_error, delta)\n",
    "    linear = abs_error - quadratic\n",
    "    unweighted_loss = 0.5 * tf.square(quadratic) + delta * linear\n",
    "    \n",
    "    # Aplicar pesos por horizonte\n",
    "    weighted_loss = unweighted_loss * weights\n",
    "    \n",
    "    # Reducir media\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "def rmse_by_horizon(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula RMSE separado para cada paso del horizonte de predicci√≥n\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tensor con valores reales [batch, horizon, ...] \n",
    "        y_pred: Tensor con predicciones [batch, horizon, ...]\n",
    "        \n",
    "    Returns:\n",
    "        Lista con RMSE para cada horizonte\n",
    "    \"\"\"\n",
    "    # Obtener n√∫mero de horizontes\n",
    "    horizon = y_true.shape[1]\n",
    "    \n",
    "    # Calcular RMSE por horizonte\n",
    "    rmse_values = []\n",
    "    for h in range(horizon):\n",
    "        # Extraer slice para este horizonte\n",
    "        y_true_h = y_true[:, h]\n",
    "        y_pred_h = y_pred[:, h]\n",
    "        \n",
    "        # Calcular MSE y RMSE\n",
    "        mse = tf.reduce_mean(tf.square(y_true_h - y_pred_h))\n",
    "        rmse = tf.sqrt(mse)\n",
    "        rmse_values.append(rmse)\n",
    "        \n",
    "    return rmse_values\n",
    "\n",
    "# Clase de m√©trica personalizada para TensorFlow\n",
    "class RMSEByHorizon(tf.keras.metrics.Metric):\n",
    "    \"\"\"\n",
    "    M√©trica personalizada para registrar RMSE por horizonte de predicci√≥n\n",
    "    \"\"\"\n",
    "    def __init__(self, horizon=12, name='rmse_by_horizon', **kwargs):\n",
    "        super(RMSEByHorizon, self).__init__(name=name, **kwargs)\n",
    "        self.horizon = horizon\n",
    "        self.errors = [self.add_weight(name=f'horizon_{h+1}', initializer='zeros') \n",
    "                      for h in range(horizon)]\n",
    "        self.counts = [self.add_weight(name=f'count_{h+1}', initializer='zeros') \n",
    "                      for h in range(horizon)]\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Aseguramos que estamos trabajando con el horizonte correcto\n",
    "        horizon = min(self.horizon, y_true.shape[1])\n",
    "        \n",
    "        # Actualizar error acumulado por horizonte\n",
    "        for h in range(horizon):\n",
    "            # Extraer slice para este horizonte\n",
    "            y_true_h = y_true[:, h]\n",
    "            y_pred_h = y_pred[:, h]\n",
    "            \n",
    "            # Actualizar suma de errores cuadr√°ticos y conteo\n",
    "            mse = tf.reduce_mean(tf.square(y_true_h - y_pred_h))\n",
    "            self.errors[h].assign_add(mse)\n",
    "            self.counts[h].assign_add(1.0)\n",
    "    \n",
    "    def result(self):\n",
    "        # Calcular RMSE por horizonte\n",
    "        results = [tf.sqrt(error / count) if count > 0 else 0.0\n",
    "                  for error, count in zip(self.errors, self.counts)]\n",
    "        # Devolver promedio\n",
    "        return tf.reduce_mean(results)\n",
    "    \n",
    "    def get_horizon_results(self):\n",
    "        \"\"\"Devuelve lista con RMSE por horizonte\"\"\"\n",
    "        return [tf.sqrt(error / count) if count > 0 else 0.0\n",
    "               for error, count in zip(self.errors, self.counts)]\n",
    "    \n",
    "    def reset_state(self):\n",
    "        for h in range(self.horizon):\n",
    "            self.errors[h].assign(0.0)\n",
    "            self.counts[h].assign(0.0)\n",
    "\n",
    "# Ejemplo de compilaci√≥n de modelo con estas funciones\n",
    "def compile_model_with_huber(model):\n",
    "    \"\"\"Compila un modelo usando p√©rdida Huber con ponderaci√≥n por horizonte\"\"\"\n",
    "    model.compile(\n",
    "        optimizer=get_adamw_optimizer(learning_rate=1e-3/25, weight_decay=1e-4),\n",
    "        loss=huber_loss_with_horizon_weight,\n",
    "        metrics=['mse', 'mae', RMSEByHorizon(horizon=12)]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de p√©rdida Huber con ponderaci√≥n por horizonte implementada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551917f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ñ∂Ô∏è Implementaci√≥n de Teacher-Forcing con Cosine Decay\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class TeacherForcingController:\n",
    "    \"\"\"\n",
    "    Controlador para Teacher-Forcing con decaimiento de coseno (0.70‚Üí0.30)\n",
    "    \n",
    "    El Teacher-Forcing es una t√©cnica para modelos secuenciales donde se usa la\n",
    "    salida real (no la predicha) como entrada del siguiente paso durante el entrenamiento.\n",
    "    \n",
    "    Esta implementaci√≥n:\n",
    "    - Comienza con probabilidad 0.70 de usar Teacher-Forcing\n",
    "    - Decrece hacia 0.30 siguiendo una curva de coseno\n",
    "    - Se puede ajustar a cualquier rango de √©pocas\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, start_prob=0.70, end_prob=0.30, total_epochs=60):\n",
    "        \"\"\"\n",
    "        Inicializa el controlador de Teacher-Forcing\n",
    "        \n",
    "        Args:\n",
    "            start_prob: Probabilidad inicial de usar Teacher-Forcing\n",
    "            end_prob: Probabilidad final de usar Teacher-Forcing\n",
    "            total_epochs: N√∫mero total de √©pocas para el decaimiento\n",
    "        \"\"\"\n",
    "        self.start_prob = start_prob\n",
    "        self.end_prob = end_prob\n",
    "        self.total_epochs = total_epochs\n",
    "        self.current_epoch = 0\n",
    "        self.current_prob = start_prob\n",
    "    \n",
    "    def update_epoch(self, epoch=None):\n",
    "        \"\"\"\n",
    "        Actualiza la √©poca actual y recalcula la probabilidad de Teacher-Forcing\n",
    "        \n",
    "        Args:\n",
    "            epoch: Nueva √©poca (opcional, si None incrementa la actual)\n",
    "            \n",
    "        Returns:\n",
    "            Probabilidad actualizada\n",
    "        \"\"\"\n",
    "        # Actualizar √©poca\n",
    "        if epoch is not None:\n",
    "            self.current_epoch = epoch\n",
    "        else:\n",
    "            self.current_epoch += 1\n",
    "        \n",
    "        # Asegurar l√≠mites\n",
    "        self.current_epoch = min(self.current_epoch, self.total_epochs)\n",
    "        \n",
    "        # Calcular progreso normalizado (0 a 1)\n",
    "        progress = self.current_epoch / self.total_epochs\n",
    "        \n",
    "        # Aplicar decaimiento de coseno\n",
    "        cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        self.current_prob = self.end_prob + (self.start_prob - self.end_prob) * cosine_decay\n",
    "        \n",
    "        return self.current_prob\n",
    "    \n",
    "    def use_teacher_forcing(self):\n",
    "        \"\"\"\n",
    "        Decide si usar Teacher-Forcing en la iteraci√≥n actual\n",
    "        \n",
    "        Returns:\n",
    "            True si se debe usar Teacher-Forcing, False en caso contrario\n",
    "        \"\"\"\n",
    "        return np.random.random() < self.current_prob\n",
    "    \n",
    "    def get_current_probability(self):\n",
    "        \"\"\"\n",
    "        Devuelve la probabilidad actual\n",
    "        \n",
    "        Returns:\n",
    "            Probabilidad actual de usar Teacher-Forcing\n",
    "        \"\"\"\n",
    "        return self.current_prob\n",
    "\n",
    "\n",
    "# Implementaci√≥n de una capa de decoder con Teacher-Forcing para GRU\n",
    "class TeacherForcingGRUDecoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Capa de decoder GRU con soporte para Teacher-Forcing\n",
    "    \n",
    "    Esta capa permite elegir entre usar la salida predicha por el modelo\n",
    "    o la salida real (Teacher-Forcing) durante el entrenamiento, seg√∫n\n",
    "    una probabilidad que puede variar durante el entrenamiento.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, units, tf_controller=None, return_sequences=True, dropout=0.2, **kwargs):\n",
    "        \"\"\"\n",
    "        Inicializa el decoder con Teacher-Forcing\n",
    "        \n",
    "        Args:\n",
    "            units: Dimensionalidad de la capa GRU\n",
    "            tf_controller: Controlador de Teacher-Forcing (opcional)\n",
    "            return_sequences: Si es True, devuelve la secuencia completa\n",
    "            dropout: Tasa de dropout\n",
    "            **kwargs: Argumentos adicionales para la capa GRU\n",
    "        \"\"\"\n",
    "        super(TeacherForcingGRUDecoder, self).__init__(**kwargs)\n",
    "        self.gru = tf.keras.layers.GRU(units, return_sequences=return_sequences, \n",
    "                                      dropout=dropout, **kwargs)\n",
    "        self.tf_controller = tf_controller or TeacherForcingController()\n",
    "        self.dense = tf.keras.layers.Dense(1)  # Capa de proyecci√≥n para las salidas\n",
    "        self.supports_masking = True\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, inputs, targets=None, initial_state=None, training=False):\n",
    "        \"\"\"\n",
    "        Ejecuta el decoder con soporte para Teacher-Forcing\n",
    "        \n",
    "        Args:\n",
    "            inputs: Entradas del decoder (salidas del encoder)\n",
    "            targets: Objetivos reales (usados en Teacher-Forcing)\n",
    "            initial_state: Estado inicial para la GRU\n",
    "            training: Si es True, estamos en modo entrenamiento\n",
    "            \n",
    "        Returns:\n",
    "            Salidas del decoder\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_length = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Solo usar Teacher-Forcing durante entrenamiento y si targets est√° disponible\n",
    "        use_tf = training and targets is not None and self.tf_controller.use_teacher_forcing()\n",
    "        \n",
    "        if not use_tf or targets is None:\n",
    "            # Ejecuci√≥n normal (sin Teacher-Forcing)\n",
    "            outputs = self.gru(inputs, initial_state=initial_state, training=training)\n",
    "            outputs = self.dropout_layer(outputs, training=training)\n",
    "            return self.dense(outputs)\n",
    "        \n",
    "        # Con Teacher-Forcing: procesamos paso a paso\n",
    "        # Primero procesamos el primer paso normalmente\n",
    "        inputs_t = inputs[:, 0:1, :]\n",
    "        state = initial_state\n",
    "        \n",
    "        # Procesar primer paso\n",
    "        output, state = self.gru(inputs_t, initial_state=state, training=training)\n",
    "        outputs = [self.dense(output)]\n",
    "        \n",
    "        # Para el resto de pasos, decidimos entre usar Teacher-Forcing o no\n",
    "        for t in range(1, seq_length):\n",
    "            # En Teacher-Forcing: usar el target real del paso anterior como entrada\n",
    "            if use_tf:\n",
    "                teacher_input = targets[:, t-1:t, :]\n",
    "                \n",
    "                # La entrada normal del decoder se combina con el target anterior\n",
    "                decoder_input = inputs[:, t:t+1, :]\n",
    "                combined_input = tf.concat([decoder_input, teacher_input], axis=-1)\n",
    "                \n",
    "                # Procesar este paso\n",
    "                output, state = self.gru(combined_input, initial_state=state, training=training)\n",
    "            else:\n",
    "                # Sin Teacher-Forcing: usar la entrada normal\n",
    "                decoder_input = inputs[:, t:t+1, :]\n",
    "                output, state = self.gru(decoder_input, initial_state=state, training=training)\n",
    "            \n",
    "            # Proyectar a espacio de salida\n",
    "            output = self.dropout_layer(output, training=training)\n",
    "            projected = self.dense(output)\n",
    "            outputs.append(projected)\n",
    "        \n",
    "        # Concatenar todas las salidas\n",
    "        return tf.concat(outputs, axis=1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Configuraci√≥n para serializaci√≥n\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'units': self.gru.units,\n",
    "            'return_sequences': self.gru.return_sequences,\n",
    "            'dropout': self.dropout_layer.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# Ejemplo de modelo GRU Encoder-Decoder con Teacher-Forcing\n",
    "class GRUEncoderDecoderWithTF(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Modelo GRU Encoder-Decoder con Teacher-Forcing\n",
    "    \n",
    "    Implementa el Teacher-Forcing con probabilidad variable seg√∫n\n",
    "    el plan experimental (decaimiento de coseno 0.70‚Üí0.30)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_units=128, dropout=0.2, total_epochs=60):\n",
    "        \"\"\"\n",
    "        Inicializa el modelo\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimensionalidad de entrada\n",
    "            hidden_units: Unidades ocultas en GRU\n",
    "            dropout: Tasa de dropout\n",
    "            total_epochs: Total de √©pocas para el decaimiento de Teacher-Forcing\n",
    "        \"\"\"\n",
    "        super(GRUEncoderDecoderWithTF, self).__init__()\n",
    "        \n",
    "        # Configurar controlador de Teacher-Forcing\n",
    "        self.tf_controller = TeacherForcingController(\n",
    "            start_prob=0.70,\n",
    "            end_prob=0.30,\n",
    "            total_epochs=total_epochs\n",
    "        )\n",
    "        \n",
    "        # Arquitectura del modelo\n",
    "        self.encoder_gru = tf.keras.layers.GRU(\n",
    "            hidden_units, return_sequences=True, return_state=True)\n",
    "        \n",
    "        self.decoder_gru = TeacherForcingGRUDecoder(\n",
    "            hidden_units, tf_controller=self.tf_controller)\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "        self.dropout = dropout\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs, targets=None, training=False):\n",
    "        \"\"\"\n",
    "        Ejecuta el modelo\n",
    "        \n",
    "        Args:\n",
    "            inputs: Tensor de entrada [batch, time_steps, features]\n",
    "            targets: Objetivos reales (para Teacher-Forcing)\n",
    "            training: Si es True, estamos en modo entrenamiento\n",
    "            \n",
    "        Returns:\n",
    "            Predicciones\n",
    "        \"\"\"\n",
    "        # Actualizar probabilidad de Teacher-Forcing en √©poca actual (solo training)\n",
    "        if training:\n",
    "            self.current_tf_prob = self.tf_controller.get_current_probability()\n",
    "            tf.summary.scalar('teacher_forcing_prob', self.current_tf_prob)\n",
    "        \n",
    "        # Codificaci√≥n (encoder)\n",
    "        encoder_outputs, encoder_state = self.encoder_gru(inputs, training=training)\n",
    "        encoder_outputs = self.dropout_layer(encoder_outputs, training=training)\n",
    "        \n",
    "        # Decodificaci√≥n (decoder) con soporte para Teacher-Forcing\n",
    "        decoder_outputs = self.decoder_gru(\n",
    "            encoder_outputs, targets=targets, \n",
    "            initial_state=encoder_state, training=training)\n",
    "        \n",
    "        # Proyecci√≥n final\n",
    "        outputs = self.output_layer(decoder_outputs)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "        Implementaci√≥n personalizada de paso de entrenamiento para Teacher-Forcing\n",
    "        \n",
    "        Args:\n",
    "            data: Tupla (x, y) con entradas y objetivos\n",
    "            \n",
    "        Returns:\n",
    "            Dict con m√©tricas de entrenamiento\n",
    "        \"\"\"\n",
    "        x, y = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass con Teacher-Forcing habilitado\n",
    "            predictions = self(x, targets=y, training=True)\n",
    "            \n",
    "            # Calcular p√©rdida\n",
    "            loss = self.compiled_loss(y, predictions, regularization_losses=self.losses)\n",
    "        \n",
    "        # Calcular gradientes y actualizar pesos\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # Actualizar m√©tricas\n",
    "        self.compiled_metrics.update_state(y, predictions)\n",
    "        \n",
    "        # A√±adir probabilidad actual de Teacher-Forcing a las m√©tricas\n",
    "        metrics_results = {m.name: m.result() for m in self.metrics}\n",
    "        metrics_results['teacher_forcing_prob'] = self.tf_controller.get_current_probability()\n",
    "        \n",
    "        return metrics_results\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Callback al final de cada √©poca para actualizar la probabilidad de Teacher-Forcing\n",
    "        \"\"\"\n",
    "        # Actualizar probabilidad de Teacher-Forcing\n",
    "        updated_prob = self.tf_controller.update_epoch(epoch)\n",
    "        \n",
    "        # Agregar al log si existe\n",
    "        if logs is not None:\n",
    "            logs['teacher_forcing_prob'] = updated_prob\n",
    "            \n",
    "        # Imprimir cambio cada 5 √©pocas\n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"√âpoca {epoch}: probabilidad de Teacher-Forcing = {updated_prob:.4f}\")\n",
    "\n",
    "# Actualizar MODEL_FACTORY para incluir los nuevos modelos avanzados\n",
    "def update_model_factory():\n",
    "    \"\"\"\n",
    "    Actualiza el diccionario MODEL_FACTORY con los nuevos modelos avanzados\n",
    "    \"\"\"\n",
    "    global MODEL_FACTORY\n",
    "    \n",
    "    # A√±adir modelos avanzados\n",
    "    updated_factory = {\n",
    "        'gru_ed': lambda input_dim, dropout=0.2: GRUEncoderDecoderWithTF(input_dim, dropout=dropout),\n",
    "        'gru_ed_pafc': lambda input_dim, dropout=0.2: GRUEncoderDecoderWithTF(input_dim, dropout=dropout),\n",
    "        'ae_fusion_gru': lambda input_dim, dropout=0.2: AEFusionGRUModel(input_dim, dropout=dropout),\n",
    "        'ae_fusion_gru_t': lambda input_dim, dropout=0.2: AEFusionGRUAttention(input_dim, dropout=dropout),\n",
    "        'ae_fusion_gru_t_mask': lambda input_dim, dropout=0.2: AEFusionGRUAttentionMask(input_dim, dropout=dropout)\n",
    "    }\n",
    "    \n",
    "    # Actualizar diccionario global\n",
    "    MODEL_FACTORY.update(updated_factory)\n",
    "    print(\"‚úÖ MODEL_FACTORY actualizado con arquitecturas avanzadas\")\n",
    "\n",
    "print(\"‚úÖ Arquitecturas avanzadas implementadas seg√∫n plan experimental\")\n",
    "print(\"   Incluyendo Conv3D-AE con bottleneck de 64 dims\")\n",
    "\n",
    "# Ejemplo de la arquitectura del autoencoder\n",
    "try:\n",
    "    # Crear modelo\n",
    "    input_shape = (48, 61, 65, 3)  # (time, height, width, channels)\n",
    "    model = Conv3DAutoencoder(input_shape)\n",
    "    \n",
    "    # Mostrar arquitectura\n",
    "    print(\"\\nüß† ARQUITECTURA DEL AUTOENCODER CONVOLUCIONAL 3D:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"ENCODER:\")\n",
    "    model.encoder.summary(line_length=100)\n",
    "    print(\"\\nDECODER:\")\n",
    "    model.decoder.summary(line_length=100)\n",
    "    \n",
    "    # Actualizar MODEL_FACTORY\n",
    "    update_model_factory()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al mostrar arquitectura: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a1c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ñ∂Ô∏è Implementaci√≥n de normalizaci√≥n con RobustScaler y StandardScaler\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "class DataNormalizer:\n",
    "    \"\"\"\n",
    "    Implementa la normalizaci√≥n de datos seg√∫n el plan experimental:\n",
    "    - RobustScaler por celda para precipitaci√≥n (m√°s resistente a outliers)\n",
    "    - StandardScaler global para variables topogr√°ficas y temporales\n",
    "    \n",
    "    Los scalers se guardan para poder aplicar la transformaci√≥n inversa\n",
    "    durante la evaluaci√≥n y visualizaci√≥n.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Inicializa el normalizador con scalers vac√≠os\"\"\"\n",
    "        # Diccionario para almacenar los scalers de precipitaci√≥n por celda\n",
    "        self.precip_scalers = {}  # (lat, lon) -> RobustScaler\n",
    "        \n",
    "        # Scalers globales para cada tipo de variable\n",
    "        self.topo_scaler = StandardScaler()       # Variables topogr√°ficas\n",
    "        self.temporal_scaler = StandardScaler()   # Variables temporales\n",
    "        self.other_scaler = StandardScaler()      # Otras variables\n",
    "        \n",
    "        # Mapeo de variables a tipos\n",
    "        self.topo_features = ['elevation', 'slope', 'roughness', 'curvature', 'aspect']\n",
    "        self.temporal_features = ['month_sin', 'month_cos', 'doy_sin', 'doy_cos']\n",
    "        self.precipitation_vars = ['total_precipitation', 'precip_hist', \n",
    "                                  'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag12',\n",
    "                                  'lag_1', 'lag_2', 'lag_12']\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, ds):\n",
    "        \"\"\"\n",
    "        Ajusta los escaladores a los datos del dataset\n",
    "        \n",
    "        Args:\n",
    "            ds: Dataset xarray con las variables\n",
    "        \"\"\"\n",
    "        print(\"üî¢ Ajustando escaladores de normalizaci√≥n...\")\n",
    "        \n",
    "        # 1. RobustScaler por celda para variables de precipitaci√≥n\n",
    "        if any(var in ds.data_vars for var in self.precipitation_vars):\n",
    "            print(\"  ‚ñ∂Ô∏è Ajustando RobustScaler por celda para precipitaci√≥n...\")\n",
    "            \n",
    "            # Encontrar una variable de precipitaci√≥n disponible\n",
    "            precip_var = next((var for var in self.precipitation_vars if var in ds.data_vars), None)\n",
    "            \n",
    "            if precip_var:\n",
    "                # Acceder a la variable de precipitaci√≥n en forma de arreglo\n",
    "                precip_data = ds[precip_var].values\n",
    "                \n",
    "                # Determinar dimensiones espaciales\n",
    "                if precip_data.ndim == 3:  # [time, lat, lon]\n",
    "                    n_times, n_lats, n_lons = precip_data.shape\n",
    "                    \n",
    "                    # Crear y ajustar un scaler para cada celda espacial\n",
    "                    for lat_idx in range(n_lats):\n",
    "                        for lon_idx in range(n_lons):\n",
    "                            # Extraer serie temporal para esta celda\n",
    "                            cell_data = precip_data[:, lat_idx, lon_idx].reshape(-1, 1)\n",
    "                            \n",
    "                            # Solo ajustar si tenemos datos v√°lidos (no todos NaN)\n",
    "                            if not np.all(np.isnan(cell_data)):\n",
    "                                # Reemplazar NaNs con 0 para el ajuste\n",
    "                                cell_data_clean = np.nan_to_num(cell_data, nan=0.0)\n",
    "                                \n",
    "                                # Crear y ajustar RobustScaler para esta celda\n",
    "                                scaler = RobustScaler(quantile_range=(10.0, 90.0))\n",
    "                                scaler.fit(cell_data_clean)\n",
    "                                \n",
    "                                # Guardar el scaler\n",
    "                                self.precip_scalers[(lat_idx, lon_idx)] = scaler\n",
    "                \n",
    "                print(f\"  ‚úì RobustScaler ajustado para {len(self.precip_scalers)} celdas\")\n",
    "        \n",
    "        # 2. StandardScaler global para variables topogr√°ficas\n",
    "        topo_arrays = []\n",
    "        for var in self.topo_features:\n",
    "            if var in ds.data_vars:\n",
    "                data = ds[var].values\n",
    "                \n",
    "                # Asegurarse que es 2D o 3D y aplanar para el ajuste\n",
    "                if data.ndim == 2:  # [lat, lon]\n",
    "                    flat_data = data.reshape(-1, 1)\n",
    "                elif data.ndim == 3:  # [time, lat, lon]\n",
    "                    # En este caso, solo necesitamos una muestra temporal\n",
    "                    flat_data = data[0].reshape(-1, 1)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # A√±adir al array de variables topogr√°ficas\n",
    "                topo_arrays.append(flat_data)\n",
    "        \n",
    "        if topo_arrays:\n",
    "            # Concatenar datos topogr√°ficos y ajustar scaler\n",
    "            topo_data = np.concatenate(topo_arrays, axis=0)\n",
    "            topo_data_clean = np.nan_to_num(topo_data, nan=0.0)\n",
    "            self.topo_scaler.fit(topo_data_clean)\n",
    "            print(f\"  ‚úì StandardScaler ajustado para variables topogr√°ficas\")\n",
    "        \n",
    "        # 3. StandardScaler para variables temporales\n",
    "        temporal_arrays = []\n",
    "        for var in self.temporal_features:\n",
    "            if var in ds.data_vars:\n",
    "                data = ds[var].values\n",
    "                \n",
    "                # Variables temporales suelen ser 3D [time, lat, lon]\n",
    "                if data.ndim == 3:\n",
    "                    # Tomar una muestra espacial (todas las veces)\n",
    "                    flat_data = data[:, 0, 0].reshape(-1, 1)\n",
    "                else:\n",
    "                    flat_data = data.reshape(-1, 1)\n",
    "                \n",
    "                temporal_arrays.append(flat_data)\n",
    "        \n",
    "        if temporal_arrays:\n",
    "            # Concatenar datos temporales y ajustar scaler\n",
    "            temporal_data = np.concatenate(temporal_arrays, axis=0)\n",
    "            self.temporal_scaler.fit(temporal_data)\n",
    "            print(f\"  ‚úì StandardScaler ajustado para variables temporales\")\n",
    "            \n",
    "        self.is_fitted = True\n",
    "        print(\"‚úÖ Normalizaci√≥n configurada correctamente\")\n",
    "    \n",
    "    def transform(self, ds):\n",
    "        \"\"\"\n",
    "        Aplica la normalizaci√≥n al dataset\n",
    "        \n",
    "        Args:\n",
    "            ds: Dataset xarray con las variables\n",
    "            \n",
    "        Returns:\n",
    "            Dataset xarray con variables normalizadas\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            print(\"‚ö†Ô∏è El normalizador no est√° ajustado. Llamando a fit() primero...\")\n",
    "            self.fit(ds)\n",
    "        \n",
    "        # Crear una copia del dataset para no modificar el original\n",
    "        ds_norm = ds.copy()\n",
    "        \n",
    "        # 1. Normalizar variables de precipitaci√≥n (por celda)\n",
    "        for var in self.precipitation_vars:\n",
    "            if var in ds.data_vars:\n",
    "                data = ds[var].values.copy()\n",
    "                \n",
    "                if data.ndim == 3:  # [time, lat, lon]\n",
    "                    n_times, n_lats, n_lons = data.shape\n",
    "                    \n",
    "                    # Normalizar cada celda por separado\n",
    "                    for lat_idx in range(n_lats):\n",
    "                        for lon_idx in range(n_lons):\n",
    "                            if (lat_idx, lon_idx) in self.precip_scalers:\n",
    "                                # Extraer y desnormalizar\n",
    "                                cell_data = data[:, lat_idx, lon_idx].reshape(-1, 1)\n",
    "                                \n",
    "                                # Normalizar (con control de NaN)\n",
    "                                cell_data_clean = np.nan_to_num(cell_data, nan=0.0)\n",
    "                                cell_data_norm = self.precip_scalers[(lat_idx, lon_idx)].transform(cell_data_clean)\n",
    "                                \n",
    "                                # Actualizar datos\n",
    "                                data[:, lat_idx, lon_idx] = cell_data_norm.flatten()\n",
    "                    \n",
    "                    # Actualizar variable en el dataset\n",
    "                    ds_norm[var].values = data\n",
    "                    print(f\"  ‚úì Variable {var} normalizada con RobustScaler por celda\")\n",
    "        \n",
    "        # 2. Normalizar variables topogr√°ficas\n",
    "        for var in self.topo_features:\n",
    "            if var in ds.data_vars:\n",
    "                data = ds[var].values.copy()\n",
    "                data_shape = data.shape\n",
    "                \n",
    "                # Aplanar para normalizaci√≥n\n",
    "                flat_data = data.reshape(-1, 1)\n",
    "                flat_data_clean = np.nan_to_num(flat_data, nan=0.0)\n",
    "                \n",
    "                # Normalizar y restaurar forma\n",
    "                normalized = self.topo_scaler.transform(flat_data_clean)\n",
    "                ds_norm[var].values = normalized.reshape(data_shape)\n",
    "                print(f\"  ‚úì Variable topogr√°fica {var} normalizada con StandardScaler\")\n",
    "        \n",
    "        # 3. Normalizar variables temporales\n",
    "        for var in self.temporal_features:\n",
    "            if var in ds.data_vars:\n",
    "                data = ds[var].values.copy()\n",
    "                data_shape = data.shape\n",
    "                \n",
    "                if data.ndim == 3:  # [time, lat, lon]\n",
    "                    # Variables temporales: mismo valor en cada punto espacial para un tiempo dado\n",
    "                    # Normalizar solo los valores √∫nicos temporales\n",
    "                    unique_temporal = np.unique(data.reshape(data.shape[0], -1), axis=1)\n",
    "                    normalized = self.temporal_scaler.transform(unique_temporal)\n",
    "                    \n",
    "                    # Recrear el array 3D\n",
    "                    for t in range(data.shape[0]):\n",
    "                        data[t, :, :] = normalized[t, 0]\n",
    "                    \n",
    "                    ds_norm[var].values = data\n",
    "                else:\n",
    "                    # Caso m√°s simple (array 1D o 2D)\n",
    "                    flat_data = data.reshape(-1, 1)\n",
    "                    normalized = self.temporal_scaler.transform(flat_data)\n",
    "                    ds_norm[var].values = normalized.reshape(data_shape)\n",
    "                \n",
    "                print(f\"  ‚úì Variable temporal {var} normalizada con StandardScaler\")\n",
    "                \n",
    "        print(\"‚úÖ Normalizaci√≥n aplicada correctamente\")\n",
    "        return ds_norm\n",
    "                \n",
    "    def inverse_transform_precip(self, data, lat_idx=None, lon_idx=None):\n",
    "        \"\"\"\n",
    "        Desnormaliza datos de precipitaci√≥n\n",
    "        \n",
    "        Args:\n",
    "            data: Datos normalizados\n",
    "            lat_idx, lon_idx: √çndices de la celda (si es None, se asume que data tiene forma [time, lat, lon])\n",
    "            \n",
    "        Returns:\n",
    "            Datos desnormalizados\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            print(\"‚ö†Ô∏è El normalizador no est√° ajustado, no se puede desnormalizar\")\n",
    "            return data\n",
    "        \n",
    "        # Caso 1: Datos 3D [time, lat, lon]\n",
    "        if lat_idx is None and lon_idx is None and data.ndim == 3:\n",
    "            result = np.zeros_like(data)\n",
    "            n_times, n_lats, n_lons = data.shape\n",
    "            \n",
    "            for lat_idx in range(n_lats):\n",
    "                for lon_idx in range(n_lons):\n",
    "                    if (lat_idx, lon_idx) in self.precip_scalers:\n",
    "                        # Extraer y desnormalizar\n",
    "                        cell_data = data[:, lat_idx, lon_idx].reshape(-1, 1)\n",
    "                        cell_denorm = self.precip_scalers[(lat_idx, lon_idx)].inverse_transform(cell_data)\n",
    "                        result[:, lat_idx, lon_idx] = cell_denorm.flatten()\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        # Caso 2: Celda espec√≠fica\n",
    "        elif lat_idx is not None and lon_idx is not None:\n",
    "            if (lat_idx, lon_idx) in self.precip_scalers:\n",
    "                # Asegurar forma 2D\n",
    "                reshaped_data = data.reshape(-1, 1) if data.ndim == 1 else data\n",
    "                return self.precip_scalers[(lat_idx, lon_idx)].inverse_transform(reshaped_data)\n",
    "        \n",
    "        # Fallback: retornar datos originales\n",
    "        return data\n",
    "\n",
    "# Integrar con los dataloaders existentes\n",
    "def apply_normalizers_to_dataloaders(train_loader, val_loader, dataset_path=None):\n",
    "    \"\"\"\n",
    "    Aplica normalizaci√≥n a los dataloaders existentes\n",
    "    \n",
    "    Args:\n",
    "        train_loader: DataLoader de entrenamiento\n",
    "        val_loader: DataLoader de validaci√≥n\n",
    "        dataset_path: Ruta al archivo NetCDF para ajustar normalizadores\n",
    "        \n",
    "    Returns:\n",
    "        Tuple de (train_loader, val_loader, normalizer)\n",
    "    \"\"\"\n",
    "    # Si no se proporciona ruta, usar la global\n",
    "    if dataset_path is None:\n",
    "        dataset_path = FULL_NC if 'FULL_NC' in globals() else './data/complete_dataset.nc'\n",
    "    \n",
    "    try:\n",
    "        # Cargar dataset y ajustar normalizers\n",
    "        print(f\"üìä Cargando dataset para normalizaci√≥n desde {dataset_path}\")\n",
    "        ds = xr.open_dataset(dataset_path)\n",
    "        \n",
    "        # Crear e inicializar normalizador\n",
    "        normalizer = DataNormalizer()\n",
    "        normalizer.fit(ds)\n",
    "        \n",
    "        # Aplicar normalizaci√≥n a los dataloaders (requiere c√≥digo espec√≠fico \n",
    "        # que depender√° de la estructura del DataLoader)\n",
    "        # ...\n",
    "        \n",
    "        return train_loader, val_loader, normalizer\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error aplicando normalizaci√≥n: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return train_loader, val_loader, None\n",
    "\n",
    "# Ejemplo de uso\n",
    "def normalize_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Normaliza un dataset seg√∫n las especificaciones del plan experimental\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Ruta al archivo NetCDF del dataset\n",
    "        \n",
    "    Returns:\n",
    "        Dataset normalizado, normalizador ajustado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar dataset\n",
    "        print(f\"üìä Cargando dataset desde {dataset_path}\")\n",
    "        ds = xr.open_dataset(dataset_path)\n",
    "        \n",
    "        # Crear y ajustar normalizador\n",
    "        normalizer = DataNormalizer()\n",
    "        normalizer.fit(ds)\n",
    "        \n",
    "        # Normalizar dataset\n",
    "        ds_normalized = normalizer.transform(ds)\n",
    "        \n",
    "        # Comparar valores antes y despu√©s\n",
    "        for var in ['total_precipitation', 'elevation', 'month_sin']:\n",
    "            if var in ds.data_vars:\n",
    "                orig = ds[var].values\n",
    "                norm = ds_normalized[var].values\n",
    "                \n",
    "                # Mostrar estad√≠sticas\n",
    "                print(f\"\\nüìä Estad√≠sticas para {var}:\")\n",
    "                print(f\"  Original: min={np.nanmin(orig):.4f}, max={np.nanmax(orig):.4f}, \"\n",
    "                      f\"mean={np.nanmean(orig):.4f}, std={np.nanstd(orig):.4f}\")\n",
    "                print(f\"  Normalizado: min={np.nanmin(norm):.4f}, max={np.nanmax(norm):.4f}, \"\n",
    "                      f\"mean={np.nanmean(norm):.4f}, std={np.nanstd(norm):.4f}\")\n",
    "        \n",
    "        return ds_normalized, normalizer\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al normalizar dataset: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Verificar normalizaci√≥n con nuestro dataset\n",
    "if 'FULL_NC' in globals():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üî¢ VERIFICACI√ìN DE NORMALIZACI√ìN DE DATOS\")\n",
    "    print(\"=\"*80)\n",
    "    normalized_ds, normalizer = normalize_dataset(FULL_NC)\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960ca9b",
   "metadata": {},
   "source": [
    "# Visualizaciones Avanzadas de M√©tricas\n",
    "\n",
    "## An√°lisis de Rendimiento por Cluster de Altitud y Comparaci√≥n Temporal\n",
    "\n",
    "Esta secci√≥n implementa las visualizaciones avanzadas seg√∫n el plan experimental:\n",
    "1. **Box-plots por cluster de altitud** (low/mid/high): Muestra c√≥mo var√≠a el error de predicci√≥n seg√∫n la elevaci√≥n\n",
    "2. **Mapas de sesgo medio** comparando folds hist√≥ricos vs recientes: Visualiza cambios en los patrones espaciales de error entre √©pocas hist√≥ricas (F4, F5) y recientes (F1, F2, F3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000aadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ñ∂Ô∏è Implementaci√≥n de visualizaciones box-plots y mapas de sesgo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "# Configuraci√≥n general de visualizaci√≥n\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "class AdvancedMetricsVisualizer:\n",
    "    \"\"\"\n",
    "    Visualizador avanzado de m√©tricas para modelos de precipitaci√≥n\n",
    "    \n",
    "    Implementa las visualizaciones especificadas en el plan experimental:\n",
    "    1. Box-plots por cluster de altitud (low/mid/high)\n",
    "    2. Mapas de sesgo medio comparando folds hist√≥ricos vs recientes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_path=None, model_results_path=None, normalizer=None):\n",
    "        \"\"\"\n",
    "        Inicializa el visualizador\n",
    "        \n",
    "        Args:\n",
    "            dataset_path: Ruta al dataset NetCDF\n",
    "            model_results_path: Ruta a resultados del modelo (opcional)\n",
    "            normalizer: Objeto DataNormalizer para desnormalizar datos\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        self.model_results_path = model_results_path\n",
    "        self.normalizer = normalizer\n",
    "        self.ds = None\n",
    "        self.results = {}\n",
    "        self.cluster_names = {1: 'Low', 2: 'Mid', 3: 'High'}\n",
    "        self.cluster_colors = {1: '#3498db', 2: '#2ecc71', 3: '#e74c3c'}\n",
    "        \n",
    "        # Definir folds hist√≥ricos y recientes\n",
    "        self.historical_folds = ['F4', 'F5']  # 1990, 2000\n",
    "        self.recent_folds = ['F1', 'F2', 'F3']  # 2022, 2023, 2024\n",
    "        \n",
    "        # Cargar dataset si se proporciona ruta\n",
    "        if dataset_path is not None:\n",
    "            self.load_dataset(dataset_path)\n",
    "            \n",
    "        # Cargar resultados de modelos si se proporciona ruta\n",
    "        if model_results_path is not None:\n",
    "            self.load_model_results(model_results_path)\n",
    "    \n",
    "    def load_dataset(self, dataset_path=None):\n",
    "        \"\"\"Carga el dataset NetCDF\"\"\"\n",
    "        if dataset_path is not None:\n",
    "            self.dataset_path = dataset_path\n",
    "            \n",
    "        if self.dataset_path is None:\n",
    "            print(\"‚ùå No se ha especificado ruta del dataset\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            print(f\"üìä Cargando dataset desde {self.dataset_path}\")\n",
    "            self.ds = xr.open_dataset(self.dataset_path)\n",
    "            \n",
    "            # Verificar si tenemos cluster_elevation\n",
    "            if 'cluster_elevation' not in self.ds:\n",
    "                print(\"‚ö†Ô∏è Variable 'cluster_elevation' no encontrada en el dataset\")\n",
    "                # Intentar crear clusters artificiales para demostraci√≥n\n",
    "                self.create_demo_clusters()\n",
    "            else:\n",
    "                print(f\"‚úÖ Dataset cargado correctamente, forma: {self.ds.dims}\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando dataset: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def create_demo_clusters(self):\n",
    "        \"\"\"Crea clusters artificiales para demostraci√≥n si no existen en el dataset\"\"\"\n",
    "        if self.ds is None or 'elevation' not in self.ds:\n",
    "            print(\"‚ùå No se puede crear clusters artificiales (dataset no cargado o sin elevation)\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Obtener elevaci√≥n y crear clusters basados en percentiles\n",
    "            elevation = self.ds['elevation'].values\n",
    "            \n",
    "            # Calcular percentiles para crear 3 clusters (low, mid, high)\n",
    "            low_threshold = np.nanpercentile(elevation, 33)\n",
    "            high_threshold = np.nanpercentile(elevation, 66)\n",
    "            \n",
    "            # Crear array de clusters\n",
    "            clusters = np.zeros_like(elevation, dtype=np.int32)\n",
    "            clusters[elevation < low_threshold] = 1  # Low\n",
    "            clusters[(elevation >= low_threshold) & (elevation < high_threshold)] = 2  # Mid\n",
    "            clusters[elevation >= high_threshold] = 3  # High\n",
    "            \n",
    "            # Agregar al dataset\n",
    "            self.ds['cluster_elevation'] = (('lat', 'lon'), clusters)\n",
    "            \n",
    "            print(\"‚úÖ Clusters artificiales creados basados en percentiles de elevaci√≥n\")\n",
    "            # Mostrar estad√≠sticas\n",
    "            for cluster_id, name in self.cluster_names.items():\n",
    "                mask = (clusters == cluster_id)\n",
    "                if np.any(mask):\n",
    "                    mean_elev = np.nanmean(elevation[mask])\n",
    "                    count = np.sum(mask)\n",
    "                    print(f\"  Cluster {name}: {count} celdas, elevaci√≥n media: {mean_elev:.1f}m\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creando clusters artificiales: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_model_results(self, model_results_path=None):\n",
    "        \"\"\"Carga resultados de modelos guardados\"\"\"\n",
    "        # Esta funci√≥n carga los resultados de CheckpointDir, por ejemplo.\n",
    "        # Por simplificaci√≥n, aqu√≠ solo configuramos algunos resultados de ejemplo\n",
    "        \n",
    "        # Estructura de resultados de ejemplo:\n",
    "        # self.results = {\n",
    "        #     'experiment_name': {\n",
    "        #         'fold': {\n",
    "        #             'predictions': array(...),\n",
    "        #             'actuals': array(...),\n",
    "        #             'rmse': float,\n",
    "        #             'bias': array(...)\n",
    "        #         }\n",
    "        #     }\n",
    "        # }\n",
    "        \n",
    "        try:\n",
    "            # Intentar cargar resultados desde CHECKPOINT_DIR\n",
    "            import pickle\n",
    "            import glob\n",
    "            from pathlib import Path\n",
    "            \n",
    "            if model_results_path is None:\n",
    "                # Usar CHECKPOINT_DIR global si est√° definido\n",
    "                if 'CHECKPOINT_DIR' in globals():\n",
    "                    checkpoint_dir = globals()['CHECKPOINT_DIR']\n",
    "                else:\n",
    "                    checkpoint_dir = Path('./checkpoints')\n",
    "            else:\n",
    "                checkpoint_dir = Path(model_results_path)\n",
    "            \n",
    "            if not checkpoint_dir.exists():\n",
    "                print(f\"‚ö†Ô∏è Directorio de checkpoints no encontrado: {checkpoint_dir}\")\n",
    "                return False\n",
    "            \n",
    "            print(f\"üìÇ Buscando resultados en: {checkpoint_dir}\")\n",
    "            \n",
    "            # Buscar archivos de resultados\n",
    "            result_files = list(checkpoint_dir.glob(\"*_result.pkl\"))\n",
    "            if not result_files:\n",
    "                print(\"‚ö†Ô∏è No se encontraron archivos de resultados\")\n",
    "                return False\n",
    "            \n",
    "            print(f\"‚úÖ Encontrados {len(result_files)} archivos de resultados\")\n",
    "            \n",
    "            # Cargar cada archivo de resultados\n",
    "            self.results = {}\n",
    "            for result_file in result_files:\n",
    "                try:\n",
    "                    # Extraer nombre del experimento y fold del nombre de archivo\n",
    "                    parts = result_file.stem.split('_')\n",
    "                    if len(parts) < 2:\n",
    "                        continue\n",
    "                        \n",
    "                    exp_name = parts[0]\n",
    "                    fold = parts[1]\n",
    "                    \n",
    "                    # Cargar datos del checkpoint\n",
    "                    with open(result_file, 'rb') as f:\n",
    "                        checkpoint_data = pickle.load(f)\n",
    "                    \n",
    "                    # Extraer modelo y m√©tricas\n",
    "                    if len(checkpoint_data) >= 3:\n",
    "                        model, history, rmse = checkpoint_data[:3]\n",
    "                        \n",
    "                        # Inicializar estructura si no existe\n",
    "                        if exp_name not in self.results:\n",
    "                            self.results[exp_name] = {}\n",
    "                        \n",
    "                        # Guardar m√©tricas\n",
    "                        self.results[exp_name][fold] = {\n",
    "                            'rmse': rmse,\n",
    "                            'history': history\n",
    "                        }\n",
    "                        \n",
    "                        print(f\"  ‚úì Cargado {exp_name} - {fold} (RMSE: {rmse:.4f})\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå Error procesando {result_file}: {e}\")\n",
    "            \n",
    "            return len(self.results) > 0\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error general cargando resultados: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False\n",
    "    \n",
    "    def extract_model_predictions(self, exp_name, folds=None):\n",
    "        \"\"\"\n",
    "        Extrae predicciones de modelos desde CHECKPOINT_DIR para visualizaci√≥n\n",
    "        \n",
    "        Args:\n",
    "            exp_name: Nombre del experimento\n",
    "            folds: Lista de folds a incluir (None = todos)\n",
    "            \n",
    "        Returns:\n",
    "            Dict con predicciones por fold\n",
    "        \"\"\"\n",
    "        # Esta funci√≥n es un placeholder - en una implementaci√≥n real, \n",
    "        # ejecutar√≠amos inferencia en los datos de validaci√≥n.\n",
    "        # Para demostraci√≥n, generaremos datos sint√©ticos basados en los RMSEs reales\n",
    "        \n",
    "        import pickle\n",
    "        from pathlib import Path\n",
    "        \n",
    "        predictions = {}\n",
    "        \n",
    "        # Determinar qu√© folds procesar\n",
    "        if folds is None and exp_name in self.results:\n",
    "            folds = list(self.results[exp_name].keys())\n",
    "        elif folds is None:\n",
    "            folds = []\n",
    "        \n",
    "        # Si no tenemos folds para procesar, usar checkpoint_dir\n",
    "        if not folds and 'CHECKPOINT_DIR' in globals():\n",
    "            checkpoint_dir = globals()['CHECKPOINT_DIR']\n",
    "            \n",
    "            # Buscar checkpoints para este experimento\n",
    "            checkpoint_files = list(checkpoint_dir.glob(f\"{exp_name}_*_result.pkl\"))\n",
    "            folds = [f.stem.split('_')[1] for f in checkpoint_files]\n",
    "            \n",
    "        print(f\"üìä Extrayendo predicciones para {exp_name} en folds: {folds}\")\n",
    "        \n",
    "        # Si no hay dataset, cargar uno para demostraci√≥n\n",
    "        if self.ds is None:\n",
    "            if 'FULL_NC' in globals():\n",
    "                self.load_dataset(globals()['FULL_NC'])\n",
    "            else:\n",
    "                print(\"‚ùå No se puede extraer predicciones sin dataset\")\n",
    "                return predictions\n",
    "        \n",
    "        # Obtener dimensiones espaciales\n",
    "        if self.ds is not None and 'lat' in self.ds and 'lon' in self.ds:\n",
    "            lat_dim = len(self.ds.lat) if 'lat' in self.ds.dims else 20\n",
    "            lon_dim = len(self.ds.lon) if 'lon' in self.ds.dims else 20\n",
    "        else:\n",
    "            lat_dim, lon_dim = 20, 20\n",
    "        \n",
    "        # Para cada fold, generar predicciones sint√©ticas\n",
    "        for fold in folds:\n",
    "            # Obtener RMSE para este experimento/fold\n",
    "            rmse = 0.5  # Valor por defecto\n",
    "            if exp_name in self.results and fold in self.results[exp_name]:\n",
    "                rmse = self.results[exp_name][fold]['rmse']\n",
    "            \n",
    "            # Generar datos sint√©ticos\n",
    "            # 1. Ground truth: Valor real (para demostraci√≥n, usar un patr√≥n espacial)\n",
    "            actuals = np.zeros((lat_dim, lon_dim))\n",
    "            \n",
    "            # Patr√≥n de gradiente para actuals\n",
    "            x, y = np.meshgrid(np.linspace(0, 1, lon_dim), np.linspace(0, 1, lat_dim))\n",
    "            # Crear patr√≥n realista (m√°s lluvia en monta√±as)\n",
    "            if 'elevation' in self.ds:\n",
    "                # Usar elevaci√≥n real para el patr√≥n\n",
    "                elevation = self.ds['elevation'].values\n",
    "                # Normalizar a [0,1]\n",
    "                elev_norm = (elevation - np.nanmin(elevation)) / (np.nanmax(elevation) - np.nanmin(elevation))\n",
    "                # Crear patr√≥n basado en elevaci√≥n y latitud\n",
    "                actuals = 2 + 3 * elev_norm + 2 * y\n",
    "                \n",
    "                # A√±adir ruido\n",
    "                noise = np.random.normal(0, 0.5, size=actuals.shape)\n",
    "                actuals += noise\n",
    "                \n",
    "                # Asegurar valores positivos (es precipitaci√≥n)\n",
    "                actuals = np.maximum(0, actuals)\n",
    "            else:\n",
    "                # Patr√≥n geom√©trico simple\n",
    "                actuals = 2 + 3 * np.sin(5 * x) * np.cos(5 * y) + 2 * y\n",
    "            \n",
    "            # 2. Predictions: A√±adir error proporcional al RMSE\n",
    "            # El error es mayor en √°reas de alta monta√±a (clusters altos)\n",
    "            error_scale = np.ones((lat_dim, lon_dim))\n",
    "            \n",
    "            if 'cluster_elevation' in self.ds:\n",
    "                clusters = self.ds['cluster_elevation'].values\n",
    "                # Cluster 1 (bajo): error bajo, Cluster 3 (alto): error alto\n",
    "                cluster_error_scale = {1: 0.7, 2: 1.0, 3: 1.3}\n",
    "                for cluster_id, scale in cluster_error_scale.items():\n",
    "                    mask = (clusters == cluster_id)\n",
    "                    error_scale[mask] = scale\n",
    "            \n",
    "            # Generar error proporcional al RMSE, con componente sistem√°tica (sesgo) y aleatoria\n",
    "            error = np.random.normal(0, rmse, size=actuals.shape)\n",
    "            \n",
    "            # A√±adir sesgo sistem√°tico seg√∫n el fold (folds hist√≥ricos tienen m√°s sesgo positivo)\n",
    "            bias = np.zeros_like(actuals)\n",
    "            if fold in self.historical_folds:\n",
    "                # Sesgo positivo en folds hist√≥ricos (subestima en √°reas altas)\n",
    "                if 'cluster_elevation' in self.ds:\n",
    "                    clusters = self.ds['cluster_elevation'].values\n",
    "                    bias[clusters == 3] = 0.8  # Subestima en monta√±as\n",
    "                    bias[clusters == 2] = 0.4  # Ligera subestima en elevaci√≥n media\n",
    "                    bias[clusters == 1] = 0.1  # Casi insesgado en elevaciones bajas\n",
    "                else:\n",
    "                    bias = 0.4 * y  # Mayor sesgo a mayor latitud (proxy de monta√±as)\n",
    "            else:\n",
    "                # Sesgo menor en folds recientes\n",
    "                if 'cluster_elevation' in self.ds:\n",
    "                    clusters = self.ds['cluster_elevation'].values\n",
    "                    bias[clusters == 3] = 0.3  # Algo de subestima en monta√±as\n",
    "                    bias[clusters == 2] = 0.1  # Casi insesgado en elevaci√≥n media\n",
    "                    bias[clusters == 1] = -0.1  # Ligera sobreestima en elevaciones bajas\n",
    "                else:\n",
    "                    bias = 0.1 * y  # Sesgo mucho menor\n",
    "            \n",
    "            # Aplicar error con componente sistem√°tica y escala variable\n",
    "            predictions[fold] = {\n",
    "                'actuals': actuals,\n",
    "                'predictions': actuals + bias + error * error_scale,\n",
    "                'bias': bias,\n",
    "                'rmse': rmse,\n",
    "                'error': error * error_scale\n",
    "            }\n",
    "            \n",
    "            # Calcular m√©tricas por cluster y a√±adirlas\n",
    "            if 'cluster_elevation' in self.ds:\n",
    "                clusters = self.ds['cluster_elevation'].values\n",
    "                cluster_metrics = {}\n",
    "                \n",
    "                for cluster_id in np.unique(clusters):\n",
    "                    if cluster_id == 0 or np.isnan(cluster_id):  # Ignorar 0 o NaN\n",
    "                        continue\n",
    "                    \n",
    "                    mask = (clusters == cluster_id)\n",
    "                    if not np.any(mask):\n",
    "                        continue\n",
    "                        \n",
    "                    act_cluster = actuals[mask]\n",
    "                    pred_cluster = predictions[fold]['predictions'][mask]\n",
    "                    error_cluster = pred_cluster - act_cluster\n",
    "                    \n",
    "                    cluster_metrics[int(cluster_id)] = {\n",
    "                        'rmse': np.sqrt(np.mean(error_cluster**2)),\n",
    "                        'bias': np.mean(error_cluster),\n",
    "                        'mad': np.mean(np.abs(error_cluster)),\n",
    "                        'actuals_mean': np.mean(act_cluster),\n",
    "                        'predictions_mean': np.mean(pred_cluster)\n",
    "                    }\n",
    "                \n",
    "                predictions[fold]['cluster_metrics'] = cluster_metrics\n",
    "            \n",
    "            print(f\"  ‚úì Generadas predicciones sint√©ticas para {fold}\")\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def plot_cluster_boxplots(self, exp_name, folds=None, normalize=True):\n",
    "        \"\"\"\n",
    "        Crea box-plots por cluster de altitud para visualizar errores\n",
    "        \n",
    "        Args:\n",
    "            exp_name: Nombre del experimento\n",
    "            folds: Lista de folds espec√≠ficos (None = todos)\n",
    "            normalize: Si normalizar los errores por la media del grupo\n",
    "        \"\"\"\n",
    "        # 1. Extraer predicciones y ground truth\n",
    "        predictions = self.extract_model_predictions(exp_name, folds)\n",
    "        \n",
    "        if not predictions:\n",
    "            print(\"‚ùå No hay predicciones disponibles para visualizar\")\n",
    "            return\n",
    "        \n",
    "        if 'cluster_elevation' not in self.ds:\n",
    "            print(\"‚ùå No hay informaci√≥n de clusters de altitud en el dataset\")\n",
    "            return\n",
    "        \n",
    "        # 2. Preparar datos para visualizaci√≥n\n",
    "        # Agrupar errores por cluster y tipo de fold\n",
    "        clusters = self.ds['cluster_elevation'].values\n",
    "        \n",
    "        # Crear figura\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Plot 1: Box-plots de RMSE por cluster y tipo de fold\n",
    "        ax1 = axes[0]\n",
    "        \n",
    "        historical_rmses = {1: [], 2: [], 3: []}  # Por cluster\n",
    "        recent_rmses = {1: [], 2: [], 3: []}      # Por cluster\n",
    "        \n",
    "        # Recopilar RMSEs por cluster y tipo de fold\n",
    "        for fold, fold_data in predictions.items():\n",
    "            if 'cluster_metrics' not in fold_data:\n",
    "                continue\n",
    "                \n",
    "            for cluster_id, metrics in fold_data['cluster_metrics'].items():\n",
    "                if fold in self.historical_folds:\n",
    "                    historical_rmses[cluster_id].append(metrics['rmse'])\n",
    "                else:\n",
    "                    recent_rmses[cluster_id].append(metrics['rmse'])\n",
    "        \n",
    "        # Preparar datos para visualizaci√≥n\n",
    "        cluster_ids = []\n",
    "        rmses = []\n",
    "        fold_types = []\n",
    "        \n",
    "        for cluster_id in [1, 2, 3]:  # Low, Mid, High\n",
    "            # Hist√≥rico\n",
    "            for rmse in historical_rmses[cluster_id]:\n",
    "                cluster_ids.append(self.cluster_names[cluster_id])\n",
    "                rmses.append(rmse)\n",
    "                fold_types.append('Historical')\n",
    "            \n",
    "            # Reciente\n",
    "            for rmse in recent_rmses[cluster_id]:\n",
    "                cluster_ids.append(self.cluster_names[cluster_id])\n",
    "                rmses.append(rmse)\n",
    "                fold_types.append('Recent')\n",
    "        \n",
    "        # Crear DataFrame\n",
    "        df_rmse = pd.DataFrame({\n",
    "            'Cluster': cluster_ids,\n",
    "            'RMSE': rmses,\n",
    "            'Fold Type': fold_types\n",
    "        })\n",
    "        \n",
    "        # Crear box-plot\n",
    "        sns.boxplot(x='Cluster', y='RMSE', hue='Fold Type', data=df_rmse, \n",
    "                    palette={'Historical': '#3498db', 'Recent': '#e74c3c'}, ax=ax1)\n",
    "        \n",
    "        ax1.set_title(f'RMSE por Cluster de Altitud - {exp_name}', fontsize=14)\n",
    "        ax1.set_ylabel('RMSE (mm/d√≠a)', fontsize=12)\n",
    "        ax1.set_xlabel('Cluster de Altitud', fontsize=12)\n",
    "        ax1.legend(title='Tipo de Fold')\n",
    "        ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # A√±adir valores medios en texto\n",
    "        for cluster_id in [1, 2, 3]:\n",
    "            hist_mean = np.mean(historical_rmses[cluster_id]) if historical_rmses[cluster_id] else np.nan\n",
    "            recent_mean = np.mean(recent_rmses[cluster_id]) if recent_rmses[cluster_id] else np.nan\n",
    "            \n",
    "            if not np.isnan(hist_mean) and not np.isnan(recent_mean):\n",
    "                # Test de significancia\n",
    "                _, p_value = ttest_ind(historical_rmses[cluster_id], recent_rmses[cluster_id])\n",
    "                \n",
    "                cluster_name = self.cluster_names[cluster_id]\n",
    "                idx = list(self.cluster_names.values()).index(cluster_name)\n",
    "                y_pos = max(hist_mean, recent_mean) + 0.15\n",
    "                \n",
    "                # Formatear texto seg√∫n significancia\n",
    "                if p_value < 0.05:\n",
    "                    diff_text = f\"Œî={recent_mean-hist_mean:.2f} (p={p_value:.3f})*\"\n",
    "                else:\n",
    "                    diff_text = f\"Œî={recent_mean-hist_mean:.2f} (p={p_value:.3f})\"\n",
    "                    \n",
    "                ax1.annotate(diff_text, xy=(idx, y_pos), ha='center', fontsize=9,\n",
    "                            bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.7))\n",
    "        \n",
    "        # Plot 2: Box-plots de sesgo (bias) por cluster y tipo de fold\n",
    "        ax2 = axes[1]\n",
    "        \n",
    "        historical_bias = {1: [], 2: [], 3: []}  # Por cluster\n",
    "        recent_bias = {1: [], 2: [], 3: []}      # Por cluster\n",
    "        \n",
    "        # Recopilar sesgo por cluster y tipo de fold\n",
    "        for fold, fold_data in predictions.items():\n",
    "            if 'cluster_metrics' not in fold_data:\n",
    "                continue\n",
    "                \n",
    "            for cluster_id, metrics in fold_data['cluster_metrics'].items():\n",
    "                if fold in self.historical_folds:\n",
    "                    historical_bias[cluster_id].append(metrics['bias'])\n",
    "                else:\n",
    "                    recent_bias[cluster_id].append(metrics['bias'])\n",
    "        \n",
    "        # Preparar datos para visualizaci√≥n\n",
    "        cluster_ids = []\n",
    "        biases = []\n",
    "        fold_types = []\n",
    "        \n",
    "        for cluster_id in [1, 2, 3]:  # Low, Mid, High\n",
    "            # Hist√≥rico\n",
    "            for bias in historical_bias[cluster_id]:\n",
    "                cluster_ids.append(self.cluster_names[cluster_id])\n",
    "                biases.append(bias)\n",
    "                fold_types.append('Historical')\n",
    "            \n",
    "            # Reciente\n",
    "            for bias in recent_bias[cluster_id]:\n",
    "                cluster_ids.append(self.cluster_names[cluster_id])\n",
    "                biases.append(bias)\n",
    "                fold_types.append('Recent')\n",
    "        \n",
    "        # Crear DataFrame\n",
    "        df_bias = pd.DataFrame({\n",
    "            'Cluster': cluster_ids,\n",
    "            'Bias': biases,\n",
    "            'Fold Type': fold_types\n",
    "        })\n",
    "        \n",
    "        # Crear box-plot\n",
    "        sns.boxplot(x='Cluster', y='Bias', hue='Fold Type', data=df_bias, \n",
    "                   palette={'Historical': '#3498db', 'Recent': '#e74c3c'}, ax=ax2)\n",
    "        \n",
    "        ax2.set_title(f'Sesgo por Cluster de Altitud - {exp_name}', fontsize=14)\n",
    "        ax2.set_ylabel('Sesgo (mm/d√≠a)', fontsize=12)\n",
    "        ax2.set_xlabel('Cluster de Altitud', fontsize=12)\n",
    "        ax2.legend(title='Tipo de Fold')\n",
    "        ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # A√±adir valores medios en texto\n",
    "        for cluster_id in [1, 2, 3]:\n",
    "            hist_mean = np.mean(historical_bias[cluster_id]) if historical_bias[cluster_id] else np.nan\n",
    "            recent_mean = np.mean(recent_bias[cluster_id]) if recent_bias[cluster_id] else np.nan\n",
    "            \n",
    "            if not np.isnan(hist_mean) and not np.isnan(recent_mean):\n",
    "                # Test de significancia\n",
    "                _, p_value = ttest_ind(historical_bias[cluster_id], recent_bias[cluster_id])\n",
    "                \n",
    "                cluster_name = self.cluster_names[cluster_id]\n",
    "                idx = list(self.cluster_names.values()).index(cluster_name)\n",
    "                y_pos = max(hist_mean, recent_mean) + 0.15\n",
    "                \n",
    "                # Formatear texto seg√∫n significancia\n",
    "                if p_value < 0.05:\n",
    "                    diff_text = f\"Œî={recent_mean-hist_mean:.2f} (p={p_value:.3f})*\"\n",
    "                else:\n",
    "                    diff_text = f\"Œî={recent_mean-hist_mean:.2f} (p={p_value:.3f})\"\n",
    "                    \n",
    "                ax2.annotate(diff_text, xy=(idx, y_pos), ha='center', fontsize=9,\n",
    "                           bbox=dict(boxstyle='round,pad=0.3', fc='white', alpha=0.7))\n",
    "        \n",
    "        # Ajustar layout\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f'An√°lisis de Error por Cluster de Altitud - {exp_name}', fontsize=16, y=1.05)\n",
    "        plt.show()\n",
    "        \n",
    "        # Mostrar conclusiones del an√°lisis\n",
    "        print(\"\\nüìä AN√ÅLISIS DE ERROR POR CLUSTER DE ALTITUD\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Experimento: {exp_name}\")\n",
    "        print(f\"Folds hist√≥ricos: {self.historical_folds}, Folds recientes: {self.recent_folds}\")\n",
    "        \n",
    "        # RMSE promedio por cluster y tipo de fold\n",
    "        print(\"\\nRMSE promedio (mm/d√≠a):\")\n",
    "        print(\"-\"*40)\n",
    "        print(f\"{'Cluster':<10} | {'Hist√≥rico':^10} | {'Reciente':^10} | {'Diferencia':^10} | {'% Cambio':^10}\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        for cluster_id in [1, 2, 3]:\n",
    "            hist_mean = np.mean(historical_rmses[cluster_id]) if historical_rmses[cluster_id] else np.nan\n",
    "            recent_mean = np.mean(recent_rmses[cluster_id]) if recent_rmses[cluster_id] else np.nan\n",
    "            \n",
    "            if not np.isnan(hist_mean) and not np.isnan(recent_mean):\n",
    "                diff = recent_mean - hist_mean\n",
    "                pct_change = 100 * diff / hist_mean\n",
    "                print(f\"{self.cluster_names[cluster_id]:<10} | {hist_mean:^10.3f} | {recent_mean:^10.3f} | {diff:^10.3f} | {pct_change:^10.1f}%\")\n",
    "        \n",
    "        # Sesgo promedio por cluster y tipo de fold\n",
    "        print(\"\\nSesgo promedio (mm/d√≠a):\")\n",
    "        print(\"-\"*40)\n",
    "        print(f\"{'Cluster':<10} | {'Hist√≥rico':^10} | {'Reciente':^10} | {'Diferencia':^10} | {'Mejora':^10}\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        for cluster_id in [1, 2, 3]:\n",
    "            hist_mean = np.mean(historical_bias[cluster_id]) if historical_bias[cluster_id] else np.nan\n",
    "            recent_mean = np.mean(recent_bias[cluster_id]) if recent_bias[cluster_id] else np.nan\n",
    "            \n",
    "            if not np.isnan(hist_mean) and not np.isnan(recent_mean):\n",
    "                diff = recent_mean - hist_mean\n",
    "                # El sesgo mejora si se acerca a cero\n",
    "                bias_reduction = np.abs(hist_mean) - np.abs(recent_mean)\n",
    "                improvement = \"S√≠\" if bias_reduction > 0 else \"No\"\n",
    "                print(f\"{self.cluster_names[cluster_id]:<10} | {hist_mean:^10.3f} | {recent_mean:^10.3f} | {diff:^10.3f} | {improvement:^10}\")\n",
    "    \n",
    "    def plot_bias_maps(self, exp_name, folds=None, mask_insignificant=True):\n",
    "        \"\"\"\n",
    "        Crea mapas de sesgo medio comparando folds hist√≥ricos vs recientes\n",
    "        \n",
    "        Args:\n",
    "            exp_name: Nombre del experimento\n",
    "            folds: Lista de folds espec√≠ficos (None = todos)\n",
    "            mask_insignificant: Si enmascarar √°reas con diferencias no significativas\n",
    "        \"\"\"\n",
    "        # 1. Extraer predicciones y ground truth\n",
    "        predictions = self.extract_model_predictions(exp_name, folds)\n",
    "        \n",
    "        if not predictions:\n",
    "            print(\"‚ùå No hay predicciones disponibles para visualizar\")\n",
    "            return\n",
    "        \n",
    "        # 2. Calcular sesgo medio para folds hist√≥ricos y recientes\n",
    "        historical_bias = []\n",
    "        recent_bias = []\n",
    "        \n",
    "        # Recopilar mapas de sesgo por tipo de fold\n",
    "        for fold, fold_data in predictions.items():\n",
    "            if 'bias' not in fold_data:\n",
    "                continue\n",
    "                \n",
    "            if fold in self.historical_folds:\n",
    "                historical_bias.append(fold_data['bias'])\n",
    "            else:\n",
    "                recent_bias.append(fold_data['bias'])\n",
    "        \n",
    "        # Verificar que tenemos datos para ambos tipos de folds\n",
    "        if not historical_bias or not recent_bias:\n",
    "            print(\"‚ö†Ô∏è No hay suficientes datos para comparar folds hist√≥ricos y recientes\")\n",
    "            if not historical_bias:\n",
    "                print(\"  ‚ùå Faltan datos para folds hist√≥ricos\")\n",
    "            if not recent_bias:\n",
    "                print(\"  ‚ùå Faltan datos para folds recientes\")\n",
    "            return\n",
    "        \n",
    "        # Convertir a arrays y calcular promedio\n",
    "        historical_bias = np.stack(historical_bias)\n",
    "        recent_bias = np.stack(recent_bias)\n",
    "        \n",
    "        historical_mean = np.mean(historical_bias, axis=0)\n",
    "        recent_mean = np.mean(recent_bias, axis=0)\n",
    "        \n",
    "        # Calcular diferencia (cambio en sesgo)\n",
    "        bias_diff = recent_mean - historical_mean\n",
    "        \n",
    "        # Obtener coordenadas para los mapas\n",
    "        if self.ds is not None and 'lat' in self.ds and 'lon' in self.ds:\n",
    "            lats = self.ds.lat.values\n",
    "            lons = self.ds.lon.values\n",
    "        else:\n",
    "            lats = np.linspace(-4.5, 4.5, historical_mean.shape[0])\n",
    "            lons = np.linspace(-80, -70, historical_mean.shape[1])\n",
    "        \n",
    "        # Crear figura\n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        \n",
    "        # Configuraci√≥n com√∫n de proyecci√≥n cartogr√°fica\n",
    "        projection = ccrs.PlateCarree()\n",
    "        \n",
    "        # Plot 1: Sesgo medio en folds hist√≥ricos\n",
    "        ax1 = fig.add_subplot(221, projection=projection)\n",
    "        \n",
    "        # Crear malla de coordenadas\n",
    "        lon_mesh, lat_mesh = np.meshgrid(lons, lats)\n",
    "        \n",
    "        # Rango de colores para sesgo (divergente)\n",
    "        vmin, vmax = -2, 2\n",
    "        cmap = 'RdBu_r'  # Rojo = subestima, Azul = sobreestima\n",
    "        \n",
    "        # Crear mapa\n",
    "        mappable1 = ax1.pcolormesh(lon_mesh, lat_mesh, historical_mean, \n",
    "                                 cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                                 transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # A√±adir caracter√≠sticas del mapa\n",
    "        ax1.coastlines(resolution='50m')\n",
    "        ax1.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        \n",
    "        # A√±adir grid y etiquetas\n",
    "        gl = ax1.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, alpha=0.5)\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        \n",
    "        # Agregar t√≠tulo\n",
    "        ax1.set_title('Sesgo medio en folds hist√≥ricos (1990, 2000)', fontsize=12)\n",
    "        \n",
    "        # Plot 2: Sesgo medio en folds recientes\n",
    "        ax2 = fig.add_subplot(222, projection=projection)\n",
    "        \n",
    "        # Crear mapa\n",
    "        mappable2 = ax2.pcolormesh(lon_mesh, lat_mesh, recent_mean, \n",
    "                                 cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                                 transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # A√±adir caracter√≠sticas del mapa\n",
    "        ax2.coastlines(resolution='50m')\n",
    "        ax2.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        \n",
    "        # A√±adir grid y etiquetas\n",
    "        gl = ax2.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, alpha=0.5)\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        \n",
    "        # Agregar t√≠tulo\n",
    "        ax2.set_title('Sesgo medio en folds recientes (2022, 2023, 2024)', fontsize=12)\n",
    "        \n",
    "        # A√±adir color bar para los dos primeros mapas\n",
    "        cbar_ax1 = fig.add_axes([0.1, 0.47, 0.8, 0.02])\n",
    "        cbar1 = plt.colorbar(mappable1, cax=cbar_ax1, orientation='horizontal')\n",
    "        cbar1.set_label('Sesgo (mm/d√≠a) [negativo = sobreestima, positivo = subestima]')\n",
    "        \n",
    "        # Plot 3: Diferencia de sesgo (reciente - hist√≥rico)\n",
    "        ax3 = fig.add_subplot(223, projection=projection)\n",
    "        \n",
    "        # Rango de colores para diferencia (divergente centrado en 0)\n",
    "        diff_vmax = max(1.0, np.nanmax(np.abs(bias_diff)))\n",
    "        diff_vmin = -diff_vmax\n",
    "        \n",
    "        # Crear mapa\n",
    "        mappable3 = ax3.pcolormesh(lon_mesh, lat_mesh, bias_diff, \n",
    "                                 cmap='PiYG', vmin=diff_vmin, vmax=diff_vmax,\n",
    "                                 transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # A√±adir caracter√≠sticas del mapa\n",
    "        ax3.coastlines(resolution='50m')\n",
    "        ax3.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        \n",
    "        # A√±adir grid y etiquetas\n",
    "        gl = ax3.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, alpha=0.5)\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        \n",
    "        # Agregar t√≠tulo\n",
    "        ax3.set_title('Cambio en sesgo: reciente - hist√≥rico', fontsize=12)\n",
    "        \n",
    "        # Plot 4: Significancia estad√≠stica del cambio\n",
    "        ax4 = fig.add_subplot(224, projection=projection)\n",
    "        \n",
    "        # Calcular significancia estad√≠stica (p-value) para cada celda\n",
    "        p_values = np.ones_like(bias_diff)  # Por defecto, 1.0 (no significativo)\n",
    "        \n",
    "        # Para cada celda, realizar test t-student entre hist√≥rico y reciente\n",
    "        for i in range(historical_bias.shape[1]):\n",
    "            for j in range(historical_bias.shape[2]):\n",
    "                hist_values = historical_bias[:, i, j]\n",
    "                recent_values = recent_bias[:, i, j]\n",
    "                \n",
    "                if len(hist_values) >= 2 and len(recent_values) >= 2:\n",
    "                    try:\n",
    "                        # Usar Mann-Whitney si tenemos pocos datos (no param√©trico)\n",
    "                        _, p_value = mannwhitneyu(hist_values, recent_values)\n",
    "                        p_values[i, j] = p_value\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # Visualizar significancia: rojo = p<0.01, naranja = p<0.05, amarillo = p<0.10\n",
    "        significance = np.zeros_like(p_values)\n",
    "        significance[p_values < 0.01] = 3  # altamente significativo\n",
    "        significance[np.logical_and(p_values >= 0.01, p_values < 0.05)] = 2  # significativo\n",
    "        significance[np.logical_and(p_values >= 0.05, p_values < 0.10)] = 1  # marginalmente significativo\n",
    "        \n",
    "        # Crear mapa de significancia\n",
    "        cmap_sig = plt.cm.get_cmap('RdYlBu_r', 4)\n",
    "        mappable4 = ax4.pcolormesh(lon_mesh, lat_mesh, significance, \n",
    "                                 cmap=cmap_sig, vmin=0, vmax=3,\n",
    "                                 transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # A√±adir caracter√≠sticas del mapa\n",
    "        ax4.coastlines(resolution='50m')\n",
    "        ax4.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        \n",
    "        # A√±adir grid y etiquetas\n",
    "        gl = ax4.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, alpha=0.5)\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        \n",
    "        # Agregar t√≠tulo\n",
    "        ax4.set_title('Significancia estad√≠stica del cambio', fontsize=12)\n",
    "        \n",
    "        # A√±adir color bar para diferencia y significancia\n",
    "        cbar_ax2 = fig.add_axes([0.1, 0.03, 0.35, 0.02])\n",
    "        cbar2 = plt.colorbar(mappable3, cax=cbar_ax2, orientation='horizontal')\n",
    "        cbar2.set_label('Cambio en sesgo (mm/d√≠a)')\n",
    "        \n",
    "        cbar_ax3 = fig.add_axes([0.55, 0.03, 0.35, 0.02])\n",
    "        cbar3 = plt.colorbar(mappable4, cax=cbar_ax3, orientation='horizontal', ticks=[0.4, 1.2, 2.0, 2.8])\n",
    "        cbar3.set_ticklabels(['No significativo', 'p < 0.10', 'p < 0.05', 'p < 0.01'])\n",
    "        cbar3.set_label('Nivel de significancia')\n",
    "        \n",
    "        # T√≠tulo general\n",
    "        plt.suptitle(f'An√°lisis Espacial de Sesgo - {exp_name}\\n'\n",
    "                     f'Comparaci√≥n entre folds hist√≥ricos ({\", \".join(self.historical_folds)}) '\n",
    "                     f'vs recientes ({\", \".join(self.recent_folds)})',\n",
    "                     fontsize=16, y=0.98)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
    "        plt.show()\n",
    "        \n",
    "        # An√°lisis de significancia global\n",
    "        print(\"\\nüìä AN√ÅLISIS DE CAMBIOS EN SESGO MEDIO\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Experimento: {exp_name}\")\n",
    "        print(f\"Folds hist√≥ricos: {self.historical_folds}, Folds recientes: {self.recent_folds}\")\n",
    "        \n",
    "        # RMSE promedio por cluster y tipo de fold\n",
    "        print(\"\\nRMSE promedio (mm/d√≠a):\")\n",
    "        print(\"-\"*40)\n",
    "        print(f\"{'Cluster':<10} | {'Hist√≥rico':^10} | {'Reciente':^10} | {'Diferencia':^10} | {'% Cambio':^10}\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        for cluster_id in [1, 2, 3]:\n",
    "            hist_mean = np.mean(historical_rmses[cluster_id]) if historical_rmses[cluster_id] else np.nan\n",
    "            recent_mean = np.mean(recent_rmses[cluster_id]) if recent_rmses[cluster_id] else np.nan\n",
    "            \n",
    "            if not np.isnan(hist_mean) and not np.isnan(recent_mean):\n",
    "                diff = recent_mean - hist_mean\n",
    "                pct_change = 100 * diff / hist_mean\n",
    "                print(f\"{self.cluster_names[cluster_id]:<10} | {hist_mean:^10.3f} | {recent_mean:^10.3f} | {diff:^10.3f} | {pct_change:^10.1f}%\")\n",
    "        \n",
    "        # Sesgo promedio por cluster y tipo de fold\n",
    "        print(\"\\nSesgo promedio (mm/d√≠a):\")\n",
    "        print(\"-\"*40)\n",
    "        print(f\"{'Cluster':<10} | {'Hist√≥rico':^10} | {'Reciente':^10} | {'Diferencia':^10} | {'Mejora':^10}\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        for cluster_id in [1, 2, 3]:\n",
    "            hist_mean = np.mean(historical_bias[cluster_id]) if historical_bias[cluster_id] else np.nan\n",
    "            recent_mean = np.mean(recent_bias[cluster_id]) if recent_bias[cluster_id] else np.nan\n",
    "            \n",
    "            if not np.isnan(hist_mean) and not np.isnan(recent_mean):\n",
    "                diff = recent_mean - hist_mean\n",
    "                # El sesgo mejora si se acerca a cero\n",
    "                bias_reduction = np.abs(hist_mean) - np.abs(recent_mean)\n",
    "                improvement = \"S√≠\" if bias_reduction > 0 else \"No\"\n",
    "                print(f\"{self.cluster_names[cluster_id]:<10} | {hist_mean:^10.3f} | {recent_mean:^10.3f} | {diff:^10.3f} | {improvement:^10}\")\n",
    "                    \n",
    "                hist_cluster = np.nanmean(historical_mean[mask])\n",
    "                recent_cluster = np.nanmean(recent_mean[mask])\n",
    "                diff_cluster = np.nanmean(bias_diff[mask])\n",
    "                \n",
    "                # Porcentaje de √°rea con cambio significativo\n",
    "                total_cluster_cells = np.sum(mask)\n",
    "                sig_cluster_cells = np.sum(np.logical_and(mask, p_values < 0.05))\n",
    "                sig_cluster_pct = 100 * sig_cluster_cells / total_cluster_cells\n",
    "                \n",
    "                print(f\"{self.cluster_names[cluster_id]:<10} | {hist_cluster:^10.3f} | {recent_cluster:^10.3f} | {diff_cluster:^10.3f} | {sig_cluster_pct:^10.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
