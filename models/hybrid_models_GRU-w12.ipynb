{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43cdeb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH = /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction\n",
      "\n",
      "ğŸ“Š  Resumen global de NaNs\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "total_precipitation_lag1    :    3,965 / 2,101,450  ( 0.19%)\n",
      "total_precipitation_lag2    :    7,930 / 2,101,450  ( 0.38%)\n",
      "total_precipitation_lag12   :   47,580 / 2,101,450  ( 2.26%)\n",
      "\n",
      "ğŸ•’  Fechas con NaNs por variable\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "total_precipitation_lag1\n",
      "      time  na_cells\n",
      "1981-01-01      3965\n",
      "      time  na_cells\n",
      "1981-01-01      3965\n",
      "   â‡¢  Ãºltima fecha con NaNs: 1981-01\n",
      "\n",
      "total_precipitation_lag2\n",
      "      time  na_cells\n",
      "1981-01-01      3965\n",
      "1981-02-01      3965\n",
      "      time  na_cells\n",
      "1981-01-01      3965\n",
      "1981-02-01      3965\n",
      "   â‡¢  Ãºltima fecha con NaNs: 1981-02\n",
      "\n",
      "total_precipitation_lag12\n",
      "      time  na_cells\n",
      "1981-01-01      3965\n",
      "1981-02-01      3965\n",
      "1981-03-01      3965\n",
      "   â€¦\n",
      "      time  na_cells\n",
      "1981-10-01      3965\n",
      "1981-11-01      3965\n",
      "1981-12-01      3965\n",
      "   â‡¢  Ãºltima fecha con NaNs: 1981-12\n",
      "\n",
      "Primera fecha 100 % libre de NaNs en TODOS los lags: 1982-01\n",
      "ğŸ”  Timestamps antes : 530\n",
      "ğŸ”  Timestamps despuÃ©s: 518\n",
      "ğŸ’¾  Dataset sin 1981 guardado en /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction/data/output/complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc\n",
      "\n",
      "ğŸ“Š  NaNs restantes tras quitar 1981\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "total_precipitation_lag1    : 0 NaNs\n",
      "total_precipitation_lag2    : 0 NaNs\n",
      "total_precipitation_lag12   : 0 NaNs\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, ConvLSTM2D, GRU, Flatten, RepeatVector, Reshape, TimeDistributed,\n",
    "    Dense, MultiHeadAttention, Add, LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "## â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â RutasÂ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "# â–¶ï¸ Path configuration\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
    "        if (p / '.git').exists():\n",
    "            BASE_PATH = p; break\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "print('BASE_PATH =', BASE_PATH)\n",
    "\n",
    "# Dataset paths\n",
    "DATA_DIR = BASE_PATH/'data'/'output'\n",
    "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
    "MODEL_DIR = BASE_PATH/'models'/'output'/'HybridLSTMModels'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
    "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMAGE_DIR = MODEL_DIR/'images'\n",
    "IMAGE_DIR.mkdir(exist_ok=True)\n",
    "FULL_NC = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation.nc'\n",
    "FULL_NC_CLEAN = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "dept_gdf = gpd.read_file(MODEL_INPUT_DIR/'MGN_Departamento.shp')\n",
    "\n",
    "BASE_MODEL_DIR = MODEL_DIR\n",
    "GIF_DIR        = MODEL_DIR / \"gifs\"\n",
    "GIF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Dataset & Shapes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "ds          = xr.open_dataset(FULL_NC)\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "LAG_VARS = ['total_precipitation_lag1',\n",
    "            'total_precipitation_lag2',\n",
    "            'total_precipitation_lag12']\n",
    "\n",
    "# ============================================================\n",
    "print(\"\\nğŸ“Š  Resumen global de NaNs\")\n",
    "print(\"â”€\"*55)\n",
    "for var in LAG_VARS:\n",
    "    arr    = ds[var].values\n",
    "    total  = arr.size\n",
    "    n_nans = int(np.isnan(arr).sum())\n",
    "    print(f\"{var:<28}: {n_nans:>8,} / {total:,}  ({n_nans/total:6.2%})\")\n",
    "\n",
    "# ============================================================\n",
    "print(\"\\nğŸ•’  Fechas con NaNs por variable\")\n",
    "print(\"â”€\"*55)\n",
    "for var in LAG_VARS:\n",
    "    arr         = ds[var].values\n",
    "    nan_per_ts  = np.isnan(arr).reshape(len(ds.time), -1).sum(axis=1)\n",
    "    if nan_per_ts.sum() == 0:\n",
    "        print(f\"{var}: sin NaNs âœ”ï¸\")\n",
    "        continue\n",
    "\n",
    "    df_nan = (pd\n",
    "              .DataFrame({\"time\": pd.to_datetime(ds.time.values),\n",
    "                          \"na_cells\": nan_per_ts})\n",
    "              .query(\"na_cells > 0\"))\n",
    "\n",
    "    # primeras 3 y Ãºltimas 3 fechas con NaNs\n",
    "    head = df_nan.head(3).to_string(index=False)\n",
    "    tail = df_nan.tail(3).to_string(index=False)\n",
    "    last = df_nan[\"time\"].iloc[-1].strftime(\"%Y-%m\")\n",
    "\n",
    "    print(f\"\\n{var}\")\n",
    "    print(head)\n",
    "    if len(df_nan) > 6:\n",
    "        print(\"   â€¦\")\n",
    "    print(tail)\n",
    "    print(f\"   â‡¢  Ãºltima fecha con NaNs: {last}\")\n",
    "\n",
    "# ============================================================\n",
    "# Primera fecha en la que las TRES variables estÃ¡n 100 % limpias\n",
    "# ------------------------------------------------------------\n",
    "def last_nan_index(var: str) -> int:\n",
    "    \"\"\"Ãndice del Ãºltimo timestamp que contiene al menos un NaN en `var`.\"\"\"\n",
    "    nan_per_ts = np.isnan(ds[var].values).reshape(len(ds.time), -1).sum(axis=1)\n",
    "    idxs       = np.where(nan_per_ts > 0)[0]\n",
    "    return idxs[-1] if len(idxs) else -1\n",
    "\n",
    "last_nan_any = max(last_nan_index(v) for v in LAG_VARS)\n",
    "first_clean  = pd.to_datetime(ds.time.values[last_nan_any + 1])\n",
    "\n",
    "print(\"\\nPrimera fecha 100 % libre de NaNs en TODOS los lags:\",\n",
    "      first_clean.strftime(\"%Y-%m\"))\n",
    "\n",
    "ds_clean = ds.sel(time=~(ds['time.year'] == 1981))   # descarta TODO 1981\n",
    "\n",
    "print(\"ğŸ”  Timestamps antes :\", len(ds.time))\n",
    "print(\"ğŸ”  Timestamps despuÃ©s:\", len(ds_clean.time))\n",
    "\n",
    "# 3) Guarda nuevo archivo NetCDF\n",
    "ds_clean.to_netcdf(FULL_NC_CLEAN, mode='w')\n",
    "print(f\"ğŸ’¾  Dataset sin 1981 guardado en {FULL_NC_CLEAN}\")\n",
    "\n",
    "# 4) (-- opcional --)  verifica que ya no queden NaNs en los lags\n",
    "LAG_VARS = ['total_precipitation_lag1',\n",
    "            'total_precipitation_lag2',\n",
    "            'total_precipitation_lag12']\n",
    "\n",
    "print(\"\\nğŸ“Š  NaNs restantes tras quitar 1981\")\n",
    "print(\"â”€\"*50)\n",
    "for var in LAG_VARS:\n",
    "    n_nan = int(np.isnan(ds_clean[var].values).sum())\n",
    "    print(f\"{var:<28}: {n_nan:,} NaNs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, ConvLSTM2D, GRU, Flatten, RepeatVector, Reshape, TimeDistributed,\n",
    "    Dense, MultiHeadAttention, Add, LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "## â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â RutasÂ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "# â–¶ï¸ Path configuration\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
    "        if (p / '.git').exists():\n",
    "            BASE_PATH = p; break\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "print('BASE_PATH =', BASE_PATH)\n",
    "\n",
    "# Dataset paths\n",
    "DATA_DIR = BASE_PATH/'data'/'output'\n",
    "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
    "MODEL_DIR = BASE_PATH/'models'/'output'/'HybridLSTMModels'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
    "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMAGE_DIR = MODEL_DIR/'images'\n",
    "IMAGE_DIR.mkdir(exist_ok=True)\n",
    "FULL_NC = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "dept_gdf = gpd.read_file(MODEL_INPUT_DIR/'MGN_Departamento.shp')\n",
    "\n",
    "BASE_MODEL_DIR = MODEL_DIR\n",
    "GIF_DIR        = MODEL_DIR / \"gifs\"\n",
    "GIF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Dataset & Shapes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "ds          = xr.open_dataset(FULL_NC)\n",
    "lat, lon    = len(ds.latitude), len(ds.longitude)\n",
    "cells       = lat * lon\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hyperâ€‘parÃ¡metros globales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "INPUT_WINDOW   = 60\n",
    "HORIZON        = 3\n",
    "TARGET_VAR     = 'total_precipitation'\n",
    "EPOCHS         = 12\n",
    "BATCH_SIZE     = 4           # tamaÃ±o pequeÃ±oÂ â†’Â menor RAMÂ GPU\n",
    "PATIENCE       = 10\n",
    "LR             = 1e-3\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â Modelo base ConvLSTMÂ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "\n",
    "def _build_convlstm_ed(*,input_window: int,output_horizon: int,spatial_height: int,spatial_width: int,n_features: int,n_filters: int = 64,n_heads: int = 4,use_attention: bool = True,lr: float = LR) -> Model:\n",
    "    \"\"\"Construye un Encoderâ€‘Decoder ConvLSTM.\n",
    "\n",
    "    Si `use_attention=False` se omite la capa Multiâ€‘HeadÂ Attention.\n",
    "    La salida es `(B,Â T_out,Â H,Â W,Â 1)`.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(input_window, spatial_height, spatial_width, n_features), name=\"enc_input\")\n",
    "\n",
    "    # â”€â”€ Encoder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    x = ConvLSTM2D(n_filters,   (3, 3), padding='same', return_sequences=True,  name=\"enc_lstm_1\")(inputs)\n",
    "    x = ConvLSTM2D(n_filters//2,(3, 3), padding='same', return_sequences=False, name=\"enc_lstm_2\")(x)\n",
    "\n",
    "    # â”€â”€ Flatten + contexto temporal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    flat = Flatten(name=\"flatten_spatial\")(x)\n",
    "    ctx  = RepeatVector(output_horizon, name=\"context\")(flat)  # (B, T_out, HÂ·WÂ·C)\n",
    "\n",
    "    # â”€â”€ Decoder GRU (temporal) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    dec = GRU(2*n_filters, return_sequences=True, name=\"dec_gru\")(ctx)\n",
    "\n",
    "    if use_attention:\n",
    "        attn = MultiHeadAttention(num_heads=n_heads, key_dim=n_filters, dropout=0.1, name=\"mha\")(dec, dec)\n",
    "        dec  = LayerNormalization(name=\"mha_norm\")(Add(name=\"mha_add\")([dec, attn]))\n",
    "\n",
    "    # â”€â”€ ProyecciÃ³n + reshape a grid â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    proj = TimeDistributed(Dense(spatial_height*spatial_width, activation='linear'), name=\"dense_proj\")(dec)\n",
    "    out  = Reshape((output_horizon, spatial_height, spatial_width, 1), name=\"reshape_out\")(proj)\n",
    "\n",
    "    model = Model(inputs, out, name=\"ConvLSTM_ED_Attn\" if use_attention else \"ConvLSTM_ED\")\n",
    "    model.compile(optimizer=Adam(lr), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Factories ---------------------------------------------------\n",
    "\n",
    "def factory_no_attn(**kw):\n",
    "    return _build_convlstm_ed(use_attention=False, **kw)\n",
    "\n",
    "def factory_attn(**kw):\n",
    "    return _build_convlstm_ed(use_attention=True, **kw)\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â MÃ©tricasÂ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "\n",
    "def evaluate(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-5))) * 100\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    return rmse, mae, mape, r2\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Quickâ€‘plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "\n",
    "def quick_plot(ax, data, cmap, title, date_label, vmin=None, vmax=None):\n",
    "    mesh = ax.pcolormesh(ds.longitude, ds.latitude, data, cmap=cmap, shading='nearest', vmin=vmin, vmax=vmax, transform=ccrs.PlateCarree())\n",
    "    ax.coastlines(); ax.add_geometries(dept_gdf.geometry, ccrs.PlateCarree(), edgecolor='black', facecolor='none', linewidth=1)\n",
    "    gl = ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
    "    ax.set_title(f\"{title}\\n{date_label}\", pad=12)\n",
    "    return mesh\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Experiments &Â Folds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "# â–¸ Solo mostramos los tres primeros niveles; aÃ±ade los demÃ¡s igual\n",
    "BASE_FEATURES = [\n",
    "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
    "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
    "    'elevation','slope','aspect'\n",
    "]\n",
    "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
    "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
    "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
    "\n",
    "FOLDS = {'F1': {'year': 2018,'active': True}}\n",
    "\n",
    "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
    "    'ConvLSTM-ED': {\n",
    "        'active': True,\n",
    "        'feature_list': BASE_FEATURES,\n",
    "        'builder': factory_attn, #factory_no_attn,\n",
    "        'n_filters': 64,\n",
    "        'n_heads'  : 4\n",
    "    },\n",
    "    'ConvLSTM-ED-KCE': {\n",
    "        'active': True,\n",
    "        'feature_list': KCE_FEATURES,\n",
    "        'builder': factory_attn,\n",
    "        'n_filters': 64,\n",
    "        'n_heads'  : 4,\n",
    "    },\n",
    "    'ConvLSTM-ED-KCE-PAFC': {\n",
    "        'active': True,\n",
    "        'feature_list': PAFC_FEATURES,\n",
    "        'builder': factory_attn,\n",
    "        'n_filters': 96,\n",
    "        'n_heads'  : 6,\n",
    "    },\n",
    "}\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ventanas deslizadas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "\n",
    "def make_windows(mask:np.ndarray, allow_past_context:bool)->tuple[np.ndarray,np.ndarray]:\n",
    "    \"\"\"Genera ventanas **descartando** las que contienen NaNs.  # ğŸ”¸ NEW\"\"\"\n",
    "    seq_X, seq_y = [], []\n",
    "    lim = len(mask) - INPUT_WINDOW - HORIZON + 1\n",
    "    for start in range(lim):\n",
    "        end_w = start + INPUT_WINDOW; end_y = end_w + HORIZON\n",
    "        if allow_past_context:\n",
    "            if not mask[end_w:end_y].all():\n",
    "                continue\n",
    "        else:\n",
    "            if not mask[start:end_y].all():\n",
    "                continue\n",
    "        Xw = Xarr[start:end_w]; yw = yarr[end_w:end_y]\n",
    "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
    "            continue  # ğŸ”¸ NEWÂ â€” descarta ventana con NaNs\n",
    "        seq_X.append(Xw); seq_y.append(yw)\n",
    "    return np.array(seq_X), np.array(seq_y)\n",
    "\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Bucle principal de entrenamiento â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "RESULTS: List[Dict[str, Any]] = []\n",
    "\n",
    "# ğŸ”¸ NEW helper ------------------------------------------------\n",
    "\n",
    "def _impute_nans(a:np.ndarray, per_feature_mean:np.ndarray|None=None, is_target:bool=False)->np.ndarray:\n",
    "    \"\"\"Imputa NaNs restantes (seguridad extra).\"\"\"\n",
    "    if not np.isnan(a).any():\n",
    "        return a\n",
    "    if is_target:\n",
    "        a[np.isnan(a)] = 0.0  # ğŸ”¸ NEW â€“Â 0 para y\n",
    "        return a\n",
    "    if per_feature_mean is None:\n",
    "        raise ValueError('per_feature_mean required for imputing X')\n",
    "    flat = a.reshape(-1, a.shape[-1])\n",
    "    nan_idx = np.isnan(flat)\n",
    "    for f in range(a.shape[-1]):\n",
    "        flat[nan_idx[:,f], f] = per_feature_mean[f]  # ğŸ”¸ NEW\n",
    "    return flat.reshape(a.shape)\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "def run_all_experiments():\n",
    "    times = pd.to_datetime(ds.time.values)\n",
    "    total = sum(e['active'] for e in EXPERIMENTS.values()) * sum(f['active'] for f in FOLDS.values())\n",
    "    cnt   = 0\n",
    "\n",
    "    for exp_name, exp_cfg in EXPERIMENTS.items():\n",
    "        if not exp_cfg['active']:\n",
    "            continue\n",
    "        vars_     = exp_cfg['feature_list']\n",
    "        builder   = exp_cfg['builder']      # fÃ¡brica especÃ­fica\n",
    "        n_filters = exp_cfg.get('n_filters',64)\n",
    "        n_heads   = exp_cfg.get('n_heads',4)\n",
    "\n",
    "        # â”€ Preâ€‘load features por experimento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        global Xarr, yarr\n",
    "        Xarr = ds[vars_].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
    "        yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
    "        feats = Xarr.shape[-1]\n",
    "\n",
    "        for fold_name, fold_cfg in FOLDS.items():\n",
    "            if not fold_cfg['active']:\n",
    "                continue\n",
    "            cnt += 1\n",
    "            year_val = fold_cfg['year']\n",
    "            print(f\"\\nâ–¶ï¸  [{cnt}/{total}] {exp_name} â€“ {fold_name} (val={year_val})\")\n",
    "\n",
    "            mask_val = times.year == year_val\n",
    "            mask_tr  = ~mask_val\n",
    "            if mask_val.sum() < HORIZON:\n",
    "                print(\"âš ï¸Â AÃ±o sin pasos suficientes â†’Â skip\"); continue\n",
    "\n",
    "            X_tr, y_tr = make_windows(mask_tr,  allow_past_context=False)\n",
    "            X_va, y_va = make_windows(mask_val, allow_past_context=True)\n",
    "            print(f\"VentanasÂ train: {len(X_tr)} Â· val: {len(X_va)}\")\n",
    "            if len(X_tr)==0 or len(X_va)==0:\n",
    "                print(\"âš ï¸Â Sin ventanas vÃ¡lidas â†’Â skip\"); continue\n",
    "\n",
    "            # ğŸ”¸ NEWÂ â€” ImputaciÃ³n de seguridad\n",
    "            feat_mean = np.nanmean(X_tr.reshape(-1,feats),axis=0)\n",
    "            X_tr = _impute_nans(X_tr,feat_mean); X_va=_impute_nans(X_va,feat_mean)\n",
    "            y_tr = _impute_nans(y_tr,is_target=True); y_va=_impute_nans(y_va,is_target=True)\n",
    "            \n",
    "            # â”€ Scaling (fit solo en train) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            sx = StandardScaler().fit(X_tr.reshape(-1, feats))\n",
    "            sy = StandardScaler().fit(y_tr.reshape(-1, 1))\n",
    "            X_tr_sc = sx.transform(X_tr.reshape(-1, feats)).reshape(X_tr.shape)\n",
    "            X_va_sc = sx.transform(X_va.reshape(-1, feats)).reshape(X_va.shape)\n",
    "            y_tr_sc = sy.transform(y_tr.reshape(-1, 1)).reshape(y_tr.shape)[..., None]\n",
    "            y_va_sc = sy.transform(y_va.reshape(-1, 1)).reshape(y_va.shape)[..., None]\n",
    "\n",
    "            # â”€ Build & train model (factory) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            tag        = f\"{exp_name.replace('+','_')}_{fold_name}\"\n",
    "            model_path = BASE_MODEL_DIR / f\"{tag}.keras\"\n",
    "            if model_path.exists():\n",
    "                print(f\"â©Â {tag} ya existe â†’Â skip\"); continue\n",
    "\n",
    "            model = builder(\n",
    "                input_window=INPUT_WINDOW,\n",
    "                output_horizon=HORIZON,\n",
    "                spatial_height=lat,\n",
    "                spatial_width=lon,\n",
    "                n_features=feats,\n",
    "                n_filters=n_filters,\n",
    "                n_heads=n_heads,\n",
    "                lr=LR\n",
    "            )\n",
    "\n",
    "            es   = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "            hist = model.fit(X_tr_sc, y_tr_sc, validation_data=(X_va_sc, y_va_sc), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=1)\n",
    "\n",
    "            # â”€ EvaluaciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            y_hat_sc = model.predict(X_va_sc, verbose=0)\n",
    "            y_hat    = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(y_hat_sc.shape)\n",
    "            y_true   = sy.inverse_transform(y_va_sc.reshape(-1,1)).reshape(y_va_sc.shape)\n",
    "\n",
    "            rmse, mae, mape, r2 = evaluate(y_true.ravel(), y_hat.ravel())\n",
    "            RESULTS.append(dict(experiment=exp_name, fold=fold_name, RMSE=rmse, MAE=mae, MAPE=mape, R2=r2, epochs=len(hist.history['loss'])))\n",
    "\n",
    "            # â”€ Guardado artefactos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            model.save(model_path)\n",
    "            plt.figure(); plt.plot(hist.history['loss'], label='train'); plt.plot(hist.history['val_loss'], label='val'); plt.legend(); plt.title(tag); plt.savefig(IMAGE_DIR/f\"{tag}.png\"); plt.close()\n",
    "\n",
    "            _generate_gif(y_true[0], y_hat[0], tag)\n",
    "            print(f\"âœ…Â Guardado {model_path.name}\")\n",
    "\n",
    "    # â”€ MÃ©tricas globales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df = pd.DataFrame(RESULTS)\n",
    "    out_csv = BASE_MODEL_DIR / \"metrics_experiments_folds.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"\\nğŸ“‘Â Tabla de mÃ©tricas en {out_csv}\")\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Generador de GIF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "\n",
    "def _generate_gif(y_true_sample, y_pred_sample, tag):\n",
    "    pcm_min, pcm_max = 0, np.max(y_pred_sample)\n",
    "    frames = []\n",
    "    for h in range(HORIZON):\n",
    "        pmap = y_pred_sample[h, ..., 0]\n",
    "        fig, ax = plt.subplots(1,1, figsize=(6,5), subplot_kw={'projection':ccrs.PlateCarree()})\n",
    "        mesh = ax.pcolormesh(ds.longitude, ds.latitude, pmap, cmap='Blues', shading='nearest', vmin=pcm_min, vmax=pcm_max, transform=ccrs.PlateCarree())\n",
    "        ax.coastlines(); ax.gridlines(draw_labels=True)\n",
    "        ax.set_title(f\"{tag} â€“ H{h+1}\")\n",
    "        fig.colorbar(mesh, ax=ax, fraction=0.046, pad=0.04)\n",
    "        tmp = GIF_DIR/f\"tmp_{tag}_h{h}.png\"\n",
    "        fig.savefig(tmp, bbox_inches='tight'); plt.close(fig)\n",
    "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
    "    gif_path = GIF_DIR/f\"{tag}.gif\"\n",
    "    imageio.mimsave(gif_path, frames, fps=0.5)\n",
    "    print(f\"ğŸ’¾Â GIF {gif_path.name} listo\")\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Â Bucle principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "run_all_experiments()\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4de7eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH = /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction\n",
      "\n",
      "ğŸ” Evaluando ConvLSTM-ED-KCE-PAFC_F1 â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Â· h=1: todos los valores son NaN/Inf â†’ skip\n",
      "   Â· h=2: todos los valores son NaN/Inf â†’ skip\n",
      "   Â· h=3: todos los valores son NaN/Inf â†’ skip\n",
      "ğŸ’¾ GIF ConvLSTM-ED-KCE-PAFC_F1.gif creado\n",
      "\n",
      "ğŸ” Evaluando ConvLSTM-ED-KCE_F1 â€¦\n",
      "ğŸ’¾ GIF ConvLSTM-ED-KCE_F1.gif creado\n",
      "\n",
      "ğŸ” Evaluando ConvLSTM-ED_F1 â€¦\n",
      "ğŸ’¾ GIF ConvLSTM-ED_F1.gif creado\n",
      "ğŸ“‘Â MÃ©tricas guardadas en /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction/models/output/HybridLSTMModels/metrics_eval.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ğŸ“ˆ **Evaluador para salidas espaciales ConvLSTM**\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, xarray as xr, tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt, geopandas as gpd, imageio.v2 as imageio\n",
    "import sys\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Paths & Constantes â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# â–¶ï¸ Path configuration\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
    "        if (p / '.git').exists():\n",
    "            BASE_PATH = p; break\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "print('BASE_PATH =', BASE_PATH)\n",
    "\n",
    "# Dataset paths\n",
    "DATA_DIR = BASE_PATH/'data'/'output'\n",
    "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
    "MODEL_DIR = BASE_PATH/'models'/'output'/'HybridLSTMModels'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
    "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMAGE_DIR = MODEL_DIR/'images'\n",
    "IMAGE_DIR.mkdir(exist_ok=True)\n",
    "FULL_NC = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "departamentos = gpd.read_file(MODEL_INPUT_DIR/'MGN_Departamento.shp')\n",
    "\n",
    "BASE_MODEL_DIR = MODEL_DIR\n",
    "GIF_DIR        = MODEL_DIR / \"gifs\"\n",
    "GIF_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hyperâ€‘parÃ¡metros globales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "INPUT_WINDOW   = 60\n",
    "HORIZON        = 3\n",
    "TARGET_VAR     = 'total_precipitation'\n",
    "EPOCHS         = 12\n",
    "BATCH_SIZE     = 4           # tamaÃ±o pequeÃ±oÂ â†’Â menor RAMÂ GPU\n",
    "PATIENCE       = 10\n",
    "LR             = 1e-3\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Dataset &Â shapes â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ds = xr.open_dataset(FULL_NC); lat,lon=len(ds.latitude),len(ds.longitude)\n",
    "\n",
    "#â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Experiments &Â Folds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "\n",
    "# Factories ---------------------------------------------------\n",
    "\n",
    "def factory_no_attn(**kw):\n",
    "    return _build_convlstm_ed(use_attention=False, **kw)\n",
    "\n",
    "def factory_attn(**kw):\n",
    "    return _build_convlstm_ed(use_attention=True, **kw)\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "# â–¸ Solo mostramos los tres primeros niveles; aÃ±ade los demÃ¡s igual\n",
    "BASE_FEATURES = [\n",
    "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
    "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
    "    'elevation','slope','aspect'\n",
    "]\n",
    "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
    "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
    "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
    "\n",
    "FOLDS = {'F1': {'year': 2018,'active': True}}\n",
    "\n",
    "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
    "    'ConvLSTM-ED': {\n",
    "        'active': True,\n",
    "        'feature_list': BASE_FEATURES,\n",
    "        'builder': factory_attn, #factory_no_attn,\n",
    "        'n_filters': 64,\n",
    "        'n_heads'  : 4\n",
    "    },\n",
    "    'ConvLSTM-ED-KCE': {\n",
    "        'active': True,\n",
    "        'feature_list': KCE_FEATURES,\n",
    "        'builder': factory_attn,\n",
    "        'n_filters': 64,\n",
    "        'n_heads'  : 4,\n",
    "    },\n",
    "    'ConvLSTM-ED-KCE-PAFC': {\n",
    "        'active': True,\n",
    "        'feature_list': PAFC_FEATURES,\n",
    "        'builder': factory_attn,\n",
    "        'n_filters': 96,\n",
    "        'n_heads'  : 6,\n",
    "    },\n",
    "}\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "\n",
    "def quick_plot(ax,data,cmap,title,date_label,vmin=None,vmax=None):\n",
    "    mesh=ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
    "    ax.coastlines(); ax.add_geometries(departamentos.geometry,ccrs.PlateCarree(),edgecolor='black',facecolor='none',linewidth=1)\n",
    "    gl=ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
    "    ax.set_title(f\"{title}\\n{date_label}\",pad=10); return mesh\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Recuperamos diccionario EXPERIMENTS (del bloque de entrenamiento) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from typing import Dict\n",
    "EXPERIMENTS:Dict[str,Dict[str,Any]] = {\n",
    "    'ConvLSTM-ED':              {'feature_list': \"+\".join(BASE_FEATURES).split(\"+\")},\n",
    "    'ConvLSTM-ED-KCE':          {'feature_list': \"+\".join(KCE_FEATURES).split(\"+\")},\n",
    "    'ConvLSTM-ED-KCE-PAFC':     {'feature_list': \"+\".join(PAFC_FEATURES).split(\"+\")},\n",
    "    #Â otros experimentos\n",
    "}\n",
    "\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” EvaluaciÃ³n â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "all_metrics=[]; times=pd.to_datetime(ds.time.values)\n",
    "for mpath in sorted(BASE_MODEL_DIR.glob(\"*.keras\")):\n",
    "    tag   = mpath.stem                        # p.ej. ConvLSTM-ED_F1\n",
    "    parts = tag.split(\"_\")\n",
    "    fold  = parts[-1]                         # F1\n",
    "    exp_token = \"_\".join(parts[:-1])\n",
    "    exp_name  = exp_token.replace(\"_\",\"+\")  # vuelve al nombre original con +\n",
    "    if exp_name not in EXPERIMENTS:\n",
    "        print(\"âš ï¸Â Exp no encontrado para\",tag); continue\n",
    "    feats = EXPERIMENTS[exp_name]['feature_list']\n",
    "    print(f\"\\nğŸ” Evaluando {tag} â€¦\")\n",
    "\n",
    "    # â€” ExtracciÃ³n de arrays â€”\n",
    "    Xarr = ds[feats].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
    "    yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
    "    T,_,_,F = Xarr.shape\n",
    "    Xfull = Xarr; yfull=yarr  # mantenemos (T,H,W,F)\n",
    "\n",
    "    # ventana final (idÃ©ntica lÃ³gica del cuaderno original)\n",
    "    start=T-INPUT_WINDOW-HORIZON; end_w=start+INPUT_WINDOW; end_y=end_w+HORIZON\n",
    "    X_eval = Xfull[start:end_w]                 # (60,H,W,F)\n",
    "    y_eval = yfull[end_w:end_y]                 # (3,H,W)\n",
    "\n",
    "    # â€” Scalers (fit incremental) â€”\n",
    "    sx,sy = StandardScaler(),StandardScaler()\n",
    "    for t in range(T):\n",
    "        sx.partial_fit(Xfull[t].reshape(-1,F))\n",
    "        sy.partial_fit(yfull[t].reshape(-1,1))\n",
    "    Xe_sc = sx.transform(X_eval.reshape(-1,F)).reshape(1,INPUT_WINDOW,lat,lon,F)\n",
    "    ye_sc = sy.transform(y_eval.reshape(-1,1)).reshape(1,HORIZON,lat,lon,1)\n",
    "\n",
    "    # â€” Carga modelo y predicciÃ³n â€”\n",
    "    model=tf.keras.models.load_model(mpath,compile=False)\n",
    "    yhat_sc=model.predict(Xe_sc,verbose=0)      # (1,3,H,W,1)\n",
    "    yhat   = sy.inverse_transform(yhat_sc.reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
    "    ytrue  = y_eval\n",
    "\n",
    "    # â€” MÃ©tricas por horizonte â€”\n",
    "    for h in range(HORIZON):\n",
    "        yt = ytrue[h].ravel()\n",
    "        yp = yhat[h].ravel()\n",
    "\n",
    "        # ---------- filtro NaN / Â±âˆ ----------\n",
    "        mask = np.isfinite(yt) & np.isfinite(yp)\n",
    "        if mask.sum() == 0:          # ventana vacÃ­a â†’ se ignora\n",
    "            print(f\"   Â· h={h+1}: todos los valores son NaN/Inf â†’ skip\")\n",
    "            continue\n",
    "        yt, yp = yt[mask], yp[mask]\n",
    "        # -------------------------------------\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(yt, yp))\n",
    "        mae  = mean_absolute_error(yt, yp)\n",
    "        mape = np.mean(np.abs((yt - yp) / (yt + 1e-5))) * 100\n",
    "        r2   = r2_score(yt, yp)\n",
    "\n",
    "        all_metrics.append(dict(\n",
    "            model      = tag,\n",
    "            experiment = exp_name,\n",
    "            fold       = fold,\n",
    "            horizon    = h + 1,\n",
    "            RMSE       = rmse,\n",
    "            MAE        = mae,\n",
    "            MAPE       = mape,\n",
    "            R2         = r2\n",
    "        ))\n",
    "\n",
    "    # â€” Figura Real vsÂ Pred vsÂ MAPE â€”\n",
    "    fig,axes=plt.subplots(HORIZON,3,figsize=(14,4*HORIZON),subplot_kw={'projection':ccrs.PlateCarree()})\n",
    "    dates=pd.date_range(times[end_w],periods=HORIZON,freq='MS')\n",
    "    vmin=0; vmax=max(yhat.max(),ytrue.max())\n",
    "    for h in range(HORIZON):\n",
    "        quick_plot(axes[h,0],ytrue[h],'Blues',f\"Real h={h+1}\",dates[h].strftime('%Y-%m'),vmin,vmax)\n",
    "        quick_plot(axes[h,1],yhat [h],'Blues',f\"Pred h={h+1}\",dates[h].strftime('%Y-%m'),vmin,vmax)\n",
    "        err=np.clip(np.abs((ytrue[h]-yhat[h])/(ytrue[h]+1e-5))*100,0,100)\n",
    "        quick_plot(axes[h,2],err,'Reds',f\"MAPE% h={h+1}\",dates[h].strftime('%Y-%m'),0,100)\n",
    "    fig.suptitle(f\"{tag}  â€”Â Eval final ventana\",fontsize=16); fig.tight_layout();\n",
    "    fig.savefig(BASE_MODEL_DIR/f\"fig_{tag}.png\"); plt.close(fig)\n",
    "\n",
    "    # â€” GIF â€”\n",
    "    frames=[]; pcm_min,pcm_max=0,yhat.max()\n",
    "    for h in range(HORIZON):\n",
    "        figg,ax=plt.subplots(1,1,figsize=(6,5),subplot_kw={'projection':ccrs.PlateCarree()})\n",
    "        m=ax.pcolormesh(ds.longitude,ds.latitude,yhat[h],cmap='Blues',shading='nearest',vmin=pcm_min,vmax=pcm_max,transform=ccrs.PlateCarree())\n",
    "        ax.coastlines(); ax.set_title(f\"{tag} â€“ H{h+1}\"); figg.colorbar(m,ax=ax,fraction=0.046,pad=0.04)\n",
    "        tmp=GIF_DIR/f\"tmp_{tag}_{h}.png\"; figg.savefig(tmp,bbox_inches='tight'); plt.close(figg)\n",
    "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
    "    imageio.mimsave(GIF_DIR/f\"{tag}.gif\",frames,fps=0.5)\n",
    "    print(\"ğŸ’¾ GIF\",f\"{tag}.gif\",\"creado\")\n",
    "\n",
    "# â€”â€”â€” Guardar tabla â€”â€”â€”\n",
    "pd.DataFrame(all_metrics).to_csv(BASE_MODEL_DIR/'metrics_eval.csv',index=False)\n",
    "print(\"ğŸ“‘Â MÃ©tricas guardadas en\",BASE_MODEL_DIR/'metrics_eval.csv')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
