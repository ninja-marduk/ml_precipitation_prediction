{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/base_models_Conv_STHyMOUNTAIN_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb66731",
   "metadata": {
    "id": "3fb66731"
   },
   "source": [
    "# Version Control\n",
    "## V2.8 - 2025-12-10 10:30\n",
    "- ConvGRU2D: helper embebido en el notebook (nativo -> TFA -> custom); modelos ConvGRU usan build_convgru2d_layer y tests se apoyan en el mismo helper (sin import externo).\n",
    "- Nueva celda MAP EXPORTS: reusa pesos guardados, reconstruye ventanas de validacion, guarda predictions/targets/metadata y genera PNG 800 dpi (Real | Pred | MAPE) + GIF por modelo/horizonte en models/output/V2_Enhanced_Models/map_exports/H{H}/{exp}/{model}/.\n",
    "- Permite filtrar horizontes/experimentos/modelos; indice de muestra configurable; exportes persistentes para recalcular metricas.\n",
    "## V2.7 - 2025-11-06 07:05\n",
    "- Predicciones/evaluacion: ahora sobre todo el set de validacion; reshape con H y clipping post-inverse (y_hat = max(y_hat, 0)).\n",
    "- Opcional: CSV por ventana de validacion (activable con CONFIG['save_window_metrics']).\n",
    "## V2.6 - 2025-11-03 16:53\n",
    "- batch_predict robusto: minibatches con validacion de forma, manejo de lote final y squeeze opcional.\n",
    "## V2.5 - 2025-11-03 16:46\n",
    "- Celda TRAINING HELPERS: se definen compute_horizon_weights, validate_shapes, save_hyperparameters, TrainingMonitor, print_training_summary y plot_learning_curves (idempotentes).\n",
    "## V2.4 - 2025-11-03 16:40\n",
    "- Prediccion: reshape por horizonte (usa H del bucle en train/val).\n",
    "- MODELS: filtrado automatico de modelos ConvGRU* si ConvGRU2D no existe en esta version de TF/Keras.\n",
    "- UNIT TESTS (Enhanced): pruebas de perdidas, pesos, batch_predict y shapes; SKIP de ConvGRU si capa ausente.\n",
    "- Entorno: TF_CPP_MIN_LOG_LEVEL=2 y logger TF en ERROR.\n",
    "## V2.3 - 2025-11-02 16:03\n",
    "- Multi-horizonte estable: preprocesado UNA vez por H y uso consistente de horizon en modelos, validacion y guardados.\n",
    "- validate_shapes usa expected_horizon para train y val (formas (H, lat, lon, 1)).\n",
    "- train_model acepta horizon; logs/checkpoints/CSV con sufijo h{H}; resultados guardados via out_csv (f-string).\n",
    "- ConvGRU2DCell robusto: state_size/output_size en __init__, get_initial_state agregado y limpieza de lineas duplicadas.\n",
    "- TensorFlow logs suprimidos (TF_CPP_MIN_LOG_LEVEL=2 y logger de TF en ERROR).\n",
    "- Atencion meteorologica: Flatten + MHA + reshape espacial.\n",
    "- Visualizacion: listado de GIFs y LATEST PREDICTIONS sin mojibake.\n",
    "## V2.2 - 2025-11-02 14:10\n",
    "- Estructura multi-horizonte: enabled_horizons = [3,6,12].\n",
    "- Preprocesado por horizonte (windowing usa H dinamico).\n",
    "- Entrenamiento por H con compute_horizon_weights y rutas h{H}.\n",
    "- Resultados incluyen columna TotalHorizon; graficos y logs por H.\n",
    "- Unit tests recorren todos los horizontes habilitados.\n",
    "- ConvGRU2DCell fija state_size/output_size; limpieza de textos (ASCII).\n",
    "## V2.1 - 2025-11-02 13:55\n",
    "- oneDNN desactivado antes de importar TF.\n",
    "- Perdidas vectorizadas (MultiHorizon/TemporalConsistency/Combined).\n",
    "- _spatial_head: ultimo timestep sin condicional simbolico.\n",
    "- Atencion meteorologica con flatten + reshape.\n",
    "- Preprocesado sin fuga y dos escaladores.\n",
    "- CSVLogger(str(...)) y uso de history_dict.\n",
    "- Limpieza de textos/labels (R^2, titulos, TRAINING SUMMARY).\n",
    "- Tests: finitud en CombinedLoss y ConvGRU state_size/output_size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d24cb",
   "metadata": {
    "id": "187d24cb"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/refactored_base_models_Conv_STHyMOUNTAIN_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea17dc4",
   "metadata": {
    "id": "4ea17dc4"
   },
   "source": [
    "# Spatio-Temporal Precipitation Prediction Models V2 (Refactored)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This refactored notebook implements and compares spatio-temporal deep learning architectures for monthly precipitation prediction in mountainous regions, using CHIRPS-2.0 data. It preserves all original functionality while improving structure, robustness, and maintainability.\n",
    "\n",
    "## Model Architectures\n",
    "\n",
    "### Base Models\n",
    "- **ConvRNN**: Convolutional RNN for spatial-temporal processing\n",
    "- **ConvLSTM**: Convolutional LSTM with memory cells\n",
    "- **ConvGRU**: Convolutional GRU for efficient processing\n",
    "\n",
    "### Enhanced Models\n",
    "- **Multi-horizon training**: Optimized for 1, 2, and 3-month predictions\n",
    "- **Temporal consistency**: Regularization to prevent abrupt changes\n",
    "- **Bidirectional processing**: Forward and backward temporal analysis\n",
    "\n",
    "### Advanced Models\n",
    "- **Attention mechanisms**: Temporal and meteorological attention\n",
    "- **Residual learning**: Skip connections for gradient flow\n",
    "- **Efficient architectures**: Parameter-optimized variants\n",
    "\n",
    "## Dataset\n",
    "- **Source**: CHIRPS-2.0 precipitation data\n",
    "- **Region**: Boyaca, Colombia (mountainous terrain)\n",
    "- **Features**: Precipitation, elevation, seasonal patterns\n",
    "- **Temporal range**: 60-month input window, 3-month prediction horizon\n",
    "\n",
    "## Improvements in Refactored Version\n",
    "- Split long cells into modular, atomic units for better debugging\n",
    "- Centralized configuration with dict for constants and paths\n",
    "- Enhanced error handling (e.g., raise on missing features/files)\n",
    "- Added reproducibility (random seeds, pinned versions in Colab)\n",
    "- Removed reliance on globals with function parameters\n",
    "- Improved custom layers (vectorized ConvGRU2D using RNN wrapper)\n",
    "- Added comprehensive unit tests for layers, losses, models\n",
    "- Preserved all original functionality (models, losses, training, evals, viz)\n",
    "- Added TensorBoard callback for better monitoring\n",
    "- Ensured non-negative predictions in losses\n",
    "\n",
    "## Execution Order\n",
    "1. Environment setup and imports\n",
    "2. Configuration and constants\n",
    "3. Data loading and validation\n",
    "4. Custom layers and losses\n",
    "5. Model factories\n",
    "6. Data preprocessing\n",
    "7. Model training\n",
    "8. Unit tests\n",
    "9. Visualization and comparison\n",
    "10. Cleanup and termination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32b29bcf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32b29bcf",
    "outputId": "eff2f7ad-7cba-4205-c164-fda056729d58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cartopy or GeoPandas not available. Maps will use basic plotting.\n",
      "No GPU detected - running on CPU\n",
      "Environment setup complete\n",
      "TensorFlow version: 2.20.0\n",
      "NumPy version: 2.2.6\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# ENVIRONMENT SETUP AND IMPORTS\n",
    "# ==================================================\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Disable oneDNN before importing TensorFlow\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)  # Ensure reproducibility\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "import sys, os, gc, warnings\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output, display, Image\n",
    "\n",
    "# TensorFlow/Keras imports\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, ConvLSTM2D, SimpleRNN, Flatten, Dense, Reshape,\n",
    "    Lambda, Permute, Layer, TimeDistributed, Multiply, GlobalAveragePooling1D,\n",
    "    Dropout, BatchNormalization, Add, Concatenate, Average,\n",
    "    GlobalAveragePooling2D, MultiHeadAttention, LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint, TensorBoard, Callback\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Visualization imports\n",
    "CARTOPY_AVAILABLE = False\n",
    "ccrs = None\n",
    "gpd = None\n",
    "fiona = None\n",
    "imageio = None\n",
    "\n",
    "try:\n",
    "    import cartopy.crs as ccrs\n",
    "    import geopandas as gpd\n",
    "    import fiona\n",
    "    import imageio.v2 as imageio\n",
    "    CARTOPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Cartopy or GeoPandas not available. Maps will use basic plotting.\")\n",
    "    CARTOPY_AVAILABLE = False\n",
    "    ccrs = None\n",
    "    gpd = None\n",
    "    fiona = None\n",
    "    imageio = None\n",
    "\n",
    "# Configure GPU memory growth\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU memory growth configured for {len(gpus)} GPU(s)\")\n",
    "    else:\n",
    "        print(\"No GPU detected - running on CPU\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"GPU configuration warning: {e}\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context('notebook')\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Install dependencies in Colab with pinned versions\n",
    "if IN_COLAB:\n",
    "    print(\"Installing dependencies in Colab...\")\n",
    "    try:\n",
    "        !apt-get -qq update\n",
    "        !apt-get -qq install libproj-dev proj-data proj-bin libgeos-dev\n",
    "        !pip install -q --upgrade pip==24.0\n",
    "        !pip install -q numpy==1.24.3 pandas==2.0.3 xarray==2023.6.0 netCDF4==1.6.5\n",
    "        !pip install -q matplotlib==3.7.2 seaborn==0.12.2\n",
    "        !pip install -q scikit-learn==1.3.0\n",
    "        !pip install -q geopandas==0.13.2\n",
    "        !pip install -q --no-binary cartopy cartopy==0.22.0\n",
    "        !pip install -q imageio==2.31.1\n",
    "        print(\"Dependencies installed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error installing dependencies: {e}\")\n",
    "\n",
    "print(\"Environment setup complete\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d61e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "457d61e8",
    "outputId": "609140d2-27b8-448f-bb65-a22f9809a780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration complete\n",
      "Base path: d:\\github.com\\ninja-marduk\\ml_precipitation_prediction\n",
      "Input window: 60 months\n",
      "Prediction horizon: 6 months\n",
      "Experiments: ['BASIC', 'KCE', 'PAFC']\n",
      "Output directory: d:\\github.com\\ninja-marduk\\ml_precipitation_prediction\\models\\output\\V2_Enhanced_Models\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# CONFIGURATION AND CONSTANTS\n",
    "# ==================================================\n",
    "\n",
    "# Centralized configuration\n",
    "CONFIG = {\n",
    "    'input_window': 60,\n",
    "    'horizon': 12,\n",
    "    'epochs': 150,\n",
    "    'batch_size': 2, #8,\n",
    "    'learning_rate': 1e-3,\n",
    "    'patience': 50, #80,\n",
    "    'light_mode': False, #True,\n",
    "    'light_grid_size': 5,\n",
    "    'base_path': None,\n",
    "    'data_file': None,\n",
    "    'out_root': True,\n",
    "    'enabled_horizons': [12], # [3, 6, 12],\n",
    "    'loss_weighting': 'uniform',\n",
    "    'save_window_metrics': False,\n",
    "    'train_val_split': 0.8,\n",
    "    'effective_batch_size': None,\n",
    "    'prediction_batch_size': 1,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'feature_sets': {\n",
    "        'BASIC': ['year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "                  'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "                  'elevation', 'slope', 'aspect'],\n",
    "        'KCE': ['year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "                  'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "                  'elevation', 'slope', 'aspect', 'elev_high','elev_med','elev_low'],\n",
    "        'PAFC': ['year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "                  'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "                  'elevation', 'slope', 'aspect', 'elev_high','elev_med','elev_low',\n",
    "                  'total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set paths\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    CONFIG['base_path'] = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "else:\n",
    "    CONFIG['base_path'] = Path.cwd()\n",
    "    for p in [CONFIG['base_path'], *CONFIG['base_path'].parents]:\n",
    "        if (p / '.git').exists():\n",
    "            CONFIG['base_path'] = p\n",
    "            break\n",
    "\n",
    "CONFIG['data_file'] = CONFIG['base_path'] / 'data' / 'output' / (\n",
    "    'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    ")\n",
    "CONFIG['out_root'] = CONFIG['base_path'] / 'models' / 'output' / 'V2_Enhanced_Models'\n",
    "CONFIG['out_root'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define feature sets\n",
    "CONFIG['feature_sets']['KCE'] = CONFIG['feature_sets']['BASIC'] + ['elev_high', 'elev_med', 'elev_low']\n",
    "CONFIG['feature_sets']['PAFC'] = CONFIG['feature_sets']['KCE'] + [\n",
    "    'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag12'\n",
    "]\n",
    "\n",
    "BASE_CONTINUOUS_FEATURES = ['year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "    'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "    'elevation', 'slope', 'aspect']\n",
    "ELEVATION_ONEHOT_FEATURES = ['elev_high', 'elev_med', 'elev_low']\n",
    "PRECIP_LAG_FEATURES = ['total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag12']\n",
    "\n",
    "print(\"Configuration complete\")\n",
    "print(f\"Base path: {CONFIG['base_path']}\")\n",
    "print(f\"Input window: {CONFIG['input_window']} months\")\n",
    "print(f\"Prediction horizon: {CONFIG['horizon']} months\")\n",
    "print(f\"Experiments: {list(CONFIG['feature_sets'].keys())}\")\n",
    "print(f\"Output directory: {CONFIG['out_root']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8dadbd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8dadbd0",
    "outputId": "44453066-dd3c-491c-dbc2-ccfd9f3a5305"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "*** LIGHT MODE ENABLED: Using 5x5 pixel subset ***\n",
      "Subset extracted: lat[28:33], lon[30:35]\n",
      "Dataset loaded successfully\n",
      "Time steps: 518\n",
      "Spatial dimensions: 5 x 5\n",
      "Variables: ['total_precipitation', 'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std', 'month_sin']...\n",
      "All required features present in dataset\n",
      "Dataset ready for training\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# DATA LOADING AND VALIDATION\n",
    "# ==================================================\n",
    "\n",
    "def load_and_validate_data(config: Dict) -> Tuple[xr.Dataset, int, int]:\n",
    "    \"\"\"Load dataset and validate features, applying light mode if enabled.\"\"\"\n",
    "    data_file = config['data_file']\n",
    "    light_mode = config['light_mode']\n",
    "    light_grid_size = config['light_grid_size']\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    if not data_file.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found: {data_file}\")\n",
    "\n",
    "    try:\n",
    "        ds = xr.open_dataset(data_file)\n",
    "\n",
    "        if light_mode:\n",
    "            print(f\"\\n*** LIGHT MODE ENABLED: Using {light_grid_size}x{light_grid_size} pixel subset ***\")\n",
    "            lat_center = len(ds.latitude) // 2\n",
    "            lon_center = len(ds.longitude) // 2\n",
    "            lat_start = lat_center - light_grid_size // 2\n",
    "            lat_end = lat_start + light_grid_size\n",
    "            lon_start = lon_center - light_grid_size // 2\n",
    "            lon_end = lon_start + light_grid_size\n",
    "            ds = ds.isel(latitude=slice(lat_start, lat_end), longitude=slice(lon_start, lon_end))\n",
    "            print(f\"Subset extracted: lat[{lat_start}:{lat_end}], lon[{lon_start}:{lon_end}]\")\n",
    "\n",
    "        lat, lon = len(ds.latitude), len(ds.longitude)\n",
    "        print(f\"Dataset loaded successfully\")\n",
    "        print(f\"Time steps: {len(ds.time)}\")\n",
    "        print(f\"Spatial dimensions: {lat} x {lon}\")\n",
    "        print(f\"Variables: {list(ds.data_vars)[:5]}...\")\n",
    "\n",
    "        # Validate features\n",
    "        missing_feats = []\n",
    "        for exp_name, feats in config['feature_sets'].items():\n",
    "            for feat in feats:\n",
    "                if feat not in ds.data_vars and feat not in ds.coords:\n",
    "                    missing_feats.append(feat)\n",
    "        if missing_feats:\n",
    "            raise ValueError(f\"Missing features in dataset: {set(missing_feats)}\")\n",
    "        print(\"All required features present in dataset\")\n",
    "\n",
    "        return ds, lat, lon\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load dataset\n",
    "ds, lat, lon = load_and_validate_data(CONFIG)\n",
    "print(\"Dataset ready for training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09fbfe27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09fbfe27",
    "outputId": "4c70eded-8d1d-491f-b05a-0be3dd8dfd18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom layers and losses defined\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# CUSTOM LAYERS AND LOSSES\n",
    "# ==================================================\n",
    "\n",
    "class ConvGRU2DCell(Layer):\n",
    "    \"\"\"Robust ConvGRU2D cell implementation.\"\"\"\n",
    "    def __init__(self, filters, kernel_size, padding='same', activation='tanh',\n",
    "                 recurrent_activation='sigmoid', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "                # Provide state_size/output_size upfront for RNN wrapper\n",
    "        self.state_size = tf.TensorShape([None, None, self.filters])\n",
    "        self.output_size = tf.TensorShape([None, None, self.filters])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Set state_size as a tuple for RNN compatibility\n",
    "        height, width = input_shape[1], input_shape[2]\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(*self.kernel_size, input_dim, self.filters * 3),\n",
    "            initializer='glorot_uniform', name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(*self.kernel_size, self.filters, self.filters * 3),\n",
    "            initializer='orthogonal', name='recurrent_kernel')\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.filters * 3,), initializer='zeros', name='bias')\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        h_tm1 = states[0]\n",
    "        x_conv = K.conv2d(inputs, self.kernel, padding=self.padding)\n",
    "        x_z, x_r, x_h = tf.split(x_conv, 3, axis=-1)\n",
    "        h_conv = K.conv2d(h_tm1, self.recurrent_kernel, padding=self.padding)\n",
    "        h_z, h_r, h_h = tf.split(h_conv, 3, axis=-1)\n",
    "        b_z, b_r, b_h = tf.split(self.bias, 3)\n",
    "        z = self.recurrent_activation(x_z + h_z + b_z)\n",
    "        r = self.recurrent_activation(x_r + h_r + b_r)\n",
    "        h_candidate = self.activation(x_h + r * h_h + b_h)\n",
    "        h = (1 - z) * h_tm1 + z * h_candidate\n",
    "        return h, [h]\n",
    "\n",
    "\n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        # Provide zero initial state with dynamic spatial dims\n",
    "        if inputs is not None:\n",
    "            dtype = dtype or inputs.dtype\n",
    "            shape = tf.shape(inputs)\n",
    "            b = shape[0] if batch_size is None else batch_size\n",
    "            h = shape[-3]\n",
    "            w = shape[-2]\n",
    "            init = tf.zeros((b, h, w, self.filters), dtype=dtype)\n",
    "            return [init]\n",
    "        # Fallback: minimal spatial dims if inputs not provided\n",
    "        if batch_size is None:\n",
    "            batch_size = 1\n",
    "        init = tf.zeros((batch_size, 1, 1, self.filters), dtype=dtype or tf.keras.backend.floatx())\n",
    "        return [init]\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters, 'kernel_size': self.kernel_size, 'padding': self.padding,\n",
    "            'activation': tf.keras.activations.serialize(self.activation),\n",
    "            'recurrent_activation': tf.keras.activations.serialize(self.recurrent_activation)\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], input_shape[2], self.filters)\n",
    "\n",
    "class ConvGRU2D(Layer):\n",
    "    \"\"\"Full ConvGRU2D with vectorized RNN processing.\"\"\"\n",
    "    def __init__(self, filters, kernel_size, padding='same', activation='tanh',\n",
    "                 recurrent_activation='sigmoid', return_sequences=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.return_sequences = return_sequences\n",
    "        self.cell = ConvGRU2DCell(filters, kernel_size, padding, activation, recurrent_activation)\n",
    "        self.rnn = tf.keras.layers.RNN(self.cell, return_sequences=return_sequences)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.rnn(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters, 'kernel_size': self.kernel_size, 'padding': self.padding,\n",
    "            'activation': self.activation, 'recurrent_activation': self.recurrent_activation,\n",
    "            'return_sequences': self.return_sequences\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MultiHorizonLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Weighted loss for multi-horizon forecasting (vectorized).\"\"\"\n",
    "    def __init__(self, horizon_weights=[0.4, 0.35, 0.25], name='multi_horizon_loss'):\n",
    "        super().__init__(name=name)\n",
    "        self.horizon_weights = tf.constant(horizon_weights, dtype=tf.float32)\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.maximum(y_pred, 0.0)\n",
    "        mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=[2, 3, 4])\n",
    "        weights = tf.reshape(self.horizon_weights, [1, -1])\n",
    "        return tf.reduce_mean(tf.reduce_sum(mse * weights, axis=1))\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'horizon_weights': self.horizon_weights.numpy().tolist()})\n",
    "        return config\n",
    "\n",
    "class TemporalConsistencyLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Regularization to enforce temporal smoothness.\"\"\"\n",
    "    def __init__(self, mse_weight=1.0, consistency_weight=0.1, name='temporal_consistency_loss'):\n",
    "        super().__init__(name=name)\n",
    "        self.mse_weight = mse_weight\n",
    "        self.consistency_weight = consistency_weight\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.maximum(y_pred, 0.0)\n",
    "        mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "        temporal_diffs = tf.abs(y_pred[:, 1:, :, :, :] - y_pred[:, :-1, :, :, :])\n",
    "        consistency_loss = tf.reduce_mean(temporal_diffs)\n",
    "        return self.mse_weight * mse_loss + self.consistency_weight * consistency_loss\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'mse_weight': self.mse_weight, 'consistency_weight': self.consistency_weight})\n",
    "        return config\n",
    "\n",
    "class CombinedLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Combines multi-horizon MSE with temporal consistency (vectorized).\"\"\"\n",
    "    def __init__(self, horizon_weights=[0.4, 0.35, 0.25], consistency_weight=0.1, name='combined_loss'):\n",
    "        super().__init__(name=name)\n",
    "        self.horizon_weights = tf.constant(horizon_weights, dtype=tf.float32)\n",
    "        self.consistency_weight = consistency_weight\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.maximum(y_pred, 0.0)\n",
    "        mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=[2, 3, 4])\n",
    "        weights = tf.reshape(self.horizon_weights, [1, -1])\n",
    "        mh_loss = tf.reduce_mean(tf.reduce_sum(mse * weights, axis=1))\n",
    "        temporal_diffs = tf.abs(y_pred[:, 1:, :, :, :] - y_pred[:, :-1, :, :, :])\n",
    "        tc_loss = tf.reduce_mean(temporal_diffs)\n",
    "        return mh_loss + self.consistency_weight * tc_loss\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'horizon_weights': self.horizon_weights.numpy().tolist(), 'consistency_weight': self.consistency_weight})\n",
    "        return config\n",
    "\n",
    "class SimpleTemporalAttention(Layer):\n",
    "    \"\"\"Temporal attention for sequence processing.\"\"\"\n",
    "    def __init__(self, units=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attention_dense = Dense(1, activation='tanh')\n",
    "        self.softmax = tf.keras.layers.Softmax(axis=1)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_scores = self.attention_dense(inputs)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        context = tf.reduce_sum(inputs * attention_weights, axis=1)\n",
    "        return context, attention_weights\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config\n",
    "\n",
    "class SpatialReshapeLayer(Layer):\n",
    "    \"\"\"Reshape for attention: (batch, time, height, width, channels) -> (batch, time, height*width*channels).\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], input_shape[2] * input_shape[3] * input_shape[4])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size, time_steps, height, width, channels = tf.unstack(tf.shape(inputs))\n",
    "        return tf.reshape(inputs, [batch_size, time_steps, height * width * channels])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "class SpatialRestoreLayer(Layer):\n",
    "    \"\"\"Restore spatial dimensions after attention.\"\"\"\n",
    "    def __init__(self, height, width, channels, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.channels = channels\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], self.height, self.width, self.channels)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size, time_steps, _ = tf.unstack(tf.shape(inputs))\n",
    "        return tf.reshape(inputs, [batch_size, time_steps, self.height, self.width, self.channels])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'height': self.height, 'width': self.width, 'channels': self.channels})\n",
    "        return config\n",
    "\n",
    "print(\"Custom layers and losses defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bd2744f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bd2744f",
    "outputId": "504dbb90-fd1e-4541-c1f0-9ec60fa560ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvGRU2D not available; ConvGRU models skipped\n",
      "Model registry initialized with 10 models across all categories\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# MODEL FACTORIES\n",
    "# ==================================================\n",
    "# ConvGRU2D helper embebido (sin import al modulo); intenta nativo, luego TFA y finalmente la implementacion custom\n",
    "from typing import Tuple\n",
    "\n",
    "try:\n",
    "    from tensorflow.keras.layers import ConvGRU2D as _NativeConvGRU2D  # type: ignore\n",
    "except Exception:\n",
    "    _NativeConvGRU2D = None\n",
    "\n",
    "def _get_from_tfa():\n",
    "    try:\n",
    "        import tensorflow_addons as tfa  # type: ignore\n",
    "        return tfa.layers.ConvGRU2D\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_convgru2d_layer(\n",
    "    filters: int,\n",
    "    kernel_size: Tuple[int, int] = (3, 3),\n",
    "    return_sequences: bool = True,\n",
    "    padding: str = \"same\",\n",
    "    name: str = \"conv_gru2d\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a ConvGRU2D layer if available.\n",
    "\n",
    "    Tries native tf.keras first; if missing, tries tensorflow-addons.\n",
    "    Falls back to the custom ConvGRU2D defined in the previous cell.\n",
    "    Raises an ImportError with guidance when none are present.\n",
    "    \"\"\"\n",
    "    layer_cls = _NativeConvGRU2D or _get_from_tfa()\n",
    "    if layer_cls is not None:\n",
    "        return layer_cls(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            return_sequences=return_sequences,\n",
    "            padding=padding,\n",
    "            name=name,\n",
    "            **kwargs,\n",
    "        )\n",
    "    try:\n",
    "        return ConvGRU2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            return_sequences=return_sequences,\n",
    "            name=name,\n",
    "            **kwargs,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"ConvGRU2D is not available in this TensorFlow/Keras build. \"\n",
    "            \"Install TF >=2.12 or add tensorflow-addons.\"\n",
    "        ) from e\n",
    "\n",
    "try:\n",
    "    _probe = build_convgru2d_layer(filters=4, kernel_size=(1, 1), return_sequences=True, name=\"conv_gru2d_probe\")\n",
    "    HAS_CONVGRU2D = _probe is not None\n",
    "except Exception:\n",
    "    HAS_CONVGRU2D = False\n",
    "\n",
    "def _spatial_head(x, horizon: int, lat: int, lon: int) -> tf.Tensor:\n",
    "    if len(x.shape) == 5:\n",
    "        x = Lambda(lambda t: tf.squeeze(t, axis=1) if tf.shape(t)[1] == 1 else t[:, -1, :, :, :],\n",
    "                   name=\"take_last_step\")(x)\n",
    "    x = Conv2D(horizon, (1, 1), padding=\"same\", activation=\"linear\", name=\"head_conv1x1\")(x)\n",
    "    x = Lambda(lambda t: tf.transpose(t, [0, 3, 1, 2]), name=\"head_transpose\")(x)\n",
    "    x = Lambda(lambda t: tf.expand_dims(t, -1), name=\"head_expand_dim\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _assert_model_output(model: Model, horizon: int, lat: int, lon: int):\n",
    "    expected = (None, horizon, lat, lon, 1)\n",
    "    actual = model.output_shape\n",
    "    if actual != expected:\n",
    "        raise ValueError(f'{model.name} output_shape {actual} != expected {expected}')\n",
    "\n",
    "\n",
    "def build_conv_lstm(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"Build ConvLSTM-based model.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True)(inp)\n",
    "    x = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=False)(x)\n",
    "    out = _spatial_head(x, horizon, lat, lon)\n",
    "    model = Model(inp, out, name='ConvLSTM')\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_conv_gru(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"Build ConvGRU-based model.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    x = build_convgru2d_layer(filters=32, kernel_size=(3, 3), padding=\"same\", return_sequences=True, name=\"conv_gru2d_1\")(inp)\n",
    "    x = build_convgru2d_layer(filters=16, kernel_size=(3, 3), padding=\"same\", return_sequences=False, name=\"conv_gru2d_2\")(x)\n",
    "    out = _spatial_head(x, horizon, lat, lon)\n",
    "    model = Model(inp, out, name=\"ConvGRU\")\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_conv_rnn(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"Build ConvRNN-based model.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(inp)\n",
    "    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(x)\n",
    "    x = TimeDistributed(Flatten())(x)\n",
    "    x = SimpleRNN(128, activation='tanh', return_sequences=False)(x)\n",
    "    x = Dense(horizon * lat * lon)(x)\n",
    "    out = Reshape((horizon, lat, lon, 1))(x)\n",
    "    model = Model(inp, out, name='ConvRNN')\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_conv_lstm_enhanced(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"Enhanced ConvLSTM with dropout and batch norm.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = _spatial_head(x, horizon, lat, lon)\n",
    "    model = Model(inp, out, name='ConvLSTM_Enhanced')\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_conv_gru_enhanced(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"Enhanced ConvGRU with dropout and batch norm.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    x = build_convgru2d_layer(filters=32, kernel_size=(3, 3), padding=\"same\", return_sequences=True, name=\"conv_gru2d_enh_1\")(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = build_convgru2d_layer(filters=16, kernel_size=(3, 3), padding=\"same\", return_sequences=False, name=\"conv_gru2d_enh_2\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    out = _spatial_head(x, horizon, lat, lon)\n",
    "    model = Model(inp, out, name=\"ConvGRU_Enhanced\")\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_conv_rnn_enhanced(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"Enhanced ConvRNN with dropout.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = TimeDistributed(Flatten())(x)\n",
    "    x = SimpleRNN(128, activation='tanh', return_sequences=False)(x)\n",
    "    x = Dense(horizon * lat * lon)(x)\n",
    "    out = Reshape((horizon, lat, lon, 1))(x)\n",
    "    model = Model(inp, out, name='ConvRNN_Enhanced')\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_conv_lstm_bidirectional(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"Bidirectional ConvLSTM.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    forward = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True)(inp)\n",
    "    backward = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True, go_backwards=True)(inp)\n",
    "    x = Concatenate()([forward, backward])\n",
    "    x = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=False)(x)\n",
    "    out = _spatial_head(x, horizon, lat, lon)\n",
    "    model = Model(inp, out, name='ConvLSTM_Bidirectional')\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_conv_gru_residual(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"ConvGRU with residual connections.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    x = build_convgru2d_layer(filters=32, kernel_size=(3, 3), padding=\"same\", return_sequences=True, name=\"conv_gru2d_res_1\")(inp)\n",
    "    residual = x\n",
    "    x = build_convgru2d_layer(filters=32, kernel_size=(3, 3), padding=\"same\", return_sequences=True, name=\"conv_gru2d_res_2\")(x)\n",
    "    x = Add()([x, residual])\n",
    "    x = build_convgru2d_layer(filters=16, kernel_size=(3, 3), padding=\"same\", return_sequences=False, name=\"conv_gru2d_res_3\")(x)\n",
    "    out = _spatial_head(x, horizon, lat, lon)\n",
    "    model = Model(inp, out, name=\"ConvGRU_Residual\")\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_conv_lstm_residual(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"ConvLSTM with residual connections.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True)(inp)\n",
    "    residual = x\n",
    "    x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True)(x)\n",
    "    x = Add()([x, residual])\n",
    "    x = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=False)(x)\n",
    "    out = _spatial_head(x, horizon, lat, lon)\n",
    "    model = Model(inp, out, name='ConvLSTM_Residual')\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_conv_lstm_attention_simple(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"ConvLSTM with temporal attention.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True)(inp)\n",
    "    reshape = SpatialReshapeLayer()(x)\n",
    "    attention = SimpleTemporalAttention()(reshape)\n",
    "    x = SpatialRestoreLayer(lat, lon, 32)(attention[0][:, tf.newaxis, :])\n",
    "    x = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=False)(x)\n",
    "    out = _spatial_head(x, horizon, lat, lon)\n",
    "    model = Model(inp, out, name='ConvLSTM_Attention')\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_conv_gru_attention_simple(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"ConvGRU with temporal attention.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    x = build_convgru2d_layer(filters=32, kernel_size=(3, 3), padding='same', return_sequences=True, name=\"conv_gru2d_attn_1\")(inp)\n",
    "    reshape = SpatialReshapeLayer()(x)\n",
    "    attention = SimpleTemporalAttention()(reshape)\n",
    "    x = SpatialRestoreLayer(lat, lon, 32)(attention[0][:, tf.newaxis, :])\n",
    "    x = build_convgru2d_layer(filters=16, kernel_size=(3, 3), padding='same', return_sequences=False, name=\"conv_gru2d_attn_2\")(x)\n",
    "    out = _spatial_head(x, horizon, lat, lon)\n",
    "    model = Model(inp, out, name='ConvGRU_Attention')\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_conv_lstm_meteorological_attention_simple(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"ConvLSTM with meteorological attention (temporal over flattened spatial features).\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True)(inp)\n",
    "    x_flat = TimeDistributed(Flatten())(x)\n",
    "    x_attn = MultiHeadAttention(num_heads=4, key_dim=64, output_shape=lat*lon*32)(x_flat, x_flat)\n",
    "    x = Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], tf.shape(t)[1], lat, lon, 32]), name='mha_restore_spatial')(x_attn)\n",
    "    x = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=False)(x)\n",
    "    out = _spatial_head(x, horizon, lat, lon)\n",
    "    model = Model(inp, out, name='ConvLSTM_MeteoAttention')\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_efficient_bidirectional_convlstm_simple(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"Efficient bidirectional ConvLSTM.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    forward = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=True)(inp)\n",
    "    backward = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=True, go_backwards=True)(inp)\n",
    "    x = Concatenate()([forward, backward])\n",
    "    x = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=False)(x)\n",
    "    out = _spatial_head(x, horizon, lat, lon)\n",
    "    model = Model(inp, out, name='ConvLSTM_EfficientBidir')\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "def build_transformer_baseline_simple(n_feats: int, lat: int, lon: int, horizon: int) -> Model:\n",
    "    \"\"\"Baseline Transformer model.\"\"\"\n",
    "    inp = Input(shape=(None, lat, lon, n_feats))\n",
    "    x = TimeDistributed(Flatten())(inp)  # Shape: (batch, time, lat*lon*features)\n",
    "    x = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)  # Shape: (batch, time, lat*lon*features)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = GlobalAveragePooling1D()(x)  # Shape: (batch, lat*lon*features)\n",
    "    x = Dense(horizon * lat * lon)(x)\n",
    "    out = Reshape((horizon, lat, lon, 1))(x)\n",
    "    model = Model(inp, out, name='Transformer_Baseline')\n",
    "    _assert_model_output(model, horizon, lat, lon)\n",
    "    return model\n",
    "\n",
    "# Model registry - all original models preserved\n",
    "MODEL_REGISTRY = {\n",
    "    'original': {\n",
    "        'ConvLSTM': build_conv_lstm,\n",
    "        'ConvGRU': build_conv_gru,\n",
    "        'ConvRNN': build_conv_rnn\n",
    "    },\n",
    "    'enhanced': {\n",
    "        'ConvLSTM_Enhanced': build_conv_lstm_enhanced,\n",
    "        'ConvGRU_Enhanced': build_conv_gru_enhanced,\n",
    "        'ConvRNN_Enhanced': build_conv_rnn_enhanced\n",
    "    },\n",
    "    'advanced': {\n",
    "        'ConvLSTM_Bidirectional': build_conv_lstm_bidirectional,\n",
    "        'ConvGRU_Residual': build_conv_gru_residual,\n",
    "        'ConvLSTM_Residual': build_conv_lstm_residual\n",
    "    },\n",
    "    'attention': {\n",
    "        'ConvLSTM_Attention': build_conv_lstm_attention_simple,\n",
    "        'ConvGRU_Attention': build_conv_gru_attention_simple\n",
    "    },\n",
    "    'competitive': {\n",
    "        'ConvLSTM_MeteoAttention': build_conv_lstm_meteorological_attention_simple,\n",
    "        'ConvLSTM_EfficientBidir': build_efficient_bidirectional_convlstm_simple,\n",
    "        'Transformer_Baseline': build_transformer_baseline_simple\n",
    "    }\n",
    "}\n",
    "\n",
    "# Combine all models\n",
    "MODELS = {}\n",
    "for category in MODEL_REGISTRY.values():\n",
    "    MODELS.update(category)\n",
    "\n",
    "if not HAS_CONVGRU2D:\n",
    "    for key in [\"ConvGRU\",\"ConvGRU_Enhanced\",\"ConvGRU_Residual\",\"ConvGRU_Attention\"]:\n",
    "        MODELS.pop(key, None)\n",
    "    print(\"ConvGRU2D not available; ConvGRU models skipped\")\n",
    "print(f\"Model registry initialized with {len(MODELS)} models across all categories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9678e493",
   "metadata": {
    "id": "9678e493"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==================================================\n",
    "# DATA PREPROCESSING\n",
    "# ==================================================\n",
    "\n",
    "def windowed_arrays(X: np.ndarray, y: np.ndarray, input_window: int, horizon: int, start_indices=None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create windowed arrays for sequence-to-sequence learning without leakage.\"\"\"\n",
    "    seq_X, seq_y = [], []\n",
    "    T = len(X)\n",
    "    if T < (input_window + horizon):\n",
    "        raise ValueError(f\"Not enough timesteps ({T}) to build windows for input_window={input_window}, horizon={horizon}\")\n",
    "    iterator = start_indices if start_indices is not None else range(T - input_window - horizon + 1)\n",
    "    for start in iterator:\n",
    "        if start < 0:\n",
    "            continue\n",
    "        end_w = start + input_window\n",
    "        end_y = end_w + horizon\n",
    "        if end_y > T:\n",
    "            continue\n",
    "        Xw = X[start:end_w]\n",
    "        yw = y[end_w:end_y]\n",
    "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
    "            continue\n",
    "        seq_X.append(Xw)\n",
    "        seq_y.append(yw)\n",
    "    if not seq_X:\n",
    "        raise ValueError(\"No valid windows found after NaN filtering\")\n",
    "    return np.asarray(seq_X, dtype=np.float32), np.asarray(seq_y, dtype=np.float32)\n",
    "\n",
    "def compute_split_indices(total_steps: int, input_window: int, horizon: int, split_ratio: float) -> Tuple[list, list]:\n",
    "    \"\"\"Return train/validation start indices ensuring no temporal leakage.\"\"\"\n",
    "    if total_steps < input_window + horizon:\n",
    "        raise ValueError(\"Insufficient samples to compute split indices\")\n",
    "    cutoff = max(0, int(total_steps * split_ratio))\n",
    "    max_start = total_steps - input_window - horizon + 1\n",
    "    train_end = max(0, cutoff - input_window - horizon + 1)\n",
    "    train_indices = list(range(0, train_end))\n",
    "    val_start = min(max_start, max(cutoff, 0))\n",
    "    val_indices = list(range(val_start, max_start))\n",
    "    if not train_indices or not val_indices:\n",
    "        raise ValueError(\"Split configuration produced empty train/validation windows\")\n",
    "    return train_indices, val_indices\n",
    "\n",
    "def scale_feature_blocks(X_tr: np.ndarray, X_va: np.ndarray, features: list[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Scale continuous/lag blocks independently and keep categorical blocks bounded.\"\"\"\n",
    "    X_tr_scaled = X_tr.copy()\n",
    "    X_va_scaled = X_va.copy()\n",
    "    def apply_scaler(idxs: list[int]):\n",
    "        if not idxs:\n",
    "            return None\n",
    "        scaler = StandardScaler()\n",
    "        tr_block = X_tr[..., idxs].reshape(-1, len(idxs))\n",
    "        va_block = X_va[..., idxs].reshape(-1, len(idxs))\n",
    "        scaler.fit(tr_block)\n",
    "        X_tr_scaled[..., idxs] = scaler.transform(tr_block).reshape(X_tr[..., idxs].shape)\n",
    "        X_va_scaled[..., idxs] = scaler.transform(va_block).reshape(X_va[..., idxs].shape)\n",
    "        return scaler\n",
    "    cont_indices = [i for i, feat in enumerate(features) if feat in BASE_CONTINUOUS_FEATURES]\n",
    "    lag_indices = [i for i, feat in enumerate(features) if feat in PRECIP_LAG_FEATURES]\n",
    "    apply_scaler(cont_indices)\n",
    "    apply_scaler(lag_indices)\n",
    "    cat_indices = [i for i, feat in enumerate(features) if feat in ELEVATION_ONEHOT_FEATURES]\n",
    "    if cat_indices:\n",
    "        X_tr_scaled[..., cat_indices] = np.clip(X_tr_scaled[..., cat_indices], 0.0, 1.0)\n",
    "        X_va_scaled[..., cat_indices] = np.clip(X_va_scaled[..., cat_indices], 0.0, 1.0)\n",
    "    return X_tr_scaled, X_va_scaled\n",
    "\n",
    "def preprocess_data(ds: xr.Dataset, config: Dict, lat: int, lon: int, horizon: int) -> Dict[str, Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, StandardScaler]]:\n",
    "    \"\"\"Preprocess data per experiment with no leakage; return y_scaler.\"\"\"\n",
    "    data_splits = {}\n",
    "    split_ratio = config.get('train_val_split', 0.8)\n",
    "    for exp_name, features in config['feature_sets'].items():\n",
    "        print(f\"Preprocessing {exp_name}...\")\n",
    "        X = np.stack([ds[feat].values for feat in features], axis=-1)\n",
    "        y = ds['total_precipitation'].values[..., np.newaxis]\n",
    "        total_steps = X.shape[0]\n",
    "        train_idx, val_idx = compute_split_indices(total_steps, config['input_window'], horizon, split_ratio)\n",
    "        X_tr, y_tr = windowed_arrays(X, y, config['input_window'], horizon, train_idx)\n",
    "        X_va, y_va = windowed_arrays(X, y, config['input_window'], horizon, val_idx)\n",
    "        X_tr_s, X_va_s = scale_feature_blocks(X_tr, X_va, features)\n",
    "        y_scaler = StandardScaler()\n",
    "        y_tr_s = y_scaler.fit_transform(y_tr.reshape(-1, 1)).reshape(y_tr.shape)\n",
    "        y_va_s = y_scaler.transform(y_va.reshape(-1, 1)).reshape(y_va.shape)\n",
    "        data_splits[exp_name] = (X_tr_s.astype(np.float32), y_tr_s.astype(np.float32),\n",
    "                                 X_va_s.astype(np.float32), y_va_s.astype(np.float32), y_scaler)\n",
    "        print(f\"{exp_name} processed: {X_tr.shape[0]} train windows, {X_va.shape[0]} validation windows\")\n",
    "    return data_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fec26c90",
   "metadata": {
    "id": "fec26c90"
   },
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# TRAINING HELPERS\n",
    "# ==================================================\n",
    "\n",
    "import json, time, logging\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "if 'compute_horizon_weights' not in globals():\n",
    "    def compute_horizon_weights(H:int, strategy:str='uniform'):\n",
    "        import numpy as np\n",
    "        if H <= 0:\n",
    "            return []\n",
    "        if strategy == 'uniform':\n",
    "            w = np.ones(H, dtype=np.float32)\n",
    "        elif strategy == 'linear_decay':\n",
    "            w = np.linspace(1.0, 0.2, H, dtype=np.float32)\n",
    "        elif strategy == 'front_heavy':\n",
    "            w = np.array([1.5**(H-1-i) for i in range(H)], dtype=np.float32)\n",
    "        else:\n",
    "            w = np.ones(H, dtype=np.float32)\n",
    "        s = float(w.sum()) if float(w.sum()) != 0 else 1.0\n",
    "        return (w / s).tolist()\n",
    "\n",
    "if 'validate_shapes' not in globals():\n",
    "    def validate_shapes(X: np.ndarray, y: np.ndarray, model_name: str, expected_input_shape: tuple, expected_horizon: int):\n",
    "        try:\n",
    "            if X.shape[1:] != expected_input_shape:\n",
    "                raise ValueError(f\"Invalid input shape for {model_name}: expected {expected_input_shape}, got {X.shape[1:]}\")\n",
    "            if y.shape[1:] != (expected_horizon, lat, lon, 1):\n",
    "                raise ValueError(f\"Invalid output shape for {model_name}: expected {(expected_horizon, lat, lon, 1)}, got {y.shape[1:]}\")\n",
    "            print(f\"Shapes validated for {model_name}: X={X.shape}, y={y.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Shape validation error for {model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "if 'save_hyperparameters' not in globals():\n",
    "    def save_hyperparameters(metrics_dir: Path, model_name: str, hyperparams: dict):\n",
    "        metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "        try:\n",
    "            with open(metrics_dir / f\"{model_name}_hyperparameters.json\", 'w') as f:\n",
    "                json.dump(hyperparams, f, indent=4)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving hyperparameters for {model_name}: {e}\")\n",
    "\n",
    "if 'TrainingMonitor' not in globals():\n",
    "    class TrainingMonitor(Callback):\n",
    "        def __init__(self, model_name, exp_name):\n",
    "            super().__init__()\n",
    "            self.model_name = model_name\n",
    "            self.exp_name = exp_name\n",
    "            self.start_time = None\n",
    "            self.lrs = []\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = time.time()\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            elapsed = time.time() - self.start_time\n",
    "            try:\n",
    "                lr = float(self.model.optimizer.learning_rate)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    lr = float(self.model.optimizer.lr)\n",
    "                except Exception:\n",
    "                    lr = 0.0\n",
    "            self.lrs.append(lr)\n",
    "            loss = logs.get('loss', np.nan) if logs else np.nan\n",
    "            vloss = logs.get('val_loss', np.nan) if logs else np.nan\n",
    "            print(f\"{self.exp_name} - {self.model_name} | Epoch {epoch+1} | Loss: {loss:.4f} | Val Loss: {vloss:.4f} | LR: {lr:.2e} | Elapsed: {elapsed:.0f}s\")\n",
    "\n",
    "if 'print_training_summary' not in globals():\n",
    "    def print_training_summary(history: dict, model_name: str, exp_name: str):\n",
    "        try:\n",
    "            best_val = float(min(history['val_loss']))\n",
    "            print(f\"\\nTraining Summary - {exp_name} - {model_name}:\")\n",
    "            print(f\"Best validation loss: {best_val:.4f}\")\n",
    "            print(f\"Trained epochs: {len(history['loss'])}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in training summary for {model_name}: {e}\")\n",
    "\n",
    "if 'plot_learning_curves' not in globals():\n",
    "    def plot_learning_curves(history: dict, metrics_dir: Path, model_name: str, show: bool = True):\n",
    "        try:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            epochs = range(1, len(history['loss']) + 1)\n",
    "            ax.plot(epochs, history['loss'], 'b-', label='Train Loss')\n",
    "            ax.plot(epochs, history['val_loss'], 'r-', label='Val Loss')\n",
    "            if len(history['val_loss']) > 0:\n",
    "                import numpy as np\n",
    "                best_ep = int(np.argmin(history['val_loss'])) + 1\n",
    "                best_val = float(min(history['val_loss']))\n",
    "                ax.plot(best_ep, best_val, 'r*', markersize=12, label=f'Best: {best_val:.4f}')\n",
    "            ax.set_title(f'Learning Curve - {model_name}')\n",
    "            ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
    "            ax.grid(alpha=0.3, linestyle='--'); ax.legend(loc='upper right')\n",
    "            metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "            plt.savefig(metrics_dir / f\"{model_name}_learning_curve.png\", dpi=150, bbox_inches='tight')\n",
    "            if show: plt.show()\n",
    "            plt.close(fig)\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting learning curves for {model_name}: {e}\")\n",
    "\n",
    "def batch_predict(model: tf.keras.models.Model, X: np.ndarray, batch_size: int = 1, squeeze: bool = False) -> np.ndarray:\n",
    "    \"\"\"Predict in mini-batches robustly and return concatenated array.\n",
    "    - Validates consistent output shape across batches\n",
    "    - Handles last partial batch\n",
    "    - Accepts numpy arrays or array-like\n",
    "    \"\"\"\n",
    "    import numpy as np, math\n",
    "    try:\n",
    "        X = np.asarray(X)\n",
    "        n_samples = int(X.shape[0])\n",
    "        if n_samples == 0:\n",
    "            return np.empty((0,), dtype=X.dtype)\n",
    "        if batch_size is None or batch_size <= 0:\n",
    "            batch_size = n_samples\n",
    "        num_batches = int(math.ceil(n_samples / float(batch_size)))\n",
    "        predictions = []\n",
    "        out_shape = None\n",
    "        for b in range(num_batches):\n",
    "            start = b * batch_size\n",
    "            end = min(n_samples, start + batch_size)\n",
    "            batch_X = X[start:end]\n",
    "            print(f\"Predicting batch {b+1}/{num_batches}, shape: {batch_X.shape}\")\n",
    "            batch_pred = model.predict(batch_X, verbose=0, batch_size=batch_size)\n",
    "            bp = np.asarray(batch_pred)\n",
    "            if out_shape is None:\n",
    "                out_shape = bp.shape[1:]\n",
    "            elif bp.shape[1:] != out_shape:\n",
    "                raise ValueError(f\"Inconsistent prediction shapes across batches: {bp.shape} vs (None,{out_shape})\")\n",
    "            predictions.append(bp)\n",
    "        Y = np.concatenate(predictions, axis=0)\n",
    "        return np.squeeze(Y) if squeeze else Y\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch_predict: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "712196c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "712196c3",
    "outputId": "e844f7ec-b8a4-4374-b78a-ae643442a647"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22572\\676298073.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{k}: {v}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;31m# Run tests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m \u001b[0mtest_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_unit_tests\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22572\\676298073.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(config, lat, lon)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0mok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'{name}_H{H}'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'SUCCESS'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mok\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34mf'SHAPE_MISMATCH: {y.shape} != {exp}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'{name}_H{H}'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'FAILED: {str(e)[:120]}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# 5) batch_predict sanity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m                 )\n\u001b[0;32m    977\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m         \u001b[1;31m################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[1;31m# 8. Add a node in the graph for symbolic calls.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     56\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             )\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\models\\functional.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, training, mask, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_keras_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         outputs = self._run_through_graph(\n\u001b[0m\u001b[0;32m    184\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             operation_fn=lambda op: operation_fn(\n\u001b[0;32m    186\u001b[0m                 \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\ops\\function.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                     \u001b[1;31m# Use NNX operation mapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                     \u001b[0moperation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_operation_for_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m                     \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moperation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\models\\functional.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[1;32mand\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             ):\n\u001b[0;32m    642\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0moperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m                 )\n\u001b[0;32m    977\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m         \u001b[1;31m################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[1;31m# 8. Add a node in the graph for symbolic calls.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     56\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             )\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\layers\\rnn\\conv_lstm.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, sequences, initial_state, mask, training)\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         return super().call(\n\u001b[0m\u001b[0;32m    534\u001b[0m             \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, sequences, initial_state, mask, training)\u001b[0m\n\u001b[0;32m    402\u001b[0m         self._maybe_config_dropout_masks(\n\u001b[0;32m    403\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         )\n\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m         last_output, outputs, states = self.inner_loop(\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[0msequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, sequences, initial_state, mask, training)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         return backend.rnn(\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m             \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\rnn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask, return_all_outputs)\u001b[0m\n\u001b[0;32m    424\u001b[0m                     \u001b[0minitial_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_new_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                 )\n\u001b[0;32m    426\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_ta_t\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             final_outputs = tf.while_loop(\n\u001b[0m\u001b[0;32m    429\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0mloop_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[1;33m**\u001b[0m\u001b[0mwhile_loop_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    656\u001b[0m                   \u001b[0m_call_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m                   \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m                   ('after %s' % date), instructions)\n\u001b[1;32m--> 660\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\ops\\while_loop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[0;32m    237\u001b[0m   \u001b[1;33m...\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m   \u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m   \"\"\"\n\u001b[1;32m--> 241\u001b[1;33m   return while_loop(\n\u001b[0m\u001b[0;32m    242\u001b[0m       \u001b[0mcond\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m       \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\ops\\while_loop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m       loop_var_structure = nest.map_structure(type_spec.type_spec_from_value,\n\u001b[0;32m    486\u001b[0m                                               list(loop_vars))\n\u001b[0;32m    487\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m           \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\ops\\while_loop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(i, lv)\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\rnn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[0;32m    407\u001b[0m                     \u001b[0mTuple\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_ta_t\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \"\"\"\n\u001b[0;32m    409\u001b[0m                 \u001b[0mcurrent_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_ta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 \u001b[0mcurrent_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m                 output, new_states = step_function(\n\u001b[0m\u001b[0;32m    412\u001b[0m                     \u001b[0mcurrent_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconstants\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m                 )\n\u001b[0;32m    414\u001b[0m                 \u001b[0mflat_new_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(inputs, states)\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;31m# that would otherwise break PyTorch's autograd functionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;31m# by modifying tensors needed for gradient computation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"torch\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m                 \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcell_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mnew_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\layers\\layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m                     \u001b[1;34m\"layers will not see the mask.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m                 )\n\u001b[0;32m    977\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m             \u001b[1;31m# Destroy call context if we created it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_call_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m         \u001b[1;31m################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m         \u001b[1;31m# 8. Add a node in the graph for symbolic calls.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\ops\\operation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001b[0;32m     56\u001b[0m                 \u001b[0mcall_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             )\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;31m# Plain flow.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many_symbolic_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\keras\\src\\layers\\rnn\\conv_lstm.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs, states, training)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \u001b[0mh_o\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_tm1_o\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_kernel_o\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_i\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_f\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mc_tm1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_c\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m         \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_o\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh_o\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\framework\\override_binary_operator.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m# object that can implement the operator with knowledge of itself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# and the tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_math_operator_overrides.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         ),\n\u001b[0;32m     74\u001b[0m         \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     )  # pylint: disable=protected-access\n\u001b[1;32m---> 76\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1745\u001b[0m     new_vals = gen_sparse_ops.sparse_dense_cwise_mul(y.indices, y.values,\n\u001b[0;32m   1746\u001b[0m                                                      y.dense_shape, x, name)\n\u001b[0;32m   1747\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m    \u001b[1;33m*\u001b[0m \u001b[0mInvalidArgumentError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mWhen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mincompatible\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m   \"\"\"\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manue\\.conda\\envs\\precipitation_prediction\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6825\u001b[0m         _ctx, \"Mul\", name, x, y)\n\u001b[0;32m   6826\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6827\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6828\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6829\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6830\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6831\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6832\u001b[0m       return mul_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# UNIT TESTS (Enhanced)\n",
    "# ==================================================\n",
    "\n",
    "def run_unit_tests(config: Dict, lat: int, lon: int) -> Dict[str, str]:\n",
    "    \"\"\"Robust unit tests to catch shape and API regressions early.\"\"\"\n",
    "    import numpy as np\n",
    "    results: Dict[str, str] = {}\n",
    "    B = 2\n",
    "    T = config['input_window']\n",
    "    test_n_feats = 15\n",
    "    try:\n",
    "        _convgru_layer = build_convgru2d_layer(filters=16, kernel_size=(3, 3), padding='same', return_sequences=False, name=\"conv_gru2d_test\")\n",
    "        HAS_CONVGRU2D = True\n",
    "    except Exception:\n",
    "        HAS_CONVGRU2D = False\n",
    "\n",
    "\n",
    "    # 1) Keras ConvGRU sanity\n",
    "    try:\n",
    "        if HAS_CONVGRU2D:\n",
    "            layer = _convgru_layer\n",
    "            x = tf.random.normal((B, T, lat, lon, test_n_feats))\n",
    "            y = layer(x)\n",
    "            exp = (B, lat, lon, 16)\n",
    "            results['ConvGRU2D_layer'] = 'SUCCESS' if tuple(y.shape) == exp else f'SHAPE_MISMATCH: {y.shape}'\n",
    "            tf.keras.backend.clear_session()\n",
    "        else:\n",
    "            results['ConvGRU2D_layer'] = 'SKIPPED: ConvGRU2D not available in this TF/Keras'\n",
    "    except Exception as e:\n",
    "        results['ConvGRU2D_layer'] = f'FAILED: {str(e)[:120]}'\n",
    "\n",
    "    # 2) Losses finite + monotonicity\n",
    "    try:\n",
    "        for H in config['enabled_horizons']:\n",
    "            y_true = tf.zeros((B, H, lat, lon, 1))\n",
    "            y_pred_good = tf.zeros_like(y_true)\n",
    "            y_pred_bad = tf.ones_like(y_true)\n",
    "            mh = MultiHorizonLoss(horizon_weights=[1.0/H]*H)\n",
    "            cbl = CombinedLoss(horizon_weights=[1.0/H]*H)\n",
    "            v_good = float(tf.reduce_mean(mh(y_true, y_pred_good)).numpy())\n",
    "            v_bad = float(tf.reduce_mean(mh(y_true, y_pred_bad)).numpy())\n",
    "            assert np.isfinite(v_good) and np.isfinite(v_bad)\n",
    "            assert v_good <= v_bad + 1e-6\n",
    "            v2_good = float(tf.reduce_mean(cbl(y_true, y_pred_good)).numpy())\n",
    "            v2_bad = float(tf.reduce_mean(cbl(y_true, y_pred_bad)).numpy())\n",
    "            assert np.isfinite(v2_good) and np.isfinite(v2_bad)\n",
    "            assert v2_good <= v2_bad + 1e-6\n",
    "        results['Losses'] = 'SUCCESS'\n",
    "    except Exception as e:\n",
    "        results['Losses'] = f'FAILED: {str(e)[:120]}'\n",
    "\n",
    "    # 3) compute_horizon_weights integrity\n",
    "    try:\n",
    "        for H in config['enabled_horizons']:\n",
    "            for strat in ['uniform','linear_decay','front_heavy']:\n",
    "                w = compute_horizon_weights(H, strat)\n",
    "                assert len(w) == H\n",
    "                s = float(np.sum(w))\n",
    "                assert abs(s - 1.0) < 1e-5\n",
    "        results['HorizonWeights'] = 'SUCCESS'\n",
    "    except Exception as e:\n",
    "        results['HorizonWeights'] = f'FAILED: {str(e)[:120]}'\n",
    "\n",
    "    # 4) Model output shapes across horizons\n",
    "    for H in config['enabled_horizons']:\n",
    "        for name, builder in MODELS.items():\n",
    "            try:\n",
    "                if (not HAS_CONVGRU2D) and name.startswith('ConvGRU'):\n",
    "                    results[f'{name}_H{H}'] = 'SKIPPED: ConvGRU2D not available'; continue\n",
    "                tf.keras.backend.clear_session()\n",
    "                model = builder(test_n_feats, lat, lon, H)\n",
    "                x = tf.random.normal((B, T, lat, lon, test_n_feats))\n",
    "                y = model(x, training=False)\n",
    "                exp = (B, H, lat, lon, 1)\n",
    "                ok = tuple(y.shape) == exp\n",
    "                results[f'{name}_H{H}'] = 'SUCCESS' if ok else f'SHAPE_MISMATCH: {y.shape} != {exp}'\n",
    "            except Exception as e:\n",
    "                results[f'{name}_H{H}'] = f'FAILED: {str(e)[:120]}'\n",
    "\n",
    "    # 5) batch_predict sanity\n",
    "    try:\n",
    "        for H in config['enabled_horizons']:\n",
    "            inp = tf.keras.Input(shape=(T, lat, lon, test_n_feats))\n",
    "            out = tf.keras.layers.Lambda(lambda t: tf.zeros((tf.shape(t)[0], H, lat, lon, 1), dtype=t.dtype))(inp)\n",
    "            dummy_model = tf.keras.Model(inp, out)\n",
    "            X = tf.random.normal((3, T, lat, lon, test_n_feats)).numpy()\n",
    "            Y = batch_predict(dummy_model, X, batch_size=1)\n",
    "            assert Y.shape == (3, H, lat, lon, 1)\n",
    "        results['BatchPredict'] = 'SUCCESS'\n",
    "    except Exception as e:\n",
    "        results['BatchPredict'] = f'FAILED: {str(e)[:120]}'\n",
    "\n",
    "    print('Unit Test Results:')\n",
    "    for k,v in results.items():\n",
    "        print(f'{k}: {v}')\n",
    "    return results\n",
    "\n",
    "# Run tests\n",
    "test_results = run_unit_tests(CONFIG, lat, lon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8cd04",
   "metadata": {
    "id": "97f8cd04"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==================================================\n",
    "# MODEL TRAINING\n",
    "# ==================================================\n",
    "\n",
    "def train_model(model: tf.keras.models.Model, X_tr: np.ndarray, y_tr: np.ndarray, X_va: np.ndarray, y_va: np.ndarray,\n",
    "                config: dict, model_name: str, exp_name: str, out_dir: Path, horizon: int) -> tuple[tf.keras.models.Model, dict]:\n",
    "    \"\"\"Train a single model with callbacks and save results.\"\"\"\n",
    "    metrics_dir = out_dir / f'h{horizon}' / exp_name / 'training_metrics'\n",
    "    metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_path = metrics_dir / f\"{model_name}_best_h{horizon}.h5\"\n",
    "\n",
    "    num_features = len(config['feature_sets'][exp_name])\n",
    "    expected_input_shape = (config['input_window'], lat, lon, num_features)\n",
    "    try:\n",
    "        validate_shapes(X_tr, y_tr, model_name, expected_input_shape, expected_horizon=horizon)\n",
    "        validate_shapes(X_va, y_va, model_name, expected_input_shape, expected_horizon=horizon)\n",
    "        print(f\"Training {model_name} in {exp_name} - X_tr shape: {X_tr.shape}, y_tr shape: {y_tr.shape}, \"\n",
    "              f\"X_va shape: {X_va.shape}, y_va shape: {y_va.shape}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Shape validation error for {model_name}: {e}\")\n",
    "        return model, {}\n",
    "\n",
    "    try:\n",
    "        hyperparams = {\n",
    "            'input_shape': expected_input_shape,\n",
    "            'learning_rate': config['learning_rate'],\n",
    "            'batch_size': config['batch_size'],\n",
    "            'train_batch_size': int(max(1, config.get('effective_batch_size') or max(1, config['batch_size']//2))),\n",
    "            'epochs': config['epochs'],\n",
    "            'patience': config['patience'],\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'model_params': model.count_params(),\n",
    "            'version': 'V2_Enhanced'\n",
    "        }\n",
    "        save_hyperparameters(metrics_dir, model_name, hyperparams)\n",
    "    except (KeyError, NameError) as e:\n",
    "        print(f\"Error setting hyperparameters for {model_name}: {e}\")\n",
    "        return model, {}\n",
    "\n",
    "    try:\n",
    "        if exp_name == 'BASIC':\n",
    "            loss = 'mse'\n",
    "        else:\n",
    "            horizon_weights = compute_horizon_weights(horizon, config.get('loss_weighting', 'uniform'))\n",
    "            consistency_weight = 0.1 if exp_name == 'KCE' else 0.15\n",
    "            loss = CombinedLoss(horizon_weights=horizon_weights, consistency_weight=consistency_weight)\n",
    "    except NameError as e:\n",
    "        print(f\"Error selecting loss for {model_name}: {e}\")\n",
    "        return model, {}\n",
    "\n",
    "    try:\n",
    "        model.compile(optimizer=Adam(config['learning_rate']), loss=loss, metrics=['mae'])\n",
    "        print(f\"Model {model_name} compiled successfully\")\n",
    "        model.summary()\n",
    "    except Exception as e:\n",
    "        print(f\"Error compiling model {model_name}: {e}\")\n",
    "        return model, {}\n",
    "\n",
    "    try:\n",
    "        csv_logger = CSVLogger(str(metrics_dir / f\"{model_name}_training_log_h{horizon}.csv\"), separator=',', append=False)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=max(1, config['patience']//2), min_lr=1e-6, verbose=1)\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=config['patience'], restore_best_weights=True, verbose=1)\n",
    "        checkpoint = ModelCheckpoint(str(model_path), save_best_only=True, monitor='val_loss', verbose=1)\n",
    "        training_monitor = TrainingMonitor(model_name, exp_name)\n",
    "        tensorboard = TensorBoard(log_dir=str(metrics_dir / f\"{model_name}_logs\"))\n",
    "        callbacks = [early_stop, checkpoint, reduce_lr, csv_logger, training_monitor, tensorboard]\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting callbacks for {model_name}: {e}\")\n",
    "        return model, {}\n",
    "\n",
    "    adaptive_batch = config.get('effective_batch_size')\n",
    "    if adaptive_batch is None:\n",
    "        fallback = max(1, config['batch_size']//2)\n",
    "        adaptive_batch = fallback if fallback > 0 else config['batch_size']\n",
    "    train_batch = max(1, adaptive_batch)\n",
    "    print(f\"Starting training for {model_name} in {exp_name} with batch_size={train_batch}...\")\n",
    "    try:\n",
    "        history = model.fit(\n",
    "            X_tr, y_tr,\n",
    "            validation_data=(X_va, y_va),\n",
    "            epochs=config['epochs'], batch_size=train_batch,\n",
    "            callbacks=callbacks, verbose=0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training of {model_name}: {e}\")\n",
    "        return model, {}\n",
    "\n",
    "    try:\n",
    "        lr_values = history.history.get('lr', training_monitor.lrs)\n",
    "        val_losses = history.history['val_loss']\n",
    "        best_idx = int(np.argmin(val_losses))\n",
    "        history_dict = {\n",
    "            'loss': [float(x) for x in history.history['loss']],\n",
    "            'val_loss': [float(x) for x in val_losses],\n",
    "            'mae': [float(x) for x in history.history.get('mae', [])],\n",
    "            'val_mae': [float(x) for x in history.history.get('val_mae', [])],\n",
    "            'lr': [float(x) for x in lr_values],\n",
    "            'best_epoch': best_idx,\n",
    "            'best_val_loss': float(val_losses[best_idx]),\n",
    "            'best_train_loss': float(history.history['loss'][best_idx]),\n",
    "            'train_val_gap': float(history.history['loss'][best_idx] - val_losses[best_idx])\n",
    "        }\n",
    "        with open(metrics_dir / f\"{model_name}_history.json\", 'w') as f:\n",
    "            json.dump(history_dict, f, indent=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving history for {model_name}: {e}\")\n",
    "        history_dict = {}\n",
    "\n",
    "    print_training_summary(history_dict, model_name, exp_name)\n",
    "    plot_learning_curves(history_dict, metrics_dir, model_name, show=True)\n",
    "\n",
    "    return model, history_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081dcfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d081dcfd",
    "outputId": "39ec0497-b22f-4405-d6ef-82bc2ee099cb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "results = []\n",
    "all_histories = {}\n",
    "for horizon in CONFIG['enabled_horizons']:\n",
    "    print(f'=== Running experiments for horizon H={horizon} months ===')\n",
    "    data_splits_h = preprocess_data(ds, CONFIG, lat, lon, horizon)\n",
    "    for exp_name in CONFIG['feature_sets']:\n",
    "        try:\n",
    "            X_tr, y_tr, X_va, y_va, scaler = data_splits_h[exp_name]\n",
    "            print(f\"{exp_name}: X_tr={X_tr.shape}, y_tr={y_tr.shape}, X_va={X_va.shape}, y_va={y_va.shape}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"Error accessing data for experiment {exp_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        out_exp = CONFIG['out_root'] / exp_name\n",
    "        out_exp.mkdir(exist_ok=True)\n",
    "\n",
    "        for model_name, model_builder in MODELS.items():\n",
    "            try:\n",
    "                tf.keras.backend.clear_session()\n",
    "                num_features = len(CONFIG['feature_sets'][exp_name])\n",
    "                print(f\"Building {model_name} with {num_features} features, lat={lat}, lon={lon}, horizon={horizon}\")\n",
    "                model = model_builder(num_features, lat, lon, horizon)\n",
    "                model, history = train_model(model, X_tr, y_tr, X_va, y_va, CONFIG, model_name, exp_name, CONFIG['out_root'], horizon)\n",
    "                all_histories[f\"{exp_name}_{model_name}_H{horizon}\"] = history\n",
    "\n",
    "                if not history or not history.get('val_loss'):\n",
    "                    print(f\"Skipping predictions for {model_name} in {exp_name} due to training failure.\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Generating predictions for {model_name} in {exp_name}...\")\n",
    "                try:\n",
    "                    prediction_batch = max(1, CONFIG.get('prediction_batch_size') or max(1, CONFIG['batch_size']//2))\n",
    "                    y_hat_sc = batch_predict(model, X_va, batch_size=prediction_batch)\n",
    "                    if y_hat_sc.shape != y_va.shape:\n",
    "                        y_hat_sc = y_hat_sc.reshape(y_va.shape)\n",
    "                    y_hat = scaler.inverse_transform(y_hat_sc.reshape(-1, 1)).reshape(y_va.shape)\n",
    "                    y_true = scaler.inverse_transform(y_va.reshape(-1, 1)).reshape(y_va.shape)\n",
    "                    scale_ratio = float(np.nanmax(np.abs(y_hat)) / (np.nanmax(np.abs(y_true)) + 1e-8))\n",
    "                    if scale_ratio > 50.0:\n",
    "                        print(f\"[WARN] {exp_name} - {model_name}: prediction scale ratio {scale_ratio:.1f}x, skipping metric export.\")\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating predictions for {model_name} in {exp_name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                for h in range(horizon):\n",
    "                    try:\n",
    "                        y_true_h = y_true[:, h, ..., 0]\n",
    "                        y_pred_h = y_hat[:, h, ..., 0]\n",
    "                        rmse = np.sqrt(mean_squared_error(y_true_h.ravel(), y_pred_h.ravel()))\n",
    "                        mae = mean_absolute_error(y_true_h.ravel(), y_pred_h.ravel())\n",
    "                        r2 = r2_score(y_true_h.ravel(), y_pred_h.ravel())\n",
    "                        mean_true = float(np.mean(y_true_h))\n",
    "                        mean_pred = float(np.mean(y_pred_h))\n",
    "                        total_true = float(np.sum(y_true_h))\n",
    "                        total_pred = float(np.sum(y_pred_h))\n",
    "                        bias = abs(mean_pred - mean_true) / (abs(mean_true) + 1e-8)\n",
    "                        if bias > 0.10:\n",
    "                            print(f\"[BIAS] {exp_name} - {model_name} - H{h+1}: mean bias {bias*100:.1f}%\")\n",
    "                        results.append({\n",
    "                            'TotalHorizon': horizon,\n",
    "                            'Experiment': exp_name,\n",
    "                            'Model': model_name,\n",
    "                            'H': h + 1,\n",
    "                            'RMSE': rmse,\n",
    "                            'MAE': mae,\n",
    "                            'R^2': r2,\n",
    "                            'Mean_True_mm': mean_true,\n",
    "                            'Mean_Pred_mm': mean_pred,\n",
    "                            'TotalPrecipitation': total_true,\n",
    "                            'TotalPrecipitation_Pred': total_pred\n",
    "                        })\n",
    "                        print(f\"    H={h+1}: RMSE={rmse:.4f}, MAE={mae:.4f}, R^2={r2:.4f}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error evaluating predictions for {model_name}, horizon {h+1}: {e}\")\n",
    "\n",
    "                tf.keras.backend.clear_session()\n",
    "                gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"Error training {model_name} for {exp_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "if results:\n",
    "    try:\n",
    "        res_df = pd.DataFrame(results)\n",
    "        horizon_label = 'multi' if len(CONFIG['enabled_horizons']) > 1 else str(CONFIG['enabled_horizons'][0])\n",
    "        out_csv = CONFIG['out_root'] / f'metrics_spatial_v2_refactored_h{horizon_label}.csv'\n",
    "        res_df.to_csv(out_csv, index=False)\n",
    "        print(f'Results saved to {out_csv}')\n",
    "        res_df['mean_bias_pct'] = 100 * (res_df['Mean_Pred_mm'] - res_df['Mean_True_mm']) / (res_df['Mean_True_mm'].replace(0, np.nan))\n",
    "        flagged = res_df[res_df['mean_bias_pct'].abs() > 10]\n",
    "        if not flagged.empty:\n",
    "            print('[BIAS] The following experiment/model horizons exceeded the 10% mean bias threshold:')\n",
    "            print(flagged[['Experiment', 'Model', 'H', 'mean_bias_pct']])\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "else:\n",
    "    print(\"No results generated; skipping CSV export\")\n",
    "\n",
    "# Aggressive cleanup\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8328e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a0c8328e",
    "outputId": "859b10d5-6fdc-403e-c2fa-3185cd9af001"
   },
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# VISUALIZATION AND COMPARISON\n",
    "# ==================================================\n",
    "\n",
    "def quick_plot(ax, data, cmap, title, vmin=None, vmax=None, unit=None):\n",
    "    \"\"\"Plot spatial data with optional Cartopy support.\"\"\"\n",
    "    if CARTOPY_AVAILABLE and ccrs is not None:\n",
    "        mesh = ax.pcolormesh(ds.longitude, ds.latitude, data, cmap=cmap, shading='nearest',\n",
    "                             vmin=vmin, vmax=vmax, transform=ccrs.PlateCarree())\n",
    "        ax.coastlines()\n",
    "        if gpd is not None and DEPT_GDF is not None:\n",
    "            ax.add_geometries(DEPT_GDF.geometry, ccrs.PlateCarree(),\n",
    "                              edgecolor='black', facecolor='none', linewidth=1)\n",
    "        ax.gridlines(draw_labels=False, linewidth=0.5, linestyle='--', alpha=0.4)\n",
    "    else:\n",
    "        mesh = ax.pcolormesh(ds.longitude, ds.latitude, data, cmap=cmap, shading='nearest',\n",
    "                             vmin=vmin, vmax=vmax)\n",
    "        ax.set_xlabel('Longitude', fontsize=11)\n",
    "        ax.set_ylabel('Latitude', fontsize=11)\n",
    "    ax.set_title(title, fontsize=9, pad=15)\n",
    "    return mesh\n",
    "\n",
    "# Load shapefiles if available\n",
    "DEPT_GDF = None\n",
    "if gpd is not None:\n",
    "    shape_dir = CONFIG['base_path'] / 'data' / 'input' / 'shapes'\n",
    "    if shape_dir.exists():\n",
    "        DEPT_GDF = gpd.read_file(shape_dir / 'MGN_Departamento.shp')\n",
    "        print(\"Shape files loaded for visualization\")\n",
    "\n",
    "# Generate visualizations\n",
    "comp_dir = CONFIG['out_root'] / 'comparisons'\n",
    "comp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Ensure res_df exists and is non-empty before using it.\n",
    "# Initialize res_df_local from res_df and then overwrite per-horizon as before.\n",
    "if 'res_df' in globals() and not res_df.empty:\n",
    "    res_df_local = res_df.copy()\n",
    "    for TH in sorted(res_df['TotalHorizon'].unique()):\n",
    "        sub_df = res_df[res_df['TotalHorizon'] == TH]\n",
    "        print(f'Plotting comparisons for H={TH}')\n",
    "        res_df_local = sub_df\n",
    "    # Metrics comparison plot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(24, 15), constrained_layout=True)\n",
    "    # FIX: Changed 'R' to 'R^2' to match the DataFrame column name\n",
    "    for metric, ax in zip(['RMSE', 'MAE', 'R^2', 'TotalPrecipitation'], axes.ravel()):\n",
    "        if metric != 'TotalPrecipitation':\n",
    "            pivot = res_df_local.pivot_table(values=metric, index='Model', columns='Experiment', aggfunc='mean')\n",
    "            pivot.plot(kind='bar', ax=ax)\n",
    "            ax.set_title(f'Average {metric} by Model and Experiment', pad=12, fontsize=14, weight='bold')\n",
    "            ax.set_ylabel(metric)\n",
    "            ax.set_xlabel('Model')\n",
    "            ax.legend(title='Experiment', bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            pivot_true = res_df_local.pivot_table(values='TotalPrecipitation', index='Model', columns='Experiment', aggfunc='mean')\n",
    "            pivot_pred = res_df_local.pivot_table(values='TotalPrecipitation_Pred', index='Model', columns='Experiment', aggfunc='mean')\n",
    "            pivot_true.plot(kind='bar', ax=ax, color='skyblue', alpha=0.75)\n",
    "            pivot_pred.plot(kind='line', ax=ax, marker='o', linestyle='--', linewidth=2.5, alpha=0.9)\n",
    "            ax.set_title('Avg Total Precipitation (True vs Pred) by Model & Experiment', pad=12, fontsize=14, weight='bold')\n",
    "            ax.set_ylabel('Total Precipitation (mm)')\n",
    "            ax.set_xlabel('Model')\n",
    "            legend_labels = [f'True - {c}' for c in pivot_true.columns] + [f'Pred - {c}' for c in pivot_pred.columns]\n",
    "            ax.legend(legend_labels, title='Legend', bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    fig.subplots_adjust(wspace=0.35, hspace=0.30, right=0.80)\n",
    "    plt.savefig(comp_dir / 'metrics_comparison_h{TH}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Metrics plot saved (per horizon) at: {comp_dir / 'metrics_comparison_h{TH}.png'}\")\n",
    "\n",
    "    # Summary table of best models\n",
    "    # FIX: Changed 'R' to 'R^2' here as well\n",
    "    best_models = res_df_local.groupby('Experiment').apply(lambda x: x.loc[x['RMSE'].idxmin()])[\n",
    "        ['Model', 'RMSE', 'MAE', 'R^2', 'TotalPrecipitation', 'TotalPrecipitation_Pred']\n",
    "    ]\n",
    "    print(\"\\n SUMMARY TABLE BEST MODELS BY EXPERIMENT:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(best_models.to_string())\n",
    "\n",
    "    # Learning curves\n",
    "    if all_histories:\n",
    "        n_experiments = len(all_histories)\n",
    "        n_cols, n_rows = 3, (n_experiments + 2) // 3\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(21, 7 * n_rows), constrained_layout=True)\n",
    "        axes = axes.flatten()\n",
    "        for idx, (key, history) in enumerate(all_histories.items()):\n",
    "            if idx >= len(axes): break\n",
    "            ax = axes[idx]\n",
    "            epochs = range(1, len(history['loss']) + 1)\n",
    "            ax.plot(epochs, history['loss'], 'b-', label='Train Loss', linewidth=2.5, alpha=0.8)\n",
    "            ax.plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2.5, alpha=0.8)\n",
    "            best_ep = np.argmin(history['val_loss']) + 1\n",
    "            best_val = min(history['val_loss'])\n",
    "            ax.plot(best_ep, best_val, 'r*', markersize=15, label=f'Best: {best_val:.4f}')\n",
    "            ax.set_title(key, pad=10, fontsize=13, weight='bold')\n",
    "            ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
    "            ax.grid(alpha=0.3, linestyle='--'); ax.legend(loc='upper right')\n",
    "        for ax in axes[len(all_histories):]:\n",
    "            ax.remove()\n",
    "        plt.suptitle('Learning Curves - All Experiments', fontsize=16, fontweight='bold')\n",
    "        plt.savefig(comp_dir / 'all_learning_curves_h{TH}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    # Training summary\n",
    "    print('TRAINING SUMMARY:')\n",
    "    for exp in CONFIG['feature_sets'].keys():\n",
    "        metrics_dir = CONFIG['out_root'] / exp / 'training_metrics'\n",
    "        if metrics_dir.exists():\n",
    "            print(f\"\\n Experiment: {exp}\")\n",
    "            for model_name in MODELS:\n",
    "                hp_file = metrics_dir / f\"{model_name}_hyperparameters.json\"\n",
    "                hist_file = metrics_dir / f\"{model_name}_history.json\"\n",
    "                if hp_file.exists() and hist_file.exists():\n",
    "                    with open(hp_file) as f: hp = json.load(f)\n",
    "                    with open(hist_file) as f: hist = json.load(f)\n",
    "                    print(f\"\\n   - {model_name}:\")\n",
    "                    print(f\"     - Model parameters: {hp['model_params']:,}\")\n",
    "                    print(f\"     - Best Val Loss: {hist.get('best_val_loss', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5cb6f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8e5cb6f2",
    "outputId": "e78644f4-08d2-4339-c712-635e7022e817"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Generating enhanced evolution-by-horizon plots...\")\n",
    "\n",
    "# Helper: load res_df from saved CSV if not already in memory\n",
    "def _maybe_load_res_df_from_disk():\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    candidates = []\n",
    "    try:\n",
    "        if 'CONFIG' in globals() and 'out_root' in CONFIG:\n",
    "            candidates += sorted(Path(CONFIG['out_root']).glob('metrics_spatial_v2_refactored_*.csv'))\n",
    "    except Exception:\n",
    "        pass\n",
    "    candidates += sorted(Path('.').glob('metrics_spatial_v2_refactored_*.csv'))\n",
    "    for path in sorted(candidates, key=lambda p: p.stat().st_mtime, reverse=True):\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"Loaded res_df from {path}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Attempt lazy loading if res_df is missing or empty\n",
    "if ('res_df' not in globals()) or (res_df.empty if 'res_df' in globals() else True):\n",
    "    _tmp = _maybe_load_res_df_from_disk()\n",
    "    if _tmp is not None:\n",
    "        res_df = _tmp\n",
    "\n",
    "if 'res_df' in globals() and not res_df.empty:\n",
    "    # Iterate per TotalHorizon and produce plots for each\n",
    "    for TH in sorted(res_df['TotalHorizon'].unique()):\n",
    "        sub_df = res_df[res_df['TotalHorizon'] == TH].copy()\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Individual metrics per horizon (panel)\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(28, 6))\n",
    "        metrics = ['RMSE', 'MAE', 'R^2', 'TotalPrecipitation']\n",
    "        titles = ['RMSE by Horizon', 'MAE by Horizon', 'R^2 by Horizon', 'Total Precipitation (True vs Pred) by Horizon']\n",
    "        metric_suffix = {'RMSE': 'rmse', 'MAE': 'mae', 'R^2': 'r2', 'TotalPrecipitation': 'totprecip'}\n",
    "        model_list = list(sub_df['Model'].unique())\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, max(1, len(model_list))))\n",
    "\n",
    "        for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "            ax = axes[idx]\n",
    "            if metric != 'TotalPrecipitation':\n",
    "                data = sub_df.groupby(['H', 'Model'])[metric].mean().unstack(fill_value=np.nan)\n",
    "                for i, model in enumerate(data.columns):\n",
    "                    ax.plot(data.index, data[model], marker='o', label=model, color=colors[i % len(colors)],\n",
    "                            linewidth=2.5, markersize=8, markeredgewidth=2, markeredgecolor='white')\n",
    "            else:\n",
    "                data_true = sub_df.groupby(['H', 'Model'])['TotalPrecipitation'].mean().unstack(fill_value=np.nan)\n",
    "                data_pred = sub_df.groupby(['H', 'Model'])['TotalPrecipitation_Pred'].mean().unstack(fill_value=np.nan)\n",
    "                for i, model in enumerate(data_true.columns):\n",
    "                    ax.plot(data_true.index, data_true[model], marker='s', label=f'{model} - True',\n",
    "                            color=colors[i % len(colors)], linewidth=2.5, markersize=7, markeredgecolor='white')\n",
    "                    ax.plot(data_pred.index, data_pred[model], marker='s', label=f'{model} - Pred',\n",
    "                            color=colors[i % len(colors)], linewidth=2.5, linestyle='--', alpha=0.8, markersize=7)\n",
    "            ylabel = metric if metric not in ('TotalPrecipitation', 'TotalPrecipitation_Pred') else 'Total Precipitation (mm)'\n",
    "            ax.set_xlabel('Horizon (months)', fontsize=12)\n",
    "            ax.set_ylabel(ylabel, fontsize=12)\n",
    "            ax.set_title(title, fontsize=14, fontweight='bold', pad=10)\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            ax.set_xticks(sorted(sub_df['H'].unique()))\n",
    "            if idx == 0:\n",
    "                # Legend outside the plotting area to the right\n",
    "                ax.legend(title='Model', loc='center left', bbox_to_anchor=(1.02, 0.5), frameon=True, fancybox=True, shadow=True, ncol=1)\n",
    "            else:\n",
    "                ax.legend().remove()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(hspace=0.4, wspace=0.5)\n",
    "        panel_path = comp_dir / f'metrics_evolution_panel_h{TH}.png'\n",
    "        plt.savefig(panel_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        # Save split images (one per metric) with consistent naming\n",
    "        for metric, title in zip(metrics, titles):\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            if metric != 'TotalPrecipitation':\n",
    "                data = sub_df.groupby(['H', 'Model'])[metric].mean().unstack(fill_value=np.nan)\n",
    "                for i, model in enumerate(data.columns):\n",
    "                    ax.plot(data.index, data[model], marker='o', label=model, color=colors[i % len(colors)],\n",
    "                            linewidth=2.5, markersize=7, markeredgewidth=2, markeredgecolor='white')\n",
    "            else:\n",
    "                data_true = sub_df.groupby(['H', 'Model'])['TotalPrecipitation'].mean().unstack(fill_value=np.nan)\n",
    "                data_pred = sub_df.groupby(['H', 'Model'])['TotalPrecipitation_Pred'].mean().unstack(fill_value=np.nan)\n",
    "                for i, model in enumerate(data_true.columns):\n",
    "                    ax.plot(data_true.index, data_true[model], marker='s', label=f'{model} - True',\n",
    "                            color=colors[i % len(colors)], linewidth=2.5, markersize=7, markeredgecolor='white')\n",
    "                    ax.plot(data_pred.index, data_pred[model], marker='s', label=f'{model} - Pred',\n",
    "                            color=colors[i % len(colors)], linewidth=2.5, linestyle='--', alpha=0.8, markersize=7)\n",
    "            ylabel = metric if metric not in ('TotalPrecipitation', 'TotalPrecipitation_Pred') else 'Total Precipitation (mm)'\n",
    "            ax.set_xlabel('Horizon (months)', fontsize=12)\n",
    "            ax.set_ylabel(ylabel, fontsize=12)\n",
    "            ax.set_title(title, fontsize=14, fontweight='bold', pad=10)\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            ax.set_xticks(sorted(sub_df['H'].unique()))\n",
    "            # Legend outside to the right for every split plot\n",
    "            ax.legend(title='Model', loc='center left', bbox_to_anchor=(1.02, 0.5), frameon=True, fancybox=True, shadow=True, ncol=1)\n",
    "            plt.tight_layout()\n",
    "            out_name = f'metrics_evolution_h{TH}_{metric_suffix[metric]}.png'\n",
    "            plt.savefig(comp_dir / out_name, dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "        # Normalized multi-metric comparison (kept as a single plot)\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        for metric in ['RMSE', 'MAE', 'R^2']:\n",
    "            data = sub_df.groupby(['H', 'Model'])[metric].mean().unstack(fill_value=np.nan)\n",
    "            # Normalize so that 0 = best, 1 = worst (R^2 inverted)\n",
    "            global_min = data.min().min()\n",
    "            global_max = data.max().max()\n",
    "            if np.isfinite(global_min) and np.isfinite(global_max) and (global_max - global_min) > 0:\n",
    "                if metric == 'R^2':\n",
    "                    data_norm = 1 - (data - global_min) / (global_max - global_min + 1e-9)\n",
    "                else:\n",
    "                    data_norm = (data - global_min) / (global_max - global_min + 1e-9)\n",
    "            else:\n",
    "                data_norm = data * 0.0  # fallback if constant or NaN\n",
    "            for i, model in enumerate(data_norm.columns):\n",
    "                linestyle = '-' if metric == 'RMSE' else '--' if metric == 'MAE' else ':'\n",
    "                marker = 'o' if metric == 'RMSE' else 's' if metric == 'MAE' else '^'\n",
    "                ax.plot(data_norm.index, data_norm[model], marker=marker, linewidth=2, linestyle=linestyle,\n",
    "                        label=f'{model} - {metric}', alpha=0.8)\n",
    "        ax.set_xlabel('Horizon (months)', fontsize=12)\n",
    "        ax.set_ylabel('Normalised Metric (0 = best, 1 = worst)', fontsize=12)\n",
    "        ax.set_title('Normalised Comparison of RMSE, MAE & R^2', fontsize=14, fontweight='bold', pad=15)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "        plt.savefig(comp_dir / f'normalized_metrics_comparison_h{TH}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Enhanced plots saved to:\", comp_dir)\n",
    "else:\n",
    "    print(\"No results available in res_df to plot (res_df is missing or empty).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f8208c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f6f8208c",
    "outputId": "5792524f-6dd8-4ff4-ca6c-b30f7971ddec"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Generating visual metrics table...\")\n",
    "\n",
    "# Helper: load res_df from saved CSV if not already in memory\n",
    "def _maybe_load_res_df_from_disk():\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    candidates = []\n",
    "    try:\n",
    "        if 'CONFIG' in globals() and 'out_root' in CONFIG:\n",
    "            candidates += sorted(Path(CONFIG['out_root']).glob('metrics_spatial_v2_refactored_*.csv'))\n",
    "    except Exception:\n",
    "        pass\n",
    "    candidates += sorted(Path('.').glob('metrics_spatial_v2_refactored_*.csv'))\n",
    "    for path in sorted(candidates, key=lambda p: p.stat().st_mtime, reverse=True):\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"Loaded res_df from {path}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Attempt lazy loading if res_df is missing or empty\n",
    "if ('res_df' not in globals()) or (res_df.empty if 'res_df' in globals() else True):\n",
    "    _tmp = _maybe_load_res_df_from_disk()\n",
    "    if _tmp is not None:\n",
    "        res_df = _tmp\n",
    "\n",
    "if not res_df.empty:\n",
    "    summary_data = []\n",
    "    experiments = res_df['Experiment'].unique()\n",
    "    models = res_df['Model'].unique()\n",
    "    headers = ['Experiment', 'Model', 'RMSE', 'MAE', 'R', 'Total Pcp (True)', 'Total Pcp (Pred)', 'Best H']\n",
    "\n",
    "    for exp in experiments:\n",
    "        for model in models:\n",
    "            sub = res_df[(res_df['Experiment'] == exp) & (res_df['Model'] == model)]\n",
    "            if sub.empty:\n",
    "                continue\n",
    "            avg_rmse = sub['RMSE'].mean()\n",
    "            avg_mae = sub['MAE'].mean()\n",
    "            avg_r2 = sub['R^2'].mean()\n",
    "            avg_tp_t = sub['TotalPrecipitation'].mean()\n",
    "            avg_tp_p = sub['TotalPrecipitation_Pred'].mean()\n",
    "            best_h = sub.loc[sub['RMSE'].idxmin(), 'H']\n",
    "            summary_data.append([\n",
    "                exp, model,\n",
    "                f'{avg_rmse:.4f}', f'{avg_mae:.4f}', f'{avg_r2:.4f}',\n",
    "                f'{avg_tp_t:.1f}', f'{avg_tp_p:.1f}', f'H={best_h}'\n",
    "            ])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(17, 10))\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=summary_data, colLabels=headers, cellLoc='center', loc='center',bbox=[0, 0, 1, 0.85])  # Ajustar la posicin vertical de la tabla\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.15, 1.8)\n",
    "\n",
    "    all_rmse = [float(r[2]) for r in summary_data]\n",
    "    all_mae = [float(r[3]) for r in summary_data]\n",
    "    all_r2 = [float(r[4]) for r in summary_data]\n",
    "    all_tp_t = [float(r[5]) for r in summary_data]\n",
    "    all_tp_p = [float(r[6]) for r in summary_data]\n",
    "\n",
    "    for i, row in enumerate(summary_data):\n",
    "        rmse = float(row[2]); mae = float(row[3])\n",
    "        r2 = float(row[4]); tp_t = float(row[5]); tp_p = float(row[6])\n",
    "        rmse_norm = (rmse - min(all_rmse)) / (max(all_rmse) - min(all_rmse) + 1e-9)\n",
    "        mae_norm = (mae - min(all_mae)) / (max(all_mae) - min(all_mae) + 1e-9)\n",
    "        table[(i+1, 2)].set_facecolor(plt.cm.RdYlGn(1 - rmse_norm))\n",
    "        table[(i+1, 3)].set_facecolor(plt.cm.RdYlGn(1 - mae_norm))\n",
    "        r2_norm = (r2 - min(all_r2)) / (max(all_r2) - min(all_r2) + 1e-9)\n",
    "        table[(i+1, 4)].set_facecolor(plt.cm.RdYlGn(r2_norm))\n",
    "        tp_t_norm = (tp_t - min(all_tp_t)) / (max(all_tp_t) - min(all_tp_t) + 1e-9)\n",
    "        tp_p_norm = (tp_p - min(all_tp_p)) / (max(all_tp_p) - min(all_tp_p) + 1e-9)\n",
    "        table[(i+1, 5)].set_facecolor(plt.cm.Blues(tp_t_norm))\n",
    "        table[(i+1, 6)].set_facecolor(plt.cm.Blues(tp_p_norm))\n",
    "        pastel = {'BASIC': '#e8f4f8', 'KCE': '#f0e8f8', 'PAFC': '#f8e8f0'}\n",
    "        table[(i+1, 0)].set_facecolor(pastel.get(row[0], '#ffffff'))\n",
    "\n",
    "    for j in range(len(headers)):\n",
    "        table[(0, j)].set_facecolor('#4a86e8')\n",
    "        table[(0, j)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "    # Use the 'y' parameter to position the suptitle instead.\n",
    "    fig.suptitle('Metrics Summary by Model and Experiment (Green = Better, Red = Worse, Blue = Higher Precipitation)',\n",
    "              fontsize=16, fontweight='bold', y=0.95)\n",
    "    # footnote removed due to encoding issues\n",
    "    fig.text(0.5, 0.01, \"Note: Lower values are better for RMSE and MAE, higher values are better for R and Total Precipitation.\",\n",
    "                 ha='center', va='center',  fontsize=9, style='italic')\n",
    "    plt.subplots_adjust(top=0.85)  # Dar ms espacio arriba para el ttulo\n",
    "    plt.savefig(comp_dir / 'metrics_summary_table.png', dpi=150, bbox_inches='tight', pad_inches=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    # Best overall model\n",
    "    print(\"\\n BEST OVERALL MODEL:\")\n",
    "    res_df['score'] = (\n",
    "        (1 - (res_df['RMSE'] - res_df['RMSE'].min()) / (res_df['RMSE'].max() - res_df['RMSE'].min())) +\n",
    "        (1 - (res_df['MAE'] - res_df['MAE'].min()) / (res_df['MAE'].max() - res_df['MAE'].min())) +\n",
    "        ((res_df['R^2'] - res_df['R^2'].min()) / (res_df['R^2'].max() - res_df['R^2'].min())) +\n",
    "        ((res_df['TotalPrecipitation'] - res_df['TotalPrecipitation'].min()) / (res_df['TotalPrecipitation'].max() - res_df['TotalPrecipitation'].min()))\n",
    "    ) / 4\n",
    "    best = res_df.loc[res_df['score'].idxmax()]\n",
    "    print(f\"Model:                 {best['Model']}\")\n",
    "    print(f\"Experiment:            {best['Experiment']}\")\n",
    "    print(f\"Horizon:               {best['H']}\")\n",
    "    print(f\"RMSE:                  {best['RMSE']:.4f}\")\n",
    "    print(f\"MAE:                   {best['MAE']:.4f}\")\n",
    "    print(f\"R: {best['R^2']:.4f}\")\n",
    "    print(f\"Total Precipitation:   {best['TotalPrecipitation']:.1f}\")\n",
    "    print(f\"Composite score:       {best['score']:.4f}\")\n",
    "\n",
    "print(\"\\n All visualizations generated and saved!\")\n",
    "\n",
    "plt.subplots_adjust(top=0.88)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# MAP EXPORTS (Real vs Pred vs MAPE)\n",
    "# ==================================================\n",
    "import json\n",
    "import pandas as pd\n",
    "import imageio.v2 as imageio\n",
    "import geopandas as gpd\n",
    "\n",
    "SHAPE_DIR = CONFIG[\"base_path\"] / \"data\" / \"input\" / \"shapes\"\n",
    "DEPT_GDF = gpd.read_file(SHAPE_DIR / \"MGN_Departamento.shp\")\n",
    "BOYACA = DEPT_GDF[DEPT_GDF[\"NOMBRE_DPT\"].str.upper() == \"BOYACA\"].copy()\n",
    "\n",
    "# Configuration for map generation\n",
    "PLOT_HORIZONS = CONFIG.get('enabled_horizons', [CONFIG['horizon']])\n",
    "PLOT_EXPERIMENTS = None  # e.g., ['KCE', 'PAFC']; None => all available\n",
    "PLOT_MODELS = None       # e.g., ['ConvLSTM', 'ConvLSTM_Attention']; None => all in MODELS\n",
    "MAP_SAMPLE_INDEX = -1    # which validation window to plot (-1 = most recent)\n",
    "SAVE_EXPORTS = True      # persist predictions/targets/metadata for later reuse\n",
    "MAP_OUT_ROOT = CONFIG['out_root'] / 'map_exports'\n",
    "\n",
    "lats = ds.latitude.values\n",
    "lons = ds.longitude.values\n",
    "EXTENT = [float(lons.min()), float(lons.max()), float(lats.min()), float(lats.max())]\n",
    "\n",
    "\n",
    "def _forecast_dates(val_indices: list[int], horizon: int) -> list[list[str]]:\n",
    "    times = pd.to_datetime(ds.time.values)\n",
    "    dates = []\n",
    "    for start in val_indices:\n",
    "        base = start + CONFIG['input_window']\n",
    "        horizon_dates = []\n",
    "        for h in range(horizon):\n",
    "            idx = base + h\n",
    "            if idx < len(times):\n",
    "                horizon_dates.append(pd.Timestamp(times[idx]).strftime('%Y-%m'))\n",
    "        dates.append(horizon_dates)\n",
    "    return dates\n",
    "\n",
    "\n",
    "def _load_or_generate(exp_name: str, model_name: str, horizon: int,\n",
    "                      data_splits: dict, val_indices: list[int], forecast_dates: list[list[str]]):\n",
    "    export_dir = MAP_OUT_ROOT / f'H{horizon}' / exp_name / model_name\n",
    "    pred_f = export_dir / 'predictions.npy'\n",
    "    targ_f = export_dir / 'targets.npy'\n",
    "    meta_f = export_dir / 'metadata.json'\n",
    "\n",
    "    if pred_f.exists() and targ_f.exists() and meta_f.exists():\n",
    "        y_pred = np.load(pred_f)\n",
    "        y_true = np.load(targ_f)\n",
    "        meta = json.loads(meta_f.read_text())\n",
    "        return y_pred, y_true, meta, export_dir\n",
    "\n",
    "    X_tr, y_tr, X_va, y_va, y_scaler = data_splits[exp_name]\n",
    "    weights_path = CONFIG['out_root'] / f'h{horizon}' / exp_name / 'training_metrics' / f'{model_name}_best_h{horizon}.h5'\n",
    "    if not weights_path.exists():\n",
    "        raise FileNotFoundError(f'Weights not found: {weights_path}')\n",
    "\n",
    "    model = MODELS[model_name](len(CONFIG['feature_sets'][exp_name]), lat, lon, horizon)\n",
    "    model.load_weights(str(weights_path))\n",
    "\n",
    "    pred_batch = max(1, CONFIG.get('prediction_batch_size') or max(1, CONFIG['batch_size']))\n",
    "    y_hat_sc = batch_predict(model, X_va, batch_size=pred_batch)\n",
    "    if y_hat_sc.shape != y_va.shape:\n",
    "        y_hat_sc = y_hat_sc.reshape(y_va.shape)\n",
    "    y_pred = y_scaler.inverse_transform(y_hat_sc.reshape(-1, 1)).reshape(y_va.shape)\n",
    "    y_true = y_scaler.inverse_transform(y_va.reshape(-1, 1)).reshape(y_va.shape)\n",
    "\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "    meta = {\n",
    "        'exp': exp_name,\n",
    "        'model': model_name,\n",
    "        'horizon': horizon,\n",
    "        'input_window': CONFIG['input_window'],\n",
    "        'train_val_split': CONFIG['train_val_split'],\n",
    "        'val_indices': val_indices,\n",
    "        'forecast_dates': forecast_dates,\n",
    "        'sample_index': MAP_SAMPLE_INDEX,\n",
    "        'features': CONFIG['feature_sets'][exp_name],\n",
    "        'weights_path': str(weights_path),\n",
    "    }\n",
    "    if SAVE_EXPORTS:\n",
    "        np.save(pred_f, y_pred.astype(np.float32))\n",
    "        np.save(targ_f, y_true.astype(np.float32))\n",
    "        meta_f.write_text(json.dumps(meta, indent=2))\n",
    "    return y_pred, y_true, meta, export_dir\n",
    "\n",
    "\n",
    "def _plot_triplet(ax, data, title, cmap, vmin=None, vmax=None):\n",
    "    im = ax.imshow(data, origin='lower', extent=EXTENT, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    return im\n",
    "\n",
    "\n",
    "def plot_maps():\n",
    "    horizons = PLOT_HORIZONS\n",
    "    experiments = PLOT_EXPERIMENTS or list(CONFIG['feature_sets'].keys())\n",
    "    models_to_plot = PLOT_MODELS or list(MODELS.keys())\n",
    "\n",
    "    for horizon in horizons:\n",
    "        data_splits_h = preprocess_data(ds, CONFIG, lat, lon, horizon)\n",
    "        _, val_idx = compute_split_indices(len(ds.time), CONFIG['input_window'], horizon, CONFIG['train_val_split'])\n",
    "        forecast_dates = _forecast_dates(val_idx, horizon)\n",
    "\n",
    "        for exp_name in experiments:\n",
    "            if exp_name not in data_splits_h:\n",
    "                print(f'[SKIP] {exp_name} not available for H={horizon}')\n",
    "                continue\n",
    "            for model_name in models_to_plot:\n",
    "                if model_name not in MODELS:\n",
    "                    continue\n",
    "                try:\n",
    "                    y_pred, y_true, meta, export_dir = _load_or_generate(exp_name, model_name, horizon, data_splits_h, val_idx, forecast_dates)\n",
    "                except FileNotFoundError as e:\n",
    "                    print(f'[SKIP] {exp_name} - {model_name} - H{horizon}: {e}')\n",
    "                    continue\n",
    "                if y_pred.size == 0 or y_true.size == 0:\n",
    "                    print(f'[SKIP] {exp_name} - {model_name} - H{horizon}: empty arrays')\n",
    "                    continue\n",
    "                idx = MAP_SAMPLE_INDEX if MAP_SAMPLE_INDEX >= 0 else (len(y_true) - 1)\n",
    "                idx = max(min(idx, len(y_true) - 1), 0)\n",
    "                sample_dates = meta.get('forecast_dates', forecast_dates)[idx]\n",
    "\n",
    "                frames = []\n",
    "                for h in range(min(horizon, len(sample_dates))):\n",
    "                    real = y_true[idx, h, ..., 0]\n",
    "                    pred = y_pred[idx, h, ..., 0]\n",
    "                    err = np.abs(pred - real) / (np.abs(real) + 1e-6) * 100.0\n",
    "                    fig, axes = plt.subplots(1, 3, figsize=(14, 4.5), constrained_layout=True)\n",
    "                    im0 = _plot_triplet(axes[0], real, f'Real {sample_dates[h]}', 'Blues')\n",
    "                    im1 = _plot_triplet(axes[1], pred, f'{model_name} H{h+1} {sample_dates[h]}', 'Blues')\n",
    "                    im2 = _plot_triplet(axes[2], np.clip(err, 0, 100), f'MAPE% {model_name} H{h+1} {sample_dates[h]}', 'Reds', vmin=0, vmax=100)\n",
    "                    try:\n",
    "                        BOYACA.boundary.plot(ax=axes[0], color='k', linewidth=0.8)\n",
    "                        BOYACA.boundary.plot(ax=axes[1], color='k', linewidth=0.8)\n",
    "                        BOYACA.boundary.plot(ax=axes[2], color='k', linewidth=0.8)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] shapefile overlay failed: {e}\")\n",
    "                    cbar0 = fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "                    cbar0.set_label('Precipitation (mm)', rotation=270, labelpad=12)\n",
    "                    cbar1 = fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "                    cbar1.set_label('Precipitation (mm)', rotation=270, labelpad=12)\n",
    "                    cbar2 = fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "                    cbar2.set_label('MAPE (%)', rotation=270, labelpad=12)\n",
    "                    title = f'{exp_name} | {model_name} | H{horizon} | {sample_dates[h]}'\n",
    "                    fig.suptitle(title, fontsize=12)\n",
    "                    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "                    png_path = export_dir / f'{model_name}_H{horizon}_{sample_dates[h]}.png'\n",
    "                    fig.savefig(png_path, dpi=800, bbox_inches='tight')\n",
    "                    plt.close(fig)\n",
    "                    frames.append(imageio.imread(png_path))\n",
    "                if frames:\n",
    "                    gif_path = export_dir / f'{model_name}_H{horizon}.gif'\n",
    "                    imageio.mimsave(gif_path, frames, duration=1.5)\n",
    "                print(f'[OK] {exp_name} - {model_name} - H{horizon}: saved {len(frames)} frames to {export_dir}')\n",
    "\n",
    "plot_maps()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8ee69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "29b8ee69",
    "outputId": "dbdd6681-26f1-4409-b566-7e5f7bc00d71"
   },
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# CLEANUP AND TERMINATION\n",
    "# ==================================================\n",
    "\n",
    "print(\"TRAINING AND ANALYSIS COMPLETED!\")\n",
    "print(f\"Models trained: {len(MODELS)} architectures\")\n",
    "print(f\"Experiments completed: {len(CONFIG['feature_sets'])}\")\n",
    "print(f\"Results saved to: {CONFIG['out_root']}\")\n",
    "\n",
    "# Cleanup\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "print(\"Memory cleared\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nTerminating Colab session to save resources...\")\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()\n",
    "else:\n",
    "    print(\"\\nLocal environment detected. Session remains active.\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "nbformat": 4,
  "nbformat_minor": 0
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
