{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V4 GNN-TAT: Graph Neural Networks + Temporal Attention for Spatiotemporal Precipitation Prediction\n",
    "\n",
    "## STHyMOUNTAIN Framework - Version 4\n",
    "\n",
    "**Author:** Manuel R. Pérez  \n",
    "**Date:** January 2026  \n",
    "**Institution:** UPTC - Pedagogical and Technological University of Colombia\n",
    "\n",
    "---\n",
    "\n",
    "### Innovation Summary\n",
    "\n",
    "This notebook implements **Graph Neural Networks with Temporal Attention (GNN-TAT)** for monthly precipitation prediction. Key innovations:\n",
    "\n",
    "1. **Spatial Graph Representation**: Model grid cells as nodes with edges based on:\n",
    "   - Geographic distance (inverse weighting)\n",
    "   - Elevation similarity (orographic effects)\n",
    "   - Historical precipitation correlation\n",
    "\n",
    "2. **GNN Encoder**: GraphSAGE/GAT architecture to capture non-Euclidean spatial dependencies\n",
    "\n",
    "3. **Temporal Attention**: Multi-head attention over the temporal sequence (60 months)\n",
    "\n",
    "4. **LSTM Decoder**: Sequence-to-sequence prediction with horizon output\n",
    "\n",
    "### Why GNN-TAT?\n",
    "\n",
    "- ConvLSTM assumes regular Euclidean grid → fails to capture orographic effects in mountains\n",
    "- GNN captures non-Euclidean dependencies via learned edge weights\n",
    "- Temporal attention allows focus on relevant historical patterns\n",
    "- Interpretable: edge weights reveal spatial dependencies\n",
    "\n",
    "### Contribution Target\n",
    "\n",
    "- **Paper**: Nature Communications / Water Resources Research (Q1)\n",
    "- **Novelty**: First GNN for monthly precipitation prediction on grid data\n",
    "\n",
    "---\n",
    "\n",
    "**Version History:**\n",
    "- V4.0 (Jan 2026): Initial GNN-TAT implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Environment Detection and Package Installation\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Install PyTorch Geometric - FAST method\n",
    "    # PyG 2.4+ works without torch-scatter/torch-sparse for basic GNN operations\n",
    "    import subprocess\n",
    "\n",
    "    # Direct install - no compilation needed\n",
    "    subprocess.run([\n",
    "        sys.executable, '-m', 'pip', 'install',\n",
    "        'torch-geometric', '-q'\n",
    "    ], check=True)\n",
    "\n",
    "    # Install other dependencies\n",
    "    subprocess.run([\n",
    "        sys.executable, '-m', 'pip', 'install',\n",
    "        'xarray', 'netcdf4', '-q'\n",
    "    ], check=True)\n",
    "\n",
    "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    BASE_PATH = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "    if not BASE_PATH or BASE_PATH == '':\n",
    "        BASE_PATH = r'd:\\github.com\\ninja-marduk\\ml_precipitation_prediction'\n",
    "\n",
    "print(f\"Base path: {BASE_PATH}\")\n",
    "\n",
    "# Add project root to path\n",
    "if BASE_PATH not in sys.path:\n",
    "    sys.path.insert(0, BASE_PATH)\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Core Imports\n",
    "# =============================================================================\n",
    "\n",
    "# Disable TF oneDNN warnings before importing\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch and PyTorch Geometric for GNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# PyTorch Geometric\n",
    "try:\n",
    "    import torch_geometric\n",
    "    from torch_geometric.nn import GCNConv, GATConv, SAGEConv, global_mean_pool\n",
    "    from torch_geometric.data import Data, Batch\n",
    "    from torch_geometric.utils import dense_to_sparse, to_dense_adj\n",
    "    TORCH_GEOMETRIC_AVAILABLE = True\n",
    "    print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "except ImportError:\n",
    "    TORCH_GEOMETRIC_AVAILABLE = False\n",
    "    print(\"WARNING: PyTorch Geometric not available. Install with pip install torch-geometric\")\n",
    "\n",
    "# TensorFlow for compatibility with existing pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras.callbacks import Callback, CSVLogger, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Utilities\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    GEOPANDAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GEOPANDAS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.feature as cfeature\n",
    "    CARTOPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CARTOPY_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import imageio.v2 as imageio\n",
    "    IMAGEIO_AVAILABLE = True\n",
    "except ImportError:\n",
    "    IMAGEIO_AVAILABLE = False\n",
    "\n",
    "# Print versions\n",
    "print(f\"\\nEnvironment Summary:\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  TensorFlow: {tf.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Configuration Dictionary (V2/V3 Compatible) - OPTIMIZED\n",
    "# =============================================================================\n",
    "# Changes: Added parametrized graph construction values, reduced num_gnn_layers\n",
    "\n",
    "CONFIG = {\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Data and Paths\n",
    "    # -------------------------------------------------------------------------\n",
    "    'input_window': 60,           # Number of historical months\n",
    "    'horizon': 12,                # Prediction horizon (months ahead)\n",
    "    'base_path': Path(BASE_PATH),\n",
    "    'data_file': None,            # Auto-set below\n",
    "    'out_root': None,             # Auto-set below\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Model Training\n",
    "    # -------------------------------------------------------------------------\n",
    "    'epochs': 150,\n",
    "    'batch_size': 4,              # Slightly larger for GNN efficiency\n",
    "    'learning_rate': 1e-3,\n",
    "    'patience': 50,               # Early stopping patience\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'prediction_batch_size': 2,\n",
    "    'weight_decay': 1e-5,         # L2 regularization for GNN\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Data Handling\n",
    "    # -------------------------------------------------------------------------\n",
    "    'train_val_split': 0.8,\n",
    "    'light_mode': True,           # Subset for quick testing\n",
    "    'light_grid_size': 5,         # 5x5 pixel subset (25 nodes for GNN)\n",
    "    'loss_weighting': 'uniform',\n",
    "    'save_window_metrics': False,\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Horizons to Run\n",
    "    # -------------------------------------------------------------------------\n",
    "    'enabled_horizons': [12],     # Can be [3, 6, 12] for multi-horizon\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Feature Sets (V2/V3 Compatible)\n",
    "    # -------------------------------------------------------------------------\n",
    "    'feature_sets': {\n",
    "        'BASIC': [\n",
    "            'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "            'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "            'elevation', 'slope', 'aspect'\n",
    "        ],\n",
    "        'KCE': [\n",
    "            'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "            'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "            'elevation', 'slope', 'aspect', 'elev_high', 'elev_med', 'elev_low'\n",
    "        ],\n",
    "        'PAFC': [\n",
    "            'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "            'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "            'elevation', 'slope', 'aspect', 'elev_high', 'elev_med', 'elev_low',\n",
    "            'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag12'\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # GNN-TAT Specific Configuration\n",
    "    # -------------------------------------------------------------------------\n",
    "    'gnn_config': {\n",
    "        'hidden_dim': 64,              # GNN hidden dimension\n",
    "        'num_gnn_layers': 2,           # Number of GNN layers (reduced from 3)\n",
    "        'gnn_type': 'GAT',             # 'GAT', 'SAGE', 'GCN'\n",
    "        'num_heads': 4,                # Attention heads for GAT\n",
    "        'dropout': 0.1,                # Dropout rate\n",
    "\n",
    "        # Temporal Attention\n",
    "        'temporal_hidden_dim': 64,     # Temporal attention hidden dim\n",
    "        'num_temporal_heads': 4,       # Multi-head attention heads\n",
    "        'temporal_dropout': 0.1,\n",
    "\n",
    "        # LSTM Decoder\n",
    "        'lstm_hidden_dim': 64,         # LSTM hidden dimension\n",
    "        'num_lstm_layers': 2,          # Number of LSTM layers\n",
    "\n",
    "        # Graph Construction\n",
    "        'edge_threshold': 0.3,         # Correlation threshold for edges\n",
    "        'max_neighbors': 8,            # Maximum neighbors per node (k-NN)\n",
    "        'use_distance_edges': True,    # Include distance-based edges\n",
    "        'use_elevation_edges': True,   # Include elevation-based edges\n",
    "        'use_correlation_edges': True, # Include correlation-based edges\n",
    "\n",
    "        # Graph construction parameters (previously hardcoded)\n",
    "        'distance_scale_km': 10.0,     # Scale for distance similarity\n",
    "        'elevation_scale': 0.2,        # Scale for elevation similarity\n",
    "        'elevation_weight': 0.3,       # Weight for elevation edges\n",
    "        'correlation_weight': 0.5,     # Weight for correlation edges\n",
    "        'min_edge_weight': 0.01,       # Minimum edge weight to keep\n",
    "    }\n",
    "}\n",
    "\n",
    "# Auto-set paths (V2/V3 Compatible - same dataset)\n",
    "CONFIG['data_file'] = CONFIG['base_path'] / 'data' / 'output' / (\n",
    "    'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    ")\n",
    "CONFIG['out_root'] = CONFIG['base_path'] / 'models' / 'output' / 'V4_GNN_TAT_Models'\n",
    "\n",
    "# Create output directory\n",
    "CONFIG['out_root'].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Input window: {CONFIG['input_window']} months\")\n",
    "print(f\"  Horizon: {CONFIG['horizon']} months\")\n",
    "print(f\"  Light mode: {CONFIG['light_mode']} ({CONFIG['light_grid_size']}x{CONFIG['light_grid_size']} grid)\")\n",
    "print(f\"  GNN type: {CONFIG['gnn_config']['gnn_type']}\")\n",
    "print(f\"  Hidden dim: {CONFIG['gnn_config']['hidden_dim']}\")\n",
    "print(f\"  Output directory: {CONFIG['out_root']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Data Loading Functions (V2/V3 Compatible) - IMPROVED\n",
    "# =============================================================================\n",
    "# Changes: Better feature validation that raises errors for critical missing features\n",
    "\n",
    "def load_and_validate_data(config: Dict) -> Tuple[xr.Dataset, int, int, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load dataset and validate features, applying light mode if enabled.\n",
    "\n",
    "    Returns:\n",
    "        ds: xarray Dataset\n",
    "        lat: number of latitude points\n",
    "        lon: number of longitude points\n",
    "        lat_coords: latitude coordinates array\n",
    "        lon_coords: longitude coordinates array\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If data file doesn't exist\n",
    "        ValueError: If critical features are missing\n",
    "    \"\"\"\n",
    "    data_file = config['data_file']\n",
    "    light_mode = config['light_mode']\n",
    "    light_grid_size = config['light_grid_size']\n",
    "\n",
    "    print(f\"Loading data from: {data_file}\")\n",
    "\n",
    "    # Check if file exists\n",
    "    if not Path(data_file).exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {data_file}\")\n",
    "\n",
    "    # Load dataset\n",
    "    ds = xr.open_dataset(data_file)\n",
    "    print(f\"  Original shape: time={len(ds.time)}, lat={len(ds.latitude)}, lon={len(ds.longitude)}\")\n",
    "\n",
    "    # Optional light mode: subset to center region\n",
    "    if light_mode:\n",
    "        lat_center = len(ds.latitude) // 2\n",
    "        lon_center = len(ds.longitude) // 2\n",
    "        lat_start = lat_center - light_grid_size // 2\n",
    "        lat_end = lat_start + light_grid_size\n",
    "        lon_start = lon_center - light_grid_size // 2\n",
    "        lon_end = lon_start + light_grid_size\n",
    "\n",
    "        ds = ds.isel(\n",
    "            latitude=slice(lat_start, lat_end),\n",
    "            longitude=slice(lon_start, lon_end)\n",
    "        )\n",
    "        print(f\"  Light mode applied: lat={len(ds.latitude)}, lon={len(ds.longitude)}\")\n",
    "\n",
    "    n_lat = len(ds.latitude)\n",
    "    n_lon = len(ds.longitude)\n",
    "    lat_coords = ds.latitude.values\n",
    "    lon_coords = ds.longitude.values\n",
    "\n",
    "    # Validate all features exist\n",
    "    available_vars = set(list(ds.data_vars) + list(ds.coords))\n",
    "    missing_by_exp = {}\n",
    "    critical_missing = []\n",
    "\n",
    "    for exp_name, feats in config['feature_sets'].items():\n",
    "        missing = [f for f in feats if f not in available_vars]\n",
    "        if missing:\n",
    "            missing_by_exp[exp_name] = missing\n",
    "            # Check if critical features are missing\n",
    "            critical = ['total_precipitation']\n",
    "            for c in critical:\n",
    "                if c in missing:\n",
    "                    critical_missing.append(c)\n",
    "\n",
    "    # Report missing features\n",
    "    if missing_by_exp:\n",
    "        print(f\"  WARNING: Missing features by experiment:\")\n",
    "        for exp, feats in missing_by_exp.items():\n",
    "            print(f\"    {exp}: {feats}\")\n",
    "        print(f\"  Available variables: {sorted(list(available_vars))[:30]}...\")\n",
    "\n",
    "        # Raise error for critical missing features\n",
    "        if critical_missing:\n",
    "            raise ValueError(f\"Critical features missing: {critical_missing}. Cannot proceed.\")\n",
    "\n",
    "        # For non-critical, just warn but continue\n",
    "        print(f\"  Continuing with available features...\")\n",
    "    else:\n",
    "        print(f\"  All features validated successfully\")\n",
    "\n",
    "    return ds, n_lat, n_lon, lat_coords, lon_coords\n",
    "\n",
    "\n",
    "# Load data\n",
    "ds, lat, lon, lat_coords, lon_coords = load_and_validate_data(CONFIG)\n",
    "\n",
    "print(f\"\\nData loaded:\")\n",
    "print(f\"  Grid size: {lat} x {lon} = {lat * lon} nodes\")\n",
    "print(f\"  Time steps: {len(ds.time)}\")\n",
    "print(f\"  Latitude range: [{lat_coords.min():.4f}, {lat_coords.max():.4f}]\")\n",
    "print(f\"  Longitude range: [{lon_coords.min():.4f}, {lon_coords.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph Construction for GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Spatial Graph Construction - IMPROVED\n",
    "# =============================================================================\n",
    "# Changes: Uses CONFIG parameters instead of hardcoded magic numbers\n",
    "\n",
    "class SpatialGraphBuilder:\n",
    "    \"\"\"\n",
    "    Builds a spatial graph from precipitation grid data.\n",
    "\n",
    "    Nodes: Each grid cell (lat, lon) is a node\n",
    "    Edges: Based on:\n",
    "        1. Geographic distance (k-NN or threshold)\n",
    "        2. Elevation similarity\n",
    "        3. Historical precipitation correlation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lat_coords: np.ndarray, lon_coords: np.ndarray,\n",
    "                 elevation: np.ndarray, config: Dict):\n",
    "        \"\"\"\n",
    "        Initialize graph builder.\n",
    "\n",
    "        Args:\n",
    "            lat_coords: Latitude coordinates (1D array)\n",
    "            lon_coords: Longitude coordinates (1D array)\n",
    "            elevation: Elevation values (2D array: lat x lon)\n",
    "            config: GNN configuration dictionary\n",
    "        \"\"\"\n",
    "        self.lat_coords = lat_coords\n",
    "        self.lon_coords = lon_coords\n",
    "        self.elevation = elevation\n",
    "        self.config = config['gnn_config']\n",
    "\n",
    "        self.n_lat = len(lat_coords)\n",
    "        self.n_lon = len(lon_coords)\n",
    "        self.n_nodes = self.n_lat * self.n_lon\n",
    "\n",
    "        # Create node positions (lat, lon for each grid cell)\n",
    "        self.node_positions = self._create_node_positions()\n",
    "\n",
    "        # Flatten elevation for node features\n",
    "        self.node_elevations = elevation.flatten()\n",
    "\n",
    "        print(f\"SpatialGraphBuilder initialized:\")\n",
    "        print(f\"  Nodes: {self.n_nodes} ({self.n_lat} x {self.n_lon})\")\n",
    "\n",
    "    def _create_node_positions(self) -> np.ndarray:\n",
    "        \"\"\"Create (lat, lon) positions for each node.\"\"\"\n",
    "        positions = []\n",
    "        for i, lat in enumerate(self.lat_coords):\n",
    "            for j, lon in enumerate(self.lon_coords):\n",
    "                positions.append([lat, lon])\n",
    "        return np.array(positions)\n",
    "\n",
    "    def _node_index(self, i: int, j: int) -> int:\n",
    "        \"\"\"Convert (lat_idx, lon_idx) to flat node index.\"\"\"\n",
    "        return i * self.n_lon + j\n",
    "\n",
    "    def _flat_to_grid(self, idx: int) -> Tuple[int, int]:\n",
    "        \"\"\"Convert flat node index to (lat_idx, lon_idx).\"\"\"\n",
    "        return idx // self.n_lon, idx % self.n_lon\n",
    "\n",
    "    def compute_distance_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute pairwise distance matrix between all nodes.\n",
    "        Uses Haversine formula for geographic coordinates.\n",
    "        \"\"\"\n",
    "        # Convert to radians\n",
    "        pos_rad = np.radians(self.node_positions)\n",
    "\n",
    "        # Haversine distance\n",
    "        lat1 = pos_rad[:, 0:1]\n",
    "        lon1 = pos_rad[:, 1:2]\n",
    "        lat2 = pos_rad[:, 0:1].T\n",
    "        lon2 = pos_rad[:, 1:2].T\n",
    "\n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "\n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "        # Earth radius in km\n",
    "        R = 6371.0\n",
    "        distance_km = R * c\n",
    "\n",
    "        return distance_km\n",
    "\n",
    "    def compute_elevation_similarity(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute elevation similarity matrix.\n",
    "        Similarity = exp(-|elev_i - elev_j| / scale)\n",
    "        \"\"\"\n",
    "        elev = self.node_elevations.reshape(-1, 1)\n",
    "        elev_diff = np.abs(elev - elev.T)\n",
    "\n",
    "        # Normalize by elevation range with configurable scale\n",
    "        elev_range = elev.max() - elev.min() + 1e-6\n",
    "        elevation_scale = self.config.get('elevation_scale', 0.2)\n",
    "        similarity = np.exp(-elev_diff / (elev_range * elevation_scale))\n",
    "\n",
    "        return similarity\n",
    "\n",
    "    def compute_correlation_matrix(self, precip_series: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute precipitation correlation matrix.\n",
    "\n",
    "        Args:\n",
    "            precip_series: Precipitation time series (T, lat, lon)\n",
    "\n",
    "        Returns:\n",
    "            Correlation matrix (n_nodes, n_nodes)\n",
    "        \"\"\"\n",
    "        T = precip_series.shape[0]\n",
    "\n",
    "        # Flatten to (T, n_nodes)\n",
    "        precip_flat = precip_series.reshape(T, -1)\n",
    "\n",
    "        # Handle NaN values\n",
    "        precip_flat = np.nan_to_num(precip_flat, nan=0.0)\n",
    "\n",
    "        # Center the data\n",
    "        precip_centered = precip_flat - precip_flat.mean(axis=0, keepdims=True)\n",
    "\n",
    "        # Compute correlation\n",
    "        std = precip_flat.std(axis=0, keepdims=True) + 1e-8\n",
    "        precip_norm = precip_centered / std\n",
    "\n",
    "        correlation = (precip_norm.T @ precip_norm) / T\n",
    "\n",
    "        # Ensure values in [-1, 1]\n",
    "        correlation = np.clip(correlation, -1, 1)\n",
    "\n",
    "        return correlation\n",
    "\n",
    "    def build_adjacency_matrix(self, precip_series: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Build the weighted adjacency matrix combining all edge types.\n",
    "\n",
    "        Args:\n",
    "            precip_series: Optional precipitation series for correlation edges\n",
    "\n",
    "        Returns:\n",
    "            edge_index: (2, num_edges) tensor of edge indices\n",
    "            edge_weight: (num_edges,) tensor of edge weights\n",
    "        \"\"\"\n",
    "        max_neighbors = self.config['max_neighbors']\n",
    "        edge_threshold = self.config['edge_threshold']\n",
    "\n",
    "        # Get configurable parameters (with defaults for backward compatibility)\n",
    "        distance_scale = self.config.get('distance_scale_km', 10.0)\n",
    "        elevation_weight = self.config.get('elevation_weight', 0.3)\n",
    "        correlation_weight = self.config.get('correlation_weight', 0.5)\n",
    "        min_edge_weight = self.config.get('min_edge_weight', 0.01)\n",
    "\n",
    "        # Initialize adjacency with zeros\n",
    "        adj = np.zeros((self.n_nodes, self.n_nodes))\n",
    "\n",
    "        # 1. Distance-based edges (k-NN)\n",
    "        if self.config['use_distance_edges']:\n",
    "            dist_matrix = self.compute_distance_matrix()\n",
    "\n",
    "            # Convert distance to similarity (inverse distance)\n",
    "            dist_matrix[dist_matrix == 0] = 1e-6  # Avoid division by zero\n",
    "            dist_similarity = 1.0 / (1.0 + dist_matrix / distance_scale)\n",
    "\n",
    "            # Keep only k nearest neighbors\n",
    "            for i in range(self.n_nodes):\n",
    "                # Find k nearest neighbors (excluding self)\n",
    "                neighbors = np.argsort(dist_matrix[i])[:max_neighbors + 1]\n",
    "                neighbors = neighbors[neighbors != i][:max_neighbors]\n",
    "                adj[i, neighbors] += dist_similarity[i, neighbors]\n",
    "\n",
    "            print(f\"  Distance edges added (k={max_neighbors}, scale={distance_scale}km)\")\n",
    "\n",
    "        # 2. Elevation-based edges\n",
    "        if self.config['use_elevation_edges']:\n",
    "            elev_sim = self.compute_elevation_similarity()\n",
    "\n",
    "            # Add elevation similarity to existing edges\n",
    "            adj += elev_sim * elevation_weight\n",
    "            print(f\"  Elevation edges added (weight={elevation_weight})\")\n",
    "\n",
    "        # 3. Correlation-based edges\n",
    "        if self.config['use_correlation_edges'] and precip_series is not None:\n",
    "            corr_matrix = self.compute_correlation_matrix(precip_series)\n",
    "\n",
    "            # Only keep strong positive correlations\n",
    "            corr_edges = np.maximum(corr_matrix - edge_threshold, 0)\n",
    "            adj += corr_edges * correlation_weight\n",
    "            print(f\"  Correlation edges added (threshold={edge_threshold}, weight={correlation_weight})\")\n",
    "\n",
    "        # Remove self-loops (GCN will add them back if needed via add_self_loops=True)\n",
    "        np.fill_diagonal(adj, 0)\n",
    "\n",
    "        # Normalize\n",
    "        adj_max = adj.max()\n",
    "        if adj_max > 0:\n",
    "            adj = adj / adj_max\n",
    "\n",
    "        # Symmetrize\n",
    "        adj = (adj + adj.T) / 2\n",
    "\n",
    "        # Convert to sparse format (edge_index, edge_weight)\n",
    "        edge_index = []\n",
    "        edge_weight = []\n",
    "\n",
    "        for i in range(self.n_nodes):\n",
    "            for j in range(self.n_nodes):\n",
    "                if adj[i, j] > min_edge_weight:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(adj[i, j])\n",
    "\n",
    "        edge_index = np.array(edge_index).T  # Shape: (2, num_edges)\n",
    "        edge_weight = np.array(edge_weight)  # Shape: (num_edges,)\n",
    "\n",
    "        print(f\"  Total edges: {len(edge_weight)}\")\n",
    "        print(f\"  Average degree: {len(edge_weight) / self.n_nodes:.2f}\")\n",
    "\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "    def get_node_features(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get static node features (position, elevation).\n",
    "\n",
    "        Returns:\n",
    "            Node features (n_nodes, n_features)\n",
    "        \"\"\"\n",
    "        features = np.column_stack([\n",
    "            self.node_positions,  # (lat, lon)\n",
    "            self.node_elevations.reshape(-1, 1)  # elevation\n",
    "        ])\n",
    "\n",
    "        # Normalize\n",
    "        features = (features - features.mean(axis=0)) / (features.std(axis=0) + 1e-8)\n",
    "\n",
    "        return features.astype(np.float32)\n",
    "\n",
    "\n",
    "# Build spatial graph\n",
    "print(\"\\nBuilding spatial graph...\")\n",
    "\n",
    "# Get elevation data\n",
    "if 'elevation' in ds.data_vars:\n",
    "    elevation = ds['elevation'].values\n",
    "    if len(elevation.shape) == 3:  # (time, lat, lon)\n",
    "        elevation = elevation[0]  # Take first timestep (static)\n",
    "        print(\"  Note: Elevation has time dimension, using first timestep\")\n",
    "elif 'elevation' in ds.coords:\n",
    "    elevation = ds.coords['elevation'].values\n",
    "else:\n",
    "    print(\"WARNING: Elevation not found, using zeros\")\n",
    "    elevation = np.zeros((lat, lon))\n",
    "\n",
    "# Get precipitation data for correlation\n",
    "precip_var = 'total_precipitation' if 'total_precipitation' in ds.data_vars else list(ds.data_vars)[0]\n",
    "precip_series = ds[precip_var].values\n",
    "\n",
    "# Build graph\n",
    "graph_builder = SpatialGraphBuilder(lat_coords, lon_coords, elevation, CONFIG)\n",
    "edge_index, edge_weight = graph_builder.build_adjacency_matrix(precip_series)\n",
    "static_node_features = graph_builder.get_node_features()\n",
    "\n",
    "print(f\"\\nGraph built:\")\n",
    "print(f\"  Nodes: {graph_builder.n_nodes}\")\n",
    "print(f\"  Edges: {len(edge_weight)}\")\n",
    "print(f\"  Node features shape: {static_node_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Visualize Graph Structure\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_graph(graph_builder: SpatialGraphBuilder, edge_index: np.ndarray, \n",
    "                   edge_weight: np.ndarray, title: str = \"Spatial Graph\"):\n",
    "    \"\"\"\n",
    "    Visualize the spatial graph structure.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # 1. Node positions with edges\n",
    "    ax1 = axes[0]\n",
    "    pos = graph_builder.node_positions\n",
    "    \n",
    "    # Draw edges (sample if too many)\n",
    "    n_edges_to_draw = min(500, len(edge_weight))\n",
    "    edge_indices = np.random.choice(len(edge_weight), n_edges_to_draw, replace=False)\n",
    "    \n",
    "    for idx in edge_indices:\n",
    "        i, j = edge_index[0, idx], edge_index[1, idx]\n",
    "        ax1.plot([pos[i, 1], pos[j, 1]], [pos[i, 0], pos[j, 0]], \n",
    "                'b-', alpha=0.2, linewidth=0.5)\n",
    "    \n",
    "    # Draw nodes\n",
    "    scatter = ax1.scatter(pos[:, 1], pos[:, 0], c=graph_builder.node_elevations, \n",
    "                         cmap='terrain', s=100, edgecolors='black', linewidth=0.5)\n",
    "    plt.colorbar(scatter, ax=ax1, label='Elevation (m)')\n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    ax1.set_title('Graph Structure (colored by elevation)')\n",
    "    \n",
    "    # 2. Edge weight distribution\n",
    "    ax2 = axes[1]\n",
    "    ax2.hist(edge_weight, bins=50, color='steelblue', edgecolor='black')\n",
    "    ax2.set_xlabel('Edge Weight')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Edge Weight Distribution')\n",
    "    ax2.axvline(np.mean(edge_weight), color='red', linestyle='--', label=f'Mean: {np.mean(edge_weight):.3f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Node degree distribution\n",
    "    ax3 = axes[2]\n",
    "    degrees = np.bincount(edge_index[0], minlength=graph_builder.n_nodes)\n",
    "    ax3.hist(degrees, bins=20, color='forestgreen', edgecolor='black')\n",
    "    ax3.set_xlabel('Node Degree')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Node Degree Distribution')\n",
    "    ax3.axvline(np.mean(degrees), color='red', linestyle='--', label=f'Mean: {np.mean(degrees):.1f}')\n",
    "    ax3.legend()\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_path = CONFIG['out_root'] / 'graph_visualization.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Graph visualization saved to: {fig_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize\n",
    "visualize_graph(graph_builder, edge_index, edge_weight, \n",
    "               title=f\"GNN-TAT Spatial Graph ({lat}x{lon} grid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Data Preprocessing (V2/V3 Compatible)\n",
    "# =============================================================================\n",
    "\n",
    "def windowed_arrays(X: np.ndarray, y: np.ndarray, input_window: int, \n",
    "                   horizon: int, start_indices: List[int] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create windowed arrays for sequence-to-sequence learning.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features (T, lat, lon, n_features)\n",
    "        y: Target variable (T, lat, lon)\n",
    "        input_window: Number of input timesteps\n",
    "        horizon: Number of output timesteps\n",
    "        start_indices: Optional list of valid start indices\n",
    "    \n",
    "    Returns:\n",
    "        seq_X: (N, input_window, lat, lon, n_features)\n",
    "        seq_y: (N, horizon, lat, lon)\n",
    "    \"\"\"\n",
    "    seq_X, seq_y = [], []\n",
    "    T = len(X)\n",
    "    \n",
    "    if T < (input_window + horizon):\n",
    "        raise ValueError(f\"Not enough timesteps ({T}) for windows\")\n",
    "    \n",
    "    if start_indices is None:\n",
    "        start_indices = range(T - input_window - horizon + 1)\n",
    "    \n",
    "    for start in start_indices:\n",
    "        end_w = start + input_window\n",
    "        end_y = end_w + horizon\n",
    "        \n",
    "        if end_y > T:\n",
    "            continue\n",
    "        \n",
    "        Xw = X[start:end_w]\n",
    "        yw = y[end_w:end_y]\n",
    "        \n",
    "        # Skip windows with NaNs\n",
    "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
    "            continue\n",
    "        \n",
    "        seq_X.append(Xw)\n",
    "        seq_y.append(yw)\n",
    "    \n",
    "    if len(seq_X) == 0:\n",
    "        raise ValueError(\"No valid windows created\")\n",
    "    \n",
    "    return np.asarray(seq_X, dtype=np.float32), np.asarray(seq_y, dtype=np.float32)\n",
    "\n",
    "\n",
    "def compute_split_indices(total_steps: int, input_window: int, horizon: int, \n",
    "                         split_ratio: float) -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Return train/validation start indices ensuring no temporal leakage.\n",
    "    \n",
    "    Args:\n",
    "        total_steps: Total number of timesteps\n",
    "        input_window: Input window size\n",
    "        horizon: Prediction horizon\n",
    "        split_ratio: Train ratio (e.g., 0.8)\n",
    "    \n",
    "    Returns:\n",
    "        train_indices: List of training window start indices\n",
    "        val_indices: List of validation window start indices\n",
    "    \"\"\"\n",
    "    cutoff = int(total_steps * split_ratio)\n",
    "    max_start = total_steps - input_window - horizon + 1\n",
    "    \n",
    "    # Training indices: before cutoff\n",
    "    train_end = max(0, cutoff - input_window - horizon + 1)\n",
    "    train_indices = list(range(0, train_end))\n",
    "    \n",
    "    # Validation indices: from cutoff onward\n",
    "    val_start = min(max_start, max(cutoff, 0))\n",
    "    val_indices = list(range(val_start, max_start))\n",
    "    \n",
    "    if not train_indices:\n",
    "        raise ValueError(\"Empty training set\")\n",
    "    if not val_indices:\n",
    "        raise ValueError(\"Empty validation set\")\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "\n",
    "\n",
    "def preprocess_data(ds: xr.Dataset, config: Dict, n_lat: int, n_lon: int, \n",
    "                   horizon: int) -> Dict[str, Tuple]:\n",
    "    \"\"\"\n",
    "    Preprocess data for all feature sets.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping experiment name to (X_tr, y_tr, X_va, y_va, scaler)\n",
    "    \"\"\"\n",
    "    input_window = config['input_window']\n",
    "    split_ratio = config['train_val_split']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for exp_name, feature_list in config['feature_sets'].items():\n",
    "        print(f\"\\nProcessing experiment: {exp_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract features\n",
    "            feature_arrays = []\n",
    "            for feat in feature_list:\n",
    "                if feat in ds.data_vars:\n",
    "                    arr = ds[feat].values\n",
    "                elif feat in ds.coords:\n",
    "                    # Broadcast coordinate to full grid\n",
    "                    if feat == 'time':\n",
    "                        continue\n",
    "                    arr = np.broadcast_to(\n",
    "                        ds.coords[feat].values.reshape(-1, 1, 1),\n",
    "                        (len(ds.time), n_lat, n_lon)\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"  WARNING: Feature '{feat}' not found, skipping\")\n",
    "                    continue\n",
    "                \n",
    "                # Ensure 3D shape (T, lat, lon)\n",
    "                if len(arr.shape) == 2:\n",
    "                    # Static feature: broadcast to time\n",
    "                    arr = np.broadcast_to(arr, (len(ds.time), n_lat, n_lon))\n",
    "                \n",
    "                feature_arrays.append(arr)\n",
    "            \n",
    "            # Stack features: (T, lat, lon, n_features)\n",
    "            X = np.stack(feature_arrays, axis=-1).astype(np.float32)\n",
    "            \n",
    "            # Target: total precipitation\n",
    "            y = ds['total_precipitation'].values.astype(np.float32)\n",
    "            \n",
    "            print(f\"  X shape: {X.shape}\")\n",
    "            print(f\"  y shape: {y.shape}\")\n",
    "            \n",
    "            # Handle NaN\n",
    "            X = np.nan_to_num(X, nan=0.0)\n",
    "            y = np.nan_to_num(y, nan=0.0)\n",
    "            \n",
    "            # Compute split indices\n",
    "            T = len(ds.time)\n",
    "            train_idx, val_idx = compute_split_indices(T, input_window, horizon, split_ratio)\n",
    "            \n",
    "            print(f\"  Train windows: {len(train_idx)}, Val windows: {len(val_idx)}\")\n",
    "            \n",
    "            # Create windows\n",
    "            X_tr, y_tr = windowed_arrays(X, y, input_window, horizon, train_idx)\n",
    "            X_va, y_va = windowed_arrays(X, y, input_window, horizon, val_idx)\n",
    "            \n",
    "            # Scale features\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "            \n",
    "            # Fit on training data only\n",
    "            X_tr_flat = X_tr.reshape(-1, X_tr.shape[-1])\n",
    "            y_tr_flat = y_tr.reshape(-1, 1)\n",
    "            \n",
    "            scaler_X.fit(X_tr_flat)\n",
    "            scaler_y.fit(y_tr_flat)\n",
    "            \n",
    "            # Transform\n",
    "            X_tr_shape = X_tr.shape\n",
    "            X_va_shape = X_va.shape\n",
    "            y_tr_shape = y_tr.shape\n",
    "            y_va_shape = y_va.shape\n",
    "            \n",
    "            X_tr_sc = scaler_X.transform(X_tr.reshape(-1, X_tr.shape[-1])).reshape(X_tr_shape)\n",
    "            X_va_sc = scaler_X.transform(X_va.reshape(-1, X_va.shape[-1])).reshape(X_va_shape)\n",
    "            y_tr_sc = scaler_y.transform(y_tr.reshape(-1, 1)).reshape(y_tr_shape)\n",
    "            y_va_sc = scaler_y.transform(y_va.reshape(-1, 1)).reshape(y_va_shape)\n",
    "            \n",
    "            # Add channel dimension to y: (N, H, lat, lon) -> (N, H, lat, lon, 1)\n",
    "            y_tr_sc = np.expand_dims(y_tr_sc, -1)\n",
    "            y_va_sc = np.expand_dims(y_va_sc, -1)\n",
    "            \n",
    "            print(f\"  Final shapes: X_tr={X_tr_sc.shape}, y_tr={y_tr_sc.shape}\")\n",
    "            print(f\"                X_va={X_va_sc.shape}, y_va={y_va_sc.shape}\")\n",
    "            \n",
    "            results[exp_name] = (X_tr_sc, y_tr_sc, X_va_sc, y_va_sc, scaler_y)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Preprocess data for default horizon\n",
    "print(\"Preprocessing data...\")\n",
    "data_splits = preprocess_data(ds, CONFIG, lat, lon, CONFIG['horizon'])\n",
    "\n",
    "print(f\"\\nPreprocessing complete. Available experiments: {list(data_splits.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GNN-TAT Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: GNN-TAT Model Definition (PyTorch) - OPTIMIZED\n",
    "# =============================================================================\n",
    "# Fixes applied:\n",
    "# - Proper residual connection in TemporalAttention\n",
    "# - Batched GNN processing (no Python loop)\n",
    "# - Reduced model dimensions (~500K params instead of 54M)\n",
    "# - GCN with add_self_loops=True for stability\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Temporal Attention module with proper residual connection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "\n",
    "        # Query, Key, Value projections\n",
    "        self.q_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Residual projection (if dimensions differ)\n",
    "        self.residual_proj = nn.Linear(input_dim, hidden_dim) if input_dim != hidden_dim else nn.Identity()\n",
    "\n",
    "        # Layer norm and dropout\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Scaling factor\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Save input for residual (with projection if needed)\n",
    "        residual = self.residual_proj(x)\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "\n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_probs, V)\n",
    "\n",
    "        # Reshape and project output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        # Proper residual connection: residual + dropout(output)\n",
    "        output = self.layer_norm(residual + self.dropout(output))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SpatialGNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    GNN Encoder for spatial dependencies.\n",
    "    Supports GCN, GAT, and GraphSAGE with proper handling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int,\n",
    "                 gnn_type: str = 'GAT', num_heads: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.gnn_type = gnn_type\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # GNN layers\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            if gnn_type == 'GAT':\n",
    "                layer = GATConv(\n",
    "                    in_channels=hidden_dim,\n",
    "                    out_channels=hidden_dim // num_heads,\n",
    "                    heads=num_heads,\n",
    "                    dropout=dropout,\n",
    "                    concat=True\n",
    "                )\n",
    "            elif gnn_type == 'SAGE':\n",
    "                layer = SAGEConv(\n",
    "                    in_channels=hidden_dim,\n",
    "                    out_channels=hidden_dim,\n",
    "                    aggr='mean'\n",
    "                )\n",
    "            else:  # GCN\n",
    "                layer = GCNConv(\n",
    "                    in_channels=hidden_dim,\n",
    "                    out_channels=hidden_dim,\n",
    "                    add_self_loops=True,  # Add self-loops for GCN stability\n",
    "                    normalize=True\n",
    "                )\n",
    "\n",
    "            self.gnn_layers.append(layer)\n",
    "            self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                edge_weight: torch.Tensor = None) -> torch.Tensor:\n",
    "        # Input projection\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        # GNN layers with residual connections\n",
    "        for gnn, norm in zip(self.gnn_layers, self.norms):\n",
    "            residual = x\n",
    "\n",
    "            if self.gnn_type == 'GAT':\n",
    "                x = gnn(x, edge_index)\n",
    "            elif self.gnn_type == 'SAGE':\n",
    "                # SAGEConv does not support edge_weight\n",
    "                x = gnn(x, edge_index)\n",
    "            else:  # GCN\n",
    "                x = gnn(x, edge_index, edge_weight)\n",
    "\n",
    "            x = F.gelu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = norm(x + residual)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GNN_TAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Neural Network with Temporal Attention - OPTIMIZED.\n",
    "\n",
    "    Key optimizations:\n",
    "    - Batched GNN processing using edge_index replication\n",
    "    - Temporal processing per node (not flattened)\n",
    "    - Reduced hidden dimensions (~500K params instead of 54M)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features: int, n_nodes: int, n_lat: int, n_lon: int,\n",
    "                 horizon: int, config: Dict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        self.n_lat = n_lat\n",
    "        self.n_lon = n_lon\n",
    "        self.horizon = horizon\n",
    "\n",
    "        gnn_cfg = config['gnn_config']\n",
    "        hidden_dim = gnn_cfg['hidden_dim']\n",
    "\n",
    "        # GNN Encoder\n",
    "        self.gnn_encoder = SpatialGNNEncoder(\n",
    "            input_dim=n_features,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=gnn_cfg['num_gnn_layers'],\n",
    "            gnn_type=gnn_cfg['gnn_type'],\n",
    "            num_heads=gnn_cfg['num_heads'],\n",
    "            dropout=gnn_cfg['dropout']\n",
    "        )\n",
    "\n",
    "        # Temporal Attention (per node, efficient)\n",
    "        self.temporal_attention = TemporalAttention(\n",
    "            input_dim=hidden_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=gnn_cfg['num_temporal_heads'],\n",
    "            dropout=gnn_cfg['temporal_dropout']\n",
    "        )\n",
    "\n",
    "        # LSTM for sequence modeling (per node)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=gnn_cfg['num_lstm_layers'],\n",
    "            batch_first=True,\n",
    "            dropout=gnn_cfg['dropout'] if gnn_cfg['num_lstm_layers'] > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Output projection (per node -> horizon predictions)\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(gnn_cfg['dropout']),\n",
    "            nn.Linear(hidden_dim, horizon)\n",
    "        )\n",
    "\n",
    "        self._print_model_info(gnn_cfg)\n",
    "\n",
    "    def _print_model_info(self, gnn_cfg):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"GNN-TAT Model initialized:\")\n",
    "        print(f\"  Input: ({self.n_features} features, {self.n_nodes} nodes)\")\n",
    "        print(f\"  Output: (horizon={self.horizon}, {self.n_lat}x{self.n_lon} grid)\")\n",
    "        print(f\"  GNN type: {gnn_cfg['gnn_type']}\")\n",
    "        print(f\"  Hidden dim: {gnn_cfg['hidden_dim']}\")\n",
    "        print(f\"  Parameters: {total_params:,}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                edge_weight: torch.Tensor = None) -> torch.Tensor:\n",
    "        batch_size, seq_len = x.shape[:2]\n",
    "        device = x.device\n",
    "\n",
    "        # Reshape: (batch, seq_len, lat, lon, features) -> (batch, seq_len, n_nodes, features)\n",
    "        x = x.view(batch_size, seq_len, self.n_nodes, self.n_features)\n",
    "\n",
    "        # BATCHED GNN Processing\n",
    "        x_flat = x.view(batch_size * seq_len, self.n_nodes, self.n_features)\n",
    "\n",
    "        # Replicate edge_index for batch processing\n",
    "        batch_edge_index = self._batch_edge_index(edge_index, batch_size * seq_len, device)\n",
    "        batch_edge_weight = edge_weight.repeat(batch_size * seq_len) if edge_weight is not None else None\n",
    "\n",
    "        # Flatten nodes: (batch * seq_len * n_nodes, n_features)\n",
    "        x_nodes = x_flat.view(-1, self.n_features)\n",
    "\n",
    "        # Apply GNN\n",
    "        gnn_out = self.gnn_encoder(x_nodes, batch_edge_index, batch_edge_weight)\n",
    "\n",
    "        # Reshape back: (batch, seq_len, n_nodes, hidden_dim)\n",
    "        hidden_dim = gnn_out.shape[-1]\n",
    "        gnn_out = gnn_out.view(batch_size, seq_len, self.n_nodes, hidden_dim)\n",
    "\n",
    "        # Temporal Processing (per node)\n",
    "        temporal_in = gnn_out.permute(0, 2, 1, 3)  # (batch, n_nodes, seq_len, hidden)\n",
    "        temporal_in = temporal_in.reshape(batch_size * self.n_nodes, seq_len, hidden_dim)\n",
    "\n",
    "        # Apply temporal attention\n",
    "        temporal_out = self.temporal_attention(temporal_in)\n",
    "\n",
    "        # Apply LSTM\n",
    "        lstm_out, _ = self.lstm(temporal_out)\n",
    "\n",
    "        # Take last timestep: (batch * n_nodes, hidden)\n",
    "        lstm_last = lstm_out[:, -1, :]\n",
    "\n",
    "        # Output Projection\n",
    "        out = self.output_proj(lstm_last)  # (batch * n_nodes, horizon)\n",
    "\n",
    "        # Reshape: (batch, n_nodes, horizon) -> (batch, horizon, lat, lon, 1)\n",
    "        out = out.view(batch_size, self.n_nodes, self.horizon)\n",
    "        out = out.permute(0, 2, 1)  # (batch, horizon, n_nodes)\n",
    "        out = out.view(batch_size, self.horizon, self.n_lat, self.n_lon, 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _batch_edge_index(self, edge_index: torch.Tensor, num_graphs: int,\n",
    "                          device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"Create batched edge_index by replicating and offsetting indices.\"\"\"\n",
    "        num_edges = edge_index.shape[1]\n",
    "\n",
    "        # Create offsets for each graph\n",
    "        offsets = torch.arange(num_graphs, device=device) * self.n_nodes\n",
    "        offsets = offsets.view(-1, 1, 1).expand(-1, 2, num_edges)\n",
    "\n",
    "        # Replicate edge_index and add offsets\n",
    "        batch_edge_index = edge_index.unsqueeze(0).expand(num_graphs, -1, -1)\n",
    "        batch_edge_index = batch_edge_index + offsets\n",
    "\n",
    "        # Flatten: (num_graphs, 2, num_edges) -> (2, num_graphs * num_edges)\n",
    "        batch_edge_index = batch_edge_index.permute(1, 0, 2).reshape(2, -1)\n",
    "\n",
    "        return batch_edge_index\n",
    "\n",
    "\n",
    "# Create model\n",
    "print(\"\\nCreating GNN-TAT model...\")\n",
    "\n",
    "# Get number of features from first experiment\n",
    "first_exp = list(data_splits.keys())[0]\n",
    "n_features = data_splits[first_exp][0].shape[-1]\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = GNN_TAT(\n",
    "    n_features=n_features,\n",
    "    n_nodes=graph_builder.n_nodes,\n",
    "    n_lat=lat,\n",
    "    n_lon=lon,\n",
    "    horizon=CONFIG['horizon'],\n",
    "    config=CONFIG\n",
    ").to(device)\n",
    "\n",
    "# Convert graph to torch tensors\n",
    "edge_index_tensor = torch.tensor(edge_index, dtype=torch.long).to(device)\n",
    "edge_weight_tensor = torch.tensor(edge_weight, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: TensorFlow Wrapper for V2/V3 Compatibility\n",
    "# =============================================================================\n",
    "\n",
    "class TFGNNTATWrapper(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    TensorFlow wrapper for PyTorch GNN-TAT model.\n",
    "    Allows using the same training infrastructure as V2/V3.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pytorch_model: nn.Module, edge_index: torch.Tensor, \n",
    "                 edge_weight: torch.Tensor, device: torch.device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pytorch_model = pytorch_model\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_weight = edge_weight\n",
    "        self.device = device\n",
    "        \n",
    "        # Placeholder for compatibility\n",
    "        self._dummy_layer = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass with TensorFlow inputs.\n",
    "        \"\"\"\n",
    "        # Convert TF tensor to numpy, then to PyTorch\n",
    "        if isinstance(inputs, tf.Tensor):\n",
    "            x_np = inputs.numpy()\n",
    "        else:\n",
    "            x_np = inputs\n",
    "        \n",
    "        x_torch = torch.tensor(x_np, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Set training mode\n",
    "        if training:\n",
    "            self.pytorch_model.train()\n",
    "        else:\n",
    "            self.pytorch_model.eval()\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.set_grad_enabled(training):\n",
    "            output = self.pytorch_model(x_torch, self.edge_index, self.edge_weight)\n",
    "        \n",
    "        # Convert back to TF tensor\n",
    "        return tf.constant(output.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "def build_gnn_tat_model(n_features: int, n_lat: int, n_lon: int, horizon: int,\n",
    "                        graph_builder: SpatialGraphBuilder, config: Dict,\n",
    "                        edge_index: torch.Tensor, edge_weight: torch.Tensor,\n",
    "                        device: torch.device) -> Tuple[nn.Module, TFGNNTATWrapper]:\n",
    "    \"\"\"\n",
    "    Build GNN-TAT model and TF wrapper.\n",
    "    \n",
    "    Returns:\n",
    "        pytorch_model: The PyTorch GNN-TAT model\n",
    "        tf_wrapper: TensorFlow wrapper for compatibility\n",
    "    \"\"\"\n",
    "    pytorch_model = GNN_TAT(\n",
    "        n_features=n_features,\n",
    "        n_nodes=graph_builder.n_nodes,\n",
    "        n_lat=n_lat,\n",
    "        n_lon=n_lon,\n",
    "        horizon=horizon,\n",
    "        config=config\n",
    "    ).to(device)\n",
    "    \n",
    "    tf_wrapper = TFGNNTATWrapper(pytorch_model, edge_index, edge_weight, device)\n",
    "    \n",
    "    return pytorch_model, tf_wrapper\n",
    "\n",
    "\n",
    "# Model registry (V2/V3 compatible format)\n",
    "MODELS_V4_GNN = {\n",
    "    'GNN_TAT_GAT': lambda n_feats, n_lat, n_lon, horizon: build_gnn_tat_model(\n",
    "        n_feats, n_lat, n_lon, horizon, graph_builder, \n",
    "        {**CONFIG, 'gnn_config': {**CONFIG['gnn_config'], 'gnn_type': 'GAT'}},\n",
    "        edge_index_tensor, edge_weight_tensor, device\n",
    "    ),\n",
    "    'GNN_TAT_SAGE': lambda n_feats, n_lat, n_lon, horizon: build_gnn_tat_model(\n",
    "        n_feats, n_lat, n_lon, horizon, graph_builder,\n",
    "        {**CONFIG, 'gnn_config': {**CONFIG['gnn_config'], 'gnn_type': 'SAGE'}},\n",
    "        edge_index_tensor, edge_weight_tensor, device\n",
    "    ),\n",
    "    'GNN_TAT_GCN': lambda n_feats, n_lat, n_lon, horizon: build_gnn_tat_model(\n",
    "        n_feats, n_lat, n_lon, horizon, graph_builder,\n",
    "        {**CONFIG, 'gnn_config': {**CONFIG['gnn_config'], 'gnn_type': 'GCN'}},\n",
    "        edge_index_tensor, edge_weight_tensor, device\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"\\nV4 GNN-TAT Models registered: {list(MODELS_V4_GNN.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: PyTorch Training Functions\n",
    "# =============================================================================\n",
    "\n",
    "class PrecipitationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for precipitation prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def train_pytorch_model(model: nn.Module, X_tr: np.ndarray, y_tr: np.ndarray,\n",
    "                        X_va: np.ndarray, y_va: np.ndarray,\n",
    "                        edge_index: torch.Tensor, edge_weight: torch.Tensor,\n",
    "                        config: Dict, model_name: str, exp_name: str,\n",
    "                        out_dir: Path, horizon: int, device: torch.device) -> Tuple[nn.Module, Dict]:\n",
    "    \"\"\"\n",
    "    Train PyTorch GNN-TAT model.\n",
    "    \n",
    "    Returns:\n",
    "        Trained model and training history\n",
    "    \"\"\"\n",
    "    # Setup output directories\n",
    "    metrics_dir = out_dir / f'h{horizon}' / exp_name / 'training_metrics'\n",
    "    metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_path = metrics_dir / f\"{model_name}_best_h{horizon}.pt\"\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = PrecipitationDataset(X_tr, y_tr)\n",
    "    val_dataset = PrecipitationDataset(X_va, y_va)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config.get('weight_decay', 1e-5)\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=config['patience'] // 2\n",
    "    )\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training tracking\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_mae': [],\n",
    "        'val_mae': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    print(f\"\\nTraining {model_name} on {exp_name}...\")\n",
    "    print(f\"  Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_maes = []\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(batch_X, edge_index, edge_weight)\n",
    "            loss = criterion(output, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            train_maes.append(F.l1_loss(output, batch_y).item())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_maes = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                output = model(batch_X, edge_index, edge_weight)\n",
    "                val_losses.append(criterion(output, batch_y).item())\n",
    "                val_maes.append(F.l1_loss(output, batch_y).item())\n",
    "        \n",
    "        # Compute epoch metrics\n",
    "        epoch_train_loss = np.mean(train_losses)\n",
    "        epoch_val_loss = np.mean(val_losses)\n",
    "        epoch_train_mae = np.mean(train_maes)\n",
    "        epoch_val_mae = np.mean(val_maes)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['train_mae'].append(epoch_train_mae)\n",
    "        history['val_mae'].append(epoch_val_mae)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': best_val_loss,\n",
    "            }, model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"  Epoch {epoch+1:3d}/{config['epochs']}: \"\n",
    "                  f\"train_loss={epoch_train_loss:.4f}, val_loss={epoch_val_loss:.4f}, \"\n",
    "                  f\"MAE={epoch_val_mae:.4f}, lr={current_lr:.2e}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= config['patience']:\n",
    "            print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load(model_path, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Save training history\n",
    "    history_df = pd.DataFrame(history)\n",
    "    history_df.to_csv(metrics_dir / f\"{model_name}_training_log_h{horizon}.csv\", index=False)\n",
    "    \n",
    "    # Save summary\n",
    "    summary = {\n",
    "        'model_name': model_name,\n",
    "        'experiment': exp_name,\n",
    "        'horizon': horizon,\n",
    "        'best_epoch': best_epoch + 1,\n",
    "        'best_val_loss': float(best_val_loss),\n",
    "        'final_train_loss': float(history['train_loss'][-1]),\n",
    "        'final_val_loss': float(history['val_loss'][-1]),\n",
    "        'total_epochs': len(history['train_loss']),\n",
    "        'parameters': sum(p.numel() for p in model.parameters())\n",
    "    }\n",
    "    \n",
    "    with open(metrics_dir / f\"{model_name}_history.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"  Training complete. Best val_loss: {best_val_loss:.4f} at epoch {best_epoch+1}\")\n",
    "    \n",
    "    return model, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: Evaluation Functions (V2/V3 Compatible)\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(model: nn.Module, X_va: np.ndarray, y_va: np.ndarray,\n",
    "                   edge_index: torch.Tensor, edge_weight: torch.Tensor,\n",
    "                   scaler: StandardScaler, device: torch.device,\n",
    "                   model_name: str, exp_name: str, horizon: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate model and compute metrics per horizon step.\n",
    "    \n",
    "    Returns:\n",
    "        List of metric dictionaries (one per horizon step)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    # Predict in batches\n",
    "    batch_size = 4\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_va), batch_size):\n",
    "            batch_X = torch.tensor(X_va[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "            batch_pred = model(batch_X, edge_index, edge_weight)\n",
    "            predictions.append(batch_pred.cpu().numpy())\n",
    "    \n",
    "    y_hat_sc = np.concatenate(predictions, axis=0)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    y_hat = scaler.inverse_transform(y_hat_sc.reshape(-1, 1)).reshape(y_hat_sc.shape)\n",
    "    y_true = scaler.inverse_transform(y_va.reshape(-1, 1)).reshape(y_va.shape)\n",
    "    \n",
    "    # Compute metrics per horizon step\n",
    "    for h in range(horizon):\n",
    "        y_h_true = y_true[:, h].flatten()\n",
    "        y_h_pred = y_hat[:, h].flatten()\n",
    "        \n",
    "        # Metrics\n",
    "        rmse = np.sqrt(np.mean((y_h_pred - y_h_true) ** 2))\n",
    "        mae = np.mean(np.abs(y_h_pred - y_h_true))\n",
    "        \n",
    "        ss_res = np.sum((y_h_true - y_h_pred) ** 2)\n",
    "        ss_tot = np.sum((y_h_true - np.mean(y_h_true)) ** 2)\n",
    "        r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "        \n",
    "        results.append({\n",
    "            'TotalHorizon': horizon,\n",
    "            'Experiment': exp_name,\n",
    "            'Model': model_name,\n",
    "            'H': h + 1,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R^2': r2,\n",
    "            'Mean_True_mm': np.mean(y_h_true),\n",
    "            'Mean_Pred_mm': np.mean(y_h_pred),\n",
    "            'TotalPrecipitation': np.sum(y_h_true),\n",
    "            'TotalPrecipitation_Pred': np.sum(y_h_pred)\n",
    "        })\n",
    "    \n",
    "    return results, y_hat, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: Main Training Loop (V2/V3 Compatible) - IMPROVED\n",
    "# =============================================================================\n",
    "# Changes: Better memory management between experiments\n",
    "\n",
    "# Results collection\n",
    "all_results = []\n",
    "all_histories = {}\n",
    "all_predictions = {}\n",
    "\n",
    "for horizon in CONFIG['enabled_horizons']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running experiments for horizon H={horizon} months\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Preprocess data for this horizon\n",
    "    data_splits_h = preprocess_data(ds, CONFIG, lat, lon, horizon)\n",
    "\n",
    "    for exp_name in CONFIG['feature_sets'].keys():\n",
    "        if exp_name not in data_splits_h:\n",
    "            print(f\"\\nSkipping {exp_name}: data not available\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Experiment: {exp_name} ---\")\n",
    "\n",
    "        X_tr, y_tr, X_va, y_va, scaler = data_splits_h[exp_name]\n",
    "        n_features = X_tr.shape[-1]\n",
    "\n",
    "        for model_name in ['GNN_TAT_GAT', 'GNN_TAT_SAGE', 'GNN_TAT_GCN']:\n",
    "            print(f\"\\n>>> Model: {model_name}\")\n",
    "\n",
    "            # Clear memory BEFORE creating new model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            try:\n",
    "                # Build model\n",
    "                gnn_type = model_name.split('_')[-1]  # GAT, SAGE, or GCN\n",
    "                model_config = {**CONFIG, 'gnn_config': {**CONFIG['gnn_config'], 'gnn_type': gnn_type}}\n",
    "\n",
    "                model = GNN_TAT(\n",
    "                    n_features=n_features,\n",
    "                    n_nodes=graph_builder.n_nodes,\n",
    "                    n_lat=lat,\n",
    "                    n_lon=lon,\n",
    "                    horizon=horizon,\n",
    "                    config=model_config\n",
    "                ).to(device)\n",
    "\n",
    "                # Train\n",
    "                model, history = train_pytorch_model(\n",
    "                    model, X_tr, y_tr, X_va, y_va,\n",
    "                    edge_index_tensor, edge_weight_tensor,\n",
    "                    CONFIG, model_name, exp_name,\n",
    "                    CONFIG['out_root'], horizon, device\n",
    "                )\n",
    "\n",
    "                all_histories[f\"{exp_name}_{model_name}_H{horizon}\"] = history\n",
    "\n",
    "                # Evaluate\n",
    "                results, y_hat, y_true = evaluate_model(\n",
    "                    model, X_va, y_va,\n",
    "                    edge_index_tensor, edge_weight_tensor,\n",
    "                    scaler, device,\n",
    "                    model_name, exp_name, horizon\n",
    "                )\n",
    "\n",
    "                all_results.extend(results)\n",
    "                all_predictions[f\"{exp_name}_{model_name}_H{horizon}\"] = {\n",
    "                    'y_hat': y_hat,\n",
    "                    'y_true': y_true\n",
    "                }\n",
    "\n",
    "                # Print summary metrics\n",
    "                avg_rmse = np.mean([r['RMSE'] for r in results])\n",
    "                avg_mae = np.mean([r['MAE'] for r in results])\n",
    "                avg_r2 = np.mean([r['R^2'] for r in results])\n",
    "                print(f\"  Results: RMSE={avg_rmse:.2f}mm, MAE={avg_mae:.2f}mm, R2={avg_r2:.3f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "            finally:\n",
    "                # ALWAYS cleanup after each model\n",
    "                if 'model' in dir():\n",
    "                    del model\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "        # Cleanup after each experiment\n",
    "        del X_tr, y_tr, X_va, y_va\n",
    "        gc.collect()\n",
    "\n",
    "    # Cleanup after each horizon\n",
    "    del data_splits_h\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training complete!\")\n",
    "print(f\"Total experiments: {len(all_results) // CONFIG['horizon'] if all_results else 0}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Export and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: Save Results (V2/V3 Compatible Format)\n",
    "# =============================================================================\n",
    "\n",
    "# Create results DataFrame\n",
    "res_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Add bias metrics\n",
    "res_df['mean_bias_mm'] = res_df['Mean_Pred_mm'] - res_df['Mean_True_mm']\n",
    "res_df['mean_bias_pct'] = 100 * res_df['mean_bias_mm'] / (res_df['Mean_True_mm'].replace(0, np.nan))\n",
    "\n",
    "# Save to CSV\n",
    "horizon_label = 'multi' if len(CONFIG['enabled_horizons']) > 1 else str(CONFIG['enabled_horizons'][0])\n",
    "out_csv = CONFIG['out_root'] / f'metrics_spatial_v4_gnn_tat_h{horizon_label}.csv'\n",
    "res_df.to_csv(out_csv, index=False)\n",
    "\n",
    "print(f\"Results saved to: {out_csv}\")\n",
    "print(f\"\\nResults Summary:\")\n",
    "print(res_df.groupby(['Model', 'Experiment'])[['RMSE', 'MAE', 'R^2']].mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: Visualization - Model Comparison\n",
    "# =============================================================================\n",
    "\n",
    "def plot_model_comparison(res_df: pd.DataFrame, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Plot model comparison charts.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. RMSE by Model and Experiment\n",
    "    ax1 = axes[0, 0]\n",
    "    pivot_rmse = res_df.groupby(['Model', 'Experiment'])['RMSE'].mean().unstack()\n",
    "    pivot_rmse.plot(kind='bar', ax=ax1, width=0.8)\n",
    "    ax1.set_ylabel('RMSE (mm)')\n",
    "    ax1.set_title('RMSE by Model and Feature Set')\n",
    "    ax1.legend(title='Feature Set')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. R² by Model and Experiment\n",
    "    ax2 = axes[0, 1]\n",
    "    pivot_r2 = res_df.groupby(['Model', 'Experiment'])['R^2'].mean().unstack()\n",
    "    pivot_r2.plot(kind='bar', ax=ax2, width=0.8)\n",
    "    ax2.set_ylabel('R²')\n",
    "    ax2.set_title('R² by Model and Feature Set')\n",
    "    ax2.legend(title='Feature Set')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.axhline(y=0.437, color='red', linestyle='--', label='V2 Baseline (0.437)')\n",
    "    \n",
    "    # 3. RMSE by Horizon\n",
    "    ax3 = axes[1, 0]\n",
    "    for model in res_df['Model'].unique():\n",
    "        model_data = res_df[res_df['Model'] == model]\n",
    "        for exp in model_data['Experiment'].unique():\n",
    "            exp_data = model_data[model_data['Experiment'] == exp]\n",
    "            ax3.plot(exp_data['H'], exp_data['RMSE'], \n",
    "                    marker='o', label=f\"{model} ({exp})\")\n",
    "    ax3.set_xlabel('Horizon (H)')\n",
    "    ax3.set_ylabel('RMSE (mm)')\n",
    "    ax3.set_title('RMSE Degradation by Horizon')\n",
    "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Bias Distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    for model in res_df['Model'].unique():\n",
    "        model_bias = res_df[res_df['Model'] == model]['mean_bias_mm']\n",
    "        ax4.hist(model_bias, bins=20, alpha=0.5, label=model)\n",
    "    ax4.axvline(x=0, color='red', linestyle='--')\n",
    "    ax4.set_xlabel('Mean Bias (mm)')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Bias Distribution by Model')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    fig_path = output_dir / 'model_comparison_v4_gnn_tat.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Comparison plot saved to: {fig_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    plot_model_comparison(res_df, CONFIG['out_root'])\n",
    "else:\n",
    "    print(\"No results to plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: Graph Edge Analysis (GNN Interpretability)\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_graph_edges(graph_builder: SpatialGraphBuilder, edge_index: np.ndarray,\n",
    "                        edge_weight: np.ndarray, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Analyze and visualize graph edges for interpretability.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # 1. Edge weight heatmap (adjacency matrix)\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Reconstruct adjacency matrix\n",
    "    adj_matrix = np.zeros((graph_builder.n_nodes, graph_builder.n_nodes))\n",
    "    for idx in range(len(edge_weight)):\n",
    "        i, j = edge_index[0, idx], edge_index[1, idx]\n",
    "        adj_matrix[i, j] = edge_weight[idx]\n",
    "    \n",
    "    im1 = ax1.imshow(adj_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    plt.colorbar(im1, ax=ax1, label='Edge Weight')\n",
    "    ax1.set_xlabel('Node Index')\n",
    "    ax1.set_ylabel('Node Index')\n",
    "    ax1.set_title('Adjacency Matrix (Edge Weights)')\n",
    "    \n",
    "    # 2. Strongest edges by location\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    # Get top 10% strongest edges\n",
    "    threshold = np.percentile(edge_weight, 90)\n",
    "    strong_edges = edge_weight > threshold\n",
    "    \n",
    "    pos = graph_builder.node_positions\n",
    "    \n",
    "    # Draw all nodes\n",
    "    scatter = ax2.scatter(pos[:, 1], pos[:, 0], c=graph_builder.node_elevations,\n",
    "                         cmap='terrain', s=100, edgecolors='black', linewidth=0.5,\n",
    "                         zorder=2)\n",
    "    plt.colorbar(scatter, ax=ax2, label='Elevation (m)')\n",
    "    \n",
    "    # Draw strong edges\n",
    "    for idx in np.where(strong_edges)[0]:\n",
    "        i, j = edge_index[0, idx], edge_index[1, idx]\n",
    "        ax2.plot([pos[i, 1], pos[j, 1]], [pos[i, 0], pos[j, 0]],\n",
    "                'r-', alpha=0.6, linewidth=edge_weight[idx] * 2, zorder=1)\n",
    "    \n",
    "    ax2.set_xlabel('Longitude')\n",
    "    ax2.set_ylabel('Latitude')\n",
    "    ax2.set_title(f'Top 10% Strongest Edges (n={strong_edges.sum()})')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    fig_path = output_dir / 'graph_edge_analysis.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"Edge analysis saved to: {fig_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nGraph Statistics:\")\n",
    "    print(f\"  Total nodes: {graph_builder.n_nodes}\")\n",
    "    print(f\"  Total edges: {len(edge_weight)}\")\n",
    "    print(f\"  Average edge weight: {np.mean(edge_weight):.4f}\")\n",
    "    print(f\"  Max edge weight: {np.max(edge_weight):.4f}\")\n",
    "    print(f\"  Strong edges (top 10%): {strong_edges.sum()}\")\n",
    "\n",
    "\n",
    "analyze_graph_edges(graph_builder, edge_index, edge_weight, CONFIG['out_root'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 16: Export Predictions for Map Generation\n",
    "# =============================================================================\n",
    "\n",
    "def export_predictions(predictions_dict: Dict, output_dir: Path, horizon: int):\n",
    "    \"\"\"\n",
    "    Export predictions for map generation (V2/V3 compatible format).\n",
    "    \"\"\"\n",
    "    map_out_root = output_dir / 'map_exports'\n",
    "    \n",
    "    for key, data in predictions_dict.items():\n",
    "        parts = key.split('_')\n",
    "        exp_name = parts[0]\n",
    "        model_name = '_'.join(parts[1:-1])\n",
    "        \n",
    "        export_dir = map_out_root / f'H{horizon}' / exp_name / model_name\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save arrays\n",
    "        np.save(export_dir / 'predictions.npy', data['y_hat'])\n",
    "        np.save(export_dir / 'targets.npy', data['y_true'])\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'model': model_name,\n",
    "            'experiment': exp_name,\n",
    "            'horizon': horizon,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'shape': list(data['y_hat'].shape),\n",
    "            'rmse_mean': float(np.sqrt(np.mean((data['y_hat'] - data['y_true'])**2))),\n",
    "            'framework': 'GNN-TAT V4'\n",
    "        }\n",
    "        \n",
    "        with open(export_dir / 'metadata.json', 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"Exported: {key}\")\n",
    "    \n",
    "    print(f\"\\nAll predictions exported to: {map_out_root}\")\n",
    "\n",
    "\n",
    "if len(all_predictions) > 0:\n",
    "    export_predictions(all_predictions, CONFIG['out_root'], CONFIG['horizon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 17: Final Summary\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"V4 GNN-TAT Training Complete\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    # Summary statistics\n",
    "    summary_df = res_df.groupby(['Model', 'Experiment']).agg({\n",
    "        'RMSE': ['mean', 'std'],\n",
    "        'MAE': ['mean', 'std'],\n",
    "        'R^2': ['mean', 'std'],\n",
    "        'mean_bias_mm': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(summary_df)\n",
    "    \n",
    "    # Best model\n",
    "    best_idx = res_df.groupby(['Model', 'Experiment'])['R^2'].mean().idxmax()\n",
    "    best_model, best_exp = best_idx\n",
    "    best_r2 = res_df.groupby(['Model', 'Experiment'])['R^2'].mean().max()\n",
    "    best_rmse = res_df[(res_df['Model'] == best_model) & (res_df['Experiment'] == best_exp)]['RMSE'].mean()\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model} with {best_exp} features\")\n",
    "    print(f\"  R²: {best_r2:.3f}\")\n",
    "    print(f\"  RMSE: {best_rmse:.2f} mm\")\n",
    "    \n",
    "    # Comparison with V2 baseline\n",
    "    v2_baseline_r2 = 0.437\n",
    "    v2_baseline_rmse = 98.17\n",
    "    \n",
    "    r2_improvement = (best_r2 - v2_baseline_r2) / v2_baseline_r2 * 100\n",
    "    rmse_improvement = (v2_baseline_rmse - best_rmse) / v2_baseline_rmse * 100\n",
    "    \n",
    "    print(f\"\\nComparison with V2 ConvLSTM Baseline:\")\n",
    "    print(f\"  V2 Baseline: R²={v2_baseline_r2:.3f}, RMSE={v2_baseline_rmse:.2f}mm\")\n",
    "    print(f\"  V4 GNN-TAT:  R²={best_r2:.3f}, RMSE={best_rmse:.2f}mm\")\n",
    "    print(f\"  Improvement: R² {r2_improvement:+.1f}%, RMSE {rmse_improvement:+.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Output Files:\")\n",
    "print(f\"  Metrics CSV: {CONFIG['out_root'] / f'metrics_spatial_v4_gnn_tat_h{horizon_label}.csv'}\")\n",
    "print(f\"  Visualizations: {CONFIG['out_root']}\")\n",
    "print(f\"  Predictions: {CONFIG['out_root'] / 'map_exports'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 18: Save Notebook State\n",
    "# =============================================================================\n",
    "\n",
    "# Save complete state for reproducibility\n",
    "state = {\n",
    "    'config': {k: str(v) if isinstance(v, Path) else v for k, v in CONFIG.items() if k != 'feature_sets'},\n",
    "    'feature_sets': CONFIG['feature_sets'],\n",
    "    'gnn_config': CONFIG['gnn_config'],\n",
    "    'grid_info': {\n",
    "        'lat': lat,\n",
    "        'lon': lon,\n",
    "        'n_nodes': graph_builder.n_nodes,\n",
    "        'n_edges': len(edge_weight)\n",
    "    },\n",
    "    'training_summaries': all_histories,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "state_path = CONFIG['out_root'] / 'experiment_state_v4.json'\n",
    "with open(state_path, 'w') as f:\n",
    "    json.dump(state, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Experiment state saved to: {state_path}\")\n",
    "print(\"\\nV4 GNN-TAT notebook execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}