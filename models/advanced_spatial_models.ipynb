{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/advanced_spatial_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CozjYqgoXqrJ"
      },
      "source": [
        "# 🚀 Proposed Improvements for Spatial Precipitation Models\n",
        "\n",
        "### 1. Hyperparameter Optimization\n",
        "\n",
        "| Parameter     | Original Value | Improved Value | Justification                 |\n",
        "| ------------- | -------------- | -------------- | ----------------------------- |\n",
        "| Batch Size    | 4              | **16**         | Greater gradient stability    |\n",
        "| Learning Rate | 1e-3           | **5e-4**       | Smoother convergence          |\n",
        "| Epochs        | 50             | **100**        | More time with early stopping |\n",
        "| Patience      | 6              | **10**         | Prevent premature stopping    |\n",
        "| Dropout       | 0              | **0.2**        | Regularization                |\n",
        "| L2 Reg        | 0              | **1e-5**       | Prevent overfitting           |\n",
        "\n",
        "### 2. Improved Architectures\n",
        "\n",
        "#### Attention ConvLSTM (ConvLSTM\\_Att)\n",
        "\n",
        "```python\n",
        "- 3 ConvLSTM layers (64→32→16 filters)\n",
        "- CBAM (Channel + Spatial Attention)\n",
        "- BatchNorm + Dropout in each layer\n",
        "- Multi-scale head (1×1, 3×3, 5×5)\n",
        "```\n",
        "\n",
        "#### Residual ConvGRU (ConvGRU\\_Res)\n",
        "\n",
        "```python\n",
        "- Skip connections from input\n",
        "- Enhanced BatchNorm\n",
        "- 2 ConvGRU blocks (64→32 filters)\n",
        "- Final residual connection\n",
        "```\n",
        "\n",
        "#### Hybrid Transformer (Hybrid\\_Trans)\n",
        "\n",
        "```python\n",
        "- CNN temporal encoder\n",
        "- Multi-head attention (4 heads)\n",
        "- LSTM for temporal aggregation\n",
        "- Spatial decoder\n",
        "```\n",
        "\n",
        "### 3. Advanced Techniques\n",
        "\n",
        "#### Learning Rate Scheduling\n",
        "\n",
        "* **Warmup**: Initial 5 epochs\n",
        "* **Cosine Decay**: Smooth reduction after warmup\n",
        "* **ReduceLROnPlateau**: Additional reduction if stalled\n",
        "\n",
        "#### Data Augmentation\n",
        "\n",
        "* Gaussian noise (σ=0.005)\n",
        "* Maintains spatial and temporal coherence\n",
        "\n",
        "#### Regularization\n",
        "\n",
        "* Spatial dropout (0.2)\n",
        "* L2 regularization on all weights\n",
        "* Batch Normalization\n",
        "\n",
        "## 📈 Expected Improvements\n",
        "\n",
        "### By Horizon:\n",
        "\n",
        "* **H=1**: RMSE < 40 (\\~8% improvement)\n",
        "* **H=2**: RMSE < 30, R² > 0.5 (significant improvement)\n",
        "* **H=3**: RMSE < 65, R² > 0.65 (\\~10% improvement)\n",
        "\n",
        "### By Model:\n",
        "\n",
        "1. **ConvLSTM\\_Att**: Improved capture of relevant spatial patterns\n",
        "2. **ConvGRU\\_Res**: Greater stability and reduced temporal degradation\n",
        "3. **Hybrid\\_Trans**: Enhanced modeling of long-range dependencies\n",
        "\n",
        "## 🚀 Next Steps\n",
        "\n",
        "### Short-term:\n",
        "\n",
        "1. Train models with improved configurations\n",
        "2. Validate metric improvements\n",
        "3. Regional error analysis\n",
        "\n",
        "### Medium-term:\n",
        "\n",
        "1. **Ensemble Methods**: Combine best models\n",
        "2. **Multi-Task Learning**: Predict multiple variables simultaneously\n",
        "3. **Physics-Informed Loss**: Incorporate physical constraints\n",
        "\n",
        "### Long-term:\n",
        "\n",
        "1. **3D Models**: ConvLSTM3D to capture elevation\n",
        "2. **Graph Neural Networks**: Address irregular spatial relations\n",
        "3. **Uncertainty Quantification**: Confidence intervals\n",
        "\n",
        "## 🔍 Baseline Comparison\n",
        "\n",
        "The script automatically generates comparisons with original models, displaying:\n",
        "\n",
        "* % improvement in RMSE\n",
        "* Evolution of R² per horizon\n",
        "* Summary table of best models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiFmcqSEXqrM",
        "outputId": "a88599c4-d218-4a61-b779-1112ddc8d1e6"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── IMPORTS Y CONFIGURACIÓN ─────────────────────────\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import sys, os, gc, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, ConvLSTM2D, LSTM,SimpleRNN, LSTM, GRU, Flatten, Dense, Reshape, RepeatVector,\n",
        "    Lambda, Permute, Layer, TimeDistributed, BatchNormalization, Dropout, Add,\n",
        "    Add, Multiply, Concatenate, GlobalAveragePooling2D, Activation,\n",
        "    LayerNormalization, MultiHeadAttention, MaxPooling2D, Embedding, Conv3D\n",
        ")\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,\n",
        "    CSVLogger, Callback, LearningRateScheduler\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam, AdamW\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import geopandas as gpd\n",
        "import imageio.v2 as imageio\n",
        "from IPython.display import clear_output, display, Image\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "\n",
        "\n",
        "## ╭─────────────────────────── Paths ──────────────────────────╮\n",
        "# ▶️ Path configuration\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "    # Install necessary dependencies\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "print('BASE_PATH =', BASE_PATH)\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = BASE_PATH / 'data' / 'output'\n",
        "OUT_ROOT = BASE_PATH / 'models' / 'output' / 'advanced_spatial'\n",
        "MODEL_OUTPUT_DIR = OUT_ROOT.parent\n",
        "MODEL_ROOT = BASE_PATH / 'models'\n",
        "OUT_ROOT.mkdir(exist_ok=True)\n",
        "BASE_MODEL_DIR = OUT_ROOT / 'base_models'\n",
        "BASE_MODEL_DIR.mkdir(exist_ok=True)\n",
        "MODEL_DIR = OUT_ROOT\n",
        "SHAPE_DIR = BASE_PATH / 'data' / 'input' / 'shapes'\n",
        "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
        "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IMAGE_DIR = OUT_ROOT/'images'\n",
        "IMAGE_DIR.mkdir(exist_ok=True)\n",
        "GIF_DIR = OUT_ROOT / \"gifs\"\n",
        "GIF_DIR.mkdir(exist_ok=True)\n",
        "# Dataset paths\n",
        "FULL_NC = DATA_DIR / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation.nc'\n",
        "FULL_NC_CLEAN = DATA_DIR / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "DEPT_GDF = gpd.read_file(SHAPE_DIR/'MGN_Departamento.shp')\n",
        "\n",
        "print(f\"📁 BASE_PATH: {BASE_PATH}\")\n",
        "print(f\"📊 Dataset: {FULL_NC_CLEAN.name if FULL_NC_CLEAN.exists() else FULL_NC.name}\")\n",
        "\n",
        "# ───────────────────────── IMPROVED CONFIGURATION ─────────────────────────\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context('talk')  # Higher resolution context\n",
        "plt.rcParams['figure.figsize'] = (14, 10)  # Larger default figure size\n",
        "plt.rcParams['figure.dpi'] = 120  # Higher DPI for display\n",
        "plt.rcParams['savefig.dpi'] = 700  # Higher DPI for saved figures\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['axes.titlesize'] = 16\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 12\n",
        "plt.rcParams['lines.linewidth'] = 2\n",
        "\n",
        "# GPU config\n",
        "for g in tf.config.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(g, True)\n",
        "\n",
        "print(\"✅ Imports completados\")\n",
        "\n",
        "# ╰────────────────────────────────────────────────────────────╯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1mY4GTA1MOF",
        "outputId": "6f42e3ed-0af5-4881-8b00-fbc74d65d91d"
      },
      "outputs": [],
      "source": [
        "# Export Smoke Test: validate exports for meta-model consumption\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import json\n",
        "import sys\n",
        "\n",
        "# Resolve project root\n",
        "BASE_PATH = Path.cwd()\n",
        "for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "    if (p / '.git').exists():\n",
        "        BASE_PATH = p\n",
        "        break\n",
        "\n",
        "ADVANCED_SPATIAL_ROOT = BASE_PATH / 'models' / 'output' / 'advanced_spatial'\n",
        "META_DIR = ADVANCED_SPATIAL_ROOT / 'meta_models'\n",
        "META_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"📁 Checking exports in: {META_DIR}\")\n",
        "\n",
        "report = {\n",
        "    'root': str(META_DIR),\n",
        "    'exports_checked': 0,\n",
        "    'valid_exports': 0,\n",
        "    'errors': [],\n",
        "    'common_shape': None\n",
        "}\n",
        "\n",
        "try:\n",
        "    if not META_DIR.exists():\n",
        "        raise FileNotFoundError(f\"Meta models directory not found: {META_DIR}\")\n",
        "\n",
        "    subdirs = [d for d in META_DIR.iterdir() if d.is_dir()]\n",
        "    if not subdirs:\n",
        "        raise RuntimeError(\"No export subdirectories found. Ensure export step ran successfully.\")\n",
        "\n",
        "    shapes = []\n",
        "    for sub in sorted(subdirs):\n",
        "        pred_f = sub / 'predictions.npy'\n",
        "        targ_f = sub / 'targets.npy'\n",
        "        meta_f = sub / 'metadata.json'\n",
        "\n",
        "        report['exports_checked'] += 1\n",
        "\n",
        "        if not pred_f.exists() or not targ_f.exists():\n",
        "            report['errors'].append(f\"Missing predictions/targets in {sub}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            pred = np.load(pred_f)\n",
        "            targ = np.load(targ_f)\n",
        "        except Exception as e:\n",
        "            report['errors'].append(f\"Failed to load arrays in {sub}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Shape checks\n",
        "        if pred.ndim != 4 or targ.ndim != 4:\n",
        "            report['errors'].append(f\"Invalid dims in {sub}: pred {pred.shape}, targ {targ.shape} (expected 4D)\")\n",
        "            continue\n",
        "        if pred.shape != targ.shape:\n",
        "            report['errors'].append(f\"Shape mismatch in {sub}: pred {pred.shape} vs targ {targ.shape}\")\n",
        "            continue\n",
        "\n",
        "        # Basic content checks\n",
        "        if not np.isfinite(pred).all():\n",
        "            report['errors'].append(f\"Non-finite values in predictions: {sub}\")\n",
        "            continue\n",
        "        if not np.isfinite(targ).all():\n",
        "            report['errors'].append(f\"Non-finite values in targets: {sub}\")\n",
        "            continue\n",
        "        if pred.size == 0:\n",
        "            report['errors'].append(f\"Empty predictions array: {sub}\")\n",
        "            continue\n",
        "\n",
        "        shapes.append(pred.shape)\n",
        "        report['valid_exports'] += 1\n",
        "\n",
        "        # Metadata presence (optional but recommended)\n",
        "        if not meta_f.exists():\n",
        "            report['errors'].append(f\"Missing metadata.json in {sub}\")\n",
        "\n",
        "    if not shapes:\n",
        "        raise RuntimeError(\"No valid exports found. See errors for details.\")\n",
        "\n",
        "    # Determine common shape and store it\n",
        "    N = min(s[0] for s in shapes)\n",
        "    H = min(s[1] for s in shapes)\n",
        "    Y = min(s[2] for s in shapes)\n",
        "    X = min(s[3] for s in shapes)\n",
        "    report['common_shape'] = [N, H, Y, X]\n",
        "\n",
        "    print(f\"✅ Export smoke test passed: {report['valid_exports']} valid of {report['exports_checked']} checked\")\n",
        "    print(f\"   Common shape: {tuple(report['common_shape'])}\")\n",
        "\n",
        "except Exception as e:\n",
        "    report['errors'].append(str(e))\n",
        "    print(f\"❌ Export smoke test failed: {e}\")\n",
        "\n",
        "# Persist report for CI/traceability\n",
        "try:\n",
        "    (META_DIR / 'export_smoke_test.json').write_text(json.dumps(report, indent=2))\n",
        "    print(f\"📝 Wrote {META_DIR / 'export_smoke_test.json'}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not write smoke test report: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBjkrkZcqjtR"
      },
      "outputs": [],
      "source": [
        "# ╭──────────────────── Global hyperparameters ─────────────╮\n",
        "INPUT_WINDOW   = 60\n",
        "HORIZON        = 3  # Forecast horizon in months\n",
        "TARGET_VAR     = 'total_precipitation'\n",
        "EPOCHS         = 120\n",
        "BATCH_SIZE     = 4           # small size → less RAM GPU\n",
        "PATIENCE       = 100\n",
        "LR             = 1e-3\n",
        "L2_REG         = 1e-5 # L2 regularization\n",
        "DROPOUT        = 0.2 # Dropout\n",
        "# ╰────────────────────────────────────────────────────────────╯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMyBUMZulFhR",
        "outputId": "054d08e7-2793-486e-d0ed-4f566eb7ce0c"
      },
      "outputs": [],
      "source": [
        "# ╭──────────────────────── Datasets ────────────────────────────╮\n",
        "LAG_VARS = ['total_precipitation_lag1',\n",
        "            'total_precipitation_lag2',\n",
        "            'total_precipitation_lag12']\n",
        "\n",
        "if FULL_NC_CLEAN.exists():\n",
        "    print(f\"🟢 Clean dataset found → {FULL_NC_CLEAN.name}\")\n",
        "    ds = xr.open_dataset(FULL_NC_CLEAN)\n",
        "\n",
        "else:\n",
        "    # ============================================================\n",
        "    print(f\"🟠 Warning: clean dataset not found.\\n\")\n",
        "    ds = xr.open_dataset(FULL_NC)\n",
        "    print(\"\\n📊  Global summary of NaNs\")\n",
        "    print(\"─\"*55)\n",
        "    for var in LAG_VARS:\n",
        "        arr    = ds[var].values\n",
        "        total  = arr.size\n",
        "        n_nans = int(np.isnan(arr).sum())\n",
        "        print(f\"{var:<28}: {n_nans:>8,} / {total:,}  ({n_nans/total:6.2%})\")\n",
        "\n",
        "    # ============================================================\n",
        "    print(\"\\n🕒  Dates with NaNs by variable\")\n",
        "    print(\"─\"*55)\n",
        "    for var in LAG_VARS:\n",
        "        arr         = ds[var].values\n",
        "        nan_per_ts  = np.isnan(arr).reshape(len(ds.time), -1).sum(axis=1)\n",
        "        if nan_per_ts.sum() == 0:\n",
        "            print(f\"{var}: no NaNs ✔️\")\n",
        "            continue\n",
        "\n",
        "        df_nan = (pd\n",
        "                  .DataFrame({\"time\": pd.to_datetime(ds.time.values),\n",
        "                              \"na_cells\": nan_per_ts})\n",
        "                  .query(\"na_cells > 0\"))\n",
        "\n",
        "        # first 3 and last 3 dates with NaNs\n",
        "        head = df_nan.head(3).to_string(index=False)\n",
        "        tail = df_nan.tail(3).to_string(index=False)\n",
        "        last = df_nan[\"time\"].iloc[-1].strftime(\"%Y-%m\")\n",
        "\n",
        "        print(f\"\\n{var}\")\n",
        "        print(head)\n",
        "        if len(df_nan) > 6:\n",
        "            print(\"   …\")\n",
        "        print(tail)\n",
        "        print(f\"   ⇢  last date with NaNs: {last}\")\n",
        "\n",
        "    # ============================================================\n",
        "    # First date in which the THREE variables are 100 % clean\n",
        "    # ------------------------------------------------------------\n",
        "    def last_nan_index(var: str) -> int:\n",
        "        \"\"\"Index of the last timestamp that contains at least one NaN in `var`.\"\"\"\n",
        "        nan_per_ts = np.isnan(ds[var].values).reshape(len(ds.time), -1).sum(axis=1)\n",
        "        idxs       = np.where(nan_per_ts > 0)[0]\n",
        "        return idxs[-1] if len(idxs) else -1\n",
        "\n",
        "    last_nan_any = max(last_nan_index(v) for v in LAG_VARS)\n",
        "    first_clean  = pd.to_datetime(ds.time.values[last_nan_any + 1])\n",
        "\n",
        "    print(\"\\nFirst date 100 % free of NaNs in ALL lags:\",\n",
        "          first_clean.strftime(\"%Y-%m\"))\n",
        "\n",
        "    ds_clean = ds.sel(time=~(ds['time.year'] == 1981))   # discard ALL 1981\n",
        "\n",
        "    print(\"🔎  Timestamps before:\", len(ds.time))\n",
        "    print(\"🔎  Timestamps after:\", len(ds_clean.time))\n",
        "\n",
        "    # 3) Save new NetCDF file\n",
        "    ds_clean.to_netcdf(FULL_NC_CLEAN, mode='w')\n",
        "    print(f\"💾  Dataset sin 1981 guardado en {FULL_NC_CLEAN}\")\n",
        "\n",
        "    # 4) (-- optional --)  check that there are no NaNs in the lags\n",
        "    LAG_VARS = ['total_precipitation_lag1',\n",
        "                'total_precipitation_lag2',\n",
        "                'total_precipitation_lag12']\n",
        "\n",
        "    print(\"\\n📊  Remaining NaNs after removing 1981\")\n",
        "    print(\"─\"*50)\n",
        "    for var in LAG_VARS:\n",
        "        n_nan = int(np.isnan(ds_clean[var].values).sum())\n",
        "        print(f\"{var:<28}: {n_nan:,} NaNs\")\n",
        "\n",
        "    ds = ds_clean\n",
        "# ╰────────────────────────────────────────────────────────────╯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMdi9LC2qjtT"
      },
      "outputs": [],
      "source": [
        "# Time windows for training and validation (in months)\n",
        "VALIDATION_WINDOW = 24\n",
        "TRAINING_WINDOW = 60\n",
        "\n",
        "# Simplified fold structure with reference dates\n",
        "FOLDS = {\n",
        "    'F1': {\n",
        "        'active': True,\n",
        "        'ref_date': '2024-12'  # Reference date for the fold\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcvm08VIguq1",
        "outputId": "57a738d0-bfc5-4eb2-9e14-3f205488155c"
      },
      "outputs": [],
      "source": [
        "# ╭──────────────────────── Shapes ────────────────────────────╮\n",
        "lat, lon    = len(ds.latitude), len(ds.longitude)\n",
        "cells       = lat * lon\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭────────────────────────── Metrics ────────────────────────╮\n",
        "\n",
        "def evaluate(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae  = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-5))) * 100\n",
        "    r2   = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, mape, r2\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭────────────────────── Base ConvLSTM model ────────────────╮\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def tile_step_emb(batch_ref, step_emb_tab):\n",
        "    # Static shape (TensorShape / TensorSpec)\n",
        "    if isinstance(batch_ref, (tf.TensorShape, tf.TensorSpec)):\n",
        "        return tf.TensorShape([batch_ref[0],\n",
        "                               step_emb_tab.shape[0],\n",
        "                               step_emb_tab.shape[1]])\n",
        "    # Execution\n",
        "    b   = tf.shape(batch_ref)[0]\n",
        "    emb = tf.expand_dims(step_emb_tab, 0)\n",
        "    return tf.tile(emb, [b, 1, 1])\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "custom = {'tile_step_emb': tile_step_emb}\n",
        "\n",
        "def _build_convlstm_ed(\n",
        "        *,\n",
        "        input_window: int,\n",
        "        output_horizon: int,\n",
        "        spatial_height: int,\n",
        "        spatial_width: int,\n",
        "        n_features: int,\n",
        "        n_filters: int = 64,\n",
        "        n_heads: int = 4,\n",
        "        use_attention: bool = True,\n",
        "        use_positional_emb: bool = True,\n",
        "        lr: float = 1e-3\n",
        "    ) -> Model:\n",
        "    \"\"\"\n",
        "    Encoder-Decoder ConvLSTM + GRU.\n",
        "    If `use_positional_emb` = True, add an output step embedding\n",
        "    that prevents the model from generating the same prediction for all horizons.\n",
        "    \"\"\"\n",
        "\n",
        "    # ──────────────── Encoder ────────────────\n",
        "    enc_inputs = Input(\n",
        "        shape=(input_window, spatial_height, spatial_width, n_features),\n",
        "        name=\"enc_input\"\n",
        "    )\n",
        "\n",
        "    x = ConvLSTM2D(n_filters, (3, 3), padding='same',\n",
        "                   return_sequences=True,  name=\"enc_lstm_1\")(enc_inputs)\n",
        "    x = ConvLSTM2D(n_filters // 2, (3, 3), padding='same',\n",
        "                   return_sequences=False, name=\"enc_lstm_2\")(x)\n",
        "\n",
        "    # ── Flatten grid and repeat context T_out times ──\n",
        "    flat = Flatten(name=\"flatten_spatial\")(x)                 # (B, H·W·C)\n",
        "    ctx  = RepeatVector(output_horizon, name=\"context\")(flat) # (B, T_out, H·W·C)\n",
        "\n",
        "    # ── Positional embedding ──\n",
        "    if use_positional_emb:\n",
        "        # Create step IDs as constant input\n",
        "        step_ids_input = Input(shape=(output_horizon,), dtype=tf.int32, name=\"step_ids\")\n",
        "\n",
        "        # Embedding layer\n",
        "        step_emb_layer = Embedding(output_horizon, n_filters, name=\"step_embedding\")\n",
        "        step_emb = step_emb_layer(step_ids_input)  # (B, T_out, D)\n",
        "\n",
        "        # Concatenate with context\n",
        "        dec_in = Concatenate(name=\"dec_concat\")([ctx, step_emb])\n",
        "\n",
        "        # Update model inputs\n",
        "        model_inputs = [enc_inputs, step_ids_input]\n",
        "    else:\n",
        "        dec_in = ctx\n",
        "        model_inputs = enc_inputs\n",
        "\n",
        "    # ─────────────── Temporal decoder ───────────────\n",
        "    dec = GRU(2 * n_filters, return_sequences=True, name=\"dec_gru\")(dec_in) # (B, T_out, 2·F)\n",
        "\n",
        "    # ─────── Attention (optional) ───────\n",
        "    if use_attention:\n",
        "        attn = MultiHeadAttention(num_heads=n_heads,\n",
        "                                  key_dim=n_filters,\n",
        "                                  dropout=0.1,\n",
        "                                  name=\"mha\")(dec, dec)\n",
        "        dec  = Add(name=\"mha_residual\")([dec, attn])\n",
        "        dec  = LayerNormalization(name=\"mha_norm\")(dec)\n",
        "\n",
        "    # ───────────── Projection to grid ─────────────\n",
        "    proj = TimeDistributed(\n",
        "        Dense(spatial_height * spatial_width, activation='linear'),\n",
        "        name=\"dense_proj\"\n",
        "    )(dec)                                                    # (B, T_out, H·W)\n",
        "\n",
        "    out = Reshape(\n",
        "        (output_horizon, spatial_height, spatial_width, 1),\n",
        "        name=\"reshape_out\"\n",
        "    )(proj)\n",
        "\n",
        "    name = (\"ConvLSTM_ED_Attn_PE\" if use_attention else \"ConvLSTM_ED_PE\") \\\n",
        "           if use_positional_emb else \\\n",
        "           (\"ConvLSTM_ED_Attn\"     if use_attention else \"ConvLSTM_ED\")\n",
        "\n",
        "    model = Model(model_inputs, out, name=name)\n",
        "    model.compile(optimizer=Adam(lr), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Factories ---------------------------------------------------\n",
        "\n",
        "def factory_no_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=False, **kw)\n",
        "\n",
        "def factory_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=True, **kw)\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭────────────────────── Experiments & Folds ─────────────────╮\n",
        "# ▸ We only show the first three levels; add the others equally\n",
        "BASE_FEATURES = [\n",
        "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
        "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
        "    'elevation','slope','aspect'\n",
        "]\n",
        "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
        "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
        "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
        "\n",
        "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
        "    'ConvLSTM-ED': {\n",
        "        'active': True,\n",
        "        'feature_list': BASE_FEATURES,\n",
        "        'builder': factory_attn, #factory_no_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE': {\n",
        "        'active': True,\n",
        "        'feature_list': KCE_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4,\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE-PAFC': {\n",
        "        'active': True,\n",
        "        'feature_list': PAFC_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 96,\n",
        "        'n_heads'  : 6,\n",
        "    },\n",
        "}\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭──────────────────── Sliding windows ───────────────────╮\n",
        "\n",
        "def make_windows(mask:np.ndarray, allow_past_context:bool)->tuple[np.ndarray,np.ndarray]:\n",
        "    \"\"\"Generates windows **discarding** those containing NaNs.\n",
        "\n",
        "    Modified to handle sliding windows properly for training and validation.\n",
        "    \"\"\"\n",
        "    seq_X, seq_y = [], []\n",
        "\n",
        "    # Get indices of True values in mask\n",
        "    mask_indices = np.where(mask)[0]\n",
        "\n",
        "    if len(mask_indices) == 0:\n",
        "        print(\"Warning: Empty mask provided\")\n",
        "        return np.array(seq_X), np.array(seq_y)\n",
        "\n",
        "    # For training windows (allow_past_context=False)\n",
        "    if not allow_past_context:\n",
        "        # For training, we create sliding windows within the available data\n",
        "        # We need at least INPUT_WINDOW consecutive points to start\n",
        "        if len(mask_indices) < INPUT_WINDOW:\n",
        "            print(f\"Warning: Not enough data points in mask for training ({len(mask_indices)} < {INPUT_WINDOW})\")\n",
        "            return np.array(seq_X), np.array(seq_y)\n",
        "\n",
        "        # Check if indices are consecutive\n",
        "        consecutive_groups = []\n",
        "        current_group = [mask_indices[0]]\n",
        "\n",
        "        for i in range(1, len(mask_indices)):\n",
        "            if mask_indices[i] == mask_indices[i-1] + 1:\n",
        "                current_group.append(mask_indices[i])\n",
        "            else:\n",
        "                if len(current_group) >= INPUT_WINDOW:\n",
        "                    consecutive_groups.append(current_group)\n",
        "                current_group = [mask_indices[i]]\n",
        "\n",
        "        # Don't forget the last group\n",
        "        if len(current_group) >= INPUT_WINDOW:\n",
        "            consecutive_groups.append(current_group)\n",
        "\n",
        "        # Create windows from each consecutive group\n",
        "        for group in consecutive_groups:\n",
        "            # We can create windows up to len(group) - INPUT_WINDOW + 1\n",
        "            for i in range(len(group) - INPUT_WINDOW + 1):\n",
        "                # Get input window indices\n",
        "                start_idx = group[i]\n",
        "                end_w_idx = start_idx + INPUT_WINDOW - 1\n",
        "\n",
        "                # Extract input data\n",
        "                Xw = Xarr[start_idx:end_w_idx+1]\n",
        "\n",
        "                # For output, we'll use up to HORIZON points after the input window\n",
        "                # but only if they're within our mask\n",
        "                y_indices = []\n",
        "                for h in range(HORIZON):\n",
        "                    y_idx = end_w_idx + 1 + h\n",
        "                    if y_idx < len(yarr):  # More flexible for validation\n",
        "                        y_indices.append(y_idx)\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # Only create a window if we have at least one output point\n",
        "                if len(y_indices) > 0:\n",
        "                    yw = yarr[y_indices]\n",
        "\n",
        "                    # Check for NaNs\n",
        "                    if not (np.isnan(Xw).any() or np.isnan(yw).any()):\n",
        "                        # Pad y if necessary\n",
        "                        if len(y_indices) < HORIZON:\n",
        "                            y_padded = np.zeros((HORIZON,) + yarr.shape[1:], dtype=yarr.dtype)\n",
        "                            y_padded[:len(y_indices)] = yw\n",
        "                            yw = y_padded\n",
        "\n",
        "                        seq_X.append(Xw)\n",
        "                        seq_y.append(yw)\n",
        "\n",
        "    # For validation windows (allow_past_context=True)\n",
        "    else:\n",
        "        # For validation, we can use past context - we don't need INPUT_WINDOW consecutive points in the mask\n",
        "        # Instead, we can use the entire dataset to create input windows that end before validation targets\n",
        "\n",
        "        if len(mask_indices) == 0:\n",
        "            print(\"Warning: Empty validation mask\")\n",
        "            return np.array(seq_X), np.array(seq_y)\n",
        "\n",
        "        # For each point in the validation mask, try to create a window\n",
        "        for val_idx in mask_indices:\n",
        "            # Check if we can create a full INPUT_WINDOW before this validation point\n",
        "            if val_idx >= INPUT_WINDOW:\n",
        "                # Create input window ending at (val_idx - 1)\n",
        "                start_idx = val_idx - INPUT_WINDOW\n",
        "                end_w_idx = val_idx - 1\n",
        "\n",
        "                # Extract input data from the full dataset (not just the mask)\n",
        "                Xw = Xarr[start_idx:end_w_idx+1]\n",
        "\n",
        "                # For output, use points starting from val_idx\n",
        "                y_indices = []\n",
        "                for h in range(HORIZON):\n",
        "                    y_idx = val_idx + h\n",
        "                    if y_idx < len(yarr):\n",
        "                        y_indices.append(y_idx)\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # Create window if we have at least one output point\n",
        "                if len(y_indices) > 0:\n",
        "                    yw = yarr[y_indices]\n",
        "\n",
        "                    # Check for NaNs\n",
        "                    if not (np.isnan(Xw).any() or np.isnan(yw).any()):\n",
        "                        # Pad y if necessary\n",
        "                        if len(y_indices) < HORIZON:\n",
        "                            y_padded = np.zeros((HORIZON,) + yarr.shape[1:], dtype=yarr.dtype)\n",
        "                            y_padded[:len(y_indices)] = yw\n",
        "                            yw = y_padded\n",
        "\n",
        "                        seq_X.append(Xw)\n",
        "                        seq_y.append(yw)\n",
        "\n",
        "    print(f\"Created {len(seq_X)} windows\")\n",
        "    return np.array(seq_X), np.array(seq_y)\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭────────────────── Main training loop ────────╮\n",
        "RESULTS: List[Dict[str, Any]] = []\n",
        "\n",
        "# 🔸 NEW helper ------------------------------------------------\n",
        "\n",
        "def _impute_nans(a:np.ndarray, per_feature_mean:np.ndarray|None=None, is_target:bool=False)->np.ndarray:\n",
        "    \"\"\"Imputes remaining NaNs (extra safety).\"\"\"\n",
        "    if not np.isnan(a).any():\n",
        "        return a\n",
        "    if is_target:\n",
        "        a[np.isnan(a)] = 0.0  # 🔸 NEW – 0 for y\n",
        "        return a\n",
        "    if per_feature_mean is None:\n",
        "        raise ValueError('per_feature_mean required for imputing X')\n",
        "    flat = a.reshape(-1, a.shape[-1])\n",
        "    nan_idx = np.isnan(flat)\n",
        "    for f in range(a.shape[-1]):\n",
        "        flat[nan_idx[:,f], f] = per_feature_mean[f]  # 🔸 NEW\n",
        "    return flat.reshape(a.shape)\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭──────────────────── Run all experiments ──────────────────────╮\n",
        "def run_all_experiments():\n",
        "    times = pd.to_datetime(ds.time.values)\n",
        "    total = sum(e['active'] for e in EXPERIMENTS.values()) * sum(f['active'] for f in FOLDS.values())\n",
        "    cnt   = 0\n",
        "\n",
        "    for exp_name, exp_cfg in EXPERIMENTS.items():\n",
        "        if not exp_cfg['active']:\n",
        "            continue\n",
        "        vars_     = exp_cfg['feature_list']\n",
        "        builder   = exp_cfg['builder']      # specific factory\n",
        "        n_filters = exp_cfg.get('n_filters',64)\n",
        "        n_heads   = exp_cfg.get('n_heads',4)\n",
        "\n",
        "        # ─ Pre‑load features for experiment ─────────────────────\n",
        "        global Xarr, yarr\n",
        "        Xarr = ds[vars_].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "        yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
        "        feats = Xarr.shape[-1]\n",
        "\n",
        "        for fold_name, fold_cfg in FOLDS.items():\n",
        "            if not fold_cfg['active']:\n",
        "                continue\n",
        "            cnt += 1\n",
        "\n",
        "            # Calculate all dates from the reference date\n",
        "            ref_date = pd.to_datetime(fold_cfg['ref_date'])\n",
        "\n",
        "            # Test period starts at reference date and extends for HORIZON months\n",
        "            test_start = ref_date\n",
        "            # Use date_range to ensure we get exactly HORIZON months\n",
        "            test_end = pd.date_range(start=test_start, periods=HORIZON, freq='MS')[-1]\n",
        "\n",
        "            # Validation period is VALIDATION_WINDOW months before test period\n",
        "            val_end = test_start - pd.DateOffset(days=1)  # Day before test starts\n",
        "            # Use date_range to ensure we get exactly VALIDATION_WINDOW months\n",
        "            val_start = pd.date_range(end=val_end, periods=VALIDATION_WINDOW, freq='MS')[0]\n",
        "\n",
        "            # Training period is TRAINING_WINDOW months before validation period\n",
        "            train_end = val_start - pd.DateOffset(days=1)  # Day before validation starts\n",
        "            # Use date_range to ensure we get exactly TRAINING_WINDOW months\n",
        "            train_start = pd.date_range(end=train_end, periods=TRAINING_WINDOW, freq='MS')[0]\n",
        "\n",
        "            # Format dates for display\n",
        "            train_start_str = train_start.strftime('%Y-%m')\n",
        "            train_end_str = train_end.strftime('%Y-%m')\n",
        "            val_start_str = val_start.strftime('%Y-%m')\n",
        "            val_end_str = val_end.strftime('%Y-%m')\n",
        "            test_start_str = test_start.strftime('%Y-%m')\n",
        "            test_end_str = test_end.strftime('%Y-%m')\n",
        "\n",
        "            print(f\"\\n▶️  [{cnt}/{total}] {exp_name} – {fold_name}\")\n",
        "            print(f\"    Reference date: {fold_cfg['ref_date']}\")\n",
        "            print(f\"    Training: {train_start_str} to {train_end_str}\")\n",
        "            print(f\"    Validation: {val_start_str} to {val_end_str}\")\n",
        "            print(f\"    Test: {test_start_str} to {test_end_str}\")\n",
        "\n",
        "            # Verify that we have data for these date ranges\n",
        "            mask_tr = (times >= train_start) & (times <= train_end)\n",
        "            mask_val = (times >= val_start) & (times <= val_end)\n",
        "\n",
        "            # Check if we have enough data in the dataset\n",
        "            print(f\"    Data points in training period: {mask_tr.sum()}/{TRAINING_WINDOW}\")\n",
        "            print(f\"    Data points in validation period: {mask_val.sum()}/{VALIDATION_WINDOW}\")\n",
        "\n",
        "            # Check if we have future data for test period\n",
        "            mask_test = (times >= test_start) & (times <= test_end)\n",
        "            test_data_count = mask_test.sum()\n",
        "            print(f\"    Data points in test period: {test_data_count}/{HORIZON}\")\n",
        "\n",
        "            # Verify February 2025 data exists\n",
        "            if test_data_count < HORIZON:\n",
        "                print(f\"⚠️ Missing future data for test period. Available: {test_data_count}/{HORIZON}\")\n",
        "                # Check if we should continue anyway\n",
        "                if test_data_count == 0:\n",
        "                    print(\"⚠️ No test data available → skip\")\n",
        "                    continue\n",
        "\n",
        "            # Create windows\n",
        "            X_tr, y_tr = make_windows(mask_tr, allow_past_context=False)\n",
        "            X_va, y_va = make_windows(mask_val, allow_past_context=True)\n",
        "            print(f\"    Windows train: {len(X_tr)} · val: {len(X_va)}\")\n",
        "\n",
        "            if len(X_tr) == 0:\n",
        "                print(\"⚠️ No valid training windows → skip\")\n",
        "                continue\n",
        "\n",
        "            if len(X_va) == 0:\n",
        "                print(\"⚠️ No valid validation windows → skip\")\n",
        "                continue\n",
        "\n",
        "            # 🔸 NEW — Safety imputation\n",
        "            feat_mean = np.nanmean(X_tr.reshape(-1,feats),axis=0)\n",
        "            X_tr = _impute_nans(X_tr,feat_mean); X_va=_impute_nans(X_va,feat_mean)\n",
        "            y_tr = _impute_nans(y_tr,is_target=True); y_va=_impute_nans(y_va,is_target=True)\n",
        "\n",
        "            # ─ Scaling (fit only in train) ─────────────────────\n",
        "            sx = StandardScaler().fit(X_tr.reshape(-1, feats))\n",
        "            sy = StandardScaler().fit(y_tr.reshape(-1, 1))\n",
        "            X_tr_sc = sx.transform(X_tr.reshape(-1, feats)).reshape(X_tr.shape)\n",
        "            X_va_sc = sx.transform(X_va.reshape(-1, feats)).reshape(X_va.shape)\n",
        "            y_tr_sc = sy.transform(y_tr.reshape(-1, 1)).reshape(y_tr.shape)[..., None]\n",
        "            y_va_sc = sy.transform(y_va.reshape(-1, 1)).reshape(y_va.shape)[..., None]\n",
        "\n",
        "            # Generate horizon dates based on test period\n",
        "            horizon_dates = pd.date_range(test_start, periods=HORIZON, freq='MS')\n",
        "            horizon_dates = [date.strftime('%Y-%m') for date in horizon_dates]\n",
        "\n",
        "            # ─ Build & train model (factory) ───────────────────\n",
        "            tag        = f\"{exp_name.replace('+','_')}_{fold_name}\"\n",
        "            model_path = BASE_MODEL_DIR / f\"{tag}.keras\"\n",
        "            if model_path.exists():\n",
        "                print(f\"⏩ {tag} already exists → skip\"); continue\n",
        "\n",
        "            model = builder(\n",
        "                input_window=INPUT_WINDOW,\n",
        "                output_horizon=HORIZON,\n",
        "                spatial_height=lat,\n",
        "                spatial_width=lon,\n",
        "                n_features=feats,\n",
        "                n_filters=n_filters,\n",
        "                n_heads=n_heads,\n",
        "                lr=LR\n",
        "            )\n",
        "\n",
        "            # Prepare step_ids for training\n",
        "            step_ids_train = np.tile(np.arange(HORIZON), (len(X_tr_sc), 1))\n",
        "            step_ids_val = np.tile(np.arange(HORIZON), (len(X_va_sc), 1))\n",
        "\n",
        "            # Check if the model uses positional embedding\n",
        "            uses_pe = len(model.inputs) > 1\n",
        "\n",
        "            if uses_pe:\n",
        "                X_train_input = [X_tr_sc, step_ids_train]\n",
        "                X_val_input = [X_va_sc, step_ids_val]\n",
        "            else:\n",
        "                X_train_input = X_tr_sc\n",
        "                X_val_input = X_va_sc\n",
        "\n",
        "            es   = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
        "            hist = model.fit(X_train_input, y_tr_sc, validation_data=(X_val_input, y_va_sc), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=1)\n",
        "\n",
        "            # ─ Evaluation ─────────────────────────────────────\n",
        "            if uses_pe:\n",
        "                y_hat_sc = model.predict([X_va_sc, step_ids_val], verbose=0)\n",
        "            else:\n",
        "                y_hat_sc = model.predict(X_va_sc, verbose=0)\n",
        "            y_hat    = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(y_hat_sc.shape)\n",
        "            y_true   = sy.inverse_transform(y_va_sc.reshape(-1,1)).reshape(y_va_sc.shape)\n",
        "\n",
        "            rmse, mae, mape, r2 = evaluate(y_true.ravel(), y_hat.ravel())\n",
        "            RESULTS.append(dict(\n",
        "                experiment=exp_name,\n",
        "                fold=fold_name,\n",
        "                RMSE=rmse,\n",
        "                MAE=mae,\n",
        "                MAPE=mape,\n",
        "                R2=r2,\n",
        "                epochs=len(hist.history['loss']),\n",
        "                horizon_dates=horizon_dates\n",
        "            ))\n",
        "\n",
        "            # ─ Saving artifacts ────────────────────────────\n",
        "            model.save(model_path)\n",
        "            plt.figure(figsize=(12, 8)); plt.plot(hist.history['loss'], label='train', linewidth=2); plt.plot(hist.history['val_loss'], label='val', linewidth=2); plt.legend(fontsize=12); plt.title(tag, fontsize=14); plt.savefig(IMAGE_DIR/f\"{tag}.png\", dpi=700, bbox_inches='tight'); plt.close()\n",
        "\n",
        "            # Check that predictions vary between horizons\n",
        "            print(f\"Verification of predictions for {tag}:\")\n",
        "            for h in range(HORIZON):\n",
        "                pred_h = y_hat[0, h, ..., 0]  # First sample, horizon h\n",
        "                print(f\"  {horizon_dates[h]}: min={pred_h.min():.3f}, max={pred_h.max():.3f}, mean={pred_h.mean():.3f}, std={pred_h.std():.3f}\")\n",
        "\n",
        "            # Use the last validation window for better visualization\n",
        "            last_idx = min(len(y_hat)-1, 10)  # Use one of the last windows\n",
        "            _generate_gif(y_true[last_idx], y_hat[last_idx], tag, horizon_dates)\n",
        "            print(f\"✅ Saved {model_path.name}\")\n",
        "\n",
        "    # ─ Global metrics ────────────────────────────────────\n",
        "    df = pd.DataFrame(RESULTS)\n",
        "    out_csv = BASE_MODEL_DIR / \"metrics_experiments_folds.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"\\n📑 Metrics table in {out_csv}\")\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭──────────────────── GIF generator ──────────────────────╮\n",
        "\n",
        "def _generate_gif(y_true_sample, y_pred_sample, tag, horizon_dates=None):\n",
        "    pcm_min, pcm_max = 0, np.max(y_pred_sample)\n",
        "    frames = []\n",
        "    for h in range(HORIZON):\n",
        "        pmap = y_pred_sample[h, ..., 0]\n",
        "        fig, ax = plt.subplots(1,1, figsize=(14,12), subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "        mesh = ax.pcolormesh(ds.longitude, ds.latitude, pmap, cmap='Blues', shading='nearest', vmin=pcm_min, vmax=pcm_max, transform=ccrs.PlateCarree())\n",
        "        ax.coastlines(); ax.gridlines(draw_labels=True)\n",
        "        # Use horizon_dates if provided, otherwise use H format\n",
        "        if horizon_dates and h < len(horizon_dates):\n",
        "            ax.set_title(f\"{tag} – {horizon_dates[h]}\")\n",
        "        else:\n",
        "            ax.set_title(f\"{tag} – H{h+1}\")\n",
        "        fig.colorbar(mesh, ax=ax, fraction=0.046, pad=0.04)\n",
        "        tmp = GIF_DIR/f\"tmp_{tag}_h{h}.png\"\n",
        "        fig.savefig(tmp, dpi=700, bbox_inches='tight'); plt.close(fig)\n",
        "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
        "    gif_path = GIF_DIR/f\"{tag}.gif\"\n",
        "    imageio.mimsave(gif_path, frames, fps=0.5)\n",
        "    print(f\"💾 GIF {gif_path.name} done\")\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭────────────────────── Main loop ─────────────────────╮\n",
        "run_all_experiments()\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# 📈 **Evaluator for spatial ConvLSTM outputs**\n",
        "\n",
        "#╭────────────────────── Experiments & Folds ─────────────────╮\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def tile_step_emb(batch_ref, step_emb_tab):\n",
        "    \"\"\"\n",
        "    Replicates the embedding table (T_out, D) → (B, T_out, D).\n",
        "\n",
        "    · During inference, `batch_ref` is TensorShape\n",
        "      → we return TensorShape (None, T_out, D).\n",
        "    · During execution, `batch_ref` is tensor\n",
        "      → we return tensor (B, T_out, D).\n",
        "\n",
        "    ▸ `step_emb_tab` ALWAYS comes from the original Lambda closure,\n",
        "      so don't make it optional.\n",
        "    \"\"\"\n",
        "    # ——— 1) Static shape ———\n",
        "    if isinstance(batch_ref, (tf.TensorShape, tf.TensorSpec)):\n",
        "        return tf.TensorShape([batch_ref[0],\n",
        "                               step_emb_tab.shape[0],\n",
        "                               step_emb_tab.shape[1]])\n",
        "\n",
        "    # ——— 2) Execution ———\n",
        "    b   = tf.shape(batch_ref)[0]\n",
        "    emb = tf.expand_dims(step_emb_tab, 0)    # (1, T_out, D)\n",
        "    return tf.tile(emb, [b, 1, 1])           # (B, T_out, D)\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "custom = {'tile_step_emb': tile_step_emb}\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "def quick_plot(ax,data,cmap,title,date_label,vmin=None,vmax=None):\n",
        "    mesh=ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
        "    ax.coastlines(); ax.add_geometries(DEPT_GDF.geometry,ccrs.PlateCarree(),edgecolor='black',facecolor='none',linewidth=1)\n",
        "    gl=ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
        "    ax.set_title(f\"{title}\\n{date_label}\",pad=10); return mesh\n",
        "\n",
        "# ───────── Recovering EXPERIMENTS dictionary (from training block) ─────────\n",
        "from typing import Dict\n",
        "EXPERIMENTS:Dict[str,Dict[str,Any]] = {\n",
        "    'ConvLSTM-ED':              {'feature_list': \"+\".join(BASE_FEATURES).split(\"+\")},\n",
        "    'ConvLSTM-ED-KCE':          {'feature_list': \"+\".join(KCE_FEATURES).split(\"+\")},\n",
        "    'ConvLSTM-ED-KCE-PAFC':     {'feature_list': \"+\".join(PAFC_FEATURES).split(\"+\")},\n",
        "    # other experiments\n",
        "}\n",
        "\n",
        "# ———————————————————— Evaluation ————————————————————\n",
        "all_metrics=[]; times=pd.to_datetime(ds.time.values)\n",
        "for mpath in sorted(BASE_MODEL_DIR.glob(\"*.keras\")):\n",
        "    tag   = mpath.stem                        # p.ej. ConvLSTM-ED_F1\n",
        "    parts = tag.split(\"_\")\n",
        "    fold  = parts[-1]                         # F1\n",
        "    exp_token = \"_\".join(parts[:-1])\n",
        "    exp_name  = exp_token.replace(\"_\",\"+\")  # original name with +\n",
        "    if exp_name not in EXPERIMENTS:\n",
        "        print(\"⚠️ Exp not found for\",tag); continue\n",
        "    feats = EXPERIMENTS[exp_name]['feature_list']\n",
        "    print(f\"\\n🔍 Evaluating {tag} …\")\n",
        "\n",
        "    # — Extraction of arrays —\n",
        "    Xarr = ds[feats].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "    yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
        "    T,_,_,F = Xarr.shape\n",
        "    Xfull = Xarr; yfull=yarr  # keep (T,H,W,F)\n",
        "\n",
        "    # final window (identical logic from original notebook)\n",
        "    start=T-INPUT_WINDOW-HORIZON; end_w=start+INPUT_WINDOW; end_y=end_w+HORIZON\n",
        "    X_eval = Xfull[start:end_w]                 # (60,H,W,F)\n",
        "    y_eval = yfull[end_w:end_y]                 # (3,H,W)\n",
        "\n",
        "    # — Scalers (fit vectorizado) —\n",
        "    flat_X = Xfull.reshape(-1, F)      # (T·H·W, F)\n",
        "    flat_y = yfull.reshape(-1, 1)      # (T·H·W, 1)\n",
        "\n",
        "    sx = StandardScaler().fit(flat_X)\n",
        "    sy = StandardScaler().fit(flat_y)\n",
        "\n",
        "    Xe_sc = sx.transform(X_eval.reshape(-1, F)).reshape(1, INPUT_WINDOW, lat, lon, F)\n",
        "    ye_sc = sy.transform(y_eval.reshape(-1, 1)).reshape(1, HORIZON, lat, lon, 1)\n",
        "\n",
        "\n",
        "    model = tf.keras.models.load_model(\n",
        "    mpath,\n",
        "    compile=False,\n",
        "    custom_objects={'tile_step_emb': tile_step_emb}\n",
        "    )\n",
        "\n",
        "    # Check if the model uses positional embedding\n",
        "    uses_pe = len(model.inputs) > 1\n",
        "\n",
        "    if uses_pe:\n",
        "        step_ids_eval = np.tile(np.arange(HORIZON), (1, 1))\n",
        "        yhat_sc = model.predict([Xe_sc, step_ids_eval], verbose=0)  # (1,3,H,W,1)\n",
        "    else:\n",
        "        yhat_sc = model.predict(Xe_sc, verbose=0)  # (1,3,H,W,1)\n",
        "    # Calculate dates from fold reference date\n",
        "    if fold in FOLDS:\n",
        "        ref_date = pd.to_datetime(FOLDS[fold]['ref_date'])\n",
        "        # Test period starts at reference date\n",
        "        eval_dates = pd.date_range(ref_date, periods=HORIZON, freq='MS')\n",
        "    else:\n",
        "        # Fallback to using the last dates in the dataset\n",
        "        eval_dates = pd.date_range(times[-HORIZON], periods=HORIZON, freq='MS')\n",
        "\n",
        "    horizon_dates = [date.strftime('%Y-%m') for date in eval_dates]\n",
        "\n",
        "    print(f\"Verification of predictions for {tag}:\")\n",
        "    for h in range(HORIZON):\n",
        "        pred_h = yhat_sc[0, h, ..., 0]  # First sample, horizon h\n",
        "        print(f\"  {horizon_dates[h]}: min={pred_h.min():.3f}, max={pred_h.max():.3f}, mean={pred_h.mean():.3f}, std={pred_h.std():.3f}\")\n",
        "\n",
        "    # Check if predictions are identical\n",
        "    if HORIZON > 1:\n",
        "        diff_h1_h2 = np.abs(yhat_sc[0, 0] - yhat_sc[0, 1]).mean()\n",
        "        print(f\"  Average difference {horizon_dates[0]} vs {horizon_dates[1]}: {diff_h1_h2:.6f}\")\n",
        "\n",
        "    yhat   = sy.inverse_transform(yhat_sc.reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
        "    ytrue  = y_eval\n",
        "\n",
        "    # — Metrics by horizon —\n",
        "    for h in range(HORIZON):\n",
        "        yt = ytrue[h].ravel()\n",
        "        yp = yhat[h].ravel()\n",
        "\n",
        "        # ---------- filter NaN / ±∞ ----------\n",
        "        mask = np.isfinite(yt) & np.isfinite(yp)\n",
        "        if mask.sum() == 0:          # empty window → skip\n",
        "            print(f\"   · {horizon_dates[h]}: all values are NaN/Inf → skip\")\n",
        "            continue\n",
        "        yt, yp = yt[mask], yp[mask]\n",
        "        # -------------------------------------\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(yt, yp))\n",
        "        mae  = mean_absolute_error(yt, yp)\n",
        "        mape = np.mean(np.abs((yt - yp) / (yt + 1e-5))) * 100\n",
        "        r2   = r2_score(yt, yp)\n",
        "\n",
        "        all_metrics.append(dict(\n",
        "            model      = tag,\n",
        "            experiment = exp_name,\n",
        "            fold       = fold,\n",
        "            horizon    = horizon_dates[h],\n",
        "            RMSE       = rmse,\n",
        "            MAE        = mae,\n",
        "            MAPE       = mape,\n",
        "            R2         = r2,\n",
        "            horizon_date = horizon_dates[h]\n",
        "        ))\n",
        "\n",
        "    # — Figure Real vs Pred vs MAPE —\n",
        "    fig,axes=plt.subplots(HORIZON,3,figsize=(18,6*HORIZON),subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "    vmin=0; vmax=max(yhat.max(),ytrue.max())\n",
        "    for h in range(HORIZON):\n",
        "        quick_plot(axes[h,0],ytrue[h],'Blues',f\"Real {horizon_dates[h]}\",horizon_dates[h],vmin,vmax)\n",
        "        quick_plot(axes[h,1],yhat [h],'Blues',f\"Pred {horizon_dates[h]}\",horizon_dates[h],vmin,vmax)\n",
        "        err=np.clip(np.abs((ytrue[h]-yhat[h])/(ytrue[h]+1e-5))*100,0,100)\n",
        "        quick_plot(axes[h,2],err,'Reds',f\"MAPE% {horizon_dates[h]}\",horizon_dates[h],0,100)\n",
        "    fig.suptitle(f\"{tag}  — Eval final ventana\",fontsize=16); fig.tight_layout();\n",
        "    fig.savefig(BASE_MODEL_DIR/f\"fig_{tag}.png\", dpi=700, bbox_inches='tight'); plt.close(fig)\n",
        "\n",
        "    # — GIF —\n",
        "    frames=[]; pcm_min,pcm_max=0,yhat.max()\n",
        "    for h in range(HORIZON):\n",
        "        figg,ax=plt.subplots(1,1,figsize=(14,12),subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "        m=ax.pcolormesh(ds.longitude,ds.latitude,yhat[h],cmap='Blues',shading='nearest',vmin=pcm_min,vmax=pcm_max,transform=ccrs.PlateCarree())\n",
        "        ax.coastlines(); ax.set_title(f\"{tag} – {horizon_dates[h]}\"); figg.colorbar(m,ax=ax,fraction=0.046,pad=0.04)\n",
        "        tmp=GIF_DIR/f\"tmp_{tag}_{horizon_dates[h]}.png\"; figg.savefig(tmp, dpi=700, bbox_inches='tight'); plt.close(figg)\n",
        "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
        "    imageio.mimsave(GIF_DIR/f\"{tag}.gif\",frames,fps=0.5)\n",
        "    print(\"💾 GIF\",f\"{tag}.gif\",\"creado\")\n",
        "\n",
        "# ——— Save table ———\n",
        "pd.DataFrame(all_metrics).to_csv(BASE_MODEL_DIR/'metrics_eval.csv',index=False)\n",
        "print(\"📑 Metrics saved in\",BASE_MODEL_DIR/'metrics_eval.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmizLc0IqjtW"
      },
      "outputs": [],
      "source": [
        "# Function to calculate dates from reference date\n",
        "def calculate_dates_from_reference(ref_date):\n",
        "    \"\"\"Calculate training, validation and test dates from a reference date\"\"\"\n",
        "    # Convert to datetime if string\n",
        "    if isinstance(ref_date, str):\n",
        "        ref_date = pd.to_datetime(ref_date)\n",
        "\n",
        "    # Test period starts at reference date and extends for HORIZON months\n",
        "    test_start = ref_date\n",
        "    test_end = ref_date + pd.DateOffset(months=HORIZON-1)\n",
        "\n",
        "    # Validation period is VALIDATION_WINDOW months before test period\n",
        "    val_end = test_start - pd.DateOffset(days=1)  # Day before test starts\n",
        "    val_start = val_end - pd.DateOffset(months=VALIDATION_WINDOW-1)\n",
        "\n",
        "    # Training period is TRAINING_WINDOW months before validation period\n",
        "    train_end = val_start - pd.DateOffset(days=1)  # Day before validation starts\n",
        "    train_start = train_end - pd.DateOffset(months=TRAINING_WINDOW-1)\n",
        "\n",
        "    return {\n",
        "        'train_start': train_start,\n",
        "        'train_end': train_end,\n",
        "        'val_start': val_start,\n",
        "        'val_end': val_end,\n",
        "        'test_start': test_start,\n",
        "        'test_end': test_end\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJWbcke3qjtW"
      },
      "outputs": [],
      "source": [
        "# Update the evaluation section to use reference date approach\n",
        "def evaluate_with_ref_date(fold_name, tag, model_path):\n",
        "    \"\"\"Evaluate a model using the reference date approach\"\"\"\n",
        "    if fold_name in FOLDS:\n",
        "        # Calculate dates from fold reference date\n",
        "        ref_date = pd.to_datetime(FOLDS[fold_name]['ref_date'])\n",
        "        # Test period starts at reference date\n",
        "        horizon_dates = pd.date_range(ref_date, periods=HORIZON, freq='MS')\n",
        "        horizon_dates = [date.strftime('%Y-%m') for date in horizon_dates]\n",
        "        print(f\"Using reference date {ref_date} for evaluation\")\n",
        "    else:\n",
        "        # Fallback to using the last dates in the dataset\n",
        "        horizon_dates = pd.date_range(times[-HORIZON], periods=HORIZON, freq='MS')\n",
        "        horizon_dates = [date.strftime('%Y-%m') for date in horizon_dates]\n",
        "        print(f\"Using fallback dates for evaluation\")\n",
        "\n",
        "    return horizon_dates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbyj_vPBXqrQ"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── ATTENTION LAYERS ─────────────────────────\n",
        "\n",
        "class SpatialAttention(Layer):\n",
        "    \"\"\"Spatial attention to highlight important regions\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.conv = Conv2D(1, (7, 7), padding='same', activation='sigmoid')\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Calculate channel statistics\n",
        "        avg_pool = K.mean(inputs, axis=-1, keepdims=True)\n",
        "        max_pool = K.max(inputs, axis=-1, keepdims=True)\n",
        "        concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
        "\n",
        "        # Generate attention map\n",
        "        attention = self.conv(concat)\n",
        "\n",
        "        return Multiply()([inputs, attention])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class ChannelAttention(Layer):\n",
        "    \"\"\"Channel attention to highlight important features\"\"\"\n",
        "\n",
        "    def __init__(self, reduction_ratio=8, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "        self.fc1 = Dense(channels // self.reduction_ratio, activation='relu')\n",
        "        self.fc2 = Dense(channels, activation='sigmoid')\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Global pooling\n",
        "        avg_pool = GlobalAveragePooling2D()(inputs)\n",
        "        max_pool = K.max(inputs, axis=[1, 2])\n",
        "\n",
        "        # Shared MLP\n",
        "        avg_out = self.fc2(self.fc1(avg_pool))\n",
        "        max_out = self.fc2(self.fc1(max_pool))\n",
        "\n",
        "        # Combine\n",
        "        attention = avg_out + max_out\n",
        "        attention = K.expand_dims(K.expand_dims(attention, 1), 1)\n",
        "\n",
        "        return Multiply()([inputs, attention])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'reduction_ratio': self.reduction_ratio\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class CBAM(Layer):\n",
        "    \"\"\"Convolutional Block Attention Module\"\"\"\n",
        "\n",
        "    def __init__(self, reduction_ratio=8, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.channel_attention = ChannelAttention(self.reduction_ratio)\n",
        "        self.spatial_attention = SpatialAttention()\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.channel_attention(inputs)\n",
        "        x = self.spatial_attention(x)\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'reduction_ratio': self.reduction_ratio\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"✅ Layers of care implemented\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yqAzSOTXqrQ"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── ADVANCED LAYERS ─────────────────────────\n",
        "\n",
        "class ConvGRU2DCell(Layer):\n",
        "    \"\"\"Improved ConvGRU2D cell with BatchNorm\"\"\"\n",
        "\n",
        "    def __init__(self, filters, kernel_size, padding='same', activation='tanh',\n",
        "                 recurrent_activation='sigmoid', use_batch_norm=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.padding = padding\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.recurrent_activation = tf.keras.activations.get(recurrent_activation)\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.state_size = (filters,)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        # Kernels\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(*self.kernel_size, input_dim, self.filters * 3),\n",
        "            initializer='glorot_uniform',\n",
        "            regularizer=l1_l2(l1=0, l2=L2_REG),\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(*self.kernel_size, self.filters, self.filters * 3),\n",
        "            initializer='orthogonal',\n",
        "            regularizer=l1_l2(l1=0, l2=L2_REG),\n",
        "            name='recurrent_kernel'\n",
        "        )\n",
        "\n",
        "        self.bias = self.add_weight(\n",
        "            shape=(self.filters * 3,),\n",
        "            initializer='zeros',\n",
        "            name='bias'\n",
        "        )\n",
        "\n",
        "        if self.use_batch_norm:\n",
        "            self.bn_x = BatchNormalization()\n",
        "            self.bn_h = BatchNormalization()\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        h_tm1 = states[0]\n",
        "\n",
        "        # Convolutions\n",
        "        x_conv = K.conv2d(inputs, self.kernel, padding=self.padding)\n",
        "        h_conv = K.conv2d(h_tm1, self.recurrent_kernel, padding=self.padding)\n",
        "\n",
        "        if self.use_batch_norm:\n",
        "            x_conv = self.bn_x(x_conv, training=training)\n",
        "            h_conv = self.bn_h(h_conv, training=training)\n",
        "\n",
        "        x_z, x_r, x_h = tf.split(x_conv, 3, axis=-1)\n",
        "        h_z, h_r, h_h = tf.split(h_conv, 3, axis=-1)\n",
        "        b_z, b_r, b_h = tf.split(self.bias, 3)\n",
        "\n",
        "        # Gates\n",
        "        z = self.recurrent_activation(x_z + h_z + b_z)\n",
        "        r = self.recurrent_activation(x_r + h_r + b_r)\n",
        "\n",
        "        # Hidden state\n",
        "        h_candidate = self.activation(x_h + r * h_h + b_h)\n",
        "        h = (1 - z) * h_tm1 + z * h_candidate\n",
        "\n",
        "        return h, [h]\n",
        "\n",
        "\n",
        "class ConvGRU2D(Layer):\n",
        "    \"\"\"Improved ConvGRU2D with support for BatchNorm and Dropout\"\"\"\n",
        "\n",
        "    def __init__(self, filters, kernel_size, padding='same', activation='tanh',\n",
        "                 recurrent_activation='sigmoid', return_sequences=False,\n",
        "                 use_batch_norm=True, dropout=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.activation = activation\n",
        "        self.recurrent_activation = recurrent_activation\n",
        "        self.return_sequences = return_sequences\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.cell = ConvGRU2DCell(\n",
        "            filters, kernel_size, padding, activation,\n",
        "            recurrent_activation, use_batch_norm\n",
        "        )\n",
        "\n",
        "        if dropout > 0:\n",
        "            self.dropout_layer = Dropout(dropout)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.cell.build(input_shape[2:])\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        time_steps = tf.shape(inputs)[1]\n",
        "        height = tf.shape(inputs)[2]\n",
        "        width = tf.shape(inputs)[3]\n",
        "\n",
        "        # Initial state\n",
        "        initial_state = tf.zeros((batch_size, height, width, self.filters))\n",
        "\n",
        "        # Process sequence\n",
        "        outputs = []\n",
        "        state = initial_state\n",
        "\n",
        "        for t in range(inputs.shape[1]):\n",
        "            output, [state] = self.cell(inputs[:, t], [state], training=training)\n",
        "\n",
        "            if self.dropout > 0:\n",
        "                output = self.dropout_layer(output, training=training)\n",
        "\n",
        "            outputs.append(output)\n",
        "\n",
        "        outputs = tf.stack(outputs, axis=1)\n",
        "\n",
        "        if self.return_sequences:\n",
        "            return outputs\n",
        "        else:\n",
        "            return outputs[:, -1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'filters': self.filters,\n",
        "            'kernel_size': self.kernel_size,\n",
        "            'padding': self.padding,\n",
        "            'activation': self.activation,\n",
        "            'recurrent_activation': self.recurrent_activation,\n",
        "            'return_sequences': self.return_sequences,\n",
        "            'use_batch_norm': self.use_batch_norm,\n",
        "            'dropout': self.dropout\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"✅ Enhanced ConvGRU layers implemented\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0t1ixIxXqrR"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── ADVANCED MODEL BUILDERS ─────────────────────────\n",
        "\n",
        "def _advanced_spatial_head(x, use_attention=True):\n",
        "    \"\"\"Cabeza de proyección mejorada con atención opcional\"\"\"\n",
        "\n",
        "    if use_attention:\n",
        "        x = CBAM()(x)\n",
        "\n",
        "    # Multi-scale processing\n",
        "    conv1 = Conv2D(HORIZON, (1, 1), padding='same')(x)\n",
        "    conv3 = Conv2D(HORIZON, (3, 3), padding='same')(x)\n",
        "    conv5 = Conv2D(HORIZON, (5, 5), padding='same')(x)\n",
        "\n",
        "    # Combine multi-scale features\n",
        "    x = Add()([conv1, conv3, conv5])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('linear')(x)\n",
        "\n",
        "    # Reshape to output format using Permute and Reshape\n",
        "    x = Permute((3, 1, 2))(x)  # From (batch, H, W, HORIZON) to (batch, HORIZON, H, W)\n",
        "    x = Reshape((HORIZON, lat, lon, 1))(x)  # Add channel dimension\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_convlstm_attention(n_feats: int):\n",
        "    \"\"\"ConvLSTM with attention mechanism\"\"\"\n",
        "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
        "\n",
        "    # First layer with more filters\n",
        "    x = ConvLSTM2D(64, (3, 3), padding='same', return_sequences=True,\n",
        "                   kernel_regularizer=l1_l2(l1=0, l2=L2_REG))(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(DROPOUT)(x)\n",
        "\n",
        "    # Second layer with attention\n",
        "    x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True,\n",
        "                   kernel_regularizer=l1_l2(l1=0, l2=L2_REG))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Apply temporal attention\n",
        "    x = TimeDistributed(CBAM())(x)\n",
        "\n",
        "    # Final layer\n",
        "    x = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=False,\n",
        "                   kernel_regularizer=l1_l2(l1=0, l2=L2_REG))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    out = _advanced_spatial_head(x)\n",
        "    return Model(inp, out, name='ConvLSTM_Attention')\n",
        "\n",
        "\n",
        "def build_convgru_residual(n_feats: int):\n",
        "    \"\"\"ConvGRU with skip connections\"\"\"\n",
        "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
        "\n",
        "    # Encoder path\n",
        "    enc1 = ConvGRU2D(64, (3, 3), return_sequences=True,\n",
        "                     use_batch_norm=True, dropout=DROPOUT)(inp)\n",
        "\n",
        "    enc2 = ConvGRU2D(32, (3, 3), return_sequences=True,\n",
        "                     use_batch_norm=True, dropout=DROPOUT)(enc1)\n",
        "\n",
        "    # Bottleneck\n",
        "    bottleneck = ConvGRU2D(16, (3, 3), return_sequences=False,\n",
        "                           use_batch_norm=True)(enc2)\n",
        "\n",
        "    # Skip connection from input - use only the last timestep\n",
        "    skip = TimeDistributed(Conv2D(16, (1, 1), padding='same'))(inp)\n",
        "    # Use Lambda for slicing\n",
        "    skip = Lambda(lambda x: x[:, -1, :, :, :])(skip)  # Take last timestep\n",
        "\n",
        "    # Combine\n",
        "    x = Add()([bottleneck, skip])\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    out = _advanced_spatial_head(x)\n",
        "    return Model(inp, out, name='ConvGRU_Residual')\n",
        "\n",
        "\n",
        "def build_hybrid_transformer(n_feats: int):\n",
        "    \"\"\"Hybrid CNN + Transformer model\"\"\"\n",
        "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
        "\n",
        "    # Encoder convolucional\n",
        "    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(inp)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "\n",
        "    # Reduce spatial dimensionality\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2), padding='same'))(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # Self-attention temporal\n",
        "    x = MultiHeadAttention(num_heads=4, key_dim=32, dropout=DROPOUT)(x, x)\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    # Temporal aggregation with LSTM\n",
        "    x = LSTM(128, return_sequences=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(DROPOUT)(x)\n",
        "\n",
        "    # Spatial decoder\n",
        "    x = Dense(lat * lon * 16)(x)\n",
        "    x = Reshape((lat, lon, 16))(x)\n",
        "\n",
        "    out = _advanced_spatial_head(x)\n",
        "    return Model(inp, out, name='Hybrid_Transformer')\n",
        "\n",
        "\n",
        "# Dictionary of models\n",
        "ADVANCED_MODELS = {\n",
        "    'ConvLSTM_Att': build_convlstm_attention,\n",
        "    'ConvGRU_Res': build_convgru_residual,\n",
        "    'Hybrid_Trans': build_hybrid_transformer\n",
        "}\n",
        "\n",
        "print(\"✅ Advanced model builders created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66lB5g6BXqrR"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── ADVANCED CALLBACKS ─────────────────────────\n",
        "\n",
        "class AdvancedTrainingMonitor(Callback):\n",
        "    \"\"\"Monitor of training with real-time visualization\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, experiment_name, patience=10):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.experiment_name = experiment_name\n",
        "        self.patience = patience\n",
        "        self.history = {'loss': [], 'val_loss': [], 'lr': [], 'epoch': []}\n",
        "        self.wait = 0\n",
        "        self.best_val_loss = np.inf\n",
        "        self.converged = False\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Update history\n",
        "        self.history['loss'].append(logs.get('loss', 0))\n",
        "        self.history['val_loss'].append(logs.get('val_loss', 0))\n",
        "        self.history['lr'].append(K.get_value(self.model.optimizer.learning_rate))\n",
        "        self.history['epoch'].append(epoch + 1)\n",
        "\n",
        "        # Check improvement\n",
        "        current_val_loss = logs.get('val_loss', 0)\n",
        "        if current_val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = current_val_loss\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "\n",
        "        # Check convergence\n",
        "        if len(self.history['val_loss']) > 5:\n",
        "            recent_losses = self.history['val_loss'][-5:]\n",
        "            loss_std = np.std(recent_losses)\n",
        "            loss_mean = np.mean(recent_losses)\n",
        "            if loss_std / loss_mean < 0.01:  # Less than 1% variation\n",
        "                self.converged = True\n",
        "\n",
        "        # Visualization every 5 epochs or in the last\n",
        "        if (epoch + 1) % 5 == 0 or (epoch + 1) == self.params['epochs']:\n",
        "            self._plot_progress()\n",
        "\n",
        "    def _plot_progress(self):\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        fig = plt.figure(figsize=(24, 8))\n",
        "\n",
        "        # Loss curves\n",
        "        ax1 = plt.subplot(141)\n",
        "        ax1.plot(self.history['epoch'], self.history['loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "        ax1.plot(self.history['epoch'], self.history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title(f'{self.model_name} - {self.experiment_name}')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss ratio\n",
        "        ax2 = plt.subplot(142)\n",
        "        if len(self.history['loss']) > 0:\n",
        "            ratio = [v/t if t > 0 else 1 for v, t in zip(self.history['val_loss'], self.history['loss'])]\n",
        "            ax2.plot(self.history['epoch'], ratio, 'g-', linewidth=2)\n",
        "            ax2.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
        "            ax2.fill_between(self.history['epoch'], 1, ratio,\n",
        "                           where=[r > 1 for r in ratio],\n",
        "                           color='red', alpha=0.2, label='Overfitting')\n",
        "            ax2.set_xlabel('Epoch')\n",
        "            ax2.set_ylabel('Val Loss / Train Loss')\n",
        "            ax2.set_title('Overfitting Monitor')\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Improvement rate and convergence\n",
        "        ax3 = plt.subplot(143)\n",
        "        if len(self.history['val_loss']) > 1:\n",
        "            # Calculate improvement rate epoch to epoch\n",
        "            improvements = []\n",
        "            for i in range(1, len(self.history['val_loss'])):\n",
        "                prev_loss = self.history['val_loss'][i-1]\n",
        "                curr_loss = self.history['val_loss'][i]\n",
        "                improvement = ((prev_loss - curr_loss) / prev_loss) * 100\n",
        "                improvements.append(improvement)\n",
        "\n",
        "            # Plot improvement rate\n",
        "            ax3.plot(self.history['epoch'][1:], improvements, 'purple', linewidth=2, alpha=0.7)\n",
        "            ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "            ax3.fill_between(self.history['epoch'][1:], 0, improvements,\n",
        "                           where=[imp > 0 for imp in improvements],\n",
        "                           color='green', alpha=0.3, label='Improvement')\n",
        "            ax3.fill_between(self.history['epoch'][1:], 0, improvements,\n",
        "                           where=[imp < 0 for imp in improvements],\n",
        "                           color='red', alpha=0.3, label='Worsening')\n",
        "\n",
        "            # Trend line\n",
        "            if len(improvements) > 3:\n",
        "                z = np.polyfit(range(len(improvements)), improvements, 2)\n",
        "                p = np.poly1d(z)\n",
        "                ax3.plot(self.history['epoch'][1:], p(range(len(improvements))),\n",
        "                       'orange', linewidth=2, linestyle='--', label='Trend')\n",
        "\n",
        "            ax3.set_xlabel('Epoch')\n",
        "            ax3.set_ylabel('Improvement (%)')\n",
        "            ax3.set_title('Improvement and Convergence')\n",
        "            ax3.legend()\n",
        "            ax3.grid(True, alpha=0.3)\n",
        "\n",
        "            # Convergence indicator\n",
        "            if self.converged:\n",
        "                ax3.text(0.02, 0.98, '✓ Converged', transform=ax3.transAxes,\n",
        "                       va='top', bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n",
        "\n",
        "        # Training stats\n",
        "        ax4 = plt.subplot(144)\n",
        "        ax4.axis('off')\n",
        "        stats_text = f\"\"\"\n",
        "        {self.model_name} - {self.experiment_name}\n",
        "\n",
        "        Epoch: {self.history['epoch'][-1]}/{self.params['epochs']}\n",
        "\n",
        "        Current loss:\n",
        "        • Train: {self.history['loss'][-1]:.6f}\n",
        "        • Val: {self.history['val_loss'][-1]:.6f}\n",
        "\n",
        "        Best val loss: {self.best_val_loss:.6f}\n",
        "        Epochs without improvement: {self.wait}/{self.patience}\n",
        "\n",
        "        Learning rate: {self.history['lr'][-1]:.2e}\n",
        "\n",
        "        State: {'Converged ✓' if self.converged else 'Training...'}\n",
        "        \"\"\"\n",
        "        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes,\n",
        "                fontsize=12, verticalalignment='top', fontfamily='monospace')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def create_callbacks(model_name, experiment_name, model_path):\n",
        "    \"\"\"Create optimized callbacks for training\"\"\"\n",
        "\n",
        "    # Learning rate scheduler with warmup\n",
        "    def lr_schedule(epoch, lr):\n",
        "        warmup_epochs = 5\n",
        "        if epoch < warmup_epochs:\n",
        "            return LR * (epoch + 1) / warmup_epochs\n",
        "        else:\n",
        "            # Cosine decay after warmup\n",
        "            progress = (epoch - warmup_epochs) / (EPOCHS - warmup_epochs)\n",
        "            return LR * 0.5 * (1 + np.cos(np.pi * progress))\n",
        "\n",
        "    callbacks = [\n",
        "        # Advanced training monitor\n",
        "        AdvancedTrainingMonitor(model_name, experiment_name, patience=PATIENCE),\n",
        "\n",
        "        # Improved early stopping\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=PATIENCE,\n",
        "            restore_best_weights=True,\n",
        "            verbose=0,\n",
        "            mode='min',\n",
        "            min_delta=1e-4\n",
        "        ),\n",
        "\n",
        "        # Model checkpoint\n",
        "        ModelCheckpoint(\n",
        "            str(model_path),\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode='min',\n",
        "            verbose=0\n",
        "        ),\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        LearningRateScheduler(lr_schedule, verbose=0),\n",
        "\n",
        "        # Reduce LR on plateau as backup\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6,\n",
        "            verbose=0,\n",
        "            mode='min',\n",
        "            min_delta=1e-4\n",
        "        ),\n",
        "\n",
        "        # CSV logger for later analysis\n",
        "        CSVLogger(\n",
        "            str(model_path.parent / f\"{model_name}_training.csv\"),\n",
        "            separator=',',\n",
        "            append=False\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evb3tTGPXqrS"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── HELPERS ─────────────────────────\n",
        "\n",
        "def windowed_arrays(X:np.ndarray, y:np.ndarray):\n",
        "    \"\"\"Create sliding windows for time series\"\"\"\n",
        "    seq_X, seq_y = [], []\n",
        "    T = len(X)\n",
        "    for start in range(T-INPUT_WINDOW-HORIZON+1):\n",
        "        end_w = start+INPUT_WINDOW\n",
        "        end_y = end_w+HORIZON\n",
        "        Xw, yw = X[start:end_w], y[end_w:end_y]\n",
        "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
        "            continue\n",
        "        seq_X.append(Xw)\n",
        "        seq_y.append(yw)\n",
        "    return np.asarray(seq_X,dtype=np.float32), np.asarray(seq_y,dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_hyperparameters(exp_path, model_name, hyperparams):\n",
        "    \"\"\"Save hyperparameters in a JSON file\"\"\"\n",
        "    hp_file = exp_path / f\"{model_name}_hyperparameters.json\"\n",
        "    with open(hp_file, 'w') as f:\n",
        "        json.dump(hyperparams, f, indent=4)\n",
        "    print(f\"   💾 Hiperparámetros guardados en: {hp_file.name}\")\n",
        "\n",
        "\n",
        "def plot_learning_curves(history, exp_path, model_name, show=True):\n",
        "    \"\"\"Generate and save learning curves\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "    axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss (MSE)')\n",
        "    axes[0].set_title(f'{model_name} - Loss Evolution')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Convergence and Stability Analysis\n",
        "    val_losses = history.history['val_loss']\n",
        "    train_losses = history.history['loss']\n",
        "\n",
        "    if len(val_losses) > 1:\n",
        "        # Calculate convergence metrics\n",
        "        epochs = range(1, len(val_losses) + 1)\n",
        "\n",
        "        # 1. Overfitting ratio\n",
        "        overfit_ratio = [val_losses[i] / train_losses[i] for i in range(len(val_losses))]\n",
        "\n",
        "        # 2. Stability (moving standard deviation)\n",
        "        window = min(5, len(val_losses)//3)\n",
        "        val_std = pd.Series(val_losses).rolling(window=window).std()\n",
        "\n",
        "        # Create subplot with two Y axes\n",
        "        ax2_left = axes[1]\n",
        "        ax2_right = ax2_left.twinx()\n",
        "\n",
        "        # Plot overfitting ratio\n",
        "        line1 = ax2_left.plot(epochs, overfit_ratio, 'r-', linewidth=2,\n",
        "                             label='Ratio Val/Train', alpha=0.8)\n",
        "        ax2_left.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
        "        ax2_left.fill_between(epochs, 1.0, overfit_ratio,\n",
        "                            where=[x > 1.0 for x in overfit_ratio],\n",
        "                            color='red', alpha=0.2)\n",
        "        ax2_left.set_xlabel('Epoch')\n",
        "        ax2_left.set_ylabel('Ratio Val Loss / Train Loss', color='red')\n",
        "        ax2_left.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "        # Plot estabilidad\n",
        "        line2 = ax2_right.plot(epochs[window-1:], val_std[window-1:], 'b-',\n",
        "                             linewidth=2, label='Stability', alpha=0.8)\n",
        "        ax2_right.set_ylabel('Standard Deviation (moving window)', color='blue')\n",
        "        ax2_right.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "        # Combined title and legend\n",
        "        ax2_left.set_title(f'{model_name} - Convergence Analysis')\n",
        "\n",
        "        # Combine legends\n",
        "        lines = line1 + line2\n",
        "        labels = [l.get_label() for l in lines]\n",
        "        ax2_left.legend(lines, labels, loc='upper left')\n",
        "\n",
        "        ax2_left.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add interpretation zones\n",
        "        if max(overfit_ratio) > 1.5:\n",
        "            ax2_left.text(0.02, 0.98, '⚠️ High overfitting detected',\n",
        "                        transform=ax2_left.transAxes, va='top',\n",
        "                        bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
        "        elif min(val_std[window-1:]) < 0.001:\n",
        "            ax2_left.text(0.02, 0.98, '✓ Stable training',\n",
        "                        transform=ax2_left.transAxes, va='top',\n",
        "                        bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n",
        "    else:\n",
        "        axes[1].text(0.5, 0.5, 'Insufficient data for convergence analysis',\n",
        "                    transform=axes[1].transAxes, ha='center', va='center',\n",
        "                    fontsize=12, color='gray')\n",
        "        axes[1].set_title(f'{model_name} - Convergence Analysis')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save\n",
        "    curves_path = exp_path / f\"{model_name}_learning_curves.png\"\n",
        "    plt.savefig(curves_path, dpi=700, bbox_inches='tight')\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "    return curves_path\n",
        "\n",
        "\n",
        "def print_training_summary(history, model_name, exp_name):\n",
        "    \"\"\"Print a summary of the training\"\"\"\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    final_val_loss = history.history['val_loss'][-1]\n",
        "    best_val_loss = min(history.history['val_loss'])\n",
        "    best_epoch = history.history['val_loss'].index(best_val_loss) + 1\n",
        "\n",
        "    print(f\"\\n   📊 Training summary {model_name} - {exp_name}:\")\n",
        "    print(f\"      • Total epochs: {len(history.history['loss'])}\")\n",
        "    print(f\"      • Loss final (train): {final_loss:.6f}\")\n",
        "    print(f\"      • Loss final (val): {final_val_loss:.6f}\")\n",
        "    print(f\"      • Best loss (val): {best_val_loss:.6f} in epoch {best_epoch}\")\n",
        "    if 'lr' in history.history and len(history.history['lr']) > 0:\n",
        "        final_lr = history.history['lr'][-1]\n",
        "        print(f\"      • Learning rate final: {final_lr:.2e}\")\n",
        "    else:\n",
        "        print(f\"      • Learning rate final: Not available\")\n",
        "\n",
        "print(\"✅ Helper functions created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dU6DGcHXqrS"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── TRAIN + EVAL LOOP ─────────────────────────\n",
        "\n",
        "# Dictionary for storing training histories\n",
        "all_histories = {}\n",
        "results = []\n",
        "\n",
        "for exp_name, exp_cfg in EXPERIMENTS.items():\n",
        "\n",
        "    # Ensure the experiment is active (based on the original definition)\n",
        "    if not exp_cfg.get('active', True): # Safely get 'active', default to True if missing\n",
        "         print(f\"\\nSkipping inactive experiment: {exp_name}\")\n",
        "         continue\n",
        "\n",
        "    feat_list = exp_cfg['feature_list'] # Get the list of feature names\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"🔬 EXPERIMENT: {exp_name} ({len(feat_list)} features)\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Prepare data\n",
        "    Xarr = ds[feat_list].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "    yarr = ds['total_precipitation'].values.astype(np.float32)[...,None]\n",
        "    X, y = windowed_arrays(Xarr, yarr)\n",
        "    split = int(0.8*len(X))\n",
        "    val_split = int(0.9*len(X))\n",
        "\n",
        "    # Normalization\n",
        "    sx = StandardScaler().fit(X[:split].reshape(-1,len(feat_list)))\n",
        "    sy = StandardScaler().fit(y[:split].reshape(-1,1))\n",
        "    X_sc = sx.transform(X.reshape(-1,len(feat_list))).reshape(X.shape)\n",
        "    y_sc = sy.transform(y.reshape(-1,1)).reshape(y.shape)\n",
        "\n",
        "    # Splits\n",
        "    X_tr, X_va, X_te = X_sc[:split], X_sc[split:val_split], X_sc[val_split:]\n",
        "    y_tr, y_va, y_te = y_sc[:split], y_sc[split:val_split], y_sc[val_split:]\n",
        "\n",
        "    print(f\"   Datos: Train={len(X_tr)}, Val={len(X_va)}, Test={len(X_te)}\")\n",
        "\n",
        "    OUT_EXP = OUT_ROOT/exp_name\n",
        "    OUT_EXP.mkdir(exist_ok=True)\n",
        "\n",
        "    # Create subdirectory for training metrics\n",
        "    METRICS_DIR = OUT_EXP / 'training_metrics'\n",
        "    METRICS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "    for mdl_name, builder in ADVANCED_MODELS.items():\n",
        "        print(f\"\\n{'─'*50}\")\n",
        "        print(f\"🤖 Modelo: {mdl_name}\")\n",
        "        print(f\"{'─'*50}\")\n",
        "\n",
        "        model_path = OUT_EXP/f\"{mdl_name.lower()}_best.keras\"\n",
        "        if model_path.exists():\n",
        "            model_path.unlink()\n",
        "\n",
        "        try:\n",
        "            # Build model\n",
        "            model = builder(n_feats=len(feat_list))\n",
        "\n",
        "            # Define optimizer with explicit configuration\n",
        "            optimizer = AdamW(learning_rate=LR, weight_decay=L2_REG)\n",
        "            model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "            # Hyperparameters\n",
        "            hyperparams = {\n",
        "                'experiment': exp_name,\n",
        "                'model': mdl_name,\n",
        "                'features': [str(f) for f in feat_list],  # Convert to strings\n",
        "                'n_features': int(len(feat_list)),\n",
        "                'input_window': int(INPUT_WINDOW),\n",
        "                'horizon': int(HORIZON),\n",
        "                'batch_size': int(BATCH_SIZE),\n",
        "                'initial_lr': float(LR),\n",
        "                'epochs': int(EPOCHS),\n",
        "                'patience': int(PATIENCE),\n",
        "                'dropout': float(DROPOUT),\n",
        "                'l2_reg': float(L2_REG),\n",
        "                'train_samples': int(len(X_tr)),\n",
        "                'val_samples': int(len(X_va)),\n",
        "                'test_samples': int(len(X_te)),\n",
        "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'model_params': int(model.count_params())\n",
        "            }\n",
        "\n",
        "            # Save hyperparameters\n",
        "            save_hyperparameters(METRICS_DIR, mdl_name, hyperparams)\n",
        "\n",
        "            # Callbacks\n",
        "            callbacks = create_callbacks(mdl_name, exp_name, model_path)\n",
        "\n",
        "            # Train with verbose=0 to use our custom monitor\n",
        "            print(f\"\\n🏃 Iniciando entrenamiento...\")\n",
        "            print(f\"   📊 Visualización en tiempo real activada\")\n",
        "            print(f\"   📈 Parámetros del modelo: {model.count_params():,}\")\n",
        "\n",
        "            history = model.fit(\n",
        "                X_tr, y_tr,\n",
        "                validation_data=(X_va, y_va),\n",
        "                epochs=EPOCHS,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                callbacks=callbacks,\n",
        "                verbose=0  # Use 0 to only show our custom monitor\n",
        "            )\n",
        "\n",
        "            # Save history\n",
        "            all_histories[f\"{exp_name}_{mdl_name}\"] = history\n",
        "\n",
        "            # Show training summary\n",
        "            print_training_summary(history, mdl_name, exp_name)\n",
        "\n",
        "            # Plot and save learning curves\n",
        "            plot_learning_curves(history, METRICS_DIR, mdl_name, show=True)\n",
        "\n",
        "            # Save history as JSON\n",
        "            # Get learning rates from training monitor if not in history\n",
        "            training_monitor = [cb for cb in callbacks if isinstance(cb, AdvancedTrainingMonitor)][0]\n",
        "            lr_values = history.history.get('lr', [])\n",
        "            if not lr_values and hasattr(training_monitor, 'history'):\n",
        "                lr_values = training_monitor.history['lr']\n",
        "\n",
        "            history_dict = {\n",
        "                'loss': [float(x) for x in history.history['loss']],\n",
        "                'val_loss': [float(x) for x in history.history['val_loss']],\n",
        "                'mae': [float(x) for x in history.history.get('mae', [])],\n",
        "                'val_mae': [float(x) for x in history.history.get('val_mae', [])],\n",
        "                'lr': [float(x) for x in lr_values] if lr_values else []\n",
        "            }\n",
        "\n",
        "            with open(METRICS_DIR / f\"{mdl_name}_history.json\", 'w') as f:\n",
        "                json.dump(history_dict, f, indent=4)\n",
        "\n",
        "            # ─ Evaluation on Test Set ─\n",
        "            print(f\"\\n📊 Evaluating on test set...\")\n",
        "            test_loss, test_mae = model.evaluate(X_te, y_te, verbose=0)\n",
        "            print(f\"   Test Loss: {test_loss:.6f}, Test MAE: {test_mae:.6f}\")\n",
        "\n",
        "            # ─ Predictions and visualization ─\n",
        "            print(f\"\\n🎯 Generating predictions...\")\n",
        "            # Use the first 5 samples of the test set\n",
        "            sample_indices = min(5, len(X_te))\n",
        "            y_hat_sc = model.predict(X_te[:sample_indices], verbose=0)\n",
        "            y_hat = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(-1,HORIZON,lat,lon)\n",
        "            y_true = sy.inverse_transform(y_te[:sample_indices].reshape(-1,1)).reshape(-1,HORIZON,lat,lon)\n",
        "\n",
        "            # ─ Evaluation metrics by horizon ─\n",
        "            # Define forecast dates for visualization\n",
        "            forecast_dates = pd.date_range(ds.time.values[-HORIZON], periods=HORIZON, freq='MS')\n",
        "\n",
        "            for h in range(HORIZON):\n",
        "                rmse = np.sqrt(mean_squared_error(y_true[:,h].ravel(), y_hat[:,h].ravel()))\n",
        "                mae = mean_absolute_error(y_true[:,h].ravel(), y_hat[:,h].ravel())\n",
        "                r2 = r2_score(y_true[:,h].ravel(), y_hat[:,h].ravel())\n",
        "\n",
        "                # Get horizon date for this forecast step\n",
        "                horizon_date = forecast_dates[h].strftime('%Y-%m')\n",
        "\n",
        "                results.append({\n",
        "                    'Experiment': exp_name,\n",
        "                    'Model': mdl_name,\n",
        "                    'horizon': horizon_date,\n",
        "                    'RMSE': rmse,\n",
        "                    'MAE': mae,\n",
        "                    'R2': r2,\n",
        "                    'Test_Loss': test_loss,\n",
        "                    'Parameters': model.count_params()\n",
        "                })\n",
        "\n",
        "                print(f\"   📈 {horizon_date}: RMSE={rmse:.4f}, MAE={mae:.4f}, R²={r2:.4f}\")\n",
        "\n",
        "            # ─ Maps & GIF ─\n",
        "            print(f\"\\n🎨 Generating visualizations...\")\n",
        "            # Use the first sample for visualization\n",
        "            sample_idx = 0\n",
        "            vmin, vmax = 0, max(y_true[sample_idx].max(), y_hat[sample_idx].max())\n",
        "            frames = []\n",
        "            # Use the previously defined forecast_dates variable\n",
        "\n",
        "            for h in range(HORIZON):\n",
        "                err = np.clip(np.abs((y_true[sample_idx,h]-y_hat[sample_idx,h])/(y_true[sample_idx,h]+1e-5))*100, 0, 100)\n",
        "                try:\n",
        "                    import cartopy.crs as ccrs\n",
        "                    fig, axs = plt.subplots(1, 3, figsize=(20, 8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
        "                except ImportError:\n",
        "                    fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
        "\n",
        "                # Real\n",
        "                real_mesh = axs[0].pcolormesh(ds.longitude, ds.latitude, y_true[sample_idx,h],\n",
        "                                            cmap='Blues', shading='nearest', vmin=vmin, vmax=vmax,\n",
        "                                            transform=ccrs.PlateCarree())\n",
        "                axs[0].coastlines()\n",
        "                axs[0].add_geometries(DEPT_GDF.geometry, ccrs.PlateCarree(),\n",
        "                                    edgecolor='black', facecolor='none', linewidth=1)\n",
        "                axs[0].gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
        "                axs[0].set_title(f\"Real {forecast_dates[h].strftime('%Y-%m')}\", fontsize=11)\n",
        "                real_cbar = fig.colorbar(real_mesh, ax=axs[0], fraction=0.046, pad=0.04)\n",
        "                real_cbar.set_label('Precipitation (mm)', rotation=270, labelpad=15)\n",
        "\n",
        "                # Prediction\n",
        "                pred_mesh = axs[1].pcolormesh(ds.longitude, ds.latitude, y_hat[sample_idx,h],\n",
        "                                            cmap='Blues', shading='nearest', vmin=vmin, vmax=vmax,\n",
        "                                            transform=ccrs.PlateCarree())\n",
        "                axs[1].coastlines()\n",
        "                axs[1].add_geometries(DEPT_GDF.geometry, ccrs.PlateCarree(),\n",
        "                                     edgecolor='black', facecolor='none', linewidth=1)\n",
        "                axs[1].gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
        "                axs[1].set_title(f\"{mdl_name} {forecast_dates[h].strftime('%Y-%m')}\", fontsize=11)\n",
        "                pred_cbar = fig.colorbar(pred_mesh, ax=axs[1], fraction=0.046, pad=0.04)\n",
        "                pred_cbar.set_label('Precipitation (mm)', rotation=270, labelpad=15)\n",
        "\n",
        "                # Error\n",
        "                err_mesh = axs[2].pcolormesh(ds.longitude, ds.latitude, err,\n",
        "                                           cmap='Reds', shading='nearest', vmin=0, vmax=100,\n",
        "                                           transform=ccrs.PlateCarree())\n",
        "                axs[2].coastlines()\n",
        "                axs[2].add_geometries(DEPT_GDF.geometry, ccrs.PlateCarree(),\n",
        "                                    edgecolor='black', facecolor='none', linewidth=1)\n",
        "                axs[2].gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
        "                axs[2].set_title(f\"MAPE% {forecast_dates[h].strftime('%Y-%m')}\", fontsize=11)\n",
        "                err_cbar = fig.colorbar(err_mesh, ax=axs[2], fraction=0.046, pad=0.04)\n",
        "                err_cbar.set_label('MAPE (%)', rotation=270, labelpad=15)\n",
        "\n",
        "                fig.suptitle(f\"{mdl_name} – {exp_name} – {forecast_dates[h].strftime('%Y-%m')}\", fontsize=13)\n",
        "                horizon_date = forecast_dates[h].strftime('%Y-%m')\n",
        "                png = OUT_EXP/f\"{mdl_name}_{horizon_date}.png\"\n",
        "                fig.tight_layout()\n",
        "                fig.savefig(png, dpi=700, bbox_inches='tight')\n",
        "                plt.close(fig)\n",
        "                frames.append(imageio.imread(png))\n",
        "\n",
        "            imageio.mimsave(OUT_EXP/f\"{mdl_name}.gif\", frames, fps=0.5)\n",
        "            print(f\"   ✅ GIF saved: {OUT_EXP/f'{mdl_name}.gif'}\")\n",
        "\n",
        "            tf.keras.backend.clear_session()\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️ Error in {mdl_name}: {str(e)}\")\n",
        "            print(f\"  → Skipping {mdl_name} for {exp_name}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "# ───────────────────────── CSV FINAL ─────────────────────────\n",
        "res_df = pd.DataFrame(results)\n",
        "res_df.to_csv(OUT_ROOT/'metrics_advanced.csv', index=False)\n",
        "print(\"\\n📑 Metrics saved →\", OUT_ROOT/'metrics_advanced.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXK-acO1XqrT"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── COMPARATIVE VISUALIZATION ─────────────────────────\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📊 GENERATING COMPARATIVE VISUALIZATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create directory for comparisons\n",
        "COMP_DIR = OUT_ROOT / \"comparisons\"\n",
        "COMP_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# 1. Comparison of metrics between models\n",
        "if \"res_df\" in locals() and not res_df.empty:\n",
        "    sns.set(style=\"ticks\", context=\"paper\", font_scale=1.2)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(24, 16))\n",
        "\n",
        "    # ---------- RMSE ----------\n",
        "    (res_df\n",
        "     .pivot_table(values=\"RMSE\", index=\"Model\", columns=\"Experiment\", aggfunc=\"mean\")\n",
        "     .plot(kind=\"bar\", ax=axes[0, 0]))\n",
        "    axes[0, 0].set(title=\"RMSE Average by Model and Experiment\",\n",
        "                   ylabel=\"RMSE (mm)\", xlabel=\"\")\n",
        "    axes[0, 0].legend(title=\"Experiment\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", frameon=False)\n",
        "    axes[0, 0].grid(alpha=.3)\n",
        "    axes[0, 0].tick_params(axis=\"x\", rotation=45)\n",
        "\n",
        "    # ---------- MAE ----------\n",
        "    (res_df\n",
        "     .pivot_table(values=\"MAE\", index=\"Model\", columns=\"Experiment\", aggfunc=\"mean\")\n",
        "     .plot(kind=\"bar\", ax=axes[0, 1]))\n",
        "    axes[0, 1].set(title=\"MAE Average by Model and Experiment\",\n",
        "                   ylabel=\"MAE (mm)\", xlabel=\"\")\n",
        "    axes[0, 1].legend(title=\"Experiment\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", frameon=False)\n",
        "    axes[0, 1].grid(alpha=.3)\n",
        "    axes[0, 1].tick_params(axis=\"x\", rotation=45)\n",
        "\n",
        "    # ---------- R² ----------\n",
        "    (res_df\n",
        "     .pivot_table(values=\"R2\", index=\"Model\", columns=\"Experiment\", aggfunc=\"mean\")\n",
        "     .plot(kind=\"bar\", ax=axes[1, 0]))\n",
        "    axes[1, 0].set(title=\"R² Average by Model and Experiment\",\n",
        "                   ylabel=\"R²\", xlabel=\"\")\n",
        "    axes[1, 0].legend(title=\"Experiment\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", frameon=False)\n",
        "    axes[1, 0].grid(alpha=.3)\n",
        "    axes[1, 0].tick_params(axis=\"x\", rotation=45)\n",
        "\n",
        "        # ---------- Horizon evolution ----------\n",
        "    ax_h = axes[1, 1]\n",
        "\n",
        "    # Get unique horizons from the data\n",
        "    horizons = sorted(res_df['horizon'].unique())\n",
        "\n",
        "    for model, grp in res_df.groupby(\"Model\"):\n",
        "        (grp.groupby(\"horizon\")[\"RMSE\"].mean()\n",
        "             .sort_index()  # keep chronological order\n",
        "             .plot(ax=ax_h, marker=\"o\",\n",
        "                   linewidth=2.5, markersize=8, label=model))\n",
        "\n",
        "    ax_h.set(title=\"Evolution of RMSE by Horizon\",\n",
        "             xlabel=\"\", ylabel=\"RMSE (mm)\")\n",
        "\n",
        "    # use the mapped labels on x-axis\n",
        "    # Fix: Use the index of the horizons for xticks\n",
        "    ax_h.set_xticks(range(len(horizons)))\n",
        "    ax_h.set_xticklabels(horizons, rotation=45)\n",
        "    ax_h.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", frameon=False)\n",
        "    ax_h.grid(alpha=.3)\n",
        "\n",
        "    sns.despine()\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "    out_path = IMAGE_DIR / \"advanced_models_plot.png\"\n",
        "    plt.savefig(out_path, dpi=700, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(f\"✅ Figure saved → {out_path}\")\n",
        "\n",
        "# 2. Summary table of best models\n",
        "if \"res_df\" in locals() and not res_df.empty:\n",
        "    print(\"\\n📋 SUMMARY TABLE – BEST MODELS BY EXPERIMENT\")\n",
        "    print(\"─\" * 60)\n",
        "    best_models = (res_df\n",
        "                   .loc[res_df.groupby(\"Experiment\")[\"RMSE\"].idxmin(),\n",
        "                        [\"Experiment\", \"Model\", \"RMSE\", \"MAE\", \"R2\"]]\n",
        "                   .set_index(\"Experiment\"))\n",
        "    print(best_models.to_string())\n",
        "\n",
        "# 3. Comparison with original models if available\n",
        "old_metrics_path = OUT_ROOT.parent / \"Spatial_CONVRNN\" / \"metrics_spatial.csv\"\n",
        "if old_metrics_path.exists():\n",
        "    print(\"\\n📊 COMPARISON WITH ORIGINAL MODELS\")\n",
        "    print(\"─\" * 60)\n",
        "    old_df = pd.read_csv(old_metrics_path)\n",
        "\n",
        "    if \"res_df\" in locals() and not res_df.empty:\n",
        "        for exp in EXPERIMENTS.keys():\n",
        "            new_data = res_df[res_df[\"Experiment\"] == exp]\n",
        "            old_data = old_df[old_df[\"Experiment\"] == exp]\n",
        "\n",
        "            if new_data.empty or old_data.empty:\n",
        "                continue\n",
        "\n",
        "            new_best = (new_data.groupby(\"Model\")[\"RMSE\"].mean()\n",
        "                        .idxmin())\n",
        "            new_rmse = new_data[new_data[\"Model\"] == new_best][\"RMSE\"].mean()\n",
        "            old_best_rmse = old_data[\"RMSE\"].min()\n",
        "            imp = (old_best_rmse - new_rmse) / old_best_rmse * 100\n",
        "\n",
        "            print(f\"\\n{exp}:\")\n",
        "            print(f\"  • Best new model: {new_best} (RMSE {new_rmse:.2f})\")\n",
        "            print(f\"  • Best original RMSE: {old_best_rmse:.2f}\")\n",
        "            print(f\"  • Improvement: {imp:.2f}%\")\n",
        "\n",
        "print(\"\\n✅ Comparative visualizations completed.\")\n",
        "print(f\"📂 All outputs stored in: {COMP_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJcQwNvLXqrT"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── DETAILED ANALYSIS OF RESULTS ─────────────────────────\n",
        "if \"res_df\" in locals() and not res_df.empty:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"📊 DETAILED ANALYSIS OF RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # ------------------------------------------------------------------ #\n",
        "    # 1. Metrics vs horizon  (three panels: RMSE / MAE / R²)\n",
        "    # ------------------------------------------------------------------ #\n",
        "    import seaborn as sns, matplotlib.pyplot as plt, numpy as np\n",
        "\n",
        "    # Get the horizon dates from the data\n",
        "    month_order = sorted(res_df[\"horizon\"].unique())\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(24, 9), sharex=True)\n",
        "    sns.set(style=\"ticks\", context=\"paper\", font_scale=1.2)\n",
        "\n",
        "    metrics   = [\"RMSE\", \"MAE\", \"R2\"]\n",
        "    y_labels  = [\"RMSE (mm)\", \"MAE (mm)\", \"R²\"]\n",
        "    colors    = plt.cm.Set3(np.linspace(0, 1, res_df[\"Model\"].nunique()))\n",
        "\n",
        "    for idx, (metric, ylab) in enumerate(zip(metrics, y_labels)):\n",
        "        ax  = axes[idx]\n",
        "        piv = (res_df\n",
        "               .groupby([\"horizon\", \"Model\"])[metric]\n",
        "               .mean()\n",
        "               .unstack())\n",
        "\n",
        "        # Ensure chronological order\n",
        "        piv = piv.loc[month_order]\n",
        "\n",
        "        for i, model in enumerate(piv.columns):\n",
        "            ax.plot(piv.index, piv[model],\n",
        "                    marker=\"o\", linewidth=2.5, markersize=8,\n",
        "                    color=colors[i], label=model)\n",
        "\n",
        "        ax.set_ylabel(ylab)\n",
        "        ax.set_xlabel(\"\")\n",
        "        ax.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\", frameon=False)\n",
        "        ax.grid(alpha=.3, linestyle=\"--\")\n",
        "        ax.tick_params(axis=\"x\", rotation=45)\n",
        "\n",
        "    sns.despine()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(IMAGE_DIR / \"advanced_models_plot.png\",\n",
        "                dpi=700, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Visual table of metrics\n",
        "    fig, ax = plt.subplots(figsize=(20, 12))\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Prepare data for the table\n",
        "    summary_data = []\n",
        "    experiments = res_df['Experiment'].unique()\n",
        "    models = res_df['Model'].unique()\n",
        "\n",
        "    # Headers\n",
        "    headers = ['Experimento', 'Modelo', 'RMSE↓', 'MAE↓', 'R²↑', 'Mejor H', 'Parámetros']\n",
        "\n",
        "    for exp in experiments:\n",
        "        for model in models:\n",
        "            exp_model_data = res_df[(res_df['Experiment'] == exp) & (res_df['Model'] == model)]\n",
        "            if not exp_model_data.empty:\n",
        "                avg_rmse = exp_model_data['RMSE'].mean()\n",
        "                avg_mae = exp_model_data['MAE'].mean()\n",
        "                avg_r2 = exp_model_data['R2'].mean()\n",
        "                best_h = exp_model_data.loc[exp_model_data['RMSE'].idxmin(), 'horizon']\n",
        "                params = exp_model_data['Parameters'].iloc[0]\n",
        "\n",
        "                summary_data.append([\n",
        "                    exp, model,\n",
        "                    f'{avg_rmse:.4f}',\n",
        "                    f'{avg_mae:.4f}',\n",
        "                    f'{avg_r2:.4f}',\n",
        "                    f'H={best_h}',\n",
        "                    f'{params:,}'\n",
        "                ])\n",
        "\n",
        "    # Create table\n",
        "    table = ax.table(cellText=summary_data, colLabels=headers,\n",
        "                    cellLoc='center', loc='center')\n",
        "\n",
        "    # Style table\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1.2, 2)\n",
        "\n",
        "    # Color cells according to performance\n",
        "    for i in range(len(summary_data)):\n",
        "        # Get values for comparison\n",
        "        rmse_val = float(summary_data[i][2])\n",
        "        mae_val = float(summary_data[i][3])\n",
        "        r2_val = float(summary_data[i][4])\n",
        "\n",
        "        # Find min/max for normalization\n",
        "        all_rmse = [float(row[2]) for row in summary_data]\n",
        "        all_mae = [float(row[3]) for row in summary_data]\n",
        "        all_r2 = [float(row[4]) for row in summary_data]\n",
        "\n",
        "        # Normalize and color RMSE (lower is better)\n",
        "        rmse_norm = (rmse_val - min(all_rmse)) / (max(all_rmse) - min(all_rmse))\n",
        "        rmse_color = plt.cm.RdYlGn(1 - rmse_norm)\n",
        "        table[(i+1, 2)].set_facecolor(rmse_color)\n",
        "\n",
        "        # Normalize and color MAE (lower is better)\n",
        "        mae_norm = (mae_val - min(all_mae)) / (max(all_mae) - min(all_mae))\n",
        "        mae_color = plt.cm.RdYlGn(1 - mae_norm)\n",
        "        table[(i+1, 3)].set_facecolor(mae_color)\n",
        "\n",
        "        # Normalize and color R² (higher is better)\n",
        "        r2_norm = (r2_val - min(all_r2)) / (max(all_r2) - min(all_r2))\n",
        "        r2_color = plt.cm.RdYlGn(r2_norm)\n",
        "        table[(i+1, 4)].set_facecolor(r2_color)\n",
        "\n",
        "        # Color experiment\n",
        "        exp_colors = {'BASIC': '#e8f4f8', 'KCE': '#f0e8f8', 'PAFC': '#f8e8f0'}\n",
        "        table[(i+1, 0)].set_facecolor(exp_colors.get(summary_data[i][0], 'white'))\n",
        "\n",
        "    # Color headers\n",
        "    for j in range(len(headers)):\n",
        "        table[(0, j)].set_facecolor('#4a86e8')\n",
        "        table[(0, j)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "    plt.title('Summary of Metrics by Model and Experiment\\n(Green=Best, Red=Worst)',\n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "    # Add legend\n",
        "    plt.text(0.5, -0.05, '↓ = Lower is better, ↑ = Higher is better',\n",
        "            transform=ax.transAxes, ha='center', fontsize=10, style='italic')\n",
        "\n",
        "    plt.savefig(IMAGE_DIR/f\"advanced_models_metrics.png\", dpi=700, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Identify the best global model\n",
        "    print(\"\\n🏆 BEST GLOBAL MODEL:\")\n",
        "    print(\"─\" * 50)\n",
        "\n",
        "    # Calculate composite score (normalized)\n",
        "    res_df['score'] = (\n",
        "        (1 - (res_df['RMSE'] - res_df['RMSE'].min()) / (res_df['RMSE'].max() - res_df['RMSE'].min())) +\n",
        "        (1 - (res_df['MAE'] - res_df['MAE'].min()) / (res_df['MAE'].max() - res_df['MAE'].min())) +\n",
        "        ((res_df['R2'] - res_df['R2'].min()) / (res_df['R2'].max() - res_df['R2'].min()))\n",
        "    ) / 3\n",
        "\n",
        "    best_overall = res_df.loc[res_df['score'].idxmax()]\n",
        "    print(f\"Model: {best_overall['Model']}\")\n",
        "    print(f\"Experiment: {best_overall['Experiment']}\")\n",
        "    print(f\"Horizon: {best_overall['horizon']}\")\n",
        "    print(f\"RMSE: {best_overall['RMSE']:.4f}\")\n",
        "    print(f\"MAE: {best_overall['MAE']:.4f}\")\n",
        "    print(f\"R²: {best_overall['R2']:.4f}\")\n",
        "    print(f\"Composite score: {best_overall['score']:.4f}\")\n",
        "\n",
        "    # 4. Analysis of improvement by horizon\n",
        "    print(\"\\n📈 ANALYSIS OF IMPROVEMENT BY HORIZON:\")\n",
        "    print(\"─\" * 50)\n",
        "\n",
        "    for h in sorted(res_df['horizon'].unique()):\n",
        "        h_data = res_df[res_df['horizon'] == h]\n",
        "        best_h = h_data.loc[h_data['RMSE'].idxmin()]\n",
        "\n",
        "        print(f\"\\nHorizon {h}:\")\n",
        "        print(f\"  • Best model: {best_h['Model']} - {best_h['Experiment']}\")\n",
        "        print(f\"  • RMSE: {best_h['RMSE']:.4f}\")\n",
        "        print(f\"  • R²: {best_h['R2']:.4f}\")\n",
        "\n",
        "print(\"\\n✅ Detailed analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdvrmOioXqrT"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── SHOW RECENT PREDICTIONS ─────────────────────────\n",
        "print(\"\\n🖼️ RECENT PREDICTIONS:\")\n",
        "for exp in EXPERIMENTS.keys():\n",
        "    exp_dir = OUT_ROOT / exp\n",
        "    if exp_dir.exists():\n",
        "        print(f\"\\n{exp}:\")\n",
        "        # Show first image of each model\n",
        "        for model in ADVANCED_MODELS.keys():\n",
        "            # Try to find an image with a date-based filename\n",
        "            img_files = list(exp_dir.glob(f\"{model}_*.png\"))\n",
        "            img_path = img_files[0] if img_files else None\n",
        "            gif_path = exp_dir / f\"{model}.gif\"\n",
        "\n",
        "            if img_path and img_path.exists():\n",
        "                from IPython.display import Image, display\n",
        "                # Extract date from filename if possible\n",
        "                date_str = img_path.stem.split('_')[-1] if '_' in img_path.stem else 'first month'\n",
        "                print(f\"  {model} - Prediction for {date_str}:\")\n",
        "                display(Image(str(img_path), width=800))\n",
        "\n",
        "            if gif_path.exists():\n",
        "                print(f\"  📹 GIF available: {gif_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🎉 NOTEBOOK COMPLETED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n📊 Resultados guardados en: {OUT_ROOT}\")\n",
        "if 'res_df' in locals() and res_df is not None and len(res_df) > 0:\n",
        "    print(f\"📈 Metrics in: {OUT_ROOT/'metrics_advanced.csv'}\")\n",
        "    print(f\"🖼️ Visualizations in: {COMP_DIR if 'COMP_DIR' in locals() else 'N/A'}\")\n",
        "else:\n",
        "    print(\"⚠️ No metrics generated in this execution\")\n",
        "print(\"\\n💡 Next steps:\")\n",
        "print(\"   1. Review the metrics and select the best model\")\n",
        "print(\"   2. Fine-tune hyperparameters if necessary\")\n",
        "print(\"   3. Train an ensemble with the best models\")\n",
        "print(\"   4. Evaluate on more recent or different regions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vqMttnQHqfI"
      },
      "outputs": [],
      "source": [
        "# ╔══════════════════════════════════════════════════════════════════════╗\n",
        "# ║  MASTER FIX  —  custom layers, carga robusta y log de errores       ║\n",
        "# ╚══════════════════════════════════════════════════════════════════════╝\n",
        "from pathlib import Path\n",
        "import sys, gc, traceback, json, warnings\n",
        "import numpy as np, pandas as pd, tensorflow as tf\n",
        "from tensorflow.keras import layers, utils\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 1)  CAPAS PERSONALIZADAS (visibles para TODO el runtime)\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "try:  # Keras 3 / TF >= 2.15\n",
        "    from tensorflow.keras.saving import register_keras_serializable as _reg\n",
        "except ImportError:\n",
        "    from tensorflow.keras.utils import register_keras_serializable as _reg\n",
        "\n",
        "@_reg()  # id = \"ConvGRU2D\"\n",
        "class ConvGRU2D(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, padding=\"same\",\n",
        "                 activation=\"tanh\", recurrent_activation=\"sigmoid\",\n",
        "                 return_sequences=True, use_batch_norm=True, dropout=0.2, **kw):\n",
        "        super().__init__(**kw)\n",
        "        self.filters, self.kernel_size, self.padding = filters, kernel_size, padding\n",
        "        self.activation, self.recurrent_activation = activation, recurrent_activation\n",
        "        self.return_sequences, self.use_batch_norm, self.dropout = \\\n",
        "            return_sequences, use_batch_norm, dropout\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def build(self, shape):\n",
        "        self.gru = layers.ConvLSTM2D(self.filters, self.kernel_size,\n",
        "                                     padding=self.padding,\n",
        "                                     activation=self.activation,\n",
        "                                     recurrent_activation=self.recurrent_activation,\n",
        "                                     return_sequences=self.return_sequences)\n",
        "        if self.use_batch_norm:\n",
        "            self.bn = layers.BatchNormalization()\n",
        "        if self.dropout:\n",
        "            self.do = layers.Dropout(self.dropout)\n",
        "        super().build(shape)\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        x = self.gru(x, training=training)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.bn(x, training=training)\n",
        "        if training and self.dropout:\n",
        "            x = self.do(x, training=training)\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        cfg = super().get_config()\n",
        "        cfg.update({k: getattr(self, k) for k in (\n",
        "            \"filters\", \"kernel_size\", \"padding\", \"activation\",\n",
        "            \"recurrent_activation\", \"return_sequences\",\n",
        "            \"use_batch_norm\", \"dropout\")})\n",
        "        return cfg\n",
        "\n",
        "@_reg()  # id = \"CBAM\"\n",
        "class CBAM(layers.Layer):\n",
        "    def __init__(self, reduction_ratio=8, **kw):\n",
        "        super().__init__(**kw); self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, shape):\n",
        "        ch = shape[-1]\n",
        "        self.mlp = tf.keras.Sequential([\n",
        "            layers.Dense(ch // self.reduction_ratio, activation=\"relu\"),\n",
        "            layers.Dense(ch, activation=\"sigmoid\")\n",
        "        ])\n",
        "        self.spatial = layers.Conv2D(1, 7, padding=\"same\", activation=\"sigmoid\")\n",
        "        super().build(shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        # channel att\n",
        "        avg = tf.reduce_mean(x, [1,2], keepdims=True)\n",
        "        max_ = tf.reduce_max(x, [1,2], keepdims=True)\n",
        "        x = x * (self.mlp(avg) + self.mlp(max_))\n",
        "        # spatial att\n",
        "        avg = tf.reduce_mean(x, -1, keepdims=True)\n",
        "        max_ = tf.reduce_max(x, -1, keepdims=True)\n",
        "        x = x * self.spatial(tf.concat([avg, max_], -1))\n",
        "        return x\n",
        "\n",
        "    # necesarios para TimeDistributed\n",
        "    def compute_output_shape(self, s): return s\n",
        "    def compute_output_signature(self, sig): return sig\n",
        "\n",
        "    def get_config(self):\n",
        "        cfg = super().get_config(); cfg.update({\"reduction_ratio\": self.reduction_ratio}); return cfg\n",
        "\n",
        "# Aliases (por si el modelo fue grabado con otra mayúscula/minúscula)\n",
        "aliases = {\n",
        "    \"conv_gru2d\": ConvGRU2D, \"cbam\": CBAM,\n",
        "    \"CustomLayers>ConvGRU2D\": ConvGRU2D, \"CustomLayers>CBAM\": CBAM,\n",
        "}\n",
        "utils.get_custom_objects().update({\"ConvGRU2D\": ConvGRU2D, \"CBAM\": CBAM, **aliases})\n",
        "setattr(layers, \"ConvGRU2D\", ConvGRU2D); setattr(layers, \"CBAM\", CBAM)\n",
        "\n",
        "print(\"✅ Custom layers registradas:\", list(utils.get_custom_objects().keys() & {\"ConvGRU2D\",\"CBAM\"}))\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# 2)  LOG centralizado de errores (export_errors.txt)\n",
        "# ──────────────────────────────────────────────────────────────────────\n",
        "# ╔══════════════════════════════════════════════════════════════╗\n",
        "# ║        FIX DEFINITIVO: load_model_safe + log de errores      ║\n",
        "# ╚══════════════════════════════════════════════════════════════╝\n",
        "import tensorflow as tf, traceback, sys, gc\n",
        "from pathlib import Path\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "# Directorio y archivo de log (usa el ya creado por tu script)\n",
        "META_MODELS_DIR = OUT_ROOT / \"meta_models\"\n",
        "META_MODELS_DIR.mkdir(exist_ok=True)\n",
        "ERRORS_LOG = META_MODELS_DIR / \"export_errors.txt\"\n",
        "ERRORS_LOG.write_text(\"\")  # limpia al arrancar\n",
        "\n",
        "def _log(context: str, exc: BaseException):\n",
        "    tb = \"\".join(traceback.format_exception(type(exc), exc, exc.__traceback__))\n",
        "    block = f\"\\n╭─ {context} ─────────────────────────────────────────\\n{tb}╰───────────────────────────────────────────────────────────\\n\"\n",
        "    print(block, file=sys.stderr)\n",
        "    with open(ERRORS_LOG, \"a\") as f:\n",
        "        f.write(block)\n",
        "\n",
        "def load_model_safe(model_path: Path, model_name: str = None):\n",
        "    \"\"\"Carga robusta (.keras / .h5) con capas personalizadas y registra\n",
        "       tracebacks completos en export_errors.txt.\"\"\"\n",
        "    nm = model_name or model_path.name\n",
        "    try:\n",
        "        with tf.keras.utils.custom_object_scope(utils.get_custom_objects()):\n",
        "            model = tf.keras.models.load_model(\n",
        "                model_path,\n",
        "                custom_objects=utils.get_custom_objects(),\n",
        "                compile=False,\n",
        "                safe_mode=False,   # necesario si hay Lambda layers serializados\n",
        "            )\n",
        "        print(f\"      ✅ Loaded {nm} (safe_mode=False)\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        _log(f\"load_model({nm})\", e)\n",
        "\n",
        "    # Fallback: archivos separados .json + .h5\n",
        "    try:\n",
        "        arch, wts = model_path.with_suffix(\".json\"), model_path.with_suffix(\".h5\")\n",
        "        if arch.exists() and wts.exists():\n",
        "            with open(arch) as f:\n",
        "                model = tf.keras.models.model_from_json(\n",
        "                    f.read(), custom_objects=utils.get_custom_objects())\n",
        "            model.load_weights(wts)\n",
        "            print(f\"      ⚡ Loaded {nm} from split files\")\n",
        "            return model\n",
        "    except Exception as e2:\n",
        "        _log(f\"split‑files({nm})\", e2)\n",
        "\n",
        "    tf.keras.backend.clear_session(); gc.collect()\n",
        "    print(f\"      ❌ All loading methods failed for {nm}\")\n",
        "    return None\n",
        "\n",
        "print(\"🔧 load_model_safe actualizado (firma: path, model_name=None) y logging listo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvIL6KrkIjvZ"
      },
      "outputs": [],
      "source": [
        "# 📁 BEST MODELS PREDICTIONS EXPORT FOR META-MODELS\n",
        "# ════════════════════════════════════════════════════════════════════════════════\n",
        "#\n",
        "# ⬇️  **Esta versión corrige el AttributeError** que impedía registrar las capas\n",
        "#     personalizadas.  Añadimos un import compatible con cualquier versión de\n",
        "#     TensorFlow 2.x y sustituimos los decoradores problemáticos.\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import gc, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"🚀 STARTING BEST MODELS PREDICTIONS EXPORT...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 1) DIRECTORIO PARA EXPORTES\n",
        "# --------------------------------------------------------------------------------\n",
        "META_MODELS_DIR = OUT_ROOT / 'meta_models'\n",
        "META_MODELS_DIR.mkdir(exist_ok=True)\n",
        "print(f\"📂 Meta-models directory: {META_MODELS_DIR}\")\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 2) COMPATIBILIDAD PARA register_keras_serializable\n",
        "# --------------------------------------------------------------------------------\n",
        "try:\n",
        "    # Keras 3 / TF ≥ 2.15\n",
        "    from tensorflow.keras.saving import register_keras_serializable as register_ks\n",
        "except ImportError:\n",
        "    # TF 2.x anteriores\n",
        "    from tensorflow.keras.utils import register_keras_serializable as register_ks\n",
        "\n",
        "# Registro global de objetos personalizados\n",
        "CUSTOM_OBJECTS = utils.get_custom_objects()   # usa las de la primera celda\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 4) … EL RESTO DE LA CELDA QUEDA IGUAL …\n",
        "#     (load_best_combinations, load_model_safe, export_best_predictions, etc.)\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "def load_best_combinations():\n",
        "    \"\"\"Load and identify the best 2 model-experiment combinations\"\"\"\n",
        "\n",
        "    # Try to load existing metrics\n",
        "    metrics_file = OUT_ROOT / 'metrics_advanced.csv'\n",
        "    if not metrics_file.exists():\n",
        "        print(\"⚠️ No metrics file found. Please run training first.\")\n",
        "        return []\n",
        "\n",
        "    # Load metrics and find best combinations\n",
        "    df = pd.read_csv(metrics_file)\n",
        "\n",
        "    # Calculate average performance across horizons for each model-experiment pair\n",
        "    summary = df.groupby(['Experiment', 'Model']).agg({\n",
        "        'RMSE': 'mean',\n",
        "        'MAE': 'mean',\n",
        "        'R2': 'mean',\n",
        "        'Parameters': 'first'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Sort by RMSE (lower is better) and R2 (higher is better)\n",
        "    summary['Score'] = summary['R2'] - (summary['RMSE'] / summary['RMSE'].max())\n",
        "    summary = summary.sort_values('Score', ascending=False)\n",
        "\n",
        "    print(f\"📊 Model Performance Summary (Top 10):\")\n",
        "    print(summary.head(10)[['Experiment', 'Model', 'RMSE', 'MAE', 'R2', 'Score']].to_string(index=False))\n",
        "\n",
        "    # Select top 2 combinations ensuring no model repetition\n",
        "    selected = []\n",
        "    used_models = set()\n",
        "\n",
        "    for _, row in summary.iterrows():\n",
        "        if row['Model'] not in used_models and len(selected) < 2:\n",
        "            selected.append({\n",
        "                'experiment': row['Experiment'],\n",
        "                'model': row['Model'],\n",
        "                'rmse': row['RMSE'],\n",
        "                'mae': row['MAE'],\n",
        "                'r2': row['R2'],\n",
        "                'score': row['Score'],\n",
        "                'parameters': row['Parameters']\n",
        "            })\n",
        "            used_models.add(row['Model'])\n",
        "\n",
        "    print(f\"\\n🎯 SELECTED TOP 2 COMBINATIONS:\")\n",
        "    for i, combo in enumerate(selected, 1):\n",
        "        print(f\"   {i}. {combo['experiment']} + {combo['model']} (Score: {combo['score']:.4f})\")\n",
        "\n",
        "    return selected\n",
        "\n",
        "def load_model_safe(path):\n",
        "    \"\"\"Carga robusta de modelos con Lambda y custom layers.\"\"\"\n",
        "    import tensorflow as tf, gc\n",
        "    try:\n",
        "        # ①  Intento principal (safe_mode=False + custom_objects)\n",
        "        with tf.keras.utils.custom_object_scope(utils.get_custom_objects()):\n",
        "            model = tf.keras.models.load_model(\n",
        "                path,\n",
        "                custom_objects=utils.get_custom_objects(),\n",
        "                compile=False,\n",
        "                safe_mode=False,   #  ← desactiva las restricciones de Lambda\n",
        "            )\n",
        "        print(\"      ✅ Loaded with safe_mode=False\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(\"      ⚠️  load_model failed:\", e)\n",
        "\n",
        "    # ②  Última alternativa: sólo arquitectura, luego pesos\n",
        "    try:\n",
        "        arch_json = Path(path).with_suffix(\".json\")\n",
        "        weights_h5 = Path(path).with_suffix(\".h5\")\n",
        "        if arch_json.exists() and weights_h5.exists():\n",
        "            with open(arch_json) as f:\n",
        "                model = tf.keras.models.model_from_json(\n",
        "                    f.read(), custom_objects=utils.get_custom_objects())\n",
        "            model.load_weights(weights_h5)\n",
        "            print(\"      ⚡ Loaded from split architecture/weights\")\n",
        "            return model\n",
        "    except Exception as e2:\n",
        "        print(\"      ⚠️  split‑files fallback failed:\", e2)\n",
        "\n",
        "    tf.keras.backend.clear_session(); gc.collect()\n",
        "    print(\"      ❌ All loading methods failed\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def export_best_predictions():\n",
        "    \"\"\"Export predictions from the best model-experiment combinations\"\"\"\n",
        "\n",
        "    # Get best combinations\n",
        "    best_combos = load_best_combinations()\n",
        "\n",
        "    if len(best_combos) == 0:\n",
        "        print(\"❌ No valid combinations found!\")\n",
        "        return False\n",
        "\n",
        "    exported_models = []\n",
        "\n",
        "    for i, combo in enumerate(best_combos, 1):\n",
        "        exp_name = combo['experiment']\n",
        "        model_name = combo['model']\n",
        "\n",
        "        print(f\"\\n\" + \"─\"*60)\n",
        "        print(f\"🎯 EXPORTING {i}/2: {exp_name} + {model_name}\")\n",
        "        print(f\"   📊 Score: {combo['score']:.4f} | RMSE: {combo['rmse']:.4f} | R²: {combo['r2']:.4f}\")\n",
        "        print(\"─\"*60)\n",
        "\n",
        "        # Check if experiment configuration exists\n",
        "        if exp_name not in EXPERIMENTS:\n",
        "            print(f\"   ❌ Experiment {exp_name} not found in EXPERIMENTS\")\n",
        "            continue\n",
        "\n",
        "        exp_cfg = EXPERIMENTS[exp_name]\n",
        "        feat_list = exp_cfg['feature_list']\n",
        "\n",
        "        # Check if model file exists\n",
        "        model_path = OUT_ROOT / exp_name / f\"{model_name.lower()}_best.keras\"\n",
        "        if not model_path.exists():\n",
        "            print(f\"   ❌ Model file not found: {model_path}\")\n",
        "            # Try alternative naming patterns\n",
        "            alt_patterns = [\n",
        "                OUT_ROOT / exp_name / f\"{model_name}_best.keras\",\n",
        "                OUT_ROOT / exp_name / f\"{model_name.lower()}.keras\",\n",
        "                OUT_ROOT / exp_name / f\"{model_name}.keras\"\n",
        "            ]\n",
        "\n",
        "            for alt_path in alt_patterns:\n",
        "                if alt_path.exists():\n",
        "                    model_path = alt_path\n",
        "                    print(f\"   ✅ Found alternative: {model_path}\")\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"   📁 Available files in {OUT_ROOT / exp_name}:\")\n",
        "                exp_dir = OUT_ROOT / exp_name\n",
        "                if exp_dir.exists():\n",
        "                    for file in exp_dir.glob(\"*\"):\n",
        "                        print(f\"      📄 {file.name}\")\n",
        "                continue\n",
        "\n",
        "        try:\n",
        "            # Prepare data for this experiment\n",
        "            print(\"   📥 Preparing data...\")\n",
        "\n",
        "            # Validate that required variables exist\n",
        "            if 'ds' not in globals():\n",
        "                print(\"   ❌ Dataset 'ds' not found in global scope\")\n",
        "                continue\n",
        "\n",
        "            if 'windowed_arrays' not in globals():\n",
        "                print(\"   ❌ Function 'windowed_arrays' not found in global scope\")\n",
        "                continue\n",
        "\n",
        "            # Prepare features array\n",
        "            try:\n",
        "                Xarr = ds[feat_list].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "                yarr = ds['total_precipitation'].values.astype(np.float32)[...,None]\n",
        "                X, y = windowed_arrays(Xarr, yarr)\n",
        "            except Exception as data_error:\n",
        "                print(f\"   ❌ Data preparation failed: {data_error}\")\n",
        "                print(f\"   📋 Available features in dataset: {list(ds.data_vars.keys()) if 'ds' in globals() else 'Dataset not available'}\")\n",
        "                print(f\"   📋 Required features: {feat_list}\")\n",
        "                continue\n",
        "\n",
        "            split = int(0.8 * len(X))\n",
        "            val_split = int(0.9 * len(X))\n",
        "\n",
        "            # Normalization (same as training)\n",
        "            sx = StandardScaler().fit(X[:split].reshape(-1, len(feat_list)))\n",
        "            sy = StandardScaler().fit(y[:split].reshape(-1, 1))\n",
        "\n",
        "            X_sc = sx.transform(X.reshape(-1, len(feat_list))).reshape(X.shape)\n",
        "            y_sc = sy.transform(y.reshape(-1, 1)).reshape(y.shape)\n",
        "\n",
        "            # Test set (same split as training)\n",
        "            X_te = X_sc[val_split:]\n",
        "            y_te = y_sc[val_split:]\n",
        "            y_te_original = sy.inverse_transform(y_te.reshape(-1, 1)).reshape(y_te.shape)\n",
        "\n",
        "            print(f\"   ✅ Data prepared - Test samples: {len(X_te)}, Features: {len(feat_list)}\")\n",
        "            print(f\"   📊 Data shapes - X_te: {X_te.shape}, y_te: {y_te_original.shape}\")\n",
        "\n",
        "            # Load the trained model - CORRECCIÓN: solo pasar el path\n",
        "            model = load_model_safe(model_path)\n",
        "            if model is None:\n",
        "                print(f\"   ❌ Could not load model, skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Display model info\n",
        "            try:\n",
        "                print(f\"   🤖 Model loaded - Input shape: {model.input_shape}, Output shape: {model.output_shape}\")\n",
        "                print(f\"   🔢 Parameters: {model.count_params():,}\")\n",
        "            except:\n",
        "                print(f\"   🤖 Model loaded successfully (shape info unavailable)\")\n",
        "\n",
        "            # Generate predictions with error handling\n",
        "            print(f\"   🎯 Generating predictions...\")\n",
        "            try:\n",
        "                # Check input shape compatibility\n",
        "                expected_shape = model.input_shape\n",
        "                actual_shape = X_te.shape\n",
        "                print(f\"   📊 Shape check - Expected: {expected_shape}, Actual: {actual_shape}\")\n",
        "\n",
        "                # If there's a shape mismatch, try to fix it\n",
        "                if len(expected_shape) != len(actual_shape):\n",
        "                    print(f\"   🔧 Attempting shape adjustment...\")\n",
        "                    if len(expected_shape) == 5 and len(actual_shape) == 4:\n",
        "                        # Add time dimension for ConvLSTM/ConvGRU models\n",
        "                        X_te_adjusted = np.expand_dims(X_te, axis=1)\n",
        "                        print(f\"   ✅ Added time dimension: {X_te_adjusted.shape}\")\n",
        "                        X_te = X_te_adjusted\n",
        "\n",
        "                # Start with small batch size to avoid memory issues\n",
        "                batch_size = min(4, len(X_te))  # Very conservative\n",
        "                print(f\"   📦 Using batch size: {batch_size}\")\n",
        "\n",
        "                # Try prediction in small batches to handle potential memory issues\n",
        "                predictions = []\n",
        "                for i in range(0, len(X_te), batch_size):\n",
        "                    batch_end = min(i + batch_size, len(X_te))\n",
        "                    batch_data = X_te[i:batch_end]\n",
        "\n",
        "                    try:\n",
        "                        batch_pred = model.predict(batch_data, verbose=0)\n",
        "                        predictions.append(batch_pred)\n",
        "                        if i == 0:  # Show info for first batch\n",
        "                            print(f\"   📊 First batch prediction shape: {batch_pred.shape}\")\n",
        "                    except Exception as batch_error:\n",
        "                        print(f\"   ⚠️ Batch {i//batch_size + 1} failed: {str(batch_error)[:80]}\")\n",
        "                        # Skip failed batches\n",
        "                        continue\n",
        "\n",
        "                if not predictions:\n",
        "                    raise Exception(\"All prediction batches failed\")\n",
        "\n",
        "                # Concatenate all predictions\n",
        "                y_pred_scaled = np.concatenate(predictions, axis=0)\n",
        "                print(f\"   📊 Combined predictions shape: {y_pred_scaled.shape}\")\n",
        "\n",
        "                # Ensure we have the right number of samples\n",
        "                min_samples = min(len(y_te_original), len(y_pred_scaled))\n",
        "                if len(y_pred_scaled) != len(y_te_original):\n",
        "                    print(f\"   ⚠️ Shape mismatch - truncating to {min_samples} samples\")\n",
        "                    y_pred_scaled = y_pred_scaled[:min_samples]\n",
        "                    y_te_original = y_te_original[:min_samples]\n",
        "                    X_te = X_te[:min_samples]\n",
        "\n",
        "                # Inverse transform to original scale\n",
        "                try:\n",
        "                    y_pred = sy.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(y_pred_scaled.shape)\n",
        "                except Exception as transform_error:\n",
        "                    print(f\"   ⚠️ Inverse transform issue: {str(transform_error)}\")\n",
        "                    # Try alternative reshaping\n",
        "                    original_shape = y_pred_scaled.shape\n",
        "                    y_pred_flat = y_pred_scaled.reshape(-1, 1)\n",
        "                    y_pred_transformed = sy.inverse_transform(y_pred_flat)\n",
        "                    y_pred = y_pred_transformed.reshape(original_shape)\n",
        "\n",
        "                # Calculate test metrics for verification\n",
        "                test_rmse = np.sqrt(mean_squared_error(y_te_original.ravel(), y_pred.ravel()))\n",
        "                test_mae = mean_absolute_error(y_te_original.ravel(), y_pred.ravel())\n",
        "\n",
        "                # Handle potential R² calculation issues\n",
        "                try:\n",
        "                    test_r2 = r2_score(y_te_original.ravel(), y_pred.ravel())\n",
        "                except:\n",
        "                    test_r2 = 0.0  # Default if calculation fails\n",
        "\n",
        "                print(f\"   📊 Test Metrics - RMSE: {test_rmse:.4f}, MAE: {test_mae:.4f}, R²: {test_r2:.4f}\")\n",
        "\n",
        "                # Create export directory for this combination\n",
        "                export_name = f\"{model_name}_{exp_name}\"\n",
        "                export_dir = META_MODELS_DIR / export_name\n",
        "                export_dir.mkdir(exist_ok=True)\n",
        "\n",
        "                # Save predictions and targets\n",
        "                print(f\"   💾 Saving files...\")\n",
        "                np.save(export_dir / 'predictions.npy', y_pred.astype(np.float32))\n",
        "                np.save(export_dir / 'targets.npy', y_te_original.astype(np.float32))\n",
        "                np.save(export_dir / 'inputs.npy', X_te.astype(np.float32))\n",
        "\n",
        "                # Save scaler objects for future use\n",
        "                try:\n",
        "                    import joblib\n",
        "                    joblib.dump(sx, export_dir / 'scaler_X.pkl')\n",
        "                    joblib.dump(sy, export_dir / 'scaler_y.pkl')\n",
        "                    print(f\"   💾 Scalers saved using joblib\")\n",
        "                except ImportError:\n",
        "                    # Fallback to numpy for scalers\n",
        "                    np.save(export_dir / 'scaler_X_mean.npy', sx.mean_)\n",
        "                    np.save(export_dir / 'scaler_X_scale.npy', sx.scale_)\n",
        "                    np.save(export_dir / 'scaler_y_mean.npy', sy.mean_)\n",
        "                    np.save(export_dir / 'scaler_y_scale.npy', sy.scale_)\n",
        "                    print(f\"   💾 Scaler parameters saved as numpy arrays\")\n",
        "\n",
        "                # Save comprehensive metadata\n",
        "                metadata = {\n",
        "                    'model_name': model_name,\n",
        "                    'experiment_name': exp_name,\n",
        "                    'combination_rank': i,\n",
        "                    'selection_score': combo['score'],\n",
        "                    'training_metrics': {\n",
        "                        'rmse': combo['rmse'],\n",
        "                        'mae': combo['mae'],\n",
        "                        'r2': combo['r2'],\n",
        "                        'parameters': combo['parameters']\n",
        "                    },\n",
        "                    'test_metrics': {\n",
        "                        'rmse': float(test_rmse),\n",
        "                        'mae': float(test_mae),\n",
        "                        'r2': float(test_r2)\n",
        "                    },\n",
        "                    'data_info': {\n",
        "                        'features': [str(f) for f in feat_list],  # Ensure JSON serializable\n",
        "                        'n_features': len(feat_list),\n",
        "                        'test_samples': len(X_te),\n",
        "                        'input_shape': list(X_te.shape),\n",
        "                        'prediction_shape': list(y_pred.shape),\n",
        "                        'target_shape': list(y_te_original.shape)\n",
        "                    },\n",
        "                    'normalization': {\n",
        "                        'X_mean': sx.mean_.tolist(),\n",
        "                        'X_scale': sx.scale_.tolist(),\n",
        "                        'y_mean': sy.mean_.tolist(),\n",
        "                        'y_scale': sy.scale_.tolist()\n",
        "                    },\n",
        "                    'export_info': {\n",
        "                        'timestamp': str(pd.Timestamp.now()),\n",
        "                        'model_file': str(model_path),\n",
        "                        'export_purpose': 'meta_model_training',\n",
        "                        'tensorflow_version': tf.__version__,\n",
        "                        'custom_objects_used': list(CUSTOM_OBJECTS.keys())\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                with open(export_dir / 'metadata.json', 'w') as f:\n",
        "                    json.dump(metadata, f, indent=2)\n",
        "\n",
        "                print(f\"   ✅ Exported to: {export_dir}\")\n",
        "                print(f\"   📁 Files: predictions.npy, targets.npy, inputs.npy, scalers, metadata.json\")\n",
        "\n",
        "                exported_models.append({\n",
        "                    'name': export_name,\n",
        "                    'path': export_dir,\n",
        "                    'model': model_name,\n",
        "                    'experiment': exp_name,\n",
        "                    'test_rmse': test_rmse,\n",
        "                    'test_r2': test_r2\n",
        "                })\n",
        "\n",
        "            except Exception as pred_error:\n",
        "                print(f\"   ❌ Prediction generation failed: {str(pred_error)}\")\n",
        "                print(f\"   📋 Prediction error details:\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Export failed: {str(e)}\")\n",
        "            print(f\"   📋 Full error details:\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "        finally:\n",
        "            # Clean up\n",
        "            try:\n",
        "                if 'model' in locals():\n",
        "                    del model\n",
        "                tf.keras.backend.clear_session()\n",
        "                gc.collect()\n",
        "                print(f\"   🧹 Memory cleaned up\")\n",
        "            except Exception as cleanup_error:\n",
        "                print(f\"   ⚠️ Cleanup warning: {cleanup_error}\")\n",
        "\n",
        "    return exported_models\n",
        "\n",
        "# Execute export\n",
        "try:\n",
        "    exported = export_best_predictions()\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(f\"📊 EXPORT RESULTS\")\n",
        "    print(f\"=\"*70)\n",
        "\n",
        "    if exported:\n",
        "        print(f\"✅ Successfully exported {len(exported)} model combinations:\")\n",
        "\n",
        "        total_size = 0\n",
        "        for exp_model in exported:\n",
        "            exp_dir = exp_model['path']\n",
        "\n",
        "            # Calculate directory size\n",
        "            dir_size = sum(f.stat().st_size for f in exp_dir.rglob('*') if f.is_file())\n",
        "            total_size += dir_size\n",
        "            size_mb = dir_size / (1024 * 1024)\n",
        "\n",
        "            print(f\"\\n   🎯 {exp_model['name']}:\")\n",
        "            print(f\"      📂 Path: {exp_dir}\")\n",
        "            print(f\"      🤖 Model: {exp_model['model']}\")\n",
        "            print(f\"      🔬 Experiment: {exp_model['experiment']}\")\n",
        "            print(f\"      📊 Test RMSE: {exp_model['test_rmse']:.4f}\")\n",
        "            print(f\"      📊 Test R²: {exp_model['test_r2']:.4f}\")\n",
        "            print(f\"      💾 Size: {size_mb:.1f} MB\")\n",
        "\n",
        "            # List files\n",
        "            files = list(exp_dir.glob('*'))\n",
        "            print(f\"      📄 Files ({len(files)}):\", ', '.join(f.name for f in files))\n",
        "\n",
        "        print(f\"\\n📊 SUMMARY:\")\n",
        "        print(f\"   📁 Total exports: {len(exported)}\")\n",
        "        print(f\"   💾 Total size: {total_size/(1024*1024):.1f} MB\")\n",
        "        print(f\"   📂 Location: {META_MODELS_DIR}\")\n",
        "\n",
        "        # Save export summary\n",
        "        summary = {\n",
        "            'export_timestamp': str(pd.Timestamp.now()),\n",
        "            'total_exported': len(exported),\n",
        "            'export_purpose': 'meta_model_training',\n",
        "            'exported_combinations': [\n",
        "                {\n",
        "                    'name': exp['name'],\n",
        "                    'model': exp['model'],\n",
        "                    'experiment': exp['experiment'],\n",
        "                    'test_rmse': float(exp['test_rmse']),\n",
        "                    'test_r2': float(exp['test_r2']),\n",
        "                    'path': str(exp['path'])\n",
        "                }\n",
        "                for exp in exported\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        with open(META_MODELS_DIR / 'export_summary.json', 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "        print(f\"   📋 Summary saved: export_summary.json\")\n",
        "\n",
        "        print(f\"\\n💡 NEXT STEPS:\")\n",
        "        print(\"   1. Use the exported predictions for meta-model training\")\n",
        "        print(\"   2. Each export contains: predictions, targets, inputs, scalers, metadata\")\n",
        "        print(\"   3. Metadata includes all necessary information for meta-model setup\")\n",
        "        print(\"   4. Scalers can be used to normalize new data consistently\")\n",
        "\n",
        "    else:\n",
        "        print(\"❌ No models were successfully exported!\")\n",
        "        print(\"   Check that:\")\n",
        "        print(\"   - Training has completed and metrics_advanced.csv exists\")\n",
        "        print(\"   - Model files (.keras) exist in experiment directories\")\n",
        "        print(\"   - EXPERIMENTS and ADVANCED_MODELS are properly defined\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ CRITICAL ERROR: {e}\")\n",
        "    import traceback\n",
        "    print(\"Full traceback:\")\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\n\" + \"=\"*70)\n",
        "print(\"🏁 BEST MODELS EXPORT COMPLETED\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
