#!/usr/bin/env python3
"""
AN√ÅLISIS DETALLADO DEL NOTEBOOK PARA DETECTAR DUPLICACIONES Y PROBLEMAS
"""

import json
import re

def analyze_notebook():
    """Analiza el notebook en detalle para encontrar problemas"""
    
    print("üîç AN√ÅLISIS DETALLADO DEL NOTEBOOK")
    print("=" * 80)
    
    # Cargar notebook
    with open('models/hybrid_models_GRU-w12.ipynb', 'r') as f:
        notebook = json.load(f)
    
    print(f"üìã Notebook: {len(notebook['cells'])} celdas")
    
    # Analizar cada celda
    all_content = ""
    for i, cell in enumerate(notebook['cells']):
        content = ''.join(cell['source'])
        all_content += content + "\n"
        print(f"   Celda {i}: {len(content.splitlines())} l√≠neas")
    
    print(f"üìã Total l√≠neas: {len(all_content.splitlines())}")
    
    # 1. DETECTAR FUNCIONES DUPLICADAS
    print("\nüîç 1. FUNCIONES DUPLICADAS:")
    print("-" * 50)
    
    critical_functions = [
        'def build_dataloaders',
        'def run_complete_training_pipeline',
        'def train_all_active_experiments',
        'def train_experiment_complete',
        'def train_experiment(',  # La funci√≥n original
        'def generate_realistic_predictions',
        'def plot_monthly_predictions',
        'def plot_spatial_maps'
    ]
    
    duplicates_found = []
    for func in critical_functions:
        count = all_content.count(func)
        if count > 1:
            print(f"   üö® {func}: {count} definiciones (DUPLICADA)")
            duplicates_found.append(func)
            
            # Buscar las posiciones de las duplicaciones
            positions = []
            start = 0
            while True:
                pos = all_content.find(func, start)
                if pos == -1:
                    break
                line_num = all_content[:pos].count('\n') + 1
                positions.append(line_num)
                start = pos + 1
            print(f"      üìç L√≠neas: {positions}")
        else:
            print(f"   ‚úÖ {func}: {count} definici√≥n")
    
    # 2. DETECTAR VARIABLES DUPLICADAS
    print("\nüîç 2. VARIABLES DUPLICADAS:")
    print("-" * 50)
    
    critical_variables = [
        'EXPERIMENTS = {',
        'FOLDS = {',
        'MODEL_FACTORY = {',
        'HYPERPARAMS = {'
    ]
    
    for var in critical_variables:
        count = all_content.count(var)
        if count > 1:
            print(f"   üö® {var}: {count} definiciones (DUPLICADA)")
            duplicates_found.append(var)
        else:
            print(f"   ‚úÖ {var}: {count} definici√≥n")
    
    # 3. ANALIZAR FLUJO DE build_dataloaders
    print("\nüîç 3. AN√ÅLISIS DE build_dataloaders:")
    print("-" * 50)
    
    # Buscar todas las definiciones de build_dataloaders
    build_patterns = re.findall(
        r'def build_dataloaders\([^)]*\):(.*?)(?=\ndef [^_]|\Z)',
        all_content,
        re.DOTALL
    )
    
    if len(build_patterns) > 1:
        print(f"   üö® {len(build_patterns)} definiciones de build_dataloaders encontradas")
        
        for i, pattern in enumerate(build_patterns):
            print(f"\n   üìã Definici√≥n {i+1}:")
            
            # Verificar par√°metros
            if 'model_key' in pattern and 'exp_name' in pattern:
                print("      ‚ö†Ô∏è Usa tanto model_key como exp_name (inconsistente)")
            elif 'model_key' in pattern:
                print("      üìù Usa par√°metro: model_key")
            elif 'exp_name' in pattern:
                print("      üìù Usa par√°metro: exp_name")
            
            # Verificar extracci√≥n de features
            if "EXPERIMENTS[" in pattern and "feature_list" in pattern:
                print("      ‚úÖ Extrae features de EXPERIMENTS")
            else:
                print("      ‚ùå NO extrae features correctamente")
            
            # Verificar procesamiento de features
            if "for feature in features:" in pattern:
                print("      ‚úÖ Itera sobre features")
            else:
                print("      ‚ùå NO itera sobre features")
            
            # Verificar return
            if "return {" in pattern:
                print("      ‚úÖ Retorna diccionario")
            else:
                print("      ‚ùå NO retorna resultado")
    
    # 4. ANALIZAR FLUJO DE ENTRENAMIENTO
    print("\nüîç 4. FLUJO DE ENTRENAMIENTO:")
    print("-" * 50)
    
    # Verificar llamadas a build_dataloaders
    build_calls = re.findall(r'build_dataloaders\([^)]*\)', all_content)
    print(f"   üìã Llamadas a build_dataloaders: {len(build_calls)}")
    
    for i, call in enumerate(build_calls):
        print(f"      {i+1}. {call}")
        
        # Verificar consistencia de par√°metros
        if 'exp_name' in call and 'fold_name' in call and 'dataset' in call:
            print("         ‚úÖ Par√°metros correctos (exp_name, fold_name, dataset)")
        elif 'model_key' in call:
            print("         ‚ö†Ô∏è Usa model_key (verificar consistencia)")
        else:
            print("         ‚ùå Par√°metros incorrectos")
    
    # 5. VERIFICAR CONFIGURACI√ìN DE EXPERIMENTOS
    print("\nüîç 5. CONFIGURACI√ìN DE EXPERIMENTOS:")
    print("-" * 50)
    
    # Buscar definiciones de EXPERIMENTS
    exp_matches = re.findall(r"EXPERIMENTS\s*=\s*{([^}]*ConvGRU[^}]*)}", all_content, re.DOTALL)
    
    if len(exp_matches) > 1:
        print(f"   üö® {len(exp_matches)} definiciones de EXPERIMENTS")
        
        # Analizar la √∫ltima definici√≥n (la que se usar√°)
        if exp_matches:
            latest_exp = exp_matches[-1]
            
            # Buscar experimentos definidos
            exp_names = re.findall(r"'([^']*GRU[^']*)':", latest_exp)
            print(f"   üìã Experimentos en √∫ltima definici√≥n: {exp_names}")
            
            # Verificar feature_list para cada experimento
            for exp_name in exp_names:
                if f"'{exp_name}'" in latest_exp and 'feature_list' in latest_exp:
                    # Buscar el feature_list espec√≠fico
                    feature_pattern = rf"'{exp_name}':[^}}]*'feature_list':\s*\[([^\]]*)\]"
                    features_match = re.search(feature_pattern, latest_exp, re.DOTALL)
                    if features_match:
                        features_str = features_match.group(1)
                        feature_count = len(re.findall(r"'[^']*'", features_str))
                        print(f"      ‚Ä¢ {exp_name}: {feature_count} caracter√≠sticas")
                    else:
                        print(f"      ‚ùå {exp_name}: feature_list no encontrada")
                else:
                    print(f"      ‚ö†Ô∏è {exp_name}: configuraci√≥n incompleta")
    
    # 6. RECOMENDACIONES DE LIMPIEZA
    print("\nüîß 6. RECOMENDACIONES DE LIMPIEZA:")
    print("-" * 50)
    
    if duplicates_found:
        print("   üìã ACCIONES REQUERIDAS:")
        
        for dup in duplicates_found:
            if 'def build_dataloaders' in dup:
                print("   1. ELIMINAR build_dataloaders duplicada - mantener solo la √öLTIMA")
            elif 'EXPERIMENTS = {' in dup:
                print("   2. ELIMINAR EXPERIMENTS duplicada - mantener solo la √öLTIMA")
            elif 'def train_experiment(' in dup:
                print("   3. ELIMINAR train_experiment antigua - usar train_experiment_complete")
        
        print("\n   üìã FUNCIONES SEGURAS PARA ELIMINAR:")
        if all_content.count('def train_experiment(') > 0 and all_content.count('def train_experiment_complete') > 0:
            print("      ‚Ä¢ def train_experiment( - reemplazada por train_experiment_complete")
        
        # Buscar funciones que no se usan
        unused_functions = []
        all_defs = re.findall(r'def ([a-zA-Z_][a-zA-Z0-9_]*)', all_content)
        for func_name in set(all_defs):
            if all_defs.count(func_name) == 1:  # Solo una definici√≥n
                # Buscar si se llama en alg√∫n lado
                call_pattern = f'{func_name}\\('
                calls = len(re.findall(call_pattern, all_content))
                if calls <= 1:  # Solo la definici√≥n
                    unused_functions.append(func_name)
        
        if unused_functions:
            print("      ‚Ä¢ Funciones posiblemente sin usar:")
            for func in unused_functions[:5]:  # Primeras 5
                print(f"        - {func}")
    else:
        print("   ‚úÖ No se encontraron duplicaciones cr√≠ticas")
    
    # 7. VERIFICAR PAR√ÅMETROS EN FLUJO COMPLETO
    print("\nüîç 7. VERIFICACI√ìN DE PAR√ÅMETROS:")
    print("-" * 50)
    
    # Verificar que run_complete_training_pipeline pase dataset correctamente
    if 'run_complete_training_pipeline' in all_content:
        pipeline_section = re.search(
            r'def run_complete_training_pipeline.*?(?=\ndef|\Z)',
            all_content,
            re.DOTALL
        )
        
        if pipeline_section:
            pipeline_code = pipeline_section.group(0)
            
            if 'train_all_active_experiments(dataset)' in pipeline_code:
                print("   ‚úÖ run_complete_training_pipeline pasa dataset correctamente")
            else:
                print("   ‚ùå run_complete_training_pipeline NO pasa dataset")
            
            if 'dataset = ds_full' in pipeline_code:
                print("   ‚úÖ Usa ds_full como dataset por defecto")
            else:
                print("   ‚ö†Ô∏è No hay dataset por defecto")
    
    return duplicates_found

if __name__ == "__main__":
    duplicates = analyze_notebook()
    
    print(f"\n" + "=" * 80)
    print("üéØ RESUMEN EJECUTIVO")
    print("=" * 80)
    
    if duplicates:
        print(f"üö® PROBLEMAS ENCONTRADOS: {len(duplicates)} duplicaciones")
        print("üîß ACCI√ìN REQUERIDA: Limpiar notebook eliminando duplicaciones")
    else:
        print("‚úÖ NOTEBOOK LIMPIO: No se encontraron duplicaciones cr√≠ticas")
    
    print("üìã SIGUIENTE PASO: Implementar limpieza en el notebook") 