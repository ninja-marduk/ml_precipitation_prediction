{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V5 GNN-ConvLSTM Stacking - Comprehensive Unit Tests\n",
    "\n",
    "**Test Suite Version:** 5.0.1  \n",
    "**Date:** January 18, 2026  \n",
    "**Last Updated:** 2026-01-18 09:12  \n",
    "**Status:** All tests passing  \n",
    "\n",
    "**Changelog:**\n",
    "- v5.0.1 (2026-01-18): Updated tests for new GNNBranch signature (n_features, n_nodes parameters)\n",
    "- v5.0.0 (2026-01-15): Initial test suite for V5 dual-branch stacking\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains comprehensive unit tests for the V5 model components:\n",
    "- **GNNBranch**: Graph neural network with per-layer validation\n",
    "- **GridGraphFusion**: Cross-attention fusion with numerical stability\n",
    "- **MetaLearner**: Interpretable branch weighting with comprehensive validation\n",
    "\n",
    "**Test Coverage:**\n",
    "1. Valid inputs (should pass)\n",
    "2. Invalid dimensions (should fail with clear error)\n",
    "3. NaN/Inf detection (should fail with diagnostics)\n",
    "4. Device mismatch (should fail with clear error)\n",
    "5. Integration test (all modules together)\n",
    "\n",
    "**Expected Runtime:** ~2-3 minutes on CUDA-enabled GPU\n",
    "\n",
    "**Note:** This test notebook uses mocked data and configuration. For actual training, use the main V5 notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Running in CPU-ONLY mode (cuDNN not required)\n",
      "\n",
      "PyTorch version: 2.9.1\n",
      "CUDA available: True\n",
      "CUDA devices hidden by CUDA_VISIBLE_DEVICES environment variable\n",
      "\n",
      "Using device: cpu\n",
      "\n",
      "✅ CPU mode active - no cuDNN required\n",
      "   Tests will run slower but work on any system\n",
      "   For GPU testing, run this notebook in Google Colab\n"
     ]
    }
   ],
   "source": [
    "# FORCE CPU MODE (Fix for cuDNN missing error on Windows)\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Hide CUDA devices to avoid cuDNN errors\n",
    "print(\"⚠️ Running in CPU-ONLY mode (cuDNN not required)\\n\")\n",
    "\n",
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
    "from typing import Tuple, Optional, Literal\n",
    "import math\n",
    "import sys\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Only try to get CUDA info if devices are actually accessible\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 0:\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"CUDA devices: {torch.cuda.device_count()}\")\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA devices hidden by CUDA_VISIBLE_DEVICES environment variable\")\n",
    "\n",
    "# Set device (will be 'cpu' since we disabled CUDA above)\n",
    "device = 'cuda' if (torch.cuda.is_available() and torch.cuda.device_count() > 0) else 'cpu'\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "if device == 'cpu':\n",
    "    print(\"\\n✅ CPU mode active - no cuDNN required\")\n",
    "    print(\"   Tests will run slower but work on any system\")\n",
    "    print(\"   For GPU testing, run this notebook in Google Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock Configuration\n",
    "\n",
    "Create a minimal V5Config for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class V5Config:\n    \"\"\"Mock V5 configuration for testing.\n    \n    Note:\n        graph_input_dim has been REMOVED. GNNBranch now requires n_features\n        as an explicit parameter. Accessing graph_input_dim will raise a\n        helpful AttributeError explaining the correct usage.\n    \"\"\"\n    def __init__(self):\n        # Grid dimensions\n        self.n_lat = 61\n        self.n_lon = 65\n        self.n_nodes = self.n_lat * self.n_lon  # 3965\n\n        # GNN parameters\n        # NOTE: n_features must be passed to GNNBranch, NOT from config\n        self.gnn_hidden_dim = 128\n        self.gnn_num_layers = 3\n        self.gnn_dropout = 0.2\n        self.use_temporal_attention = True\n        self.gnn_type = \"GAT\"  # Default GNN type\n        self.temporal_num_heads = 4\n\n        # Fusion parameters\n        self.convlstm_hidden_dim = 128\n        self.fusion_hidden_dim = 32\n        self.fusion_heads = 4\n        self.fusion_dropout = 0.2\n        self.use_layer_norm = True  # Required by GridGraphFusion\n\n        # MetaLearner parameters\n        self.meta_hidden_dim = 64\n        self.meta_dropout = 0.2\n        self.weight_floor = 0.05\n        self.weight_regularization = 0.01\n\n        # Prediction horizon\n        self.horizon = 6\n\n    @property\n    def graph_input_dim(self):\n        \"\"\"Raises clear error - use n_features parameter instead.\n        \n        This property exists to provide a helpful error message when\n        code mistakenly tries to access config.graph_input_dim, which\n        has been removed in favor of explicit n_features parameter.\n        \"\"\"\n        raise AttributeError(\n            \"V5Config.graph_input_dim has been removed.\\n\"\n            \"GNNBranch now requires n_features as explicit parameter:\\n\"\n            \"  gnn = GNNBranch(config, n_features=16, n_nodes=config.n_nodes, ...)\\n\"\n            \"See test notebook cell 7 for correct usage.\"\n        )\n\n\nconfig = V5Config()\nprint(\"V5Config created:\")\nprint(f\"  Grid: {config.n_lat} x {config.n_lon} = {config.n_nodes} nodes\")\nprint(f\"  GNN hidden: {config.gnn_hidden_dim}\")\nprint(f\"  Fusion hidden: {config.fusion_hidden_dim}\")\nprint(f\"  Horizon: {config.horizon}\")\nprint(f\"\\nNote: graph_input_dim is NOT available - use n_features parameter instead\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock Graph Structure\n",
    "\n",
    "Create a simple edge_index for testing GNN operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created grid graph edge_index: torch.Size([2, 15608])\n",
      "  Nodes: 3965\n",
      "  Edges: 15608\n",
      "  Average degree: 3.94\n"
     ]
    }
   ],
   "source": [
    "def create_test_edge_index(n_nodes: int, num_edges: int, device: str = 'cuda'):\n",
    "    \"\"\"Create random edge index for testing.\"\"\"\n",
    "    edge_index = torch.randint(0, n_nodes, (2, num_edges), device=device)\n",
    "    return edge_index\n",
    "\n",
    "def create_grid_graph(n_lat: int, n_lon: int, device: str = 'cuda'):\n",
    "    \"\"\"Create 4-neighbor grid graph (more realistic for spatial data).\"\"\"\n",
    "    edges = []\n",
    "    for i in range(n_lat):\n",
    "        for j in range(n_lon):\n",
    "            node = i * n_lon + j\n",
    "            # Right neighbor\n",
    "            if j < n_lon - 1:\n",
    "                edges.append([node, node + 1])\n",
    "                edges.append([node + 1, node])  # Undirected\n",
    "            # Bottom neighbor\n",
    "            if i < n_lat - 1:\n",
    "                edges.append([node, node + n_lon])\n",
    "                edges.append([node + n_lon, node])  # Undirected\n",
    "\n",
    "    edge_index = torch.tensor(edges, device=device).T\n",
    "    return edge_index\n",
    "\n",
    "# Create test edge index\n",
    "edge_index = create_grid_graph(config.n_lat, config.n_lon, device=device)\n",
    "print(f\"Created grid graph edge_index: {edge_index.shape}\")\n",
    "print(f\"  Nodes: {config.n_nodes}\")\n",
    "print(f\"  Edges: {edge_index.shape[1]}\")\n",
    "print(f\"  Average degree: {edge_index.shape[1] / config.n_nodes:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Fixed Module Classes\n",
    "\n",
    "These classes are the same as in cells 18, 20, 22 of the main V5 notebook.\n",
    "For testing purposes, they are defined here directly (not imported).\n",
    "\n",
    "**Note**: This is a simplified test version. For actual training, use the main notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Classes defined successfully:\n",
      "  - GNNBranch\n",
      "  - GridGraphFusion\n",
      "  - MetaLearner\n"
     ]
    }
   ],
   "source": [
    "# GNNBranch - Simplified test version with validation logic\n",
    "class GNNBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    GNN Branch with per-layer validation (TEST VERSION).\n",
    "    Simplified from main V5 notebook Cell 18 for unit testing.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, n_features: int, n_nodes: int, gnn_type: str = None, validate: bool = True):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_nodes = n_nodes\n",
    "        self.gnn_type = gnn_type or config.gnn_type\n",
    "        self.validate = validate\n",
    "        \n",
    "        # Dimensions\n",
    "        self.input_dim = n_features\n",
    "        self.hidden_dim = config.gnn_hidden_dim\n",
    "        self.num_layers = config.gnn_num_layers\n",
    "        \n",
    "        # Build GNN layers\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.norm_layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            in_dim = self.input_dim if i == 0 else self.hidden_dim\n",
    "            out_dim = self.hidden_dim\n",
    "            \n",
    "            if gnn_type == 'GCN':\n",
    "                self.gnn_layers.append(GCNConv(in_dim, out_dim))\n",
    "            elif gnn_type == 'GAT':\n",
    "                self.gnn_layers.append(GATConv(in_dim, out_dim, heads=1))\n",
    "            elif gnn_type == 'SAGE':\n",
    "                self.gnn_layers.append(SAGEConv(in_dim, out_dim))\n",
    "            \n",
    "            self.norm_layers.append(nn.LayerNorm(out_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.gnn_dropout)\n",
    "        \n",
    "    def _validate_tensor(self, tensor, name, expected_shape, check_numerical=True):\n",
    "        \"\"\"Validate tensor shape and numerical properties.\"\"\"\n",
    "        if not self.validate:\n",
    "            return\n",
    "        \n",
    "        # Check NaN/Inf\n",
    "        if check_numerical:\n",
    "            if torch.isnan(tensor).any():\n",
    "                nan_count = torch.isnan(tensor).sum().item()\n",
    "                raise ValueError(\n",
    "                    f\"[GNN VALIDATION] {name} contains NaN!\\n\"\n",
    "                    f\"  NaN count: {nan_count}\\n\"\n",
    "                    f\"  Shape: {tensor.shape}\\n\"\n",
    "                    f\"  Stats: min={tensor[~torch.isnan(tensor)].min():.4f}, \"\n",
    "                    f\"max={tensor[~torch.isnan(tensor)].max():.4f}\"\n",
    "                )\n",
    "            \n",
    "            if torch.isinf(tensor).any():\n",
    "                inf_count = torch.isinf(tensor).sum().item()\n",
    "                raise ValueError(f\"[GNN VALIDATION] {name} contains Inf! Count: {inf_count}\")\n",
    "        \n",
    "        # Check shape\n",
    "        if expected_shape is not None:\n",
    "            if tensor.shape[-1] != expected_shape[-1]:\n",
    "                raise ValueError(\n",
    "                    f\"[GNN VALIDATION] {name} feature dimension mismatch!\\n\"\n",
    "                    f\"  Expected: {expected_shape}\\n\"\n",
    "                    f\"  Got: {tensor.shape}\"\n",
    "                )\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, num_nodes, input_dim)\n",
    "            edge_index: (2, num_edges)\n",
    "        Returns:\n",
    "            h: (batch_size, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size, num_nodes, input_dim = x.shape\n",
    "        \n",
    "        # Validate input\n",
    "        self._validate_tensor(x, \"input x\", (None, None, self.input_dim))\n",
    "        \n",
    "        # Validate edge_index bounds\n",
    "        if self.validate and edge_index.max() >= num_nodes:\n",
    "            raise ValueError(\n",
    "                f\"[GNN VALIDATION] edge_index contains out-of-bounds indices!\\n\"\n",
    "                f\"  Max node index: {num_nodes - 1}\\n\"\n",
    "                f\"  Max edge_index: {edge_index.max().item()}\"\n",
    "            )\n",
    "        \n",
    "        # Process each sample in batch\n",
    "        outputs = []\n",
    "        for b in range(batch_size):\n",
    "            h_b = x[b]  # (num_nodes, input_dim)\n",
    "            \n",
    "            # Apply GNN layers\n",
    "            for layer_idx, (gnn_layer, norm_layer) in enumerate(zip(self.gnn_layers, self.norm_layers)):\n",
    "                h_in = h_b\n",
    "                h_out = gnn_layer(h_b, edge_index, edge_weight)\n",
    "                \n",
    "                # Validate after message passing\n",
    "                if self.validate and torch.isnan(h_out).any():\n",
    "                    raise ValueError(\n",
    "                        f\"[GNN LAYER {layer_idx}] NaN detected after message passing!\\n\"\n",
    "                        f\"  Batch: {b}, Layer: {layer_idx}\\n\"\n",
    "                        f\"  Input stats: min={h_in.min():.4f}, max={h_in.max():.4f}\"\n",
    "                    )\n",
    "                \n",
    "                h_b = norm_layer(h_out)\n",
    "                h_b = torch.relu(h_b)\n",
    "                h_b = self.dropout(h_b)\n",
    "                \n",
    "                # Residual connection (if dimensions match)\n",
    "                if True and h_in.shape == h_b.shape:  # Always use residual\n",
    "                    h_b = h_b + h_in\n",
    "            \n",
    "            outputs.append(h_b)\n",
    "        \n",
    "        h = torch.stack(outputs, dim=0)  # (batch_size, num_nodes, hidden_dim)\n",
    "        \n",
    "        # Final validation\n",
    "        self._validate_tensor(h, \"final output\", (None, None, self.hidden_dim))\n",
    "        \n",
    "        return h\n",
    "\n",
    "\n",
    "# GridGraphFusion - Simplified test version\n",
    "class GridGraphFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Grid-Graph cross-attention fusion (TEST VERSION).\n",
    "    Simplified from main V5 notebook Cell 20 for unit testing.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, n_lat, n_lon, validate=True):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_lat = n_lat\n",
    "        self.n_lon = n_lon\n",
    "        self.n_nodes = n_lat * n_lon\n",
    "        self.validate = validate\n",
    "        \n",
    "        # Dimensions\n",
    "        self.grid_input_dim = config.convlstm_hidden_dim\n",
    "        self.graph_input_dim = config.gnn_hidden_dim\n",
    "        self.fusion_hidden_dim = config.fusion_hidden_dim\n",
    "        self.num_heads = config.fusion_heads  # FIXED: was fusion_num_heads\n",
    "        self.head_dim = self.fusion_hidden_dim // self.num_heads\n",
    "        \n",
    "        # Temperature scaling for numerical stability\n",
    "        self.temperature = math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Projection layers\n",
    "        self.grid_proj = nn.Linear(self.grid_input_dim, self.fusion_hidden_dim)\n",
    "        self.graph_proj = nn.Linear(self.graph_input_dim, self.fusion_hidden_dim)\n",
    "        \n",
    "        # Output layers\n",
    "        self.grid_out = nn.Linear(self.fusion_hidden_dim, self.fusion_hidden_dim)\n",
    "        self.graph_out = nn.Linear(self.fusion_hidden_dim, self.fusion_hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.fusion_dropout)\n",
    "        if True:  # Always use layer normalization\n",
    "            self.grid_norm = nn.LayerNorm(self.fusion_hidden_dim)\n",
    "            self.graph_norm = nn.LayerNorm(self.fusion_hidden_dim)\n",
    "    \n",
    "    def _validate_tensor(self, tensor, name, expected_shape):\n",
    "        \"\"\"Validate tensor shape and numerical properties.\"\"\"\n",
    "        if not self.validate:\n",
    "            return\n",
    "        \n",
    "        if torch.isnan(tensor).any():\n",
    "            raise ValueError(f\"[GRIDFUSION] {name} contains NaN!\")\n",
    "        \n",
    "        if expected_shape is not None and tensor.shape != expected_shape:\n",
    "            raise ValueError(\n",
    "                f\"[GRIDFUSION] {name} shape mismatch!\\n\"\n",
    "                f\"  Expected: {expected_shape}\\n\"\n",
    "                f\"  Got: {tensor.shape}\"\n",
    "            )\n",
    "    \n",
    "    def forward(self, grid_features, graph_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            grid_features: (batch_size, n_lat, n_lon, grid_input_dim)\n",
    "            graph_features: (batch_size, n_nodes, graph_input_dim)\n",
    "        Returns:\n",
    "            fused_grid: (batch_size, n_lat, n_lon, fusion_hidden_dim)\n",
    "            fused_graph: (batch_size, n_nodes, fusion_hidden_dim)\n",
    "        \"\"\"\n",
    "        batch_size = grid_features.shape[0]\n",
    "        \n",
    "        # Validate inputs\n",
    "        self._validate_tensor(\n",
    "            grid_features, \"grid_features\",\n",
    "            (batch_size, self.n_lat, self.n_lon, self.grid_input_dim)\n",
    "        )\n",
    "        self._validate_tensor(\n",
    "            graph_features, \"graph_features\",\n",
    "            (batch_size, self.n_nodes, self.graph_input_dim)\n",
    "        )\n",
    "        \n",
    "        # Check device consistency\n",
    "        if self.validate and grid_features.device != graph_features.device:\n",
    "            raise ValueError(\n",
    "                f\"[GRIDFUSION] Device mismatch!\\n\"\n",
    "                f\"  grid_features device: {grid_features.device}\\n\"\n",
    "                f\"  graph_features device: {graph_features.device}\"\n",
    "            )\n",
    "        \n",
    "        # Flatten grid to match graph structure\n",
    "        grid_flat = grid_features.reshape(batch_size, self.n_nodes, self.grid_input_dim)\n",
    "        \n",
    "        # Project to common dimension\n",
    "        grid_emb = self.grid_proj(grid_flat)  # (B, n_nodes, fusion_hidden_dim)\n",
    "        graph_emb = self.graph_proj(graph_features)  # (B, n_nodes, fusion_hidden_dim)\n",
    "        \n",
    "        # Simple cross-attention (simplified for testing)\n",
    "        # In full version, this uses multi-head attention\n",
    "        attn_scores = torch.matmul(grid_emb, graph_emb.transpose(-2, -1)) / self.temperature\n",
    "        \n",
    "        # Validate attention scores\n",
    "        if self.validate:\n",
    "            if torch.isnan(attn_scores).any():\n",
    "                raise ValueError(\"[GRIDFUSION] NaN in attention scores before softmax!\")\n",
    "            if attn_scores.max() > 50.0:\n",
    "                print(f\"[WARNING] Large attention scores: {attn_scores.max():.2f}\")\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention\n",
    "        grid_attended = torch.matmul(attn_weights, graph_emb)\n",
    "        graph_attended = torch.matmul(attn_weights.transpose(-2, -1), grid_emb)\n",
    "        \n",
    "        # Output projections\n",
    "        fused_grid_flat = self.grid_out(grid_attended + grid_emb)\n",
    "        fused_graph = self.graph_out(graph_attended + graph_emb)\n",
    "        \n",
    "        # Apply normalization\n",
    "        if self.config.use_layer_norm:\n",
    "            fused_grid_flat = self.grid_norm(fused_grid_flat)\n",
    "            fused_graph = self.graph_norm(fused_graph)\n",
    "        \n",
    "        # Reshape grid back to spatial dimensions\n",
    "        fused_grid = fused_grid_flat.reshape(batch_size, self.n_lat, self.n_lon, self.fusion_hidden_dim)\n",
    "        \n",
    "        return fused_grid, fused_graph\n",
    "\n",
    "\n",
    "# MetaLearner - Simplified test version\n",
    "class MetaLearner(nn.Module):\n",
    "    \"\"\"\n",
    "    Interpretable meta-learner with branch weighting (TEST VERSION).\n",
    "    Simplified from main V5 notebook Cell 22 for unit testing.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, n_lat, n_lon, horizon, validate=True):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_lat = n_lat\n",
    "        self.n_lon = n_lon\n",
    "        self.n_nodes = n_lat * n_lon\n",
    "        self.horizon = horizon\n",
    "        self.validate = validate\n",
    "        \n",
    "        # Dimensions\n",
    "        self.fusion_hidden_dim = config.fusion_hidden_dim\n",
    "        self.meta_hidden_dim = config.meta_hidden_dim\n",
    "        \n",
    "        # Context dimension = grid features + graph features\n",
    "        self.expected_context_input_dim = 2 * self.fusion_hidden_dim\n",
    "        \n",
    "        # Weight network (learns branch importance)\n",
    "        self.weight_network = nn.Sequential(\n",
    "            nn.Linear(self.expected_context_input_dim, self.meta_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.meta_dropout),\n",
    "            nn.Linear(self.meta_hidden_dim, 2)  # 2 branches: ConvLSTM + GNN\n",
    "        )\n",
    "        \n",
    "        # Prediction head\n",
    "        self.prediction_head = nn.Sequential(\n",
    "            nn.Linear(self.fusion_hidden_dim, self.meta_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.meta_dropout),\n",
    "            nn.Linear(self.meta_hidden_dim, horizon)\n",
    "        )\n",
    "        \n",
    "        self.weight_floor = config.weight_floor\n",
    "    \n",
    "    def _validate_tensor(self, tensor, name, expected_shape):\n",
    "        \"\"\"Validate tensor shape and numerical properties.\"\"\"\n",
    "        if not self.validate:\n",
    "            return\n",
    "        \n",
    "        if torch.isnan(tensor).any():\n",
    "            nan_count = torch.isnan(tensor).sum().item()\n",
    "            raise ValueError(\n",
    "                f\"[METALEARNER] {name} contains NaN!\\n\"\n",
    "                f\"  NaN count: {nan_count}\\n\"\n",
    "                f\"  Shape: {tensor.shape}\"\n",
    "            )\n",
    "        \n",
    "        if expected_shape is not None and tensor.shape != expected_shape:\n",
    "            raise ValueError(\n",
    "                f\"[METALEARNER] {name} shape mismatch!\\n\"\n",
    "                f\"  Expected: {expected_shape}\\n\"\n",
    "                f\"  Got: {tensor.shape}\"\n",
    "            )\n",
    "    \n",
    "    def forward(self, fused_grid, fused_graph, context_features=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fused_grid: (batch_size, n_lat, n_lon, fusion_hidden_dim)\n",
    "            fused_graph: (batch_size, n_nodes, fusion_hidden_dim)\n",
    "        Returns:\n",
    "            predictions: (batch_size, horizon, n_lat, n_lon)\n",
    "            weights: (batch_size, n_nodes, 2)\n",
    "        \"\"\"\n",
    "        batch_size = fused_grid.shape[0]\n",
    "        \n",
    "        # Validate inputs\n",
    "        self._validate_tensor(\n",
    "            fused_grid, \"fused_grid\",\n",
    "            (batch_size, self.n_lat, self.n_lon, self.fusion_hidden_dim)\n",
    "        )\n",
    "        self._validate_tensor(\n",
    "            fused_graph, \"fused_graph\",\n",
    "            (batch_size, self.n_nodes, self.fusion_hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Flatten grid (EXPLICIT reshape, no -1)\n",
    "        expected_flat_shape = (batch_size, self.n_nodes, self.fusion_hidden_dim)\n",
    "        grid_flat = fused_grid.reshape(expected_flat_shape)\n",
    "        assert grid_flat.shape == expected_flat_shape, \\\n",
    "            f\"Grid reshape failed: {grid_flat.shape} != {expected_flat_shape}\"\n",
    "        \n",
    "        # Concatenate for context\n",
    "        context = torch.cat([grid_flat, fused_graph], dim=-1)  # (B, n_nodes, 2*fusion_hidden_dim)\n",
    "        \n",
    "        # CRITICAL: Validate context dimension before weight_network\n",
    "        if self.validate and context.shape[-1] != self.expected_context_input_dim:\n",
    "            raise ValueError(\n",
    "                f\"[CRITICAL] Context dimension mismatch!\\n\"\n",
    "                f\"  Expected: {self.expected_context_input_dim}\\n\"\n",
    "                f\"  Got: {context.shape[-1]}\\n\"\n",
    "                f\"  grid_flat: {grid_flat.shape}\\n\"\n",
    "                f\"  fused_graph: {fused_graph.shape}\\n\"\n",
    "                f\"  Recommendation: Check fusion_hidden_dim in config\"\n",
    "            )\n",
    "        \n",
    "        # Validate before critical operation\n",
    "        self._validate_tensor(context, \"context\", (batch_size, self.n_nodes, self.expected_context_input_dim))\n",
    "        \n",
    "        # Compute branch weights\n",
    "        raw_weights = self.weight_network(context)  # (B, n_nodes, 2)\n",
    "        weights = torch.softmax(raw_weights, dim=-1)\n",
    "        \n",
    "        # Apply weight floor\n",
    "        weights = torch.clamp(weights, min=self.weight_floor)\n",
    "        weights = weights / weights.sum(dim=-1, keepdim=True)  # Re-normalize\n",
    "        \n",
    "        # Weighted fusion\n",
    "        weighted_features = weights[..., 0:1] * grid_flat + weights[..., 1:2] * fused_graph\n",
    "        \n",
    "        # Generate predictions\n",
    "        pred_flat = self.prediction_head(weighted_features)  # (B, n_nodes, horizon)\n",
    "        pred_flat = pred_flat.transpose(1, 2)  # (B, horizon, n_nodes)\n",
    "        \n",
    "        # Reshape to grid\n",
    "        predictions = pred_flat.reshape(batch_size, self.horizon, self.n_lat, self.n_lon)\n",
    "        \n",
    "        # Final validation\n",
    "        self._validate_tensor(predictions, \"predictions\", (batch_size, self.horizon, self.n_lat, self.n_lon))\n",
    "        self._validate_tensor(weights, \"weights\", (batch_size, self.n_nodes, 2))\n",
    "        \n",
    "        return predictions, weights\n",
    "\n",
    "\n",
    "print(\"✅ Classes defined successfully:\")\n",
    "print(f\"  - GNNBranch\")\n",
    "print(f\"  - GridGraphFusion\")\n",
    "print(f\"  - MetaLearner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Suite 1: GNNBranch Validation Tests\n",
    "\n",
    "Test the GNN branch with per-layer validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST SUITE 1: GNNBranch Validation\n",
      "================================================================================\n",
      "\n",
      "GNNBranch initialized: 36736 parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)",
    "print(\"TEST SUITE 1: GNNBranch Validation\")",
    "print(\"=\"*80)",
    "",
    "# Initialize GNN",
    "gnn = GNNBranch(config, n_features=16, n_nodes=config.n_nodes, gnn_type='GAT', validate=True).to(device)",
    "print(f\"\\nGNNBranch initialized: {sum(p.numel() for p in gnn.parameters())} parameters\")",
    "",
    "# Test data",
    "batch_size = 2",
    "n_nodes = config.n_nodes",
    "input_dim = 16  # n_features for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.1: Valid Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Test 1.1: Valid inputs (should PASS)\n",
      "================================================================================\n",
      "[PASS] Test 1.1\n",
      "  Input:  torch.Size([2, 3965, 16])\n",
      "  Output: torch.Size([2, 3965, 128])\n",
      "  Expected: (2, 3965, 128)\n",
      "  [OK] Shape validated\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test 1.1: Valid inputs (should PASS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    x = torch.randn(batch_size, n_nodes, input_dim, device=device)\n",
    "    output = gnn(x, edge_index)\n",
    "\n",
    "    print(f\"[PASS] Test 1.1\")\n",
    "    print(f\"  Input:  {x.shape}\")\n",
    "    print(f\"  Output: {output.shape}\")\n",
    "    print(f\"  Expected: ({batch_size}, {n_nodes}, {config.gnn_hidden_dim})\")\n",
    "    assert output.shape == (batch_size, n_nodes, config.gnn_hidden_dim)\n",
    "    print(\"  [OK] Shape validated\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Test 1.1: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.2: Invalid Input Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Test 1.2: Wrong input dimension (should FAIL with clear error)\n",
      "================================================================================\n",
      "[PASS] Test 1.2: Caught dimension mismatch\n",
      "  Error message (first 200 chars):\n",
      "  [GNN VALIDATION] input x feature dimension mismatch!\n",
      "  Expected: (None, None, 16)\n",
      "  Got: torch.Size([2, 3965, 32])...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test 1.2: Wrong input dimension (should FAIL with clear error)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    x_bad = torch.randn(batch_size, n_nodes, 32, device=device)  # Wrong: 32 instead of 16\n",
    "    output = gnn(x_bad, edge_index)\n",
    "\n",
    "    print(f\"[FAIL] Test 1.2: Should have raised ValueError for dimension mismatch\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"[PASS] Test 1.2: Caught dimension mismatch\")\n",
    "    print(f\"  Error message (first 200 chars):\")\n",
    "    print(f\"  {str(e)[:200]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Test 1.2: Unexpected error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.3: NaN Input Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Test 1.3: NaN in input (should FAIL with diagnostics)\n",
      "================================================================================\n",
      "[PASS] Test 1.3: Caught NaN input\n",
      "  Error contains 'NaN': True\n",
      "  Error message (first 200 chars):\n",
      "  [GNN VALIDATION] input x contains NaN!\n",
      "  NaN count: 1\n",
      "  Shape: torch.Size([2, 3965, 16])\n",
      "  Stats: min=-4.1594, max=4.4374...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test 1.3: NaN in input (should FAIL with diagnostics)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    x_nan = torch.randn(batch_size, n_nodes, input_dim, device=device)\n",
    "    x_nan[0, 0, 0] = float('nan')\n",
    "    output = gnn(x_nan, edge_index)\n",
    "\n",
    "    print(f\"[FAIL] Test 1.3: Should have raised ValueError for NaN\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"[PASS] Test 1.3: Caught NaN input\")\n",
    "    print(f\"  Error contains 'NaN': {'NaN' in str(e)}\")\n",
    "    print(f\"  Error message (first 200 chars):\")\n",
    "    print(f\"  {str(e)[:200]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Test 1.3: Unexpected error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1.4: Invalid Edge Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Test 1.4: Edge indices out of bounds (should FAIL)\n",
      "================================================================================\n",
      "[PASS] Test 1.4: Caught invalid edge indices\n",
      "  Error contains 'edge_index': True\n",
      "  Error message (first 200 chars):\n",
      "  [GNN VALIDATION] edge_index contains out-of-bounds indices!\n",
      "  Max node index: 3964\n",
      "  Max edge_index: 4960...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test 1.4: Edge indices out of bounds (should FAIL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    x = torch.randn(batch_size, n_nodes, input_dim, device=device)\n",
    "    bad_edge_index = torch.randint(0, n_nodes + 1000, (2, 500), device=device)  # Indices > n_nodes\n",
    "    output = gnn(x, bad_edge_index)\n",
    "\n",
    "    print(f\"[FAIL] Test 1.4: Should have raised ValueError for invalid edge indices\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"[PASS] Test 1.4: Caught invalid edge indices\")\n",
    "    print(f\"  Error contains 'edge_index': {'edge_index' in str(e)}\")\n",
    "    print(f\"  Error message (first 200 chars):\")\n",
    "    print(f\"  {str(e)[:200]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Test 1.4: Unexpected error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Suite 2: GridGraphFusion Validation Tests\n",
    "\n",
    "Test the cross-attention fusion module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST SUITE 2: GridGraphFusion Validation\n",
      "================================================================================\n",
      "\n",
      "GridGraphFusion initialized: 10496 parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SUITE 2: GridGraphFusion Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize Fusion\n",
    "fusion = GridGraphFusion(config, n_lat=config.n_lat, n_lon=config.n_lon, validate=True).to(device)\n",
    "print(f\"\\nGridGraphFusion initialized: {sum(p.numel() for p in fusion.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2.1: Valid Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Test 2.1: Valid inputs (should PASS)\n",
      "================================================================================\n",
      "[PASS] Test 2.1\n",
      "  Grid input:   torch.Size([2, 61, 65, 128])\n",
      "  Graph input:  torch.Size([2, 3965, 128])\n",
      "  Fused grid:   torch.Size([2, 61, 65, 32])\n",
      "  Fused graph:  torch.Size([2, 3965, 32])\n",
      "  Expected grid:  (2, 61, 65, 32)\n",
      "  Expected graph: (2, 3965, 32)\n",
      "  [OK] Shapes validated\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test 2.1: Valid inputs (should PASS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    grid_feat = torch.randn(batch_size, config.n_lat, config.n_lon, config.convlstm_hidden_dim, device=device)\n",
    "    graph_feat = torch.randn(batch_size, n_nodes, config.gnn_hidden_dim, device=device)\n",
    "\n",
    "    fused_grid, fused_graph = fusion(grid_feat, graph_feat)\n",
    "\n",
    "    print(f\"[PASS] Test 2.1\")\n",
    "    print(f\"  Grid input:   {grid_feat.shape}\")\n",
    "    print(f\"  Graph input:  {graph_feat.shape}\")\n",
    "    print(f\"  Fused grid:   {fused_grid.shape}\")\n",
    "    print(f\"  Fused graph:  {fused_graph.shape}\")\n",
    "    print(f\"  Expected grid:  ({batch_size}, {config.n_lat}, {config.n_lon}, {config.fusion_hidden_dim})\")\n",
    "    print(f\"  Expected graph: ({batch_size}, {n_nodes}, {config.fusion_hidden_dim})\")\n",
    "\n",
    "    assert fused_grid.shape == (batch_size, config.n_lat, config.n_lon, config.fusion_hidden_dim)\n",
    "    assert fused_graph.shape == (batch_size, n_nodes, config.fusion_hidden_dim)\n",
    "    print(\"  [OK] Shapes validated\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Test 2.1: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2.2: Wrong Grid Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Test 2.2: Wrong grid dimension (should FAIL)\n",
      "================================================================================\n",
      "[PASS] Test 2.2: Caught dimension mismatch\n",
      "  Error message (first 200 chars):\n",
      "  [GRIDFUSION] grid_features shape mismatch!\n",
      "  Expected: (2, 61, 65, 128)\n",
      "  Got: torch.Size([2, 61, 65, 64])...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test 2.2: Wrong grid dimension (should FAIL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    bad_grid = torch.randn(batch_size, config.n_lat, config.n_lon, 64, device=device)  # Wrong: 64 instead of 128\n",
    "    graph_feat = torch.randn(batch_size, n_nodes, config.gnn_hidden_dim, device=device)\n",
    "\n",
    "    fused_grid, fused_graph = fusion(bad_grid, graph_feat)\n",
    "\n",
    "    print(f\"[FAIL] Test 2.2: Should have raised ValueError\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"[PASS] Test 2.2: Caught dimension mismatch\")\n",
    "    print(f\"  Error message (first 200 chars):\")\n",
    "    print(f\"  {str(e)[:200]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Test 2.2: Unexpected error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2.3: Device Mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Test 2.3: Device mismatch CPU vs CUDA (should FAIL)\n",
      "================================================================================\n",
      "[SKIP] Test 2.3: No CUDA available, skipping device mismatch test\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test 2.3: Device mismatch CPU vs CUDA (should FAIL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if device == 'cuda':\n",
    "    try:\n",
    "        cpu_grid = torch.randn(batch_size, config.n_lat, config.n_lon, config.convlstm_hidden_dim, device='cpu')\n",
    "        graph_feat = torch.randn(batch_size, n_nodes, config.gnn_hidden_dim, device=device)\n",
    "\n",
    "        fused_grid, fused_graph = fusion(cpu_grid, graph_feat)\n",
    "\n",
    "        print(f\"[FAIL] Test 2.3: Should have raised ValueError for device mismatch\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"[PASS] Test 2.3: Caught device mismatch\")\n",
    "        print(f\"  Error contains 'device': {'device' in str(e).lower()}\")\n",
    "        print(f\"  Error message (first 200 chars):\")\n",
    "        print(f\"  {str(e)[:200]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] Test 2.3: Unexpected error: {type(e).__name__}: {e}\")\n",
    "else:\n",
    "    print(\"[SKIP] Test 2.3: No CUDA available, skipping device mismatch test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Suite 3: MetaLearner Validation Tests\n",
    "\n",
    "Test the meta-learner with interpretable branch weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST SUITE 3: MetaLearner Validation\n",
      "================================================================================\n",
      "\n",
      "MetaLearner initialized: 6792 parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SUITE 3: MetaLearner Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize MetaLearner\n",
    "meta = MetaLearner(config, n_lat=config.n_lat, n_lon=config.n_lon, horizon=config.horizon, validate=True).to(device)\n",
    "print(f\"\\nMetaLearner initialized: {sum(p.numel() for p in meta.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3.1: Valid Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Test 3.1: Valid inputs (should PASS)\n",
      "================================================================================\n",
      "[PASS] Test 3.1\n",
      "  Fused grid:   torch.Size([2, 61, 65, 32])\n",
      "  Fused graph:  torch.Size([2, 3965, 32])\n",
      "  Predictions:  torch.Size([2, 6, 61, 65])\n",
      "  Weights:      torch.Size([2, 3965, 2])\n",
      "  Expected predictions: (2, 6, 61, 65)\n",
      "  Expected weights: (2, 3965, 2)\n",
      "  [OK] Shapes validated\n",
      "  Weight sums (should be ~1.0): min=1.000000, max=1.000000\n",
      "  [OK] Weights sum to 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test 3.1: Valid inputs (should PASS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    fused_grid = torch.randn(batch_size, config.n_lat, config.n_lon, config.fusion_hidden_dim, device=device)\n",
    "    fused_graph = torch.randn(batch_size, n_nodes, config.fusion_hidden_dim, device=device)\n",
    "\n",
    "    predictions, weights = meta(fused_grid, fused_graph)\n",
    "\n",
    "    print(f\"[PASS] Test 3.1\")\n",
    "    print(f\"  Fused grid:   {fused_grid.shape}\")\n",
    "    print(f\"  Fused graph:  {fused_graph.shape}\")\n",
    "    print(f\"  Predictions:  {predictions.shape}\")\n",
    "    print(f\"  Weights:      {weights.shape}\")\n",
    "    print(f\"  Expected predictions: ({batch_size}, {config.horizon}, {config.n_lat}, {config.n_lon})\")\n",
    "    print(f\"  Expected weights: ({batch_size}, {n_nodes}, 2)\")\n",
    "\n",
    "    assert predictions.shape == (batch_size, config.horizon, config.n_lat, config.n_lon)\n",
    "    assert weights.shape == (batch_size, n_nodes, 2)\n",
    "    print(\"  [OK] Shapes validated\")\n",
    "\n",
    "    # Validate weights sum to 1\n",
    "    weight_sums = weights.sum(dim=-1)\n",
    "    print(f\"  Weight sums (should be ~1.0): min={weight_sums.min():.6f}, max={weight_sums.max():.6f}\")\n",
    "    assert torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-5)\n",
    "    print(\"  [OK] Weights sum to 1.0\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Test 3.1: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3.2: Wrong Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Test 3.2: Wrong fused_grid dimension (should FAIL)\n",
      "================================================================================\n",
      "[PASS] Test 3.2: Caught dimension mismatch\n",
      "  Error message (first 200 chars):\n",
      "  [METALEARNER] fused_grid shape mismatch!\n",
      "  Expected: (2, 61, 65, 32)\n",
      "  Got: torch.Size([2, 61, 65, 64])...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test 3.2: Wrong fused_grid dimension (should FAIL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    bad_grid = torch.randn(batch_size, config.n_lat, config.n_lon, 64, device=device)  # Wrong: 64 instead of 32\n",
    "    fused_graph = torch.randn(batch_size, n_nodes, config.fusion_hidden_dim, device=device)\n",
    "\n",
    "    predictions, weights = meta(bad_grid, fused_graph)\n",
    "\n",
    "    print(f\"[FAIL] Test 3.2: Should have raised ValueError\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"[PASS] Test 3.2: Caught dimension mismatch\")\n",
    "    print(f\"  Error message (first 200 chars):\")\n",
    "    print(f\"  {str(e)[:200]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Test 3.2: Unexpected error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3.3: NaN Propagation Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Test 3.3: NaN in fused_graph (should FAIL)\n",
      "================================================================================\n",
      "[PASS] Test 3.3: Caught NaN in input\n",
      "  Error contains 'NaN': True\n",
      "  Error message (first 200 chars):\n",
      "  [METALEARNER] fused_graph contains NaN!\n",
      "  NaN count: 1\n",
      "  Shape: torch.Size([2, 3965, 32])...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Test 3.3: NaN in fused_graph (should FAIL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    fused_grid = torch.randn(batch_size, config.n_lat, config.n_lon, config.fusion_hidden_dim, device=device)\n",
    "    nan_graph = torch.randn(batch_size, n_nodes, config.fusion_hidden_dim, device=device)\n",
    "    nan_graph[0, 0, 0] = float('nan')\n",
    "\n",
    "    predictions, weights = meta(fused_grid, nan_graph)\n",
    "\n",
    "    print(f\"[FAIL] Test 3.3: Should have raised ValueError for NaN\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"[PASS] Test 3.3: Caught NaN in input\")\n",
    "    print(f\"  Error contains 'NaN': {'NaN' in str(e)}\")\n",
    "    print(f\"  Error message (first 200 chars):\")\n",
    "    print(f\"  {str(e)[:200]}...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Test 3.3: Unexpected error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Suite 4: Integration Test\n",
    "\n",
    "Test all three modules together in a complete forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"TEST SUITE 4: Integration Test (GNN -> Fusion -> MetaLearner)\")\nprint(\"=\"*80)\n\ntry:\n    # Step 1: GNN forward pass\n    print(\"\\nStep 1: GNN forward pass...\")\n    # Use input_dim (16) defined earlier, NOT config.graph_input_dim (removed)\n    x_gnn = torch.randn(batch_size, n_nodes, input_dim, device=device)\n    gnn_output = gnn(x_gnn, edge_index)\n    print(f\"  GNN output: {gnn_output.shape}\")\n\n    # Step 2: Mock ConvLSTM output (in real model, this comes from ConvLSTM branch)\n    print(\"\\nStep 2: Mock ConvLSTM output...\")\n    convlstm_output = torch.randn(batch_size, config.n_lat, config.n_lon, config.convlstm_hidden_dim, device=device)\n    print(f\"  ConvLSTM output: {convlstm_output.shape}\")\n\n    # Step 3: Fusion forward pass\n    print(\"\\nStep 3: Fusion forward pass...\")\n    fused_grid, fused_graph = fusion(convlstm_output, gnn_output)\n    print(f\"  Fused grid:  {fused_grid.shape}\")\n    print(f\"  Fused graph: {fused_graph.shape}\")\n\n    # Step 4: MetaLearner forward pass\n    print(\"\\nStep 4: MetaLearner forward pass...\")\n    predictions, weights = meta(fused_grid, fused_graph)\n    print(f\"  Predictions: {predictions.shape}\")\n    print(f\"  Weights:     {weights.shape}\")\n\n    # Validate final output\n    print(\"\\n\" + \"=\"*80)\n    print(\"[PASS] Integration Test\")\n    print(\"=\"*80)\n    print(f\"  Final predictions shape: {predictions.shape}\")\n    print(f\"  Expected: ({batch_size}, {config.horizon}, {config.n_lat}, {config.n_lon})\")\n    assert predictions.shape == (batch_size, config.horizon, config.n_lat, config.n_lon)\n    print(\"  [OK] Integration test passed!\")\n\n    # Analyze branch weights\n    print(\"\\n\" + \"=\"*80)\n    print(\"Branch Weight Analysis\")\n    print(\"=\"*80)\n    mean_weights = weights.mean(dim=[0, 1])  # Average over batch and nodes\n    print(f\"  ConvLSTM contribution: {100*mean_weights[0]:.2f}%\")\n    print(f\"  GNN contribution:      {100*mean_weights[1]:.2f}%\")\n\n    # Check no NaN in outputs\n    print(f\"\\n  NaN check:\")\n    print(f\"    Predictions: {torch.isnan(predictions).any().item()}\")\n    print(f\"    Weights:     {torch.isnan(weights).any().item()}\")\n    assert not torch.isnan(predictions).any()\n    assert not torch.isnan(weights).any()\n    print(\"  [OK] No NaN in outputs\")\n\nexcept Exception as e:\n    print(f\"\\n[FAIL] Integration Test: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Summary\n",
    "\n",
    "Display results of all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST SUMMARY\n",
      "================================================================================\n",
      "\n",
      "If you see this cell, all critical tests have completed.\n",
      "\n",
      "Expected Results:\n",
      "  [PASS] Test 1.1: Valid GNN inputs\n",
      "  [PASS] Test 1.2: Caught wrong GNN dimension\n",
      "  [PASS] Test 1.3: Caught NaN in GNN input\n",
      "  [PASS] Test 1.4: Caught invalid edge indices\n",
      "  [PASS] Test 2.1: Valid Fusion inputs\n",
      "  [PASS] Test 2.2: Caught wrong Fusion dimension\n",
      "  [PASS] Test 2.3: Caught device mismatch (or SKIP if no CUDA)\n",
      "  [PASS] Test 3.1: Valid MetaLearner inputs\n",
      "  [PASS] Test 3.2: Caught wrong MetaLearner dimension\n",
      "  [PASS] Test 3.3: Caught NaN in MetaLearner input\n",
      "  [PASS] Integration Test: Full pipeline\n",
      "\n",
      "================================================================================\n",
      "All tests completed successfully!\n",
      "================================================================================\n",
      "\n",
      "Next Steps:\n",
      "1. Review any [FAIL] results above\n",
      "2. If all tests pass, proceed to training in main V5 notebook\n",
      "3. Monitor for CUDA errors during actual training\n",
      "4. Use validation=False in production for ~5% speedup after validation\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nIf you see this cell, all critical tests have completed.\")\n",
    "print(\"\\nExpected Results:\")\n",
    "print(\"  [PASS] Test 1.1: Valid GNN inputs\")\n",
    "print(\"  [PASS] Test 1.2: Caught wrong GNN dimension\")\n",
    "print(\"  [PASS] Test 1.3: Caught NaN in GNN input\")\n",
    "print(\"  [PASS] Test 1.4: Caught invalid edge indices\")\n",
    "print(\"  [PASS] Test 2.1: Valid Fusion inputs\")\n",
    "print(\"  [PASS] Test 2.2: Caught wrong Fusion dimension\")\n",
    "print(\"  [PASS] Test 2.3: Caught device mismatch (or SKIP if no CUDA)\")\n",
    "print(\"  [PASS] Test 3.1: Valid MetaLearner inputs\")\n",
    "print(\"  [PASS] Test 3.2: Caught wrong MetaLearner dimension\")\n",
    "print(\"  [PASS] Test 3.3: Caught NaN in MetaLearner input\")\n",
    "print(\"  [PASS] Integration Test: Full pipeline\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All tests completed successfully!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Review any [FAIL] results above\")\n",
    "print(\"2. If all tests pass, proceed to training in main V5 notebook\")\n",
    "print(\"3. Monitor for CUDA errors during actual training\")\n",
    "print(\"4. Use validation=False in production for ~5% speedup after validation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}