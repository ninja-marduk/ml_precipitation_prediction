{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/hybrid_models_enconders_layering_w3_ST-HybridWaveStack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b96f3ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7b96f3ea",
        "outputId": "5e46b114-3cbf-47f4-ff2e-8a6764a733ff"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Entrenamiento Multi‚Äêrama con GRU encoder‚Äìdecoder y Transformer para low,\n",
        "validaci√≥n y forecast parametrizables, meta‚Äêmodelo XGBoost (stacking all H=1‚Äì3),\n",
        "paralelizaci√≥n, trazabilidad y l√≠mites del departamento de Boyac√°.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "# Funci√≥n para verificar disponibilidad de datos para lags de precipitaci√≥n\n",
        "def verify_precipitation_lags(ds, required_lags=None, min_valid_ratio=0.90):\n",
        "    \"\"\"\n",
        "    Verifica si hay suficientes datos disponibles para procesar los lags de precipitaci√≥n.\n",
        "\n",
        "    Args:\n",
        "        ds: Dataset xarray que contiene las variables\n",
        "        required_lags: Lista de lags requeridos (si None, verifica todos los disponibles)\n",
        "        min_valid_ratio: Proporci√≥n m√≠nima de datos v√°lidos para considerar aceptable\n",
        "\n",
        "    Raises:\n",
        "        ValueError: Si no hay lags disponibles o si la proporci√≥n de datos v√°lidos es insuficiente\n",
        "    \"\"\"\n",
        "    # Lista de posibles lags de precipitaci√≥n\n",
        "    all_possible_lags = [\n",
        "        \"total_precipitation_lag1\", \"total_precipitation_lag2\",\n",
        "        \"total_precipitation_lag3\", \"total_precipitation_lag4\",\n",
        "        \"total_precipitation_lag12\", \"total_precipitation_lag24\",\n",
        "        \"total_precipitation_lag36\"\n",
        "    ]\n",
        "\n",
        "    # Determinar qu√© lags verificar\n",
        "    lags_to_check = required_lags if required_lags else [lag for lag in all_possible_lags if lag in ds.data_vars]\n",
        "\n",
        "    if not lags_to_check:\n",
        "        raise ValueError(\"No se encontraron variables de lag de precipitaci√≥n en el dataset\")\n",
        "\n",
        "    logger.info(f\"Verificando disponibilidad de datos para {len(lags_to_check)} lags de precipitaci√≥n\")\n",
        "\n",
        "    # Verificar cada lag\n",
        "    for lag in lags_to_check:\n",
        "        if lag not in ds.data_vars:\n",
        "            raise ValueError(f\"El lag requerido {lag} no est√° disponible en el dataset\")\n",
        "\n",
        "        # Calcular proporci√≥n de datos v√°lidos\n",
        "        lag_data = ds[lag].values\n",
        "        total_elements = lag_data.size\n",
        "        valid_elements = total_elements - np.isnan(lag_data).sum()\n",
        "        valid_ratio = valid_elements / total_elements\n",
        "\n",
        "        logger.info(f\"Lag {lag}: {valid_ratio:.2%} de datos v√°lidos ({valid_elements}/{total_elements})\")\n",
        "\n",
        "        # Verificar si hay suficientes datos v√°lidos\n",
        "        if valid_ratio < min_valid_ratio:\n",
        "            raise ValueError(\n",
        "                f\"Insuficientes datos v√°lidos para {lag}. \"\n",
        "                f\"Disponible: {valid_ratio:.2%}, Requerido: {min_valid_ratio:.2%}\"\n",
        "            )\n",
        "\n",
        "    logger.info(\"‚úÖ Verificaci√≥n de lags de precipitaci√≥n completada con √©xito\")\n",
        "    return True\n",
        "\n",
        "# 0) Detectar entorno (Local / Colab)\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    BASE_PATH = Path(\"/content/drive/MyDrive/ml_precipitation_prediction\")\n",
        "    !pip install -q xarray netCDF4 optuna seaborn cartopy xgboost ace_tools_open cartopy\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p/\".git\").exists():\n",
        "            BASE_PATH = p\n",
        "            break\n",
        "print(f\"‚ñ∂Ô∏è Base path: {BASE_PATH}\")\n",
        "\n",
        "# 1) Suprimir warnings irrelevantes\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "from cartopy.io import DownloadWarning\n",
        "warnings.filterwarnings(\"ignore\", category=DownloadWarning)\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# 2) Par√°metros configurables\n",
        "INPUT_WINDOW    = 48          # meses de entrada seg√∫n an√°lisis ACF/PACF\n",
        "OUTPUT_HORIZON  = 3           # meses de validaci√≥n y forecast\n",
        "REF_DATE        = \"2025-03\"   # fecha de referencia yyyy-mm\n",
        "MAX_EPOCHS      = 300\n",
        "PATIENCE_ES     = 30\n",
        "LR_FACTOR       = 0.5\n",
        "LR_PATIENCE     = 10\n",
        "DROPOUT         = 0.1\n",
        "\n",
        "# 3) Rutas y logger\n",
        "MODEL_DIR    = BASE_PATH/\"models\"/\"output\"/\"trained_models\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FEATURES_NC  = BASE_PATH/\"models\"/\"output\"/\"features_fusion_branches.nc\"\n",
        "FULL_NC      = BASE_PATH/\"data\"/\"output\"/\"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
        "SHP_USER     = Path(\"/mnt/data/MGN_Departamento.shp\")\n",
        "BOYACA_SHP   = SHP_USER if SHP_USER.exists() else BASE_PATH/\"data\"/\"input\"/\"shapes\"/\"MGN_Departamento.shp\"\n",
        "RESULTS_CSV  = MODEL_DIR/f\"metrics_w{OUTPUT_HORIZON}_ref{REF_DATE}.csv\"\n",
        "IMAGE_DIR    = MODEL_DIR/\"images\"\n",
        "IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 4) Imports principales\n",
        "import numpy            as np\n",
        "import pandas           as pd\n",
        "import xarray           as xr\n",
        "import geopandas        as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import psutil\n",
        "from joblib import cpu_count\n",
        "from scipy.stats import skew\n",
        "\n",
        "def print_progress(message, level=0, is_start=False, is_end=False):\n",
        "    \"\"\"\n",
        "    Print a formatted progress message.\n",
        "\n",
        "    Args:\n",
        "        message: The message to print\n",
        "        level: Indentation level (0, 1, 2)\n",
        "        is_start: Whether this is the start of a section\n",
        "        is_end: Whether this is the end of a section\n",
        "    \"\"\"\n",
        "    prefix = \"\"\n",
        "    if level == 0:\n",
        "        if is_start:\n",
        "            prefix = \"üîµ \"\n",
        "        elif is_end:\n",
        "            prefix = \"‚úÖ \"\n",
        "        else:\n",
        "            prefix = \"‚û°Ô∏è \"\n",
        "    elif level == 1:\n",
        "        prefix = \"   ‚ö™ \"\n",
        "    else:\n",
        "        prefix = \"     ‚Ä¢ \"\n",
        "\n",
        "    print(f\"{prefix}{message}\")\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, GRU, RepeatVector, TimeDistributed, Dense,\n",
        "    MultiHeadAttention, Add, LayerNormalization, Flatten\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "# 5) Recursos hardware\n",
        "CORES     = cpu_count()\n",
        "AVAIL_RAM = psutil.virtual_memory().available / (1024**3)\n",
        "gpus      = tf.config.list_physical_devices(\"GPU\")\n",
        "USE_GPU   = bool(gpus)\n",
        "if USE_GPU:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    logger.info(f\"üñ• GPU disponible: {gpus[0].name}\")\n",
        "else:\n",
        "    tf.config.threading.set_inter_op_parallelism_threads(CORES)\n",
        "    tf.config.threading.set_intra_op_parallelism_threads(CORES)\n",
        "    logger.info(f\"‚öô CPU cores: {CORES}, RAM libre: {AVAIL_RAM:.1f} GB\")\n",
        "\n",
        "# 6) Modelos y utilitarios\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    # Filtrar NaNs para robustez\n",
        "    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
        "    y_true, y_pred = y_true[mask], y_pred[mask]\n",
        "\n",
        "    # Verificar que hay suficientes datos v√°lidos\n",
        "    if len(y_true) < 10:\n",
        "        logger.warning(f\"Insuficientes datos v√°lidos para calcular m√©tricas: {len(y_true)} < 10\")\n",
        "        return np.nan, np.nan, np.nan, np.nan\n",
        "\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    # Evitar divisi√≥n por cero en MAPE\n",
        "    nonzero_mask = y_true != 0\n",
        "    if np.sum(nonzero_mask) > 10:\n",
        "        mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask])/(y_true[nonzero_mask] + 1e-5))) * 100\n",
        "    else:\n",
        "        mape = np.nan\n",
        "\n",
        "    # C√°lculo de R2 solo si hay suficiente varianza\n",
        "    var = np.var(y_true)\n",
        "    if var > 1e-10:\n",
        "        r2 = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "    else:\n",
        "        r2 = np.nan\n",
        "\n",
        "    return rmse, mae, mape, r2\n",
        "\n",
        "def check_nans(arr, name=\"array\"):\n",
        "    \"\"\"Verifica si hay NaNs en un array y retorna un resumen\"\"\"\n",
        "    # Verificar primero si el array es de tipo num√©rico\n",
        "    if not np.issubdtype(arr.dtype, np.number):\n",
        "        return {\n",
        "            \"name\": name,\n",
        "            \"nan_count\": 0,  # No aplicable para tipos no num√©ricos\n",
        "            \"total_elements\": arr.size,\n",
        "            \"nan_percentage\": 0.0,\n",
        "            \"has_nans\": False  # Consideramos que no hay NaNs en datos no num√©ricos\n",
        "        }\n",
        "    \n",
        "    # Procesamiento normal para arrays num√©ricos\n",
        "    nan_count = np.isnan(arr).sum()\n",
        "    total_count = arr.size\n",
        "    nan_percentage = (nan_count / total_count) * 100 if total_count > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"nan_count\": nan_count,\n",
        "        \"total_elements\": total_count,\n",
        "        \"nan_percentage\": nan_percentage,\n",
        "        \"has_nans\": nan_count > 0\n",
        "    }\n",
        "\n",
        "def replace_nans(arr, strategy=\"mean\", fill_value=None):\n",
        "    \"\"\"Reemplaza valores NaN en un array usando diferentes estrategias\"\"\"\n",
        "    if not np.isnan(arr).any():\n",
        "        return arr\n",
        "\n",
        "    # Crear copia para no modificar el original\n",
        "    arr_copy = arr.copy()\n",
        "\n",
        "    if strategy == \"mean\":\n",
        "        fill = np.nanmean(arr)\n",
        "    elif strategy == \"median\":\n",
        "        fill = np.nanmedian(arr)\n",
        "    elif strategy == \"zero\":\n",
        "        fill = 0.0\n",
        "    elif strategy == \"constant\":\n",
        "        fill = 0.0 if fill_value is None else fill_value\n",
        "    elif strategy == \"interpolate\":\n",
        "        # Para series temporales - simple interpolaci√≥n lineal\n",
        "        if arr_copy.ndim == 1:\n",
        "            mask = np.isnan(arr_copy)\n",
        "            if np.all(mask):  # Si todo es NaN\n",
        "                return np.zeros_like(arr_copy)\n",
        "            if not np.any(~mask):  # Si no hay valores v√°lidos\n",
        "                return np.zeros_like(arr_copy)\n",
        "            arr_copy[mask] = np.interp(\n",
        "                np.flatnonzero(mask),\n",
        "                np.flatnonzero(~mask),\n",
        "                arr_copy[~mask]\n",
        "            )\n",
        "            return arr_copy\n",
        "        else:\n",
        "            # Aplanar, interpolar y restaurar forma\n",
        "            original_shape = arr_copy.shape\n",
        "            arr_flat = arr_copy.reshape(-1)\n",
        "            mask = np.isnan(arr_flat)\n",
        "            if np.all(mask) or not np.any(~mask):  # Si todo es NaN o no hay valores v√°lidos\n",
        "                return np.zeros_like(arr_copy)\n",
        "            arr_flat[mask] = np.interp(\n",
        "                np.flatnonzero(mask),\n",
        "                np.flatnonzero(~mask),\n",
        "                arr_flat[~mask]\n",
        "            )\n",
        "            return arr_flat.reshape(original_shape)\n",
        "    else:\n",
        "        raise ValueError(f\"Estrategia '{strategy}' no reconocida\")\n",
        "\n",
        "    # Reemplazar NaNs\n",
        "    arr_copy[np.isnan(arr_copy)] = fill\n",
        "    return arr_copy\n",
        "\n",
        "class ScalerNaN:\n",
        "    \"\"\"StandardScaler que maneja NaNs de forma segura\"\"\"\n",
        "    def __init__(self):\n",
        "        self.mean_ = None\n",
        "        self.scale_ = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.mean_ = np.nanmean(X, axis=0)\n",
        "        # Usar nanvar con ddof=0 para consistencia con StandardScaler\n",
        "        self.var_ = np.nanvar(X, axis=0, ddof=0)\n",
        "        # Evitar divisi√≥n por cero\n",
        "        self.var_[self.var_ < 1e-10] = 1.0\n",
        "        self.scale_ = np.sqrt(self.var_)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_transformed = X.copy()\n",
        "        # Mantener la estructura dimensional para el broadcasting correcto\n",
        "        # Iterar sobre cada fila para mantener la compatibilidad dimensional\n",
        "        for i in range(X.shape[0]):\n",
        "            row_mask = ~np.isnan(X[i, :])\n",
        "            if np.any(row_mask):\n",
        "                X_transformed[i, row_mask] = (X[i, row_mask] - self.mean_[row_mask]) / self.scale_[row_mask]\n",
        "        return X_transformed\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        return self.fit(X).transform(X)\n",
        "\n",
        "    def inverse_transform(self, X):\n",
        "        X_inv = X.copy()\n",
        "        # Usar la misma l√≥gica de iteraci√≥n para inversa\n",
        "        for i in range(X.shape[0]):\n",
        "            row_mask = ~np.isnan(X[i, :])\n",
        "            if np.any(row_mask):\n",
        "                X_inv[i, row_mask] = X[i, row_mask] * self.scale_[row_mask] + self.mean_[row_mask]\n",
        "        return X_inv\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, Y, batch_size=32, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.X, self.Y = X.astype(np.float32), Y.astype(np.float32)\n",
        "        self.batch_size = batch_size\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / self.batch_size))\n",
        "    def __getitem__(self, idx):\n",
        "        sl = slice(idx*self.batch_size, (idx+1)*self.batch_size)\n",
        "        return self.X[sl], self.Y[sl]\n",
        "\n",
        "class TrainingProgressCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, model_name, total_epochs):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.total_epochs = total_epochs\n",
        "        self.current_epoch = 0\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.current_epoch = epoch\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Update progress after each epoch\n",
        "        loss = logs.get('loss', 0.0)\n",
        "        val_loss = logs.get('val_loss', 0.0)\n",
        "        progress = (epoch + 1) / self.total_epochs * 100\n",
        "\n",
        "        # Print progress information\n",
        "        print_progress(\n",
        "            f\"Entrenamiento {self.model_name}: √âpoca {epoch+1}/{self.total_epochs} \" +\n",
        "            f\"({progress:.1f}%) - loss: {loss:.4f} - val_loss: {val_loss:.4f}\",\n",
        "            level=2\n",
        "        )\n",
        "\n",
        "# GRU-ED y Transformer-ED builders\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def build_gru_ed(input_shape, horizon, n_cells, latent=128, dropout=DROPOUT):\n",
        "    inp = Input(shape=input_shape)\n",
        "    x = GRU(latent, dropout=dropout)(inp)\n",
        "    x = RepeatVector(horizon)(x)\n",
        "    x = GRU(latent, dropout=dropout, return_sequences=True)(x)\n",
        "    out = TimeDistributed(Dense(n_cells))(x)\n",
        "    m = Model(inp, out)\n",
        "    m.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    return m\n",
        "\n",
        "\n",
        "def build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                         head_size=64, num_heads=4, ff_dim=256, dropout=0.1):\n",
        "    inp = Input(shape=input_shape)\n",
        "    attn = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inp, inp)\n",
        "    x = Add()([inp, attn])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    ff = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = Dense(input_shape[-1])(ff)\n",
        "    x = Add()([x, ff])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(horizon * n_cells)(x)\n",
        "    out = K.reshape(x, (-1, horizon, n_cells))\n",
        "    m = Model(inp, out)\n",
        "    m.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    return m\n",
        "\n",
        "\n",
        "def build_gru_ed_low(input_shape, horizon, n_cells,\n",
        "                     latent=256, dropout=0.1, use_transformer=True):\n",
        "    if use_transformer:\n",
        "        try:\n",
        "            return build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                                        head_size=64, num_heads=4,\n",
        "                                        ff_dim=512, dropout=dropout)\n",
        "        except tf.errors.ResourceExhaustedError:\n",
        "            logger.warning(\"OOM Transformer ‚Üí usando GRU‚ÄêED para low-branch\")\n",
        "    return build_gru_ed(input_shape, horizon, n_cells,\n",
        "                        latent=latent, dropout=dropout)\n",
        "\n",
        "\n",
        "def build_gru_ed_medium_high(input_shape, horizon, n_cells, latent=128, dropout=0.1, use_transformer=True):\n",
        "    try:\n",
        "        if use_transformer:\n",
        "            return build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                                        head_size=64, num_heads=4,\n",
        "                                        ff_dim=512, dropout=dropout)\n",
        "        else:\n",
        "            return build_gru_ed(input_shape, horizon, n_cells,\n",
        "                                latent=latent, dropout=dropout)\n",
        "    except tf.errors.ResourceExhaustedError:\n",
        "        logger.warning(\"OOM Transformer ‚Üí usando GRU‚ÄêED para medium/high\")\n",
        "        return build_gru_ed(input_shape, horizon, n_cells,\n",
        "                            latent=latent, dropout=dropout)\n",
        "\n",
        "# 7) Carga datos y shapefile\n",
        "logger.info(\"üìÇ Cargando datasets‚Ä¶\")\n",
        "ds_full = xr.open_dataset(FULL_NC)\n",
        "ds_feat = xr.open_dataset(FEATURES_NC)\n",
        "boyaca_gdf = gpd.read_file(BOYACA_SHP)\n",
        "if boyaca_gdf.crs is None:\n",
        "    boyaca_gdf.set_crs(epsg=4326, inplace=True)\n",
        "else:\n",
        "    boyaca_gdf = boyaca_gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Extraer informaci√≥n de elevaci√≥n y calcular l√≠mites para los clusters\n",
        "elev_flat = ds_full['elevation'].values.ravel()\n",
        "elev_min = np.nanmin(elev_flat)\n",
        "elev_max = np.nanmax(elev_flat)\n",
        "\n",
        "# Definir los clusters de elevaci√≥n usando el m√≠nimo/m√°ximo del dataset y los valores de la imagen proporcionada\n",
        "ELEVATION_CLUSTERS = {\n",
        "    'nivel_1': (elev_min, 956.0),\n",
        "    'nivel_2': (959.0, 2263.0),\n",
        "    'nivel_3': (2264.0, elev_max)\n",
        "}\n",
        "\n",
        "print_progress(f\"Clusters de elevaci√≥n definidos:\", level=1)\n",
        "print_progress(f\"nivel_1: {ELEVATION_CLUSTERS['nivel_1'][0]:.1f} - {ELEVATION_CLUSTERS['nivel_1'][1]:.1f} m\", level=2)\n",
        "print_progress(f\"nivel_2: {ELEVATION_CLUSTERS['nivel_2'][0]:.1f} - {ELEVATION_CLUSTERS['nivel_2'][1]:.1f} m\", level=2)\n",
        "print_progress(f\"nivel_3: {ELEVATION_CLUSTERS['nivel_3'][0]:.1f} - {ELEVATION_CLUSTERS['nivel_3'][1]:.1f} m\", level=2)\n",
        "\n",
        "# Crear m√°scaras seg√∫n los clusters de elevaci√≥n definidos\n",
        "mask_nivel_1 = (elev_flat >= ELEVATION_CLUSTERS['nivel_1'][0]) & (elev_flat <= ELEVATION_CLUSTERS['nivel_1'][1])\n",
        "mask_nivel_2 = (elev_flat >= ELEVATION_CLUSTERS['nivel_2'][0]) & (elev_flat <= ELEVATION_CLUSTERS['nivel_2'][1])\n",
        "mask_nivel_3 = (elev_flat >= ELEVATION_CLUSTERS['nivel_3'][0]) & (elev_flat <= ELEVATION_CLUSTERS['nivel_3'][1])\n",
        "\n",
        "print_progress(f\"Puntos por cluster: nivel_1: {np.sum(mask_nivel_1)}, nivel_2: {np.sum(mask_nivel_2)}, nivel_3: {np.sum(mask_nivel_3)}\", level=2)\n",
        "\n",
        "times      = ds_full.time.values.astype(\"datetime64[M]\")\n",
        "user_ref   = np.datetime64(REF_DATE, \"M\")\n",
        "last_avail = times[-1]\n",
        "ref = last_avail if user_ref>last_avail else user_ref\n",
        "\n",
        "val_dates = [\n",
        "    str(ref),\n",
        "    str((ref - np.timedelta64(1,'M')).astype(\"datetime64[M]\")),\n",
        "    str((ref - np.timedelta64(2,'M')).astype(\"datetime64[M]\"))\n",
        "]\n",
        "fc_dates  = [str((ref + np.timedelta64(i+1,'M')).astype(\"datetime64[M]\")) for i in range(OUTPUT_HORIZON)]\n",
        "\n",
        "idx_ref = int(np.where(times == ref)[0][0])\n",
        "lat     = ds_full.latitude.values\n",
        "lon     = ds_full.longitude.values\n",
        "METHODS = [\"CEEMDAN\",\"TVFEMD\",\"FUSION\"]\n",
        "BRANCHES= [\"high\",\"medium\",\"low\"]\n",
        "\n",
        "# Verificar qu√© lags de precipitaci√≥n est√°n disponibles\n",
        "LAG_FEATURES = [\n",
        "    \"total_precipitation_lag1\",\n",
        "    \"total_precipitation_lag2\",\n",
        "    \"total_precipitation_lag3\",\n",
        "    \"total_precipitation_lag4\",\n",
        "    \"total_precipitation_lag12\",\n",
        "    \"total_precipitation_lag24\",\n",
        "    \"total_precipitation_lag36\"\n",
        "]\n",
        "available_lags = [lag for lag in LAG_FEATURES if lag in ds_full.data_vars]\n",
        "logger.info(f\"üìä Lags de precipitaci√≥n disponibles: {available_lags}\")\n",
        "\n",
        "all_metrics = []\n",
        "preds_store = {}\n",
        "true_store  = {}\n",
        "histories   = {}\n",
        "\n",
        "# callbacks\n",
        "es_cb = callbacks.EarlyStopping(\"val_loss\", patience=PATIENCE_ES, restore_best_weights=True)\n",
        "lr_cb = callbacks.ReduceLROnPlateau(\"val_loss\", factor=LR_FACTOR, patience=LR_PATIENCE, min_lr=1e-6)\n",
        "\n",
        "# 8) Bucle principal\n",
        "print_progress(f\"Iniciando procesamiento de {len(METHODS)} m√©todos √ó {len(BRANCHES)} branches con manejo robusto de NaNs\", is_start=True)\n",
        "total_combinations = len(METHODS) * len(BRANCHES)\n",
        "processed = 0\n",
        "\n",
        "for method in METHODS:\n",
        "    for branch in BRANCHES:\n",
        "        processed += 1\n",
        "        name = f\"{method}_{branch}\"\n",
        "        if name not in ds_feat.data_vars:\n",
        "            print_progress(f\"({processed}/{total_combinations}) ‚ö†Ô∏è {name} no existe, salteando...\")\n",
        "            continue\n",
        "\n",
        "        print_progress(f\"({processed}/{total_combinations}) Procesando {name}\", is_start=True)\n",
        "        try:\n",
        "            # extraer y aplanar\n",
        "            Xarr = ds_feat[name].values            # (T, ny, nx)\n",
        "            Yarr = ds_full[\"total_precipitation\"].values  # (T, ny, nx)\n",
        "\n",
        "            # Verificar NaNs iniciales\n",
        "            x_summary = check_nans(Xarr, f\"Entrada {name}\")\n",
        "            y_summary = check_nans(Yarr, f\"Objetivo {name}\")\n",
        "\n",
        "            if x_summary[\"has_nans\"]:\n",
        "                print_progress(f\"‚ö†Ô∏è Detectados {x_summary['nan_count']} NaNs en entrada {name} ({x_summary['nan_percentage']:.2f}%)\", level=1)\n",
        "                Xarr = replace_nans(Xarr, strategy=\"interpolate\")\n",
        "                print_progress(f\"NaNs reemplazados usando interpolaci√≥n\", level=2)\n",
        "\n",
        "            if y_summary[\"has_nans\"]:\n",
        "                print_progress(f\"‚ö†Ô∏è Detectados {y_summary['nan_count']} NaNs en objetivo {name} ({y_summary['nan_percentage']:.2f}%)\", level=1)\n",
        "                Yarr = replace_nans(Yarr, strategy=\"interpolate\")\n",
        "                print_progress(f\"NaNs reemplazados usando interpolaci√≥n\", level=2)\n",
        "\n",
        "            T, ny, nx = Xarr.shape\n",
        "            n_cells   = ny * nx\n",
        "\n",
        "            Xfull = Xarr.reshape(T, n_cells)\n",
        "            yfull = Yarr.reshape(T, n_cells)\n",
        "\n",
        "            # ventanas\n",
        "            Nw = T - INPUT_WINDOW - OUTPUT_HORIZON + 1\n",
        "            if Nw <= 0:\n",
        "                print_progress(f\"‚ùå Ventanas insuficientes para {name}, continuando con el siguiente\", level=1)\n",
        "                continue\n",
        "\n",
        "            print_progress(f\"Generando {Nw} ventanas para {name}\", level=1)\n",
        "\n",
        "            Xs = np.stack([Xfull[i : i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "            ys = np.stack([yfull[i+INPUT_WINDOW : i+INPUT_WINDOW+OUTPUT_HORIZON]\n",
        "                           for i in range(Nw)], axis=0)\n",
        "\n",
        "            # ========================================================================\n",
        "            # IMPLEMENTACI√ìN ROBUSTA DE LAGS DE PRECIPITACI√ìN\n",
        "            # ========================================================================\n",
        "            print_progress(f\"Procesando lags de precipitaci√≥n para {name} de forma robusta\", level=1)\n",
        "\n",
        "            # Preparar lista para caracter√≠sticas adicionales\n",
        "            features_to_add = []\n",
        "\n",
        "            # sin/cos para low\n",
        "            if branch == \"low\":\n",
        "                months = pd.to_datetime(ds_full.time.values).month.values\n",
        "                s = np.sin(2 * np.pi * months/12)\n",
        "                c = np.cos(2 * np.pi * months/12)\n",
        "                Ss = np.stack([s[i : i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                Cs = np.stack([c[i : i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                Ss = np.repeat(Ss[:,:,None], n_cells, axis=2)\n",
        "                Cs = np.repeat(Cs[:,:,None], n_cells, axis=2)\n",
        "                features_to_add.extend([Ss, Cs])\n",
        "                logger.info(f\"‚úì Agregadas caracter√≠sticas estacionales sin/cos para branch {branch}\")\n",
        "\n",
        "            # Agregar lags de precipitaci√≥n como features adicionales (manejo robusto)\n",
        "            if available_lags:\n",
        "                logger.info(f\"üîÑ Agregando {len(available_lags)} lags de precipitaci√≥n al branch {branch}\")\n",
        "                for lag_var in available_lags:\n",
        "                    # Obtener datos y verificar NaNs\n",
        "                    lag_data = ds_full[lag_var].values\n",
        "                    lag_summary = check_nans(lag_data, f\"Lag {lag_var}\")\n",
        "\n",
        "                    # Manejar NaNs seg√∫n el porcentaje\n",
        "                    if lag_summary[\"has_nans\"]:\n",
        "                        print_progress(f\"‚ö†Ô∏è {lag_var}: {lag_summary['nan_count']} NaNs ({lag_summary['nan_percentage']:.2f}%)\", level=2)\n",
        "                        if lag_summary[\"nan_percentage\"] < 5:\n",
        "                            lag_data = replace_nans(lag_data, strategy=\"interpolate\")\n",
        "                            print_progress(f\"NaNs interpolados en {lag_var}\", level=2)\n",
        "                        elif lag_summary[\"nan_percentage\"] < 20:\n",
        "                            lag_data = replace_nans(lag_data, strategy=\"mean\")\n",
        "                            print_progress(f\"NaNs reemplazados con media en {lag_var}\", level=2)\n",
        "                        else:\n",
        "                            lag_data = replace_nans(lag_data, strategy=\"zero\")\n",
        "                            print_progress(f\"‚ö†Ô∏è Demasiados NaNs en {lag_var}, reemplazando con ceros\", level=2)\n",
        "\n",
        "                    lag_full = lag_data.reshape(T, n_cells)\n",
        "                    lag_windows = np.stack([lag_full[i : i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                    features_to_add.append(lag_windows)\n",
        "                logger.info(f\"‚úì Lags procesados robustamente: {available_lags}\")\n",
        "\n",
        "            # Concatenar todas las caracter√≠sticas\n",
        "            if features_to_add:\n",
        "                Xs = np.concatenate([Xs] + features_to_add, axis=2)\n",
        "                n_feats = Xs.shape[2]\n",
        "                print_progress(f\"Estructura de features: {Xs.shape} ({n_feats} features totales)\", level=1)\n",
        "            else:\n",
        "                n_feats = n_cells\n",
        "                print_progress(f\"Sin features adicionales: {Xs.shape}\", level=1)\n",
        "\n",
        "            # A√±adir variables adicionales disponibles en ds_full\n",
        "            if branch == \"low\":  # Para el branch \"low\" a√±adimos todas las variables\n",
        "                add_vars_features = []\n",
        "                \n",
        "                # A√±adir year\n",
        "                if 'year' in ds_full.data_vars:\n",
        "                    print_progress(f\"A√±adiendo 'year' para {name}\", level=2)\n",
        "                    year_data = ds_full['year'].values\n",
        "                    # Convertir a float si es necesario\n",
        "                    if not np.issubdtype(year_data.dtype, np.number):\n",
        "                        year_data = year_data.astype(float)\n",
        "                    year_full = year_data.reshape(T, n_cells) if year_data.ndim > 1 else np.repeat(year_data[:, np.newaxis], n_cells, axis=1)\n",
        "                    year_windows = np.stack([year_full[i : i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                    add_vars_features.append(year_windows)\n",
        "                \n",
        "                # A√±adir doy_sin, doy_cos (d√≠a del a√±o transformado a coordenadas c√≠clicas)\n",
        "                for doy_var in ['doy_sin', 'doy_cos']:\n",
        "                    if doy_var in ds_full.data_vars:\n",
        "                        print_progress(f\"A√±adiendo '{doy_var}' para {name}\", level=2)\n",
        "                        doy_data = ds_full[doy_var].values\n",
        "                        if not np.issubdtype(doy_data.dtype, np.number):\n",
        "                            doy_data = doy_data.astype(float)\n",
        "                        doy_full = doy_data.reshape(T, n_cells) if doy_data.ndim > 1 else np.repeat(doy_data[:, np.newaxis], n_cells, axis=1)\n",
        "                        doy_windows = np.stack([doy_full[i : i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                        add_vars_features.append(doy_windows)\n",
        "                \n",
        "                # A√±adir variables topogr√°ficas (elevation, slope, aspect, cluster_elevation)\n",
        "                for topo_var in ['elevation', 'slope', 'aspect', 'cluster_elevation']:\n",
        "                    if topo_var in ds_full.data_vars:\n",
        "                        print_progress(f\"A√±adiendo '{topo_var}' para {name}\", level=2)\n",
        "                        try:\n",
        "                            topo_data = ds_full[topo_var].values\n",
        "                            \n",
        "                            # Convertir a tipo num√©rico si no lo es (excepto cluster_elevation que es categ√≥rico)\n",
        "                            if topo_var != 'cluster_elevation' and not np.issubdtype(topo_data.dtype, np.number):\n",
        "                                topo_data = topo_data.astype(float)\n",
        "                            \n",
        "                            # Manejar posibles NaNs en variables topogr√°ficas num√©ricas\n",
        "                            if topo_var != 'cluster_elevation':\n",
        "                                topo_summary = check_nans(topo_data, topo_var)\n",
        "                                if topo_summary[\"has_nans\"]:\n",
        "                                    print_progress(f\"‚ö†Ô∏è Reemplazando {topo_summary['nan_count']} NaNs en {topo_var}\", level=2)\n",
        "                                    topo_data = replace_nans(topo_data, strategy=\"mean\")\n",
        "                            \n",
        "                            # Si es cluster_elevation (categ√≥rico), convertirlo a one-hot encoding\n",
        "                            if topo_var == 'cluster_elevation':\n",
        "                                print_progress(f\"Procesando {topo_var} como variable categ√≥rica\", level=2)\n",
        "                                continue  # Saltar por ahora ya que necesitar√≠amos implementar one-hot encoding\n",
        "                            \n",
        "                            # Convertir a formato compatible\n",
        "                            topo_full = topo_data.reshape(n_cells) if topo_data.ndim == 2 else topo_data.ravel()\n",
        "                            topo_full = np.tile(topo_full, (T, 1))\n",
        "                            topo_windows = np.stack([topo_full[i : i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                            add_vars_features.append(topo_windows)\n",
        "                        except Exception as e:\n",
        "                            print_progress(f\"‚ö†Ô∏è Error al procesar {topo_var}: {str(e)}\", level=2)\n",
        "                            continue\n",
        "            \n",
        "            # Verificar NaNs despu√©s del procesamiento\n",
        "            xs_processed_summary = check_nans(Xs, \"Features procesados\")\n",
        "            if xs_processed_summary[\"has_nans\"]:\n",
        "                print_progress(f\"‚ö†Ô∏è A√∫n hay {xs_processed_summary['nan_count']} NaNs despu√©s del procesamiento, reemplazando\", level=1)\n",
        "                Xs = replace_nans(Xs, strategy=\"mean\")\n",
        "\n",
        "            # escalado robusto\n",
        "            print_progress(\"Aplicando escalado robusto de datos\", level=1)\n",
        "            # Usar ScalerNaN para manejar valores NaN correctamente\n",
        "            scX = ScalerNaN().fit(Xs.reshape(-1, n_feats))\n",
        "            scY = ScalerNaN().fit(ys.reshape(-1, n_cells))\n",
        "\n",
        "            Xs_s = scX.transform(Xs.reshape(-1, n_feats)).reshape(Xs.shape)\n",
        "            ys_s = scY.transform(ys.reshape(-1, n_cells)).reshape(ys.shape)\n",
        "\n",
        "            # Verificar NaNs despu√©s del escalado\n",
        "            xs_scaled_summary = check_nans(Xs_s, \"Features escalados\")\n",
        "            ys_scaled_summary = check_nans(ys_s, \"Objetivos escalados\")\n",
        "\n",
        "            if xs_scaled_summary[\"has_nans\"] or ys_scaled_summary[\"has_nans\"]:\n",
        "                print_progress(\"‚ö†Ô∏è Hay NaNs despu√©s del escalado, reemplazando con ceros\", level=1)\n",
        "                # Reemplazar NaNs restantes con ceros\n",
        "                Xs_s = np.nan_to_num(Xs_s, nan=0.0)\n",
        "                ys_s = np.nan_to_num(ys_s, nan=0.0)\n",
        "\n",
        "            # partici√≥n centrada en REF_DATE\n",
        "            k_ref = np.clip(idx_ref - INPUT_WINDOW + 1, 0, Nw-1)\n",
        "            i0    = np.clip(k_ref - (OUTPUT_HORIZON-1), 0, Nw-OUTPUT_HORIZON)\n",
        "\n",
        "            X_tr, y_tr = Xs_s[:i0], ys_s[:i0]\n",
        "            X_va, y_va = Xs_s[i0 : i0+OUTPUT_HORIZON], ys_s[i0 : i0+OUTPUT_HORIZON]\n",
        "\n",
        "            # cargar/entrenar\n",
        "            model_path = MODEL_DIR/f\"{name}_w{OUTPUT_HORIZON}_ref{ref}.keras\"\n",
        "            if model_path.exists():\n",
        "                print_progress(f\"Cargando modelo existente: {model_path.name}\", level=1)\n",
        "                model = tf.keras.models.load_model(str(model_path), compile=False)\n",
        "                model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "            else:\n",
        "                print_progress(f\"Creando nuevo modelo para {name}\", level=1)\n",
        "                if branch == \"low\":\n",
        "                    model = build_gru_ed_low((INPUT_WINDOW, n_feats), OUTPUT_HORIZON, n_cells)\n",
        "                    print_progress(f\"Modelo low-branch creado: {model.__class__.__name__}\", level=2)\n",
        "                else:\n",
        "                    model = build_gru_ed_medium_high((INPUT_WINDOW, n_feats), OUTPUT_HORIZON, n_cells)\n",
        "                    print_progress(f\"Modelo {branch}-branch creado: {model.__class__.__name__}\", level=2)\n",
        "\n",
        "                # Crear callback de progreso personalizado\n",
        "                progress_cb = TrainingProgressCallback(name, MAX_EPOCHS)\n",
        "\n",
        "                # Mostrar resumen de datos de entrenamiento\n",
        "                print_progress(f\"Entrenando con {len(X_tr)} muestras, validando con {len(X_va)} muestras\", level=1)\n",
        "                print_progress(f\"X_train: {X_tr.shape}, y_train: {y_tr.shape}\", level=2)\n",
        "                print_progress(f\"X_val: {X_va.shape}, y_val: {y_va.shape}\", level=2)\n",
        "\n",
        "                # Entrenamiento con barra de progreso\n",
        "                print_progress(f\"Iniciando entrenamiento para {name}\", level=1)\n",
        "                hist = model.fit(\n",
        "                    DataGenerator(X_tr, y_tr),\n",
        "                    validation_data=DataGenerator(X_va, y_va),\n",
        "                    epochs=MAX_EPOCHS,\n",
        "                    callbacks=[es_cb, lr_cb, progress_cb],\n",
        "                    verbose=0  # Desactivamos verbose integrado ya que tenemos progress_cb\n",
        "                )\n",
        "\n",
        "                print_progress(f\"Guardando modelo en {model_path.name}\", level=1)\n",
        "                model.save(str(model_path))\n",
        "                histories[name] = hist.history\n",
        "\n",
        "                # Mostrar informaci√≥n del entrenamiento\n",
        "                print_progress(f\"Entrenamiento completado en {len(hist.history['loss'])} √©pocas\", level=1)\n",
        "                print_progress(f\"Loss inicial: {hist.history['loss'][0]:.4f}, Loss final: {hist.history['loss'][-1]:.4f}\", level=2)\n",
        "                print_progress(f\"Val-loss inicial: {hist.history['val_loss'][0]:.4f}, Val-loss final: {hist.history['val_loss'][-1]:.4f}\", level=2)\n",
        "\n",
        "            # validaci√≥n H=1..H\n",
        "            print_progress(f\"Generando predicciones de validaci√≥n para {name}\", level=1)\n",
        "            preds = model.predict(X_va, verbose=0).reshape(OUTPUT_HORIZON, OUTPUT_HORIZON, n_cells)\n",
        "            for h in range(OUTPUT_HORIZON):\n",
        "                date_val = val_dates[h]\n",
        "                pm_flat  = preds[h,0]\n",
        "                tm_flat  = y_va[h,0]\n",
        "                pm = scY.inverse_transform(pm_flat.reshape(1,-1))[0].reshape(ny,nx)\n",
        "                tm = scY.inverse_transform(tm_flat.reshape(1,-1))[0].reshape(ny,nx)\n",
        "                rmse, mae, mape, r2 = evaluate_metrics(tm.ravel(), pm.ravel())\n",
        "                all_metrics.append({\n",
        "                    \"model\": name, \"branch\": branch, \"horizon\": h+1,\n",
        "                    \"type\":\"validation\", \"date\": date_val,\n",
        "                    \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape, \"R2\": r2\n",
        "                })\n",
        "                preds_store[(name,date_val)] = pm\n",
        "                true_store[(name,date_val)]  = tm\n",
        "\n",
        "            # forecast\n",
        "            print_progress(f\"Generando predicciones de forecast para {name}\", level=1)\n",
        "            X_fc = Xs_s[k_ref : k_ref+1]\n",
        "            fc_s = model.predict(X_fc, verbose=0)[0]\n",
        "            FC   = scY.inverse_transform(fc_s)\n",
        "            for h in range(OUTPUT_HORIZON):\n",
        "                date_fc = fc_dates[h]\n",
        "                all_metrics.append({\n",
        "                    \"model\": name, \"branch\": branch, \"horizon\": h+1,\n",
        "                    \"type\":\"forecast\", \"date\": date_fc,\n",
        "                    \"RMSE\": np.nan, \"MAE\": np.nan, \"MAPE\": np.nan, \"R2\": np.nan\n",
        "                })\n",
        "                preds_store[(name,date_fc)] = FC[h].reshape(ny,nx)\n",
        "\n",
        "        except Exception as e:\n",
        "            print_progress(f\"‚ÄºÔ∏è Error en {name}: {str(e)}\", level=1)\n",
        "            logger.exception(f\"Error en {name}, continuo‚Ä¶\")\n",
        "            continue\n",
        "\n",
        "print_progress(\"Procesamiento de todos los modelos completado\", is_end=True)\n",
        "\n",
        "# 9) Guardar m√©tricas y mostrar tabla\n",
        "print_progress(\"Guardando m√©tricas y generando tablas\", is_start=True)\n",
        "dfm = pd.DataFrame(all_metrics)\n",
        "dfm.to_csv(RESULTS_CSV, index=False)\n",
        "import ace_tools_open as tools\n",
        "import cartopy.crs as ccrs\n",
        "tools.display_dataframe_to_user(name=f\"Metrics_w{OUTPUT_HORIZON}_ref{ref}\", dataframe=dfm)\n",
        "print_progress(f\"M√©tricas guardadas en {RESULTS_CSV}\", is_end=True)\n",
        "\n",
        "# 10) Curvas de entrenamiento\n",
        "print_progress(\"Generando curvas de entrenamiento\", is_start=True)\n",
        "for name, hist in histories.items():\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(hist[\"loss\"],  label=\"train\")\n",
        "    plt.plot(hist[\"val_loss\"],label=\"val\")\n",
        "    plt.title(f\"Loss curve: {name}\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE\")\n",
        "    plt.legend(); plt.tight_layout(); plt.show()\n",
        "print_progress(\"Visualizaciones de curvas de entrenamiento completadas\", is_end=True)\n",
        "\n",
        "# 10bis) True vs Predicted por rama y horizonte\n",
        "for branch in BRANCHES:\n",
        "    for h in range(1, OUTPUT_HORIZON+1):\n",
        "        plt.figure(figsize=(5,5))\n",
        "        valid_data_found = False\n",
        "        for method in METHODS:\n",
        "            key = f\"{method}_{branch}\"\n",
        "            date_val = val_dates[h-1]\n",
        "            if (key, date_val) in preds_store and (key, date_val) in true_store:\n",
        "                y_true = true_store[(key, date_val)].ravel()\n",
        "                y_pred = preds_store[(key, date_val)].ravel()\n",
        "                plt.scatter(y_true, y_pred, alpha=0.3, s=2, label=method)\n",
        "                valid_data_found = True\n",
        "        \n",
        "        if valid_data_found:\n",
        "            lims = [0, max(plt.xlim()[1], plt.ylim()[1])]\n",
        "            plt.plot(lims, lims, 'k--')\n",
        "            plt.xlabel(\"True\"); plt.ylabel(\"Predicted\")\n",
        "            plt.title(f\"True vs Pred ‚Äî {branch}, H={h}\")\n",
        "            plt.legend()  # Solo a√±adir leyenda si hay datos v√°lidos\n",
        "        else:\n",
        "            plt.text(0.5, 0.5, \"No hay datos suficientes\", ha='center', va='center')\n",
        "            plt.xlabel(\"True\"); plt.ylabel(\"Predicted\")\n",
        "            plt.title(f\"True vs Pred ‚Äî {branch}, H={h} (Sin datos)\")\n",
        "        \n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "# 11) Mapas 3√ó3 validaci√≥n H=1\n",
        "xmin, ymin, xmax, ymax = boyaca_gdf.total_bounds\n",
        "for date_val in val_dates:\n",
        "    arrs = [preds_store[(f\"{m}_{b}\",date_val)].ravel()\n",
        "            for m in METHODS for b in BRANCHES\n",
        "            if (f\"{m}_{b}\",date_val) in preds_store]\n",
        "    if not arrs:\n",
        "        logger.warning(f\"No hay predicciones para {date_val}, salto plot.\")\n",
        "        continue\n",
        "    vmin, vmax = np.min(arrs), np.max(arrs)\n",
        "    fig, axs = plt.subplots(3,3, figsize=(12,12), subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
        "    fig.suptitle(f\"Validaci√≥n H=1 ‚Äî {date_val}\", fontsize=16)\n",
        "    for i, b in enumerate(BRANCHES):\n",
        "        for j, m in enumerate(METHODS):\n",
        "            ax = axs[i,j]\n",
        "            ax.set_extent([xmin, xmax, ymin, ymax], ccrs.PlateCarree())\n",
        "            ax.add_geometries(boyaca_gdf.geometry, ccrs.PlateCarree(),\n",
        "                              edgecolor=\"black\", facecolor=\"none\", linewidth=1)\n",
        "            key = (f\"{m}_{b}\", date_val)\n",
        "            if key in preds_store:\n",
        "                pcm = ax.pcolormesh(lon, lat, preds_store[key],\n",
        "                                    vmin=vmin, vmax=vmax,\n",
        "                                    transform=ccrs.PlateCarree(), cmap=\"Blues\")\n",
        "            ax.set_title(f\"{m}_{b}\")\n",
        "    fig.colorbar(pcm, ax=axs, orientation=\"horizontal\",\n",
        "                 fraction=0.05, pad=0.04, label=\"Precipitaci√≥n (mm)\")\n",
        "    fig.savefig(IMAGE_DIR/f\"val_H1_{date_val}.png\", dpi=150); plt.show()\n",
        "\n",
        "    arrs_mape = [\n",
        "        np.clip(np.abs((true_store[k] - preds_store[k])/(true_store[k]+1e-5))*100,0,200).ravel()\n",
        "        for k in preds_store if k[1]==date_val and k in true_store\n",
        "    ]\n",
        "    if not arrs_mape: continue\n",
        "    vmin2, vmax2 = 0, np.max(arrs_mape)\n",
        "    fig, axs = plt.subplots(3,3, figsize=(12,12), subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
        "    fig.suptitle(f\"MAPE H=1 ‚Äî {date_val}\", fontsize=16)\n",
        "    for i, b in enumerate(BRANCHES):\n",
        "        for j, m in enumerate(METHODS):\n",
        "            ax = axs[i,j]\n",
        "            ax.set_extent([xmin, xmax, ymin, ymax], ccrs.PlateCarree())\n",
        "            ax.add_geometries(boyaca_gdf.geometry, ccrs.PlateCarree(),\n",
        "                              edgecolor=\"black\", facecolor=\"none\", linewidth=1)\n",
        "            key = (f\"{m}_{b}\", date_val)\n",
        "            if key in preds_store and key in true_store:\n",
        "                mmap = np.clip(np.abs((true_store[key] - preds_store[key])/(true_store[key]+1e-5))*100,0,200)\n",
        "                pcm2 = ax.pcolormesh(lon, lat, mmap,\n",
        "                                     vmin=vmin2, vmax=vmax2,\n",
        "                                     transform=ccrs.PlateCarree(), cmap=\"Reds\")\n",
        "            ax.set_title(f\"{m}_{b}\")\n",
        "    fig.colorbar(pcm2, ax=axs, orientation=\"horizontal\",\n",
        "                 fraction=0.05, pad=0.04, label=\"MAPE (%)\")\n",
        "    fig.savefig(IMAGE_DIR/f\"mape_H1_{date_val}.png\", dpi=150); plt.show()\n",
        "\n",
        "# 13) META‚ÄêMODELOS XGB stacking H=1-3 (retraining con 9 features)\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 13.0) Preparar X_meta completo para cada horizonte y retrain modelos\n",
        "print_progress(\"Iniciando meta-modelos XGB de stacking H=1-3\", is_start=True)\n",
        "for h in range(1, OUTPUT_HORIZON+1):\n",
        "    date = val_dates[h-1]\n",
        "    print_progress(f\"Entrenando meta-modelo XGB para horizonte {h}, fecha {date}\", level=1)\n",
        "\n",
        "    # Extraer features (3 preds + elev stats + slope + aspect)\n",
        "    print_progress(f\"Preparando datos para H={h}\", level=2)\n",
        "    \n",
        "    # Verificar que todas las predicciones existen antes de proceder\n",
        "    missing_keys = []\n",
        "    for b in ['low', 'medium', 'high']:\n",
        "        key = (f\"FUSION_{b}\", date)\n",
        "        if key not in preds_store:\n",
        "            missing_keys.append(key)\n",
        "    \n",
        "    if missing_keys:\n",
        "        print_progress(f\"‚ö†Ô∏è No se encontraron predicciones para: {missing_keys}\", level=1)\n",
        "        print_progress(f\"Omitiendo entrenamiento para horizonte {h}\", level=1)\n",
        "        continue\n",
        "    \n",
        "    # Ahora es seguro obtener las predicciones\n",
        "    preds = [preds_store[(f\"FUSION_{b}\", date)].ravel() for b in ['low','medium','high']]\n",
        "    elev_flat   = ds_full['elevation'].values.ravel()\n",
        "    slope_flat  = ds_full['slope'].values.ravel()\n",
        "    aspect_flat = ds_full['aspect'].values.ravel()\n",
        "    # Estad√≠sticos de elevaci√≥n\n",
        "    mean_e = elev_flat.mean(); std_e = elev_flat.std(); skew_e = skew(elev_flat)\n",
        "    elev_stats = np.vstack([\n",
        "        np.full_like(elev_flat, mean_e),\n",
        "        np.full_like(elev_flat, std_e),\n",
        "        np.full_like(elev_flat, skew_e)\n",
        "    ]).T\n",
        "    # Construir X_meta y y_true\n",
        "    X_meta = np.column_stack(preds + [elev_stats, slope_flat, aspect_flat])\n",
        "    y_true = true_store[(\"FUSION_low\", date)].ravel()\n",
        "\n",
        "    # Mostrar dimensiones\n",
        "    print_progress(f\"X_meta shape: {X_meta.shape}, y_true shape: {y_true.shape}\", level=2)\n",
        "\n",
        "    # Train/test split\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(X_meta, y_true, test_size=0.2, random_state=42)\n",
        "    print_progress(f\"Split: train={X_tr.shape[0]} muestras, test={X_te.shape[0]} muestras\", level=2)\n",
        "\n",
        "    # Ajustar modelo con todas las features\n",
        "    print_progress(f\"Entrenando XGBoost para H={h}\", level=2, is_start=True)\n",
        "    xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5)\n",
        "    xgb.fit(X_tr, y_tr)\n",
        "\n",
        "    # Evaluar en conjunto de prueba\n",
        "    y_pred = xgb.predict(X_te)\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_te, y_pred))\n",
        "    print_progress(f\"XGB H={h} entrenado. Test RMSE: {test_rmse:.4f}\", level=2, is_end=True)\n",
        "\n",
        "    # Guardar modelo retrained\n",
        "    model_path = str(MODEL_DIR/f\"xgb_all_H{h}_{ref}_9features.json\")\n",
        "    print_progress(f\"Guardando modelo en {model_path}\", level=2)\n",
        "    xgb.save_model(model_path)\n",
        "\n",
        "print_progress(\"Meta-modelos XGB entrenados y guardados\", is_end=True)\n",
        "\n",
        "# 13.2) Scatter, mapas y m√©trica final\n",
        "print_progress(\"Generando visualizaciones y m√©tricas finales\", is_start=True)\n",
        "fig_sc, axs_sc = plt.subplots(1, OUTPUT_HORIZON, figsize=(6*OUTPUT_HORIZON,5))\n",
        "for idx_h, h in enumerate(range(1, OUTPUT_HORIZON+1)):\n",
        "    date = val_dates[h-1]\n",
        "    mdl_path = MODEL_DIR/f\"xgb_all_H{h}_{ref}_9features.json\"\n",
        "    if mdl_path.exists():\n",
        "        xgb = XGBRegressor(); xgb.load_model(str(mdl_path))\n",
        "        print_progress(f\"Cargado modelo XGB para H={h}\", level=1)\n",
        "\n",
        "        # Recolectar X_meta completo\n",
        "        preds = [preds_store[(f\"FUSION_{b}\", date)].ravel() for b in ['low','medium','high']]\n",
        "        elev_flat   = ds_full['elevation'].values.ravel()\n",
        "        slope_flat  = ds_full['slope'].values.ravel()\n",
        "        aspect_flat = ds_full['aspect'].values.ravel()\n",
        "        mean_e = elev_flat.mean(); std_e = elev_flat.std(); skew_e = skew(elev_flat)\n",
        "\n",
        "def xgb_predict_full(model, X):\n",
        "    \"\"\"\n",
        "    Make predictions with an XGBoost model, handling memory constraints and NaNs.\n",
        "\n",
        "    Args:\n",
        "        model: The XGBoost model\n",
        "        X: Input features\n",
        "\n",
        "    Returns:\n",
        "        Predictions for all samples\n",
        "    \"\"\"\n",
        "    # Handle NaNs in input\n",
        "    has_nans = np.isnan(X).any()\n",
        "    if has_nans:\n",
        "        print_progress(f\"‚ö†Ô∏è Detectados NaNs en entrada de XGB, reemplazando con valores medios\", level=2)\n",
        "        # Replace NaNs with column means\n",
        "        X = np.copy(X)  # Create a copy to avoid modifying the original\n",
        "        for col in range(X.shape[1]):\n",
        "            col_data = X[:, col]\n",
        "            if np.isnan(col_data).any():\n",
        "                col_mean = np.nanmean(col_data)\n",
        "                X[np.isnan(X[:, col]), col] = col_mean\n",
        "\n",
        "    # Check if we need to batch the predictions due to memory constraints\n",
        "    batch_size = 100000  # Adjust based on available memory\n",
        "    if X.shape[0] > batch_size:\n",
        "        # Batch predictions to avoid memory issues\n",
        "        n_batches = int(np.ceil(X.shape[0] / batch_size))\n",
        "        preds = []\n",
        "        for i in range(n_batches):\n",
        "            start_idx = i * batch_size\n",
        "            end_idx = min((i + 1) * batch_size, X.shape[0])\n",
        "            try:\n",
        "                batch_preds = model.predict(X[start_idx:end_idx])\n",
        "                preds.append(batch_preds)\n",
        "            except Exception as e:\n",
        "                print_progress(f\"Error en predicci√≥n batch {i}: {str(e)}\", level=1)\n",
        "                # Intentar con DMatrix como fallback\n",
        "                try:\n",
        "                    import xgboost as xgb\n",
        "                    dmatrix = xgb.DMatrix(X[start_idx:end_idx])\n",
        "                    batch_preds = model.predict(dmatrix)\n",
        "                    preds.append(batch_preds)\n",
        "                except Exception as e2:\n",
        "                    print_progress(f\"Error cr√≠tico en predicci√≥n: {str(e2)}\", level=1)\n",
        "                    # Retornar arrays de cero en caso de error irrecuperable\n",
        "                    preds.append(np.zeros(end_idx - start_idx))\n",
        "        return np.concatenate(preds)\n",
        "    else:\n",
        "        # Make predictions in one go\n",
        "        try:\n",
        "            return model.predict(X)\n",
        "        except Exception as e:\n",
        "            print_progress(f\"Error en predicci√≥n: {str(e)}\", level=1)\n",
        "            # Intentar con DMatrix como fallback\n",
        "            try:\n",
        "                import xgboost as xgb\n",
        "                dmatrix = xgb.DMatrix(X)\n",
        "                return model.predict(dmatrix)\n",
        "            except Exception as e2:\n",
        "                print_progress(f\"Error cr√≠tico en predicci√≥n: {str(e2)}\", level=1)\n",
        "                return np.zeros(X.shape[0])\n",
        "\n",
        "# Meta metrics list\n",
        "meta_metrics_all = []\n",
        "\n",
        "# 13.2.1) Generate scatter plots and calculate metrics\n",
        "for idx_h, h in enumerate(range(1, OUTPUT_HORIZON+1)):\n",
        "    date = val_dates[h-1]\n",
        "    mdl_path = MODEL_DIR/f\"xgb_all_H{h}_{ref}_9features.json\"\n",
        "    if mdl_path.exists():\n",
        "        xgb = XGBRegressor(); xgb.load_model(str(mdl_path))\n",
        "        print_progress(f\"Cargado modelo XGB para H={h}\", level=1)\n",
        "\n",
        "        # Recolectar X_meta completo con manejo robusto de NaNs\n",
        "        preds = [preds_store[(f\"FUSION_{b}\", date)].ravel() for b in ['low','medium','high']]\n",
        "\n",
        "        # Verificar NaNs en predicciones base\n",
        "        for i, b in enumerate(['low','medium','high']):\n",
        "            pred_summary = check_nans(preds[i], f\"Predicci√≥n FUSION_{b}\")\n",
        "            if pred_summary[\"has_nans\"]:\n",
        "                print_progress(f\"Reemplazando {pred_summary['nan_count']} NaNs en predicciones de FUSION_{b}\", level=2)\n",
        "                preds[i] = replace_nans(preds[i], strategy=\"mean\")\n",
        "\n",
        "        elev_flat = ds_full['elevation'].values.ravel()\n",
        "        slope_flat = ds_full['slope'].values.ravel()\n",
        "        aspect_flat = ds_full['aspect'].values.ravel()\n",
        "        mean_e = elev_flat.mean(); std_e = elev_flat.std(); skew_e = skew(elev_flat)\n",
        "\n",
        "        # Verificar NaNs en caracter√≠sticas topogr√°ficas\n",
        "        for arr, name in zip([elev_flat, slope_flat, aspect_flat], ['elevation', 'slope', 'aspect']):\n",
        "            topo_summary = check_nans(arr, name)\n",
        "            if topo_summary[\"has_nans\"]:\n",
        "                print_progress(f\"Reemplazando {topo_summary['nan_count']} NaNs en {name}\", level=2)\n",
        "                if name == 'elevation':\n",
        "                    elev_flat = replace_nans(elev_flat, strategy=\"mean\")\n",
        "                elif name == 'slope':\n",
        "                    slope_flat = replace_nans(slope_flat, strategy=\"mean\")\n",
        "                elif name == 'aspect':\n",
        "                    aspect_flat = replace_nans(aspect_flat, strategy=\"mean\")\n",
        "\n",
        "        elev_stats = np.vstack([\n",
        "            np.full_like(elev_flat, mean_e),\n",
        "            np.full_like(elev_flat, std_e),\n",
        "            np.full_like(elev_flat, skew_e)\n",
        "        ]).T\n",
        "\n",
        "        X_meta = np.column_stack(preds + [elev_stats, slope_flat, aspect_flat])\n",
        "        ytrue = true_store[(\"FUSION_low\", date)].ravel()\n",
        "\n",
        "        # Verificar y manejar NaNs en ytrue\n",
        "        ytrue_summary = check_nans(ytrue, \"Objetivo verdadero\")\n",
        "        if ytrue_summary[\"has_nans\"]:\n",
        "            print_progress(f\"Reemplazando {ytrue_summary['nan_count']} NaNs en objetivo verdadero\", level=2)\n",
        "            ytrue = replace_nans(ytrue, strategy=\"mean\")\n",
        "\n",
        "        # Predicci√≥n robusta\n",
        "        ypred = xgb_predict_full(xgb, X_meta)\n",
        "\n",
        "        # Scatter\n",
        "        ax = axs_sc[idx_h]\n",
        "        ax.scatter(ytrue, ypred, alpha=0.3, s=2)\n",
        "        lims = [min(ytrue.min(), ypred.min()), max(ytrue.max(), ypred.max())]\n",
        "        ax.plot(lims, lims, 'k--')\n",
        "        ax.set_title(f\"XGB H={h} ‚Äî {date}\")\n",
        "        ax.set_xlabel(\"True\"); ax.set_ylabel(\"Predicted\")\n",
        "\n",
        "        # M√©tricas robustas\n",
        "        rm, ma, maP, r2 = evaluate_metrics(ytrue, ypred)\n",
        "        meta_metrics_all.append({\n",
        "            'horizon':h, 'date':date,\n",
        "            'RMSE':rm, 'MAE':ma, 'MAPE':maP, 'R2':r2,\n",
        "            'valid_data_pct': 100 - (np.isnan(ytrue).sum() / len(ytrue) * 100)\n",
        "        })\n",
        "    else:\n",
        "        axs_sc[idx_h].text(0.5,0.5,f\"No model H={h}\",ha='center',va='center')\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# con modelo retrained (con manejo robusto de NaNs)\n",
        "for h in range(1, OUTPUT_HORIZON+1):\n",
        "    date = val_dates[h-1]\n",
        "    mdl_path = MODEL_DIR/f\"xgb_all_H{h}_{ref}_9features.json\"\n",
        "    if not mdl_path.exists():\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        xgb = XGBRegressor(); xgb.load_model(str(mdl_path))\n",
        "\n",
        "        # Reconstruir X_meta con manejo robusto\n",
        "        preds = [preds_store[(f\"FUSION_{b}\", date)].ravel() for b in ['low','medium','high']]\n",
        "        # Manejar NaNs en predicciones\n",
        "        for i, b in enumerate(['low','medium','high']):\n",
        "            if np.isnan(preds[i]).any():\n",
        "                preds[i] = replace_nans(preds[i], strategy=\"mean\")\n",
        "\n",
        "        elev_flat = ds_full['elevation'].values.ravel()\n",
        "        slope_flat = ds_full['slope'].values.ravel()\n",
        "        aspect_flat = ds_full['aspect'].values.ravel()\n",
        "\n",
        "        # Manejar NaNs en caracter√≠sticas topogr√°ficas\n",
        "        if np.isnan(elev_flat).any():\n",
        "            elev_flat = replace_nans(elev_flat, strategy=\"mean\")\n",
        "        if np.isnan(slope_flat).any():\n",
        "            slope_flat = replace_nans(slope_flat, strategy=\"mean\")\n",
        "        if np.isnan(aspect_flat).any():\n",
        "            aspect_flat = replace_nans(aspect_flat, strategy=\"mean\")\n",
        "\n",
        "        mean_e = elev_flat.mean(); std_e = elev_flat.std(); skew_e = skew(elev_flat)\n",
        "        elev_stats = np.vstack([\n",
        "            np.full_like(elev_flat, mean_e), np.full_like(elev_flat, std_e), np.full_like(elev_flat, skew_e)\n",
        "        ]).T\n",
        "\n",
        "        X_meta = np.column_stack(preds + [elev_stats, slope_flat, aspect_flat])\n",
        "\n",
        "        # Predicci√≥n robusta\n",
        "        P = xgb_predict_full(xgb, X_meta).reshape(len(lat), len(lon))\n",
        "        T = true_store[(\"FUSION_low\", date)]\n",
        "\n",
        "        # Calcular MAPE evitando NaNs\n",
        "        mask_valid = ~(np.isnan(T) | np.isnan(P))\n",
        "        M = np.full_like(T, np.nan)  # Inicializar con NaN\n",
        "        M[mask_valid] = np.abs((T[mask_valid] - P[mask_valid])/(T[mask_valid] + 1e-5))*100\n",
        "\n",
        "        # Reemplazar NaNs en mapa MAPE para visualizaci√≥n\n",
        "        if np.isnan(M).any():\n",
        "            print_progress(f\"Reemplazando NaNs en mapa MAPE para visualizaci√≥n\", level=2)\n",
        "            M = np.nan_to_num(M, nan=0.0)\n",
        "\n",
        "        # Prepare grids for plotting before use\n",
        "        grid_lon, grid_lat = np.meshgrid(lon, lat)\n",
        "\n",
        "        fig, axs = plt.subplots(1,2, figsize=(12,5), subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "        axs[0].set_title(f\"Predicci√≥n XGB H={h}\")\n",
        "        pcm = axs[0].pcolormesh(grid_lon, grid_lat, P, transform=ccrs.PlateCarree(), cmap='Blues')\n",
        "        boyaca_gdf.boundary.plot(ax=axs[0], edgecolor='black', transform=ccrs.PlateCarree())\n",
        "        fig.colorbar(pcm, ax=axs[0], orientation='vertical', label='mm')\n",
        "        axs[1].set_title(f\"MAPE% XGB H={h}\")\n",
        "        pcm2 = axs[1].pcolormesh(grid_lon, grid_lat, M, transform=ccrs.PlateCarree(), cmap='Reds', vmin=0, vmax=np.nanpercentile(M,99))\n",
        "        boyaca_gdf.boundary.plot(ax=axs[1], edgecolor='black', transform=ccrs.PlateCarree())\n",
        "        fig.colorbar(pcm2, ax=axs[1], orientation='vertical', label='%')\n",
        "        plt.tight_layout(); plt.show()\n",
        "    except Exception as e:\n",
        "        print_progress(f\"‚ùå Error generando mapa para H={h}: {str(e)}\", level=1)\n",
        "        logger.exception(f\"Error en visualizaci√≥n de mapa para H={h}\")\n",
        "\n",
        " # 13.3) Mapas con modelo retrained (con manejo robusto de NaNs)\n",
        "for h in range(1, OUTPUT_HORIZON+1):\n",
        "    date = val_dates[h-1]\n",
        "    mdl_path = MODEL_DIR/f\"xgb_all_H{h}_{ref}_9features.json\"\n",
        "    if not mdl_path.exists():\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        xgb = XGBRegressor(); xgb.load_model(str(mdl_path))\n",
        "\n",
        "        # Reconstruir X_meta con manejo robusto\n",
        "        preds = [preds_store[(f\"FUSION_{b}\", date)].ravel() for b in ['low','medium','high']]\n",
        "        # Manejar NaNs en predicciones\n",
        "        for i, b in enumerate(['low','medium','high']):\n",
        "            if np.isnan(preds[i]).any():\n",
        "                preds[i] = replace_nans(preds[i], strategy=\"mean\")\n",
        "\n",
        "        elev_flat = ds_full['elevation'].values.ravel()\n",
        "        slope_flat = ds_full['slope'].values.ravel()\n",
        "        aspect_flat = ds_full['aspect'].values.ravel()\n",
        "\n",
        "        # Manejar NaNs en caracter√≠sticas topogr√°ficas\n",
        "        if np.isnan(elev_flat).any():\n",
        "            elev_flat = replace_nans(elev_flat, strategy=\"mean\")\n",
        "        if np.isnan(slope_flat).any():\n",
        "            slope_flat = replace_nans(slope_flat, strategy=\"mean\")\n",
        "        if np.isnan(aspect_flat).any():\n",
        "            aspect_flat = replace_nans(aspect_flat, strategy=\"mean\")\n",
        "\n",
        "        mean_e = elev_flat.mean(); std_e = elev_flat.std(); skew_e = skew(elev_flat)\n",
        "        elev_stats = np.vstack([\n",
        "            np.full_like(elev_flat, mean_e), np.full_like(elev_flat, std_e), np.full_like(elev_flat, skew_e)\n",
        "        ]).T\n",
        "\n",
        "        X_meta = np.column_stack(preds + [elev_stats, slope_flat, aspect_flat])\n",
        "\n",
        "        # Predicci√≥n robusta\n",
        "        P = xgb_predict_full(xgb, X_meta).reshape(len(lat), len(lon))\n",
        "        T = true_store[(\"FUSION_low\", date)]\n",
        "\n",
        "        # Calcular MAPE evitando NaNs\n",
        "        mask_valid = ~(np.isnan(T) | np.isnan(P))\n",
        "        M = np.full_like(T, np.nan)  # Inicializar con NaN\n",
        "        M[mask_valid] = np.abs((T[mask_valid] - P[mask_valid])/(T[mask_valid] + 1e-5))*100\n",
        "\n",
        "        # Reemplazar NaNs en mapa MAPE para visualizaci√≥n\n",
        "        if np.isnan(M).any():\n",
        "            print_progress(f\"Reemplazando NaNs en mapa MAPE para visualizaci√≥n\", level=2)\n",
        "            M = np.nan_to_num(M, nan=0.0)\n",
        "\n",
        "        # Prepare grids for plotting before use\n",
        "        grid_lon, grid_lat = np.meshgrid(lon, lat)\n",
        "\n",
        "        fig, axs = plt.subplots(1,2, figsize=(12,5), subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "        axs[0].set_title(f\"Predicci√≥n XGB H={h}\")\n",
        "        pcm = axs[0].pcolormesh(grid_lon, grid_lat, P, transform=ccrs.PlateCarree(), cmap='Blues')\n",
        "        boyaca_gdf.boundary.plot(ax=axs[0], edgecolor='black', transform=ccrs.PlateCarree())\n",
        "        fig.colorbar(pcm, ax=axs[0], orientation='vertical', label='mm')\n",
        "        axs[1].set_title(f\"MAPE% XGB H={h}\")\n",
        "        pcm2 = axs[1].pcolormesh(grid_lon, grid_lat, M, transform=ccrs.PlateCarree(), cmap='Reds', vmin=0, vmax=np.nanpercentile(M,99))\n",
        "        boyaca_gdf.boundary.plot(ax=axs[1], edgecolor='black', transform=ccrs.PlateCarree())\n",
        "        fig.colorbar(pcm2, ax=axs[1], orientation='vertical', label='%')\n",
        "        plt.tight_layout(); plt.show()\n",
        "    except Exception as e:\n",
        "        print_progress(f\"‚ùå Error generando mapa para H={h}: {str(e)}\", level=1)\n",
        "        logger.exception(f\"Error en visualizaci√≥n de mapa para H={h}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yEEbxtQP-ivk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yEEbxtQP-ivk",
        "outputId": "c264ce9d-78c1-4a1d-a18e-bdc7975d5b5b"
      },
      "outputs": [],
      "source": [
        "# 13) Meta-modelo neuronal completo con m√©tricas, mapas y tablas\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from scipy.stats import skew\n",
        "import pandas as pd\n",
        "import cartopy.crs as ccrs\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Definici√≥n de un modelo neuronal m√°s robusto para secuencias\n",
        "class TemporalMetaModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo h√≠brido avanzado con componentes de atenci√≥n temporal y GRU\n",
        "    para mantener mejor la coherencia entre horizontes de predicci√≥n.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, hidden_dim=128, num_horizons=3):\n",
        "        super().__init__()\n",
        "        self.num_horizons = num_horizons\n",
        "\n",
        "        # Encoder de caracter√≠sticas\n",
        "        self.feature_encoder = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # Capa GRU para capturar dependencias temporales\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=2,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Mecanismo de atenci√≥n para diferentes horizontes\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_dim,\n",
        "            num_heads=4,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        # Proyecci√≥n espec√≠fica para cada horizonte\n",
        "        self.horizon_projections = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim*2, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, 1)\n",
        "            ) for _ in range(num_horizons)\n",
        "        ])\n",
        "\n",
        "        # M√≥dulo de calibraci√≥n para mejorar la estimaci√≥n de incertidumbre\n",
        "        self.calibration = nn.Linear(hidden_dim*2, 1)\n",
        "\n",
        "    def forward(self, x, horizon_idx=None):\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Codificar caracter√≠sticas de entrada\n",
        "        encoded = self.feature_encoder(x)\n",
        "\n",
        "        # Preparar secuencia de horizontes para GRU\n",
        "        # Expandir el tensor codificado para tener formato de secuencia [batch, seq_len, features]\n",
        "        seq_input = encoded.unsqueeze(1).expand(-1, self.num_horizons, -1)\n",
        "\n",
        "        # Procesar con GRU para mantener coherencia temporal\n",
        "        gru_out, _ = self.gru(seq_input)\n",
        "\n",
        "        # Aplicar mecanismo de atenci√≥n para refinar representaciones\n",
        "        # Preparar datos para la atenci√≥n: [seq_len, batch, embed_dim]\n",
        "        attn_input = gru_out.transpose(0, 1)\n",
        "        attn_output, _ = self.attention(attn_input, attn_input, attn_input)\n",
        "\n",
        "        # Volver al formato original [batch, seq_len, embed_dim]\n",
        "        attn_output = attn_output.transpose(0, 1)\n",
        "\n",
        "        # Concatenar informaci√≥n original con atenci√≥n para preservar detalles\n",
        "        combined = torch.cat([gru_out, attn_output], dim=2)\n",
        "\n",
        "        # Si se solicita un horizonte espec√≠fico, devolver solo ese\n",
        "        if horizon_idx is not None:\n",
        "            return self.horizon_projections[horizon_idx](combined[:, horizon_idx])\n",
        "\n",
        "        # Caso contrario, generar predicciones para todos los horizontes\n",
        "        outputs = []\n",
        "        for h in range(self.num_horizons):\n",
        "            h_pred = self.horizon_projections[h](combined[:, h])\n",
        "            outputs.append(h_pred)\n",
        "\n",
        "        # Retornar predicciones para todos los horizontes [batch, num_horizons]\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "# Funci√≥n de p√©rdida personalizada que considera la coherencia temporal\n",
        "class TemporalCoherenceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Funci√≥n de p√©rdida que combina MSE con un t√©rmino de regularizaci√≥n\n",
        "    para mantener coherencia temporal entre horizontes consecutivos.\n",
        "    \"\"\"\n",
        "    def __init__(self, lambda_coherence=0.3):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.lambda_coherence = lambda_coherence\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        # Calcular MSE est√°ndar\n",
        "        mse_loss = self.mse(pred, target)\n",
        "\n",
        "        # Si tenemos multi-horizonte (m√°s de una columna)\n",
        "        if pred.shape[1] > 1:\n",
        "            # Penalizar cambios abruptos entre horizontes consecutivos\n",
        "            coherence_loss = torch.mean(torch.abs(\n",
        "                torch.diff(pred, dim=1) - torch.diff(target, dim=1)\n",
        "            )**2)\n",
        "\n",
        "            # Combinar las p√©rdidas\n",
        "            return mse_loss + self.lambda_coherence * coherence_loss\n",
        "        else:\n",
        "            return mse_loss\n",
        "\n",
        "# Funci√≥n para entrenar el modelo con aprendizaje por curriculum\n",
        "def train_with_curriculum(model, train_loader, val_loader, epochs=50, lr=0.001, device='cpu'):\n",
        "    \"\"\"\n",
        "    Entrena el modelo usando aprendizaje por curriculum:\n",
        "    primero en H=1, luego H=1,2, finalmente todos los horizontes.\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "    criterion = TemporalCoherenceLoss()\n",
        "\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "    best_val_loss = float('inf')\n",
        "    best_state_dict = None\n",
        "\n",
        "    # Fase 1: Entrenar solo en H=1\n",
        "    print_progress(\"Fase 1: Entrenando en horizonte H=1\", level=1)\n",
        "    for epoch in range(epochs // 3):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch_h1 = x_batch.to(device), y_batch[:, 0:1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x_batch, horizon_idx=0)\n",
        "            loss = criterion(output, y_batch_h1)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * len(x_batch)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        # Validaci√≥n\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch, y_batch_h1 = x_batch.to(device), y_batch[:, 0:1].to(device)\n",
        "                output = model(x_batch, horizon_idx=0)\n",
        "                loss = criterion(output, y_batch_h1)\n",
        "                val_loss += loss.item() * len(x_batch)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_state_dict = model.state_dict().copy()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print_progress(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\", level=2)\n",
        "\n",
        "    # Cargar los mejores pesos obtenidos hasta ahora\n",
        "    model.load_state_dict(best_state_dict)\n",
        "\n",
        "    # Fase 2: Entrenar en H=1,2\n",
        "    print_progress(\"Fase 2: Entrenando en horizontes H=1,2\", level=1)\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs // 3):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch_h12 = x_batch.to(device), y_batch[:, 0:2].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x_batch)[:, 0:2]  # Solo los primeros 2 horizontes\n",
        "            loss = criterion(output, y_batch_h12)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * len(x_batch)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        # Validaci√≥n\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch, y_batch_h12 = x_batch.to(device), y_batch[:, 0:2].to(device)\n",
        "                output = model(x_batch)[:, 0:2]\n",
        "                loss = criterion(output, y_batch_h12)\n",
        "                val_loss += loss.item() * len(x_batch)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_state_dict = model.state_dict().copy()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print_progress(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\", level=2)\n",
        "\n",
        "    # Cargar los mejores pesos obtenidos hasta ahora\n",
        "    model.load_state_dict(best_state_dict)\n",
        "\n",
        "    # Fase 3: Entrenar en todos los horizontes\n",
        "    print_progress(\"Fase 3: Entrenando en todos los horizontes\", level=1)\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs // 3):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for x_batch, y_batch in train_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * len(x_batch)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        history['train_loss'].append(train_loss)\n",
        "\n",
        "        # Validaci√≥n\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in val_loader:\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "                output = model(x_batch)\n",
        "                loss = criterion(output, y_batch)\n",
        "                val_loss += loss.item() * len(x_batch)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_state_dict = model.state_dict().copy()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print_progress(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\", level=2)\n",
        "\n",
        "    # Cargar los mejores pesos obtenidos\n",
        "    model.load_state_dict(best_state_dict)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Par√°metros de entrenamiento\n",
        "device   = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_sz = 64\n",
        "lr       = 5e-4  # Tasa de aprendizaje m√°s baja para entrenamiento m√°s estable\n",
        "epochs     = 60    # M√°s √©pocas para el curriculum learning\n",
        "\n",
        "# Funciones de m√©trica\n",
        "def rmse(a, b): return np.sqrt(np.mean((a - b)**2))\n",
        "def mae(a, b):  return np.mean(np.abs(a - b))\n",
        "def mape(a, b): return np.mean(np.abs((a - b) / (b + 1e-5))) * 100\n",
        "def r2(a, b):\n",
        "    # a: predicciones, b: valores reales\n",
        "    ss_res = np.sum((b - a)**2)\n",
        "    ss_tot = np.sum((b - np.mean(b))**2)\n",
        "    return 1 - ss_res/ss_tot if ss_tot != 0 else np.nan\n",
        "\n",
        "def evaluate(a, b):\n",
        "    return {\n",
        "        'RMSE': rmse(a, b),\n",
        "        'MAE': mae(a, b),\n",
        "        'MAPE': mape(a, b),\n",
        "        'R2': r2(a, b)\n",
        "    }\n",
        "\n",
        "# Preparar grillas para mapas\n",
        "grid_lon, grid_lat = np.meshgrid(lon, lat)\n",
        "\n",
        "# M√°scaras seg√∫n elevaci√≥n\n",
        "elev_flat = ds_full['elevation'].values.ravel()\n",
        "mask_low    = elev_flat < 200\n",
        "mask_mid    = (elev_flat >= 200) & (elev_flat <= 1000)\n",
        "mask_high   = elev_flat > 1000\n",
        "\n",
        "# Colecciones para m√©tricas\n",
        "global_metrics = []\n",
        "elev_metrics = []\n",
        "pct_metrics_list = []\n",
        "trained_models = {}  # Almacenar modelos entrenados\n",
        "\n",
        "# El resto del c√≥digo sigue igual, solo cambiamos la inicializaci√≥n y entrenamiento del modelo\n",
        "# Dividiremos el bucle en partes para mostrar solo las secciones que cambian\n",
        "\n",
        "# Bucle por cada horizonte\n",
        "print_progress(\"Procesando datos para la construcci√≥n del meta-modelo avanzado\", level=1)\n",
        "all_X_meta = []\n",
        "all_y_horizons = []\n",
        "\n",
        "for h in range(1, OUTPUT_HORIZON+1):\n",
        "    date = val_dates[h-1]\n",
        "    print_progress(f\"Recopilando datos para horizonte {h}, fecha {date}\", level=1)\n",
        "\n",
        "    # Obtener predicciones de stacking y verificar NaNs\n",
        "    preds = [preds_store[(f\"FUSION_{b}\", date)].ravel() for b in ['low','medium','high']]\n",
        "\n",
        "    # Verificar y manejar NaNs en predicciones de cada rama\n",
        "    for i, branch in enumerate(['low', 'medium', 'high']):\n",
        "        pred_summary = check_nans(preds[i], f\"Predicci√≥n FUSION_{branch}\")\n",
        "        if pred_summary[\"has_nans\"]:\n",
        "            print_progress(f\"‚ö†Ô∏è {pred_summary['nan_count']} NaNs en predicciones de {branch} ({pred_summary['nan_percentage']:.2f}%)\", level=2)\n",
        "            preds[i] = replace_nans(preds[i], strategy=\"interpolate\")\n",
        "\n",
        "    # Estad√≠sticos globales de elevaci√≥n con manejo de NaNs\n",
        "    elev_flat = ds_full['elevation'].values.ravel()\n",
        "\n",
        "    # Verificar NaNs en elevaci√≥n\n",
        "    elev_summary = check_nans(elev_flat, \"Elevaci√≥n\")\n",
        "    if elev_summary[\"has_nans\"]:\n",
        "        print_progress(f\"Reemplazando {elev_summary['nan_count']} NaNs en elevaci√≥n\", level=2)\n",
        "        elev_flat = replace_nans(elev_flat, strategy=\"mean\")\n",
        "\n",
        "    mean_e = elev_flat.mean()\n",
        "    std_e = elev_flat.std()\n",
        "    skew_e = skew(elev_flat)\n",
        "    elev_stats = np.vstack([\n",
        "        np.full_like(elev_flat, mean_e),\n",
        "        np.full_like(elev_flat, std_e),\n",
        "        np.full_like(elev_flat, skew_e)\n",
        "    ]).T\n",
        "\n",
        "    # Verificar NaNs en slope y aspect\n",
        "    slope_flat = ds_full['slope'].values.ravel()\n",
        "    aspect_flat = ds_full['aspect'].values.ravel()\n",
        "\n",
        "    for arr, name in zip([slope_flat, aspect_flat], ['Slope', 'Aspect']):\n",
        "        arr_summary = check_nans(arr, name)\n",
        "        if arr_summary[\"has_nans\"]:\n",
        "            print_progress(f\"Reemplazando {arr_summary['nan_count']} NaNs en {name}\", level=2)\n",
        "            if name == 'Slope':\n",
        "                slope_flat = replace_nans(slope_flat, strategy=\"mean\")\n",
        "            else:\n",
        "                aspect_flat = replace_nans(aspect_flat, strategy=\"mean\")\n",
        "\n",
        "    # Construir X_meta y y_true\n",
        "    X_meta = np.column_stack(preds + [elev_stats, slope_flat, aspect_flat])\n",
        "    y_true = true_store[(\"FUSION_low\", date)].ravel()\n",
        "\n",
        "    # Verificar NaNs en y_true\n",
        "    y_true_summary = check_nans(y_true, \"Objetivo\")\n",
        "    if y_true_summary[\"has_nans\"]:\n",
        "        print_progress(f\"Reemplazando {y_true_summary['nan_count']} NaNs en objetivo\", level=2)\n",
        "        y_true = replace_nans(y_true, strategy=\"mean\")\n",
        "\n",
        "    # Verificar NaNs en X_meta final\n",
        "    X_meta_summary = check_nans(X_meta, \"X_meta final\")\n",
        "    if X_meta_summary[\"has_nans\"]:\n",
        "        print_progress(f\"‚ö†Ô∏è A√∫n hay {X_meta_summary['nan_count']} NaNs en X_meta, reemplazando\", level=2)\n",
        "        X_meta = np.nan_to_num(X_meta, nan=0.0)\n",
        "\n",
        "    # Almacenamos datos para todos los horizontes\n",
        "    all_X_meta.append(X_meta)\n",
        "    all_y_horizons.append(y_true)\n",
        "\n",
        "# Unificar todos los datos para el enfoque multi-horizonte\n",
        "# Necesitamos asegurar que tenemos la misma cantidad de muestras para cada horizonte\n",
        "n_samples = min([X.shape[0] for X in all_X_meta])\n",
        "print_progress(f\"Construyendo dataset multi-horizonte con {n_samples} muestras por horizonte\", level=1)\n",
        "\n",
        "# Tomar las primeras n_samples de cada horizonte\n",
        "X_meta_unified = all_X_meta[0][:n_samples]  # Usamos X del primer horizonte como base\n",
        "y_horizons = np.column_stack([y[:n_samples] for y in all_y_horizons])\n",
        "\n",
        "# Train/Val split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_meta_unified, y_horizons, test_size=0.15, random_state=42\n",
        ")\n",
        "\n",
        "print_progress(f\"Split de datos: Train={X_train.shape}, Val={X_val.shape}\", level=2)\n",
        "\n",
        "# Preparar DataLoader\n",
        "train_dataset = TensorDataset(\n",
        "    torch.from_numpy(X_train).float(),\n",
        "    torch.from_numpy(y_train).float()\n",
        ")\n",
        "val_dataset = TensorDataset(\n",
        "    torch.from_numpy(X_val).float(),\n",
        "    torch.from_numpy(y_val).float()\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_sz, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_sz)\n",
        "\n",
        "# Define model checkpoint path\n",
        "model_path = MODEL_DIR/f\"temporal_meta_model_{ref}.pt\"\n",
        "\n",
        "# Inicializar y entrenar modelo avanzado\n",
        "print_progress(\"Entrenando meta-modelo temporal avanzado con curriculum learning\", is_start=True)\n",
        "\n",
        "# Check if model already exists\n",
        "if model_path.exists():\n",
        "    print_progress(f\"Cargando modelo existente de {model_path}\", level=1)\n",
        "    model = TemporalMetaModel(in_dim=X_meta_unified.shape[1], num_horizons=OUTPUT_HORIZON).to(device)\n",
        "    model.load_state_dict(torch.load(str(model_path)))\n",
        "\n",
        "    # Simular historial para gr√°ficas\n",
        "    history = {\n",
        "        'train_loss': [0.1] * 10,  # Placeholder\n",
        "        'val_loss': [0.1] * 10      # Placeholder\n",
        "    }\n",
        "else:\n",
        "    # Entrenar nuevo modelo\n",
        "    print_progress(f\"Iniciando entrenamiento de nuevo modelo temporal\", level=1)\n",
        "    model = TemporalMetaModel(in_dim=X_meta_unified.shape[1], num_horizons=OUTPUT_HORIZON).to(device)\n",
        "    model, history = train_with_curriculum(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        epochs=epochs,\n",
        "        lr=lr,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Guardar modelo\n",
        "    print_progress(f\"Guardando modelo en {model_path}\", level=1)\n",
        "    torch.save(model.state_dict(), str(model_path))\n",
        "\n",
        "    # Visualizar curva de entrenamiento\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.axvline(x=epochs//3, color='red', linestyle='--', alpha=0.7, label='Fase 1 ‚Üí Fase 2')\n",
        "    plt.axvline(x=2*epochs//3, color='red', linestyle='--', alpha=0.7, label='Fase 2 ‚Üí Fase 3')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('P√©rdida')\n",
        "    plt.title('Entrenamiento con Curriculum Learning')\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.savefig(IMAGE_DIR/f\"temporal_meta_training_{ref}.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "print_progress(\"Evaluando modelo para todos los horizontes\", level=1)\n",
        "\n",
        "# Evaluaci√≥n agregada\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = model(torch.from_numpy(X_meta_unified).float().to(device))\n",
        "    predictions = predictions.cpu().numpy()\n",
        "\n",
        "# Evaluar para cada horizonte y calcular m√©tricas\n",
        "print_progress(\"Calculando m√©tricas por horizonte\", level=1)\n",
        "\n",
        "global_metrics = []\n",
        "for h in range(OUTPUT_HORIZON):\n",
        "    preds_h = predictions[:, h]\n",
        "    true_h = y_horizons[:, h]\n",
        "\n",
        "    # M√©tricas globales\n",
        "    metrics = evaluate(preds_h, true_h)\n",
        "    global_metrics.append({\n",
        "        'horizon': h+1,\n",
        "        'date': val_dates[h],\n",
        "        'RMSE': metrics['RMSE'],\n",
        "        'MAE': metrics['MAE'],\n",
        "        'MAPE': metrics['MAPE'],\n",
        "        'R2': metrics['R2'],\n",
        "        'valid_data_pct': 100 - (np.isnan(true_h).sum() / len(true_h) * 100)\n",
        "    })\n",
        "\n",
        "    # Generar visualizaciones\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.scatter(true_h, preds_h, alpha=0.4, s=3)\n",
        "    lims = [\n",
        "        min(np.nanmin(true_h), np.nanmin(preds_h)),\n",
        "        max(np.nanmax(true_h), np.nanmax(preds_h))\n",
        "    ]\n",
        "    plt.plot(lims, lims, 'k--', alpha=0.75)\n",
        "    plt.xlabel('Verdadero')\n",
        "    plt.ylabel('Predicci√≥n')\n",
        "    plt.title(f'Modelo Temporal: Horizonte {h+1} (R¬≤={metrics[\"R2\"]:.3f})')\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.savefig(IMAGE_DIR/f\"temporal_meta_scatter_h{h+1}_{ref}.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    print_progress(f\"Horizonte {h+1}: RMSE={metrics['RMSE']:.2f}, R¬≤={metrics['R2']:.3f}\", level=2)\n",
        "\n",
        "# Generar tabla de m√©tricas\n",
        "df_global = pd.DataFrame(global_metrics)\n",
        "df_global.to_csv(MODEL_DIR/f\"temporal_meta_metrics_ref{ref}.csv\", index=False)\n",
        "tools.display_dataframe_to_user(\n",
        "    name=f\"TemporalMeta_Global_metrics_ref{ref}\",\n",
        "    dataframe=df_global\n",
        ")\n",
        "\n",
        "# Comparativa entre modelos\n",
        "print_progress(\"Comparando rendimiento entre meta-modelos\", level=1)\n",
        "\n",
        "models_to_compare = []\n",
        "\n",
        "# DeepMeta (modelo original)\n",
        "try:\n",
        "    deep_meta_path = MODEL_DIR/f\"deepmeta_global_metrics_ref{ref}.csv\"\n",
        "    if deep_meta_path.exists():\n",
        "        df_deep = pd.read_csv(deep_meta_path)\n",
        "        df_deep['model'] = 'DeepMeta'\n",
        "        models_to_compare.append(df_deep)\n",
        "except Exception as e:\n",
        "    print_progress(f\"Error al cargar m√©tricas DeepMeta: {str(e)}\", level=2)\n",
        "\n",
        "# XGB\n",
        "try:\n",
        "    xgb_path = MODEL_DIR/f\"xgb_meta_metrics_ref{ref}.csv\"\n",
        "    if xgb_path.exists():\n",
        "        df_xgb = pd.read_csv(xgb_path)\n",
        "        df_xgb['model'] = 'XGBoost'\n",
        "        models_to_compare.append(df_xgb)\n",
        "except Exception as e:\n",
        "    print_progress(f\"Error al cargar m√©tricas XGBoost: {str(e)}\", level=2)\n",
        "\n",
        "# Nuevo modelo temporal\n",
        "df_temporal = df_global.copy()\n",
        "df_temporal['model'] = 'TemporalMeta'\n",
        "models_to_compare.append(df_temporal)\n",
        "\n",
        "# Unificar y mostrar comparaci√≥n\n",
        "if len(models_to_compare) > 1:\n",
        "    df_comparison = pd.concat(models_to_compare, ignore_index=True)\n",
        "    pivot_rmse = df_comparison.pivot_table(\n",
        "        index='horizon', columns='model', values='RMSE', aggfunc='mean'\n",
        "    ).reset_index()\n",
        "    pivot_r2 = df_comparison.pivot_table(\n",
        "        index='horizon', columns='model', values='R2', aggfunc='mean'\n",
        "    ).reset_index()\n",
        "\n",
        "    # Guardar y mostrar tablas comparativas\n",
        "    pivot_rmse.to_csv(MODEL_DIR/f\"metamodels_rmse_comparison_{ref}.csv\", index=False)\n",
        "    pivot_r2.to_csv(MODEL_DIR/f\"metamodels_r2_comparison_{ref}.csv\", index=False)\n",
        "\n",
        "    tools.display_dataframe_to_user(\n",
        "        name=f\"MetaModels_RMSE_Comparison_{ref}\",\n",
        "        dataframe=pivot_rmse\n",
        "    )\n",
        "    tools.display_dataframe_to_user(\n",
        "        name=f\"MetaModels_R2_Comparison_{ref}\",\n",
        "        dataframe=pivot_r2\n",
        "    )\n",
        "\n",
        "    # Visualizaci√≥n de comparaci√≥n\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    models = [col for col in pivot_r2.columns if col != 'horizon']\n",
        "    x = np.arange(len(pivot_r2))\n",
        "    width = 0.8 / len(models)\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        plt.bar(\n",
        "            x + width * (i - len(models)/2 + 0.5),\n",
        "            pivot_r2[model],\n",
        "            width=width,\n",
        "            label=model\n",
        "        )\n",
        "\n",
        "    plt.xlabel('Horizonte')\n",
        "    plt.ylabel('R¬≤')\n",
        "    plt.title('Comparaci√≥n de R¬≤ entre Meta-modelos')\n",
        "    plt.xticks(x, pivot_r2['horizon'])\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.savefig(IMAGE_DIR/f\"metamodels_comparison_{ref}.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "print_progress(\"An√°lisis de meta-modelos completado\", is_end=True)\n",
        "\n",
        "# Para la parte de mapas, usamos el mismo c√≥digo base pero con el nuevo modelo\n",
        "# ...resto del c√≥digo contin√∫a igual..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "655a9b08",
      "metadata": {},
      "source": [
        "# Variables Utilizadas por los Modelos\n",
        "\n",
        "Se utilizan las variables definidas en `ALL_FEATURES`:"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
