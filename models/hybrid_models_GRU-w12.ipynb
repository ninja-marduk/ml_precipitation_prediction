{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b176146b",
      "metadata": {},
      "source": [
        "\n",
        "# Spatiotemporal Precipitation Prediction\n",
        "**5‚ÄØ√ó‚ÄØ5 Experiments Notebook**  \n",
        "Train & validate five architectures across five temporal folds (48‚ÄØm train ‚Üí 12‚ÄØm val).  Designed to run **locally or on Google¬†Colab** ‚Äî auto‚Äëdetects GPU/CPU and adapts parallelism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "48b7a139",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing pytorch-lightning ...\n",
            "Installing netcdf4 ...\n",
            "Installing netcdf4 ...\n",
            "Installing scikit-learn ...\n",
            "Installing scikit-learn ...\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mEl kernel se bloque√≥ al ejecutar c√≥digo en la celda actual o en una celda anterior. \n",
            "\u001b[1;31mRevise el c√≥digo de las celdas para identificar una posible causa del error. \n",
            "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqu√≠</a> para obtener m√°s informaci√≥n. \n",
            "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener m√°s detalles."
          ]
        }
      ],
      "source": [
        "# ‚ñ∂Ô∏è Environment setup (PyTorch + TF + XGBoost)\n",
        "import sys, subprocess, importlib, os, multiprocessing, logging, warnings, json\n",
        "basic_pkgs = [\"torch\",\"torchvision\",\"torchaudio\",\"pytorch-lightning\",\n",
        "              \"xarray\",\"netcdf4\",\"scikit-learn\",\"tqdm\",\"xgboost\",\n",
        "              \"tensorflow\",\"geopandas\",\"cartopy\",\"torchmetrics\",\"pytorch_lightning\"]\n",
        "def _install(pkg):\n",
        "    if importlib.util.find_spec(pkg) is None:\n",
        "        print(f\"Installing {pkg} ...\")\n",
        "        subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"-q\",pkg])\n",
        "for p in basic_pkgs: _install(p)\n",
        "\n",
        "import torch, xarray as xr, numpy as np, pandas as pd, pytorch_lightning as pl\n",
        "import tensorflow as tf, geopandas as gpd, cartopy.crs as ccrs\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "# ‚ñ∂Ô∏è Funciones para curvas de aprendizaje y visualizaci√≥n de predicciones\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import numpy as np\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from pathlib import Path\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "N_GPU  = torch.cuda.device_count()\n",
        "CPU_CORES = multiprocessing.cpu_count()\n",
        "NUM_WORKERS = max(1, int(CPU_CORES // 2))  # Use half of CPU cores for data loading\n",
        "print(f\"‚úÖ Torch device: {DEVICE} | GPUs: {N_GPU} | CPU cores: {CPU_CORES} | Workers: {NUM_WORKERS}\")\n",
        "\n",
        "\n",
        "# ‚ñ∂Ô∏è Path configuration (Colab vs Local)\n",
        "from pathlib import Path\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    # climb to project root if inside subfolder\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "print('BASE_PATH =', BASE_PATH)\n",
        "\n",
        "# centralised dataset / model paths\n",
        "DATA_DIR      = BASE_PATH/'data'/'output'\n",
        "MODEL_DIR     = BASE_PATH/'models'/'output'/'trained_models'; MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IMAGE_DIR     = MODEL_DIR/'images'; IMAGE_DIR.mkdir(exist_ok=True)\n",
        "FEATURES_NC   = BASE_PATH/'models'/'output'/'features_fusion_branches.nc'\n",
        "FULL_NC       = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_with_windows.nc'\n",
        "PRECIP_NC     = DATA_DIR/'precip_topo.nc'  # for PyTorch toy experiments\n",
        "print('Using FULL_NC  :', FULL_NC)\n",
        "print('Using FEATURES :', FEATURES_NC)\n",
        "\n",
        "\n",
        "\n",
        "FOLDS = {'F1':2024,'F2':2023,'F3':2022,'F4':2000,'F5':1990}\n",
        "\n",
        "# Actualizar diccionario de experimentos seg√∫n la nueva nomenclatura\n",
        "EXPERIMENTS = {\n",
        "    'GRU-ED': {'model':'gru_ed', 'use_lags':False},\n",
        "    'GRU-ED-PAFC': {'model':'gru_ed', 'use_lags':True},\n",
        "    'AE-FUSION-GRU-ED-PAFC': {'model':'ae_fusion_gru', 'use_lags':True},\n",
        "    'AE-FUSION-GRU-ED-PAFC-T': {'model':'ae_fusion_gru_t', 'use_lags':True},\n",
        "    'AE-FUSION-GRU-ED-PAFC-T-TopoMask': {'model':'ae_fusion_gru_t_mask', 'use_lags':True},\n",
        "}\n",
        "\n",
        "# ‚ñ∂Ô∏è Add variable definitions consistent with documentation\n",
        "FULL_FEATURES = [\n",
        "    'precip_hist','lag_1','lag_2','lag_12',\n",
        "    'month_sin','month_cos','doy_sin','doy_cos',\n",
        "    'elevation','slope','roughness','curvature','aspect',\n",
        "    'alt_cluster','ceemdan_imf1','ceemdan_imf2','ceemdan_imf3',\n",
        "    'tvfemd_imf1','tvfemd_imf2','tvfemd_imf3'\n",
        "]\n",
        "\n",
        "BASE_FEATURES = [\n",
        "    'precip_hist','lag_1','lag_2','lag_12',\n",
        "    'month_sin','month_cos','doy_sin','doy_cos',\n",
        "    'elevation','slope','roughness','curvature',\n",
        "    'alt_cluster'\n",
        "]\n",
        "\n",
        "\n",
        "# ‚ñ∂Ô∏è Helper functions\n",
        "import pandas as pd, numpy as np\n",
        "def add_time_encodings(ds: xr.Dataset):\n",
        "    '''Add month/day-of-year sinusoidal encodings'''\n",
        "    dates = pd.to_datetime(ds['time'].values)\n",
        "    month = dates.month\n",
        "    doy = dates.dayofyear\n",
        "    ds['month_sin'] = ('time', np.sin(2*np.pi*month/12))\n",
        "    ds['month_cos'] = ('time', np.cos(2*np.pi*month/12))\n",
        "    ds['doy_sin']   = ('time', np.sin(2*np.pi*doy/365.25))\n",
        "    ds['doy_cos']   = ('time', np.cos(2*np.pi*doy/365.25))\n",
        "    return ds\n",
        "\n",
        "# ‚ñ∂Ô∏è Logger & helper prints\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s [%(levelname)s] %(message)s',\n",
        "                    datefmt='%H:%M:%S')\n",
        "logger = logging.getLogger('precip')\n",
        "\n",
        "def print_progress(msg, level=0, is_start=False, is_end=False):\n",
        "    prefix={0:'üîµ ' if is_start else '‚úÖ ' if is_end else '‚û°Ô∏è ',\n",
        "            1:'  ‚ö™ ',2:'    ‚Ä¢ '}.get(level,'')\n",
        "    print(f'{prefix}{msg}')\n",
        "\n",
        "# (Reuse code from earlier minimal pipeline, but path variable PRECIP_NC)\n",
        "DATASET_PATH = str(PRECIP_NC)\n",
        "INPUT_WINDOW=48; HORIZON=12; BATCH_SIZE=32\n",
        "FOLDS={'F1':2024,'F2':2023,'F3':2022,'F4':2000,'F5':1990}\n",
        "# ... (insert PyTorch dataset, model, training utils from earlier) ...\n",
        "print_progress('‚ö†Ô∏è   PyTorch quick baseline section trimmed for brevity ‚Äî insert from earlier if desired', level=1)\n",
        "\n",
        "# ‚ñ∂Ô∏è Verify precipitation lags utility\n",
        "def verify_precipitation_lags(ds, required_lags=None, min_valid_ratio=0.9):\n",
        "    all_possible = [f\"total_precipitation_lag{i}\" for i in [1,2,3,4,12,24,36]]\n",
        "    lags = required_lags or [l for l in all_possible if l in ds.data_vars]\n",
        "    if not lags: raise ValueError('No lag variables found.')\n",
        "    for lag in lags:\n",
        "        arr = ds[lag].values\n",
        "        valid = np.count_nonzero(~np.isnan(arr))\n",
        "        ratio = valid/arr.size\n",
        "        logger.info(f'{lag}: {ratio:.1%} valid')\n",
        "        if ratio<min_valid_ratio:\n",
        "            raise ValueError(f'{lag} has only {ratio:.1%} valid data (<{min_valid_ratio})')\n",
        "    logger.info('Lag verification ‚úÖ')\n",
        "\n",
        "# ‚ñ∂Ô∏è NaN‚Äërobust scaling utils\n",
        "def check_nans(arr, name='array'):\n",
        "    nan_cnt=np.isnan(arr).sum(); tot=arr.size\n",
        "    return {'name':name,'nan':nan_cnt,'total':tot,'pct':nan_cnt/tot*100,'has':nan_cnt>0}\n",
        "\n",
        "def replace_nans(arr, strategy='mean'):\n",
        "    if not np.isnan(arr).any(): return arr\n",
        "    arr=arr.copy()\n",
        "    if strategy=='mean':\n",
        "        fill=np.nanmean(arr); arr[np.isnan(arr)]=fill\n",
        "    elif strategy=='median':\n",
        "        fill=np.nanmedian(arr); arr[np.isnan(arr)]=fill\n",
        "    else:\n",
        "        arr=np.nan_to_num(arr)\n",
        "    return arr\n",
        "\n",
        "class ScalerNaN:\n",
        "    def fit(self,X):\n",
        "        self.mean_=np.nanmean(X,0); var=np.nanvar(X,0); var[var<1e-9]=1\n",
        "        self.scale_=np.sqrt(var); return self\n",
        "    def transform(self,X):\n",
        "        return (X-self.mean_)/self.scale_\n",
        "    def fit_transform(self,X): self.fit(X); return self.transform(X)\n",
        "    def inverse_transform(self,X):\n",
        "        return X*self.scale_+self.mean_\n",
        "\n",
        "# ‚ñ∂Ô∏è Dataset & DataLoader builder\n",
        "class PrecipDataset(Dataset):\n",
        "    def __init__(self, ds, idx_list, input_window, horizon,\n",
        "                 sc_p, sc_x, features):\n",
        "        self.ds = ds\n",
        "        self.idx = idx_list\n",
        "        self.w = input_window\n",
        "        self.h = horizon\n",
        "        self.scp = sc_p\n",
        "        self.scx = sc_x\n",
        "        self.features = features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        t,y,x = self.idx[i]\n",
        "        win = self.ds.isel(time=slice(t-self.w, t), y=y, x=x)\n",
        "        tgt = self.ds.isel(time=slice(t, t+self.h), y=y, x=x)['precip'].values.astype(np.float32)\n",
        "\n",
        "        feats=[]\n",
        "        ph = self.scp.transform(win['precip'].values.reshape(-1,1)).flatten()\n",
        "        feats.append(ph)\n",
        "\n",
        "        for var in self.features:\n",
        "            if var.startswith('lag') or var=='precip_hist':  # already included\n",
        "                continue\n",
        "            if var in win:\n",
        "                arr = win[var].values\n",
        "                arr = self.scx.transform(arr.reshape(-1,1)).flatten()\n",
        "                feats.append(arr)\n",
        "\n",
        "        X = np.concatenate(feats).astype(np.float32)\n",
        "        return torch.tensor(X), torch.tensor(tgt)\n",
        "\n",
        "def build_dataloaders(val_year, use_lags, batch_size=BATCH_SIZE):\n",
        "    ds = xr.open_dataset(DATASET_PATH)\n",
        "    ds = add_time_encodings(ds)\n",
        "\n",
        "    train_start = np.datetime64(f'{val_year-4}-01-01')\n",
        "    train_end   = np.datetime64(f'{val_year-1}-12-31')\n",
        "    val_start   = np.datetime64(f'{val_year}-01-01')\n",
        "    val_end     = np.datetime64(f'{val_year}-12-31')\n",
        "\n",
        "    train_mask = (ds['time']>=train_start)&(ds['time']<=train_end)\n",
        "    val_mask   = (ds['time']>=val_start)&(ds['time']<=val_end)\n",
        "\n",
        "    sc_p = RobustScaler().fit(ds['precip'].where(train_mask).values.reshape(-1,1))\n",
        "    preds=[]\n",
        "    for var in ['month_sin','month_cos','doy_sin','doy_cos','elevation',\n",
        "               'slope','roughness','curvature','alt_cluster']:\n",
        "        if var in ds.data_vars:  # Make sure variable exists in dataset\n",
        "            preds.append(ds[var].where(train_mask).values.flatten())\n",
        "    sc_x = StandardScaler().fit(np.concatenate(preds).reshape(-1,1))\n",
        "\n",
        "    def make_idx(mask):\n",
        "        idx=[]\n",
        "        for t in range(INPUT_WINDOW, len(ds['time'])-HORIZON):\n",
        "            if mask[t+HORIZON-1]:\n",
        "                for y in range(ds.dims['y']):\n",
        "                    for x in range(ds.dims['x']):\n",
        "                        idx.append((t,y,x))\n",
        "        return idx\n",
        "\n",
        "    train_idx = make_idx(train_mask)\n",
        "    val_idx   = make_idx(val_mask)\n",
        "\n",
        "    feats = BASE_FEATURES.copy()\n",
        "    if not use_lags:\n",
        "        feats = [f for f in feats if not f.startswith('lag')]\n",
        "\n",
        "    train_ds = PrecipDataset(ds, train_idx, INPUT_WINDOW, HORIZON, sc_p, sc_x, feats)\n",
        "    val_ds   = PrecipDataset(ds, val_idx, INPUT_WINDOW, HORIZON, sc_p, sc_x, feats)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
        "                              num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    return train_loader, val_loader, len(feats)\n",
        "\n",
        "\n",
        "# ‚ñ∂Ô∏è Model definitions\n",
        "class GRUEncoderDecoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size=128, num_layers=2, dropout=0.2, horizon=HORIZON):\n",
        "        super().__init__()\n",
        "        self.enc = nn.GRU(input_dim, hidden_size, num_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "        self.dec = nn.GRU(1, hidden_size, num_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "        self.fc  = nn.Linear(hidden_size,1)\n",
        "        self.hor = horizon\n",
        "\n",
        "    def forward(self, x, teacher_forcing_ratio=0.5, y=None):\n",
        "        _, h = self.enc(x)\n",
        "        dec_in = x[:, -1:, 0:1]\n",
        "        outs=[]\n",
        "        for t in range(self.hor):\n",
        "            o, h = self.dec(dec_in, h)\n",
        "            pred = self.fc(o.squeeze(1))\n",
        "            outs.append(pred)\n",
        "            if self.training and y is not None and torch.rand(1)<teacher_forcing_ratio:\n",
        "                dec_in = y[:, t:t+1].unsqueeze(-1)\n",
        "            else:\n",
        "                dec_in = pred.unsqueeze(1)\n",
        "        return torch.stack(outs, dim=1)\n",
        "\n",
        "# Implementation aligned with documentation\n",
        "class Conv3DAutoEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=3, bottleneck_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=2),\n",
        "            nn.Conv3d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=2),\n",
        "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*6*6*6, bottleneck_dim)  # Adjust dimensions based on your input\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class AEFusionGRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size=128, num_layers=2, dropout=0.2, horizon=HORIZON):\n",
        "        super().__init__()\n",
        "        self.ae = Conv3DAutoEncoder(in_channels=3, bottleneck_dim=64)\n",
        "        \n",
        "        # Combined dim: original features + bottleneck\n",
        "        combined_dim = input_dim + 64\n",
        "        \n",
        "        self.backbone = GRUEncoderDecoder(combined_dim, hidden_size, num_layers, dropout, horizon)\n",
        "    \n",
        "    def forward(self, x, teacher_forcing_ratio=0.5, y=None):\n",
        "        # Assuming x_imfs is processed elsewhere and passed with x\n",
        "        # This is a placeholder for the actual implementation\n",
        "        ae_features = torch.zeros((x.size(0), 64), device=x.device)\n",
        "        \n",
        "        # Concatenate features\n",
        "        combined = torch.cat([x, ae_features.unsqueeze(1).expand(-1, x.size(1), -1)], dim=2)\n",
        "        \n",
        "        return self.backbone(combined, teacher_forcing_ratio, y)\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(hidden_dim, n_heads, dropout=dropout)\n",
        "        self.norm = nn.LayerNormalization(hidden_dim)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n",
        "        return self.norm(x + attn_out)\n",
        "\n",
        "class AEFusionGRUT(AEFusionGRU):\n",
        "    def __init__(self, input_dim, hidden_size=128, num_layers=2, dropout=0.2, horizon=HORIZON):\n",
        "        super().__init__(input_dim, hidden_size, num_layers, dropout, horizon)\n",
        "        self.attention = MultiHeadAttentionLayer(hidden_size, n_heads=4, dropout=dropout)\n",
        "        \n",
        "    def forward(self, x, teacher_forcing_ratio=0.5, y=None):\n",
        "        # Similar implementation as AEFusionGRU but with attention\n",
        "        # This is placeholder for the actual implementation with attention\n",
        "        return super().forward(x, teacher_forcing_ratio, y)\n",
        "\n",
        "class AEFusionGRUTMask(AEFusionGRUT):\n",
        "    def __init__(self, input_dim, hidden_size=128, num_layers=2, dropout=0.2, horizon=HORIZON):\n",
        "        super().__init__(input_dim, hidden_size, num_layers, dropout, horizon)\n",
        "        \n",
        "    def forward(self, x, teacher_forcing_ratio=0.5, y=None):\n",
        "        # Similar implementation but with causal masking for attention\n",
        "        # This is placeholder for the actual implementation with causal masking\n",
        "        return super().forward(x, teacher_forcing_ratio, y)\n",
        "\n",
        "# Update MODEL_FACTORY with proper implementations\n",
        "MODEL_FACTORY = {\n",
        "    'gru_ed': GRUEncoderDecoder,\n",
        "    'ae_fusion_gru': AEFusionGRU,\n",
        "    'ae_fusion_gru_t': AEFusionGRUT,\n",
        "    'ae_fusion_gru_t_mask': AEFusionGRUTMask,\n",
        "}\n",
        "\n",
        "\n",
        "# ‚ñ∂Ô∏è Training utilities\n",
        "from torchmetrics.functional import mean_squared_error\n",
        "def huber_weighted(preds, target):\n",
        "    h = torch.arange(1, target.size(1)+1, device=preds.device).float()\n",
        "    weights = 1 + h/12.0\n",
        "    loss = torch.nn.functional.huber_loss(preds, target, reduction='none')\n",
        "    return (loss*weights).mean()\n",
        "\n",
        "def train_one_epoch(model, loader, opt, tf_ratio, scheduler=None):\n",
        "    model.train()\n",
        "    losses=[]\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
        "        preds = model(X, teacher_forcing_ratio=tf_ratio, y=y)\n",
        "        loss = huber_weighted(preds, y)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    rmses=[]\n",
        "    for X,y in loader:\n",
        "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
        "        preds = model(X, teacher_forcing_ratio=0.0)\n",
        "        rmse = mean_squared_error(preds, y, squared=False)\n",
        "        rmses.append(rmse.item())\n",
        "    return np.mean(rmses)\n",
        "\n",
        "# Actualizar la funci√≥n de entrenamiento con visualizaci√≥n de curvas de aprendizaje\n",
        "def train_with_history(model, train_loader, val_loader, epochs=60, patience=20, \n",
        "                      lr=1e-3, weight_decay=1e-4, fold='', exp_name=''):\n",
        "    \"\"\"\n",
        "    Entrena el modelo con captura de historial para curvas de aprendizaje\n",
        "    \n",
        "    Args:\n",
        "        model: Modelo PyTorch a entrenar\n",
        "        train_loader: DataLoader de entrenamiento\n",
        "        val_loader: DataLoader de validaci√≥n\n",
        "        epochs: N√∫mero m√°ximo de √©pocas\n",
        "        patience: √âpocas para early stopping\n",
        "        lr: Tasa de aprendizaje\n",
        "        weight_decay: Par√°metro de regularizaci√≥n\n",
        "        fold: Identificador del fold para logs\n",
        "        exp_name: Nombre del experimento para logs\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (mejor_modelo, historial_entrenamiento, mejor_rmse)\n",
        "    \"\"\"\n",
        "    print_progress(f\"Iniciando entrenamiento de {exp_name} en fold {fold}\", is_start=True)\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=lr, total_steps=epochs*len(train_loader),\n",
        "                         pct_start=0.3, anneal_strategy='cos')\n",
        "    \n",
        "    # Inicializar historial para curvas de aprendizaje\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_rmse': [],\n",
        "        'learning_rate': [],\n",
        "        'teacher_forcing': []\n",
        "    }\n",
        "    \n",
        "    best_rmse = float('inf')\n",
        "    best_model_state = None\n",
        "    counter = 0\n",
        "    \n",
        "    for epoch in range(1, epochs+1):\n",
        "        # Calcular teacher forcing ratio con decaimiento coseno (0.7‚Üí0.3)\n",
        "        tf_ratio = 0.7 - (epoch-1)*(0.4)/(epochs-1)\n",
        "        history['teacher_forcing'].append(tf_ratio)\n",
        "        \n",
        "        # Entrenamiento\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X, teacher_forcing_ratio=tf_ratio, y=y)\n",
        "            loss = huber_weighted(preds, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            train_losses.append(loss.item())\n",
        "            \n",
        "        # Obtener learning rate actual\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        history['learning_rate'].append(current_lr)\n",
        "        \n",
        "        # Evaluaci√≥n\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        val_rmses = []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                preds = model(X, teacher_forcing_ratio=0)\n",
        "                val_loss = huber_weighted(preds, y).item()\n",
        "                val_rmse = mean_squared_error(preds, y, squared=False).item()\n",
        "                val_losses.append(val_loss)\n",
        "                val_rmses.append(val_rmse)\n",
        "        \n",
        "        # Actualizar historia\n",
        "        epoch_train_loss = np.mean(train_losses)\n",
        "        epoch_val_loss = np.mean(val_losses)\n",
        "        epoch_val_rmse = np.mean(val_rmses)\n",
        "        \n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['val_rmse'].append(epoch_val_rmse)\n",
        "        \n",
        "        # Imprimir progreso\n",
        "        print(f\"√âpoca {epoch}/{epochs} - Train loss: {epoch_train_loss:.4f} - Val RMSE: {epoch_val_rmse:.4f} - LR: {current_lr:.6f}\")\n",
        "        \n",
        "        # Comprobar early stopping (‚àÜRMSE < 1%)\n",
        "        if epoch_val_rmse < best_rmse * 0.99:  # Mejora de al menos 1%\n",
        "            best_rmse = epoch_val_rmse\n",
        "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            print_progress(f\"√âpoca {epoch}: Nuevo mejor modelo con RMSE {best_rmse:.4f}\", level=1)\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "        \n",
        "        if counter >= patience:\n",
        "            print_progress(f\"Early stopping en √©poca {epoch}\", level=1)\n",
        "            break\n",
        "    \n",
        "    # Restaurar mejor modelo\n",
        "    model.load_state_dict(best_model_state)\n",
        "    \n",
        "    # Visualizar curvas de aprendizaje\n",
        "    plot_learning_curves(history, exp_name, fold)\n",
        "    \n",
        "    print_progress(f\"Entrenamiento de {exp_name} en fold {fold} completado. Mejor RMSE: {best_rmse:.4f}\", is_end=True)\n",
        "    \n",
        "    # Guardar modelo\n",
        "    torch.save(model.state_dict(), MODEL_DIR / f\"{exp_name}_{fold}_model.pt\")\n",
        "    \n",
        "    return model, history, best_rmse\n",
        "\n",
        "def plot_learning_curves(history, exp_name, fold):\n",
        "    \"\"\"\n",
        "    Genera visualizaciones de curvas de aprendizaje durante el entrenamiento\n",
        "    \n",
        "    Args:\n",
        "        history: Diccionario con historiales de entrenamiento\n",
        "        exp_name: Nombre del experimento\n",
        "        fold: ID del fold\n",
        "    \"\"\"\n",
        "    curves_dir = IMAGE_DIR / \"learning_curves\"\n",
        "    curves_dir.mkdir(exist_ok=True, parents=True)\n",
        "    \n",
        "    fig = plt.figure(figsize=(16, 12))\n",
        "    gs = GridSpec(2, 2, figure=fig)\n",
        "    \n",
        "    # 1. P√©rdida de entrenamiento y validaci√≥n\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    ax1.plot(history['train_loss'], label='Entrenamiento', color='#3498db', linewidth=2)\n",
        "    if 'val_loss' in history and len(history['val_loss']) > 0:\n",
        "        ax1.plot(history['val_loss'], label='Validaci√≥n', color='#e74c3c', linewidth=2)\n",
        "    ax1.set_title('P√©rdida durante entrenamiento', fontsize=14)\n",
        "    ax1.set_xlabel('√âpoca', fontsize=12)\n",
        "    ax1.set_ylabel('P√©rdida', fontsize=12)\n",
        "    ax1.grid(alpha=0.3)\n",
        "    ax1.legend(fontsize=12)\n",
        "    \n",
        "    # 2. RMSE de validaci√≥n\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    if 'val_rmse' in history and len(history['val_rmse']) > 0:\n",
        "        ax2.plot(history['val_rmse'], color='#9b59b6', linewidth=2)\n",
        "        min_rmse = min(history['val_rmse'])\n",
        "        min_epoch = history['val_rmse'].index(min_rmse)\n",
        "        ax2.scatter(min_epoch, min_rmse, c='red', s=100, zorder=10, label=f'Mejor: {min_rmse:.4f}')\n",
        "    ax2.set_title('RMSE de validaci√≥n', fontsize=14)\n",
        "    ax2.set_xlabel('√âpoca', fontsize=12)\n",
        "    ax2.set_ylabel('RMSE', fontsize=12)\n",
        "    ax2.grid(alpha=0.3)\n",
        "    ax2.legend(fontsize=12)\n",
        "    \n",
        "    # 3. Tasa de aprendizaje y Teacher Forcing\n",
        "    ax3 = fig.add_subplot(gs[1, 0])\n",
        "    if 'learning_rate' in history and len(history['learning_rate']) > 0:\n",
        "        ax3.plot(history['learning_rate'], color='#2ecc71', linewidth=2)\n",
        "        ax3.set_title('Tasa de aprendizaje (OneCycleLR)', fontsize=14)\n",
        "        ax3.set_xlabel('√âpoca', fontsize=12)\n",
        "        ax3.set_ylabel('Learning Rate', fontsize=12)\n",
        "        ax3.set_yscale('log')\n",
        "        ax3.grid(alpha=0.3)\n",
        "    \n",
        "    ax4 = fig.add_subplot(gs[1, 1])\n",
        "    if 'teacher_forcing' in history and len(history['teacher_forcing']) > 0:\n",
        "        ax4.plot(history['teacher_forcing'], color='#f39c12', linewidth=2)\n",
        "        ax4.set_title('Teacher Forcing Ratio (0.7 ‚Üí 0.3)', fontsize=14)\n",
        "        ax4.set_xlabel('√âpoca', fontsize=12)\n",
        "        ax4.set_ylabel('Teacher Forcing', fontsize=12)\n",
        "        ax4.set_ylim(0, 1)\n",
        "        ax4.grid(alpha=0.3)\n",
        "    \n",
        "    plt.suptitle(f'{exp_name} - Fold {fold}', fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
        "    plt.savefig(curves_dir / f'{exp_name}_{fold}_learning_curves.png', dpi=100, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    \n",
        "    print_progress(f\"Curvas de aprendizaje guardadas en: {curves_dir / f'{exp_name}_{fold}_learning_curves.png'}\", level=1)\n",
        "\n",
        "# ‚ñ∂Ô∏è Main experiment loop con curvas de aprendizaje y visualizaci√≥n\n",
        "RESULTS = []\n",
        "ALL_HISTORIES = {}\n",
        "ALL_MODELS = {}\n",
        "\n",
        "# Crear carpeta para m√©tricas agregadas\n",
        "metrics_dir = MODEL_DIR / \"metrics\"\n",
        "metrics_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "for exp_name, cfg in EXPERIMENTS.items():\n",
        "    print_progress(f\"Ejecutando experimento: {exp_name}\", is_start=True)\n",
        "    exp_histories = {}\n",
        "    exp_models = {}\n",
        "    exp_metrics = []\n",
        "    \n",
        "    for fold, val_year in FOLDS.items():\n",
        "        print_progress(f\"Procesando fold {fold} (validaci√≥n: {val_year})\", level=1)\n",
        "        \n",
        "        # Construir dataloaders\n",
        "        train_loader, val_loader, in_dim = build_dataloaders(val_year, cfg['use_lags'])\n",
        "        \n",
        "        # Ajustar dropout seg√∫n documentaci√≥n (0.25 para F4-F5, 0.20 para los dem√°s)\n",
        "        dropout = 0.25 if fold in ['F4', 'F5'] else 0.20\n",
        "        print_progress(f\"Usando dropout={dropout} para fold {fold}\", level=2)\n",
        "        \n",
        "        # Crear y entrenar modelo con seguimiento de historia\n",
        "        model = MODEL_FACTORY[cfg['model']](in_dim, dropout=dropout).to(DEVICE)\n",
        "        model, history, best_rmse = train_with_history(\n",
        "            model, train_loader, val_loader, \n",
        "            epochs=60, patience=20, \n",
        "            lr=1e-3, weight_decay=1e-4,\n",
        "            fold=fold, exp_name=exp_name\n",
        "        )\n",
        "        \n",
        "        # Guardar resultados\n",
        "        RESULTS.append({\n",
        "            'exp': exp_name,\n",
        "            'fold': fold,\n",
        "            'rmse': best_rmse\n",
        "        })\n",
        "        \n",
        "        # Almacenar modelo e historial\n",
        "        exp_histories[fold] = history\n",
        "        exp_models[fold] = model\n",
        "        \n",
        "        # Generar visualizaci√≥n de predicciones si est√° implementada prepare_grid_data\n",
        "        try:\n",
        "            # Descomentar las siguientes l√≠neas cuando prepare_grid_data est√© implementada\n",
        "            # visualize_predictions(model, xr.open_dataset(DATASET_PATH), val_year, exp_name, fold)\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            print_progress(f\"Error en visualizaci√≥n: {str(e)}\", level=1)\n",
        "    \n",
        "    # Almacenar historias y modelos\n",
        "    ALL_HISTORIES[exp_name] = exp_histories\n",
        "    ALL_MODELS[exp_name] = exp_models\n",
        "    \n",
        "    print_progress(f\"Experimento {exp_name} completado\", is_end=True)\n",
        "\n",
        "# ‚ñ∂Ô∏è Visualizar tabla de resultados\n",
        "df = pd.DataFrame(RESULTS)\n",
        "pivot_table = df.pivot(index='exp', columns='fold', values='rmse')\n",
        "print_progress(\"Resumen de resultados RMSE:\", is_start=True)\n",
        "display(pivot_table)\n",
        "\n",
        "plt.title('Comparaci√≥n de RMSE por experimento y fold', fontsize=14)\n",
        "plt.xlabel('Experimento')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(IMAGE_DIR / \"experiment_comparison.png\", dpi=100)\n",
        "plt.show()\n",
        "\n",
        "def visualize_predictions(model, dataset, val_year, exp_name, fold, scalers=None):\n",
        "    \"\"\"\n",
        "    Genera mapas de predicciones y errores MAPE para los 12 meses de validaci√≥n\n",
        "    \n",
        "    Args:\n",
        "        model: Modelo entrenado\n",
        "        dataset: Dataset xarray completo\n",
        "        val_year: A√±o de validaci√≥n\n",
        "        exp_name: Nombre del experimento\n",
        "        fold: ID del fold\n",
        "        scalers: Tuple (sc_p, sc_x) de escaladores para transformar datos\n",
        "    \"\"\"\n",
        "    print_progress(f\"Generando visualizaciones para {exp_name}, fold {fold}\", is_start=True)\n",
        "    \n",
        "    # Preparar directorio para guardar visualizaciones\n",
        "    vis_dir = IMAGE_DIR / f\"{exp_name}_{fold}_maps\"\n",
        "    vis_dir.mkdir(exist_ok=True, parents=True)\n",
        "    \n",
        "    # Obtener meses del per√≠odo de validaci√≥n\n",
        "    months = pd.date_range(f\"{val_year}-01-01\", f\"{val_year}-12-31\", freq='MS')\n",
        "    month_names = ['Ene', 'Feb', 'Mar', 'Abr', 'May', 'Jun', 'Jul', 'Ago', 'Sep', 'Oct', 'Nov', 'Dic']\n",
        "    \n",
        "    # Extraer coordenadas\n",
        "    lats = dataset.latitude.values\n",
        "    lons = dataset.longitude.values\n",
        "    \n",
        "    # Crear matrices para almacenar resultados\n",
        "    predictions = np.zeros((len(months), len(lats), len(lons)))\n",
        "    true_values = np.zeros((len(months), len(lats), len(lons)))\n",
        "    mape_values = np.zeros((len(months), len(lats), len(lons)))\n",
        "    \n",
        "    # Obtener √≠ndices de tiempos para validaci√≥n\n",
        "    val_times = dataset['time'].sel(time=slice(f\"{val_year}-01-01\", f\"{val_year}-12-31\")).values\n",
        "    \n",
        "    # Configure plots size\n",
        "    plt.rcParams['figure.figsize'] = (20, 10)\n",
        "    \n",
        "    # Generar predicciones para cada punto de grilla\n",
        "    print_progress(f\"Generando predicciones\", level=1)\n",
        "    \n",
        "    # Esta secci√≥n depende de c√≥mo est√©n organizados tus datos\n",
        "    # Ejemplo simplificado usando una funci√≥n helper\n",
        "    input_tensor, target_tensor = prepare_grid_data(dataset, val_year, INPUT_WINDOW, HORIZON)\n",
        "    \n",
        "    # Hacer predicciones\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        preds = model(input_tensor.to(DEVICE)).cpu().numpy()\n",
        "    \n",
        "    # Desescalar predicciones si tenemos los escaladores\n",
        "    if scalers:\n",
        "        sc_p, _ = scalers\n",
        "        preds = sc_p.inverse_transform(preds.reshape(-1, HORIZON)).reshape(-1, len(lats), len(lons), HORIZON)\n",
        "        # Y reacomodar ejes para formato (month, lat, lon)\n",
        "        preds = np.moveaxis(preds, 3, 0)\n",
        "    \n",
        "    # Tambi√©n necesitamos extraer los valores reales y reacomodar\n",
        "    true_vals = target_tensor.numpy().reshape(-1, len(lats), len(lons), HORIZON)\n",
        "    true_vals = np.moveaxis(true_vals, 3, 0)\n",
        "    \n",
        "    # Calcular MAPE\n",
        "    for m in range(HORIZON):\n",
        "        valid_mask = true_vals[m] > 0.1  # Evitar divisiones por ~0\n",
        "        mape_values[m, valid_mask] = np.abs((preds[m, valid_mask] - true_vals[m, valid_mask]) / true_vals[m, valid_mask]) * 100\n",
        "    \n",
        "    # Visualizar mapas para cada mes\n",
        "    print_progress(f\"Generando mapas mensuales\", level=1)\n",
        "    \n",
        "    for m in range(HORIZON):\n",
        "        fig = plt.figure(figsize=(18, 10))\n",
        "        plt.suptitle(f\"{exp_name} - {fold} - {month_names[m]} {val_year}\", fontsize=16)\n",
        "        \n",
        "        # Preparar l√≠mites para colorbar\n",
        "        vmin_pred = np.nanpercentile(true_vals, 1)\n",
        "        vmax_pred = np.nanpercentile(true_vals, 99)\n",
        "        vmin_mape = 0\n",
        "        vmax_mape = min(100, np.nanpercentile(mape_values, 95))\n",
        "        \n",
        "        # Crear grid para lat/lon\n",
        "        lon2d, lat2d = np.meshgrid(lons, lats)\n",
        "        \n",
        "        # Plot de predicci√≥n\n",
        "        ax1 = plt.subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
        "        ax1.set_title(f\"Precipitaci√≥n Predicha (mm)\")\n",
        "        pcm = ax1.pcolormesh(lon2d, lat2d, preds[m], cmap='Blues', \n",
        "                           vmin=vmin_pred, vmax=vmax_pred, \n",
        "                           transform=ccrs.PlateCarree())\n",
        "        ax1.coastlines(resolution='10m')\n",
        "        ax1.add_feature(cfeature.BORDERS, linestyle=':')\n",
        "        gl = ax1.gridlines(draw_labels=True, linewidth=0.5)\n",
        "        gl.top_labels = False\n",
        "        gl.right_labels = False\n",
        "        plt.colorbar(pcm, ax=ax1, shrink=0.7, label='mm')\n",
        "        \n",
        "        # Plot de MAPE\n",
        "        ax2 = plt.subplot(1, 2, 2, projection=ccrs.PlateCarree())\n",
        "        ax2.set_title(f\"Error MAPE (%)\")\n",
        "        pcm2 = ax2.pcolormesh(lon2d, lat2d, mape_values[m], cmap='Reds', \n",
        "                             vmin=vmin_mape, vmax=vmax_mape, \n",
        "                             transform=ccrs.PlateCarree())\n",
        "        ax2.coastlines(resolution='10m')\n",
        "        ax2.add_feature(cfeature.BORDERS, linestyle=':')\n",
        "        gl = ax2.gridlines(draw_labels=True, linewidth=0.5)\n",
        "        gl.top_labels = False\n",
        "        gl.right_labels = False\n",
        "        plt.colorbar(pcm2, ax=ax2, shrink=0.7, label='%')\n",
        "        \n",
        "        # Guardar figura\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "        fig.savefig(vis_dir / f\"map_{month_names[m]}.png\", dpi=120, bbox_inches='tight')\n",
        "        plt.close(fig)\n",
        "    \n",
        "    # Generar visualizaci√≥n resumida (promedio)\n",
        "    print_progress(f\"Generando mapa resumen\", level=1)\n",
        "    \n",
        "    # Calcular promedios\n",
        "    avg_pred = np.nanmean(preds, axis=0)\n",
        "    avg_true = np.nanmean(true_vals, axis=0)\n",
        "    avg_mape = np.nanmean(mape_values, axis=0)\n",
        "    \n",
        "    # Plot resumen\n",
        "    fig = plt.figure(figsize=(18, 10))\n",
        "    plt.suptitle(f\"{exp_name} - {fold} - Promedio Anual {val_year}\", fontsize=16)\n",
        "    \n",
        "    # Plot predicci√≥n promedio\n",
        "    ax1 = plt.subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
        "    ax1.set_title(f\"Precipitaci√≥n Media Anual (mm)\")\n",
        "    pcm = ax1.pcolormesh(lon2d, lat2d, avg_pred, cmap='Blues', transform=ccrs.PlateCarree())\n",
        "    ax1.coastlines(resolution='10m')\n",
        "    ax1.add_feature(cfeature.BORDERS, linestyle=':')\n",
        "    gl = ax1.gridlines(draw_labels=True, linewidth=0.5)\n",
        "    gl.top_labels = False\n",
        "    gl.right_labels = False\n",
        "    plt.colorbar(pcm, ax=ax1, shrink=0.7, label='mm')\n",
        "    \n",
        "    # Plot MAPE promedio\n",
        "    ax2 = plt.subplot(1, 2, 2, projection=ccrs.PlateCarree())\n",
        "    ax2.set_title(f\"MAPE Promedio (%)\")\n",
        "    pcm2 = ax2.pcolormesh(lon2d, lat2d, avg_mape, cmap='Reds', \n",
        "                         vmin=0, vmax=min(100, np.nanpercentile(avg_mape, 95)), \n",
        "                         transform=ccrs.PlateCarree())\n",
        "    ax2.coastlines(resolution='10m')\n",
        "    ax2.add_feature(cfeature.BORDERS, linestyle=':')\n",
        "    gl = ax2.gridlines(draw_labels=True, linewidth=0.5)\n",
        "    gl.top_labels = False\n",
        "    gl.right_labels = False\n",
        "    plt.colorbar(pcm2, ax=ax2, shrink=0.7, label='%')\n",
        "    \n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    fig.savefig(vis_dir / f\"map_annual_summary.png\", dpi=120, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    \n",
        "    # Gr√°fico de RMSE por horizonte (1-12)\n",
        "    rmse_by_horizon = [np.sqrt(np.nanmean((preds[h] - true_vals[h])**2)) for h in range(HORIZON)]\n",
        "    \n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, HORIZON+1), rmse_by_horizon, marker='o', linewidth=2)\n",
        "    plt.title(f\"{exp_name} - {fold} - RMSE por Horizonte\", fontsize=14)\n",
        "    plt.xlabel('Horizonte de Predicci√≥n (meses)', fontsize=12)\n",
        "    plt.ylabel('RMSE', fontsize=12)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.xticks(range(1, HORIZON+1))\n",
        "    plt.tight_layout()\n",
        "    fig.savefig(vis_dir / f\"rmse_by_horizon.png\", dpi=120)\n",
        "    plt.close(fig)\n",
        "    \n",
        "    print_progress(f\"Visualizaciones guardadas en {vis_dir}\", is_end=True)\n",
        "    return preds, true_vals, mape_values\n",
        "\n",
        "# Funci√≥n auxiliar para preparar datos en formato de grilla\n",
        "def prepare_grid_data(dataset, val_year, input_window, horizon):\n",
        "    \"\"\"\n",
        "    Prepara datos de entrada y objetivo para predicciones en grilla\n",
        "    \n",
        "    Esta funci√≥n es un placeholder - necesitar√°s implementarla seg√∫n\n",
        "    tu estructura espec√≠fica de datos\n",
        "    \"\"\"\n",
        "    print_progress(\"Esta funci√≥n necesita implementaci√≥n espec√≠fica para el dataset!\", level=2)\n",
        "    # Placeholder - devuelve tensores vac√≠os\n",
        "    return torch.zeros((1, input_window, 10)), torch.zeros((1, horizon))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "precipitation_prediction",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
