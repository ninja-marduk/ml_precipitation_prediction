{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fpreprocessing-techniques/notebooks/preprocessing_techniques_parallel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20481089",
      "metadata": {
        "id": "20481089"
      },
      "source": [
        "# Configuraci√≥n para ejecuci√≥n local y en Google Colab\n",
        "Este notebook est√° dise√±ado para ejecutarse tanto en un entorno local como en Google Colab. A continuaci√≥n, se incluyen las configuraciones necesarias para ambos entornos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fb418c0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fb418c0d",
        "outputId": "cbbfd712-bbee-447e-98c0-2c82b1674f8d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-69e05db21346>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mIN_COLAB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Si estamos en Colab, clonar el repositorio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Configuraci√≥n del entorno (compatible con Colab y local)\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# Detectar si estamos en Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Si estamos en Colab, clonar el repositorio\n",
        "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
        "    %cd ml_precipitation_prediction\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn tqdm PyEMD pyemd EMD-signal emd PyWavelets acstools\n",
        "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
        "else:\n",
        "    # Si estamos en local, usar la ruta actual\n",
        "    if '/notebooks' in os.getcwd():\n",
        "        BASE_PATH = Path('..')\n",
        "    else:\n",
        "        BASE_PATH = Path('.')\n",
        "\n",
        "print(f\"Entorno configurado. Usando ruta base: {BASE_PATH}\")\n",
        "\n",
        "# Si BASE_PATH viene como string, lo convertimos\n",
        "BASE_PATH = Path(BASE_PATH)\n",
        "\n",
        "# Ahora puedes concatenar correctamente\n",
        "model_output_dir = BASE_PATH / 'models' / 'output'\n",
        "model_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation.nc'\n",
        "print(f\"Buscando archivo en: {data_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca01a94",
      "metadata": {
        "id": "eca01a94"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PyEMD import EMD\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "\n",
        "# üìÇ Cargar dataset\n",
        "ds = xr.open_dataset(data_file)\n",
        "precip = ds[\"total_precipitation\"]\n",
        "df = precip.to_dataframe().reset_index()\n",
        "df[\"date\"] = pd.to_datetime(df[\"time\"])\n",
        "df_monthly = df.groupby(\"date\")[\"total_precipitation\"].mean().interpolate()\n",
        "\n",
        "# üß† Descomposici√≥n EMD\n",
        "signal = df_monthly.values\n",
        "emd = EMD()\n",
        "imfs = emd.emd(signal)\n",
        "\n",
        "# üî¢ Obtener rango de a√±os\n",
        "start_year = df_monthly.index.min().year\n",
        "end_year = df_monthly.index.max().year\n",
        "\n",
        "# üé® Graficar EMD (versi√≥n corregida de layout)\n",
        "fig, axes = plt.subplots(\n",
        "    imfs.shape[0] + 1, 1,\n",
        "    figsize=(16, 3 * (imfs.shape[0] + 1)),\n",
        "    sharex=True\n",
        ")\n",
        "\n",
        "# Ponemos el t√≠tulo general un poco m√°s alto\n",
        "fig.suptitle(\n",
        "    f\"Descomposici√≥n EMD de la precipitaci√≥n mensual en Boyac√° ({start_year}‚Äì{end_year})\",\n",
        "    fontsize=18,\n",
        "    y=0.97      # subir el suptitle\n",
        ")\n",
        "\n",
        "# Ajustamos espacio vertical entre subplots y arriba\n",
        "fig.subplots_adjust(\n",
        "    top=0.90,  # reservamos el 10% superior para el suptitle\n",
        "    hspace=0.4  # aumentamos el espacio entre filas\n",
        ")\n",
        "\n",
        "# --- resto id√©ntico ---\n",
        "# Serie original\n",
        "axes[0].plot(df_monthly.index, signal, color='black', label=\"Serie original\")\n",
        "axes[0].set_ylabel(\"mm\", fontsize=12)\n",
        "axes[0].set_title(\"Serie original de precipitaci√≥n mensual\", fontsize=14)\n",
        "axes[0].legend(loc=\"upper right\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "# IMFs\n",
        "for i in range(imfs.shape[0]):\n",
        "    axes[i+1].plot(df_monthly.index, imfs[i], label=f\"IMF {i+1}\")\n",
        "    axes[i+1].set_title(f\"IMF {i+1} - Componente Intr√≠nseca de Modo\", fontsize=12)\n",
        "    axes[i+1].set_ylabel(\"mm\", fontsize=10)\n",
        "    axes[i+1].grid(True)\n",
        "    axes[i+1].legend(loc=\"upper right\")\n",
        "\n",
        "# Eje de fechas\n",
        "axes[-1].xaxis.set_major_locator(mdates.YearLocator(base=5))\n",
        "axes[-1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "plt.setp(axes[-1].xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
        "\n",
        "plt.xlabel(\"A√±o\", fontsize=13)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b45b75f7",
      "metadata": {
        "id": "b45b75f7"
      },
      "outputs": [],
      "source": [
        "# CELDA 1: Procesamiento paralelo usando joblib para CEEMDAN y TVF-EMD y guardar archivos independientes .nc\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PyEMD import CEEMDAN\n",
        "from emd import sift\n",
        "import pywt\n",
        "from tqdm import tqdm\n",
        "import contextlib\n",
        "import io\n",
        "from joblib import Parallel, delayed\n",
        "import seaborn as sns\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Cargar dataset base\n",
        "# ----------------------------\n",
        "DATASET_PATH = data_file\n",
        "ds = xr.open_dataset(DATASET_PATH)\n",
        "precip = ds[\"total_precipitation\"]\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Estad√≠sticas mensuales\n",
        "# ----------------------------\n",
        "time_diffs = np.diff(precip.time.values) / np.timedelta64(1, 'D')\n",
        "monthly_sum = precip if np.median(time_diffs) > 2 else precip.resample(time='1M').sum(dim='time')\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Funciones auxiliares\n",
        "# ----------------------------\n",
        "latitudes = monthly_sum.latitude.values\n",
        "longitudes = monthly_sum.longitude.values\n",
        "ceemdan = CEEMDAN()\n",
        "\n",
        "def wavelet_energy(signal, wavelet='db4', level=4):\n",
        "    coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
        "    return [np.sum(np.square(c)) for c in coeffs]\n",
        "\n",
        "def compute_imfs_and_energy(ts, method='CEEMDAN'):\n",
        "    with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
        "        imfs = sift.sift(ts, max_imfs=8) if method == 'TVF-EMD' else ceemdan.ceemdan(ts)\n",
        "    energy_by_imf = [wavelet_energy(imf, level=4) for imf in imfs]\n",
        "    return imfs, energy_by_imf, len(imfs)\n",
        "\n",
        "def classify_frequency(row):\n",
        "    if row['L1'] > max(row['L2'], row['L3'], row['L4'], row['Approx']):\n",
        "        return 'Alta'\n",
        "    elif row['L2'] > row['L3'] and row['L2'] > row['Approx']:\n",
        "        return 'Media'\n",
        "    else:\n",
        "        return 'Baja'\n",
        "\n",
        "# Versi√≥n modificada de la funci√≥n para evitar visualizaciones dentro del proceso paralelo\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Procesamiento por m√©todo y por CHUNKS (con resume)\n",
        "# ----------------------------\n",
        "def process_method_chunked(method, chunk_latitudes):\n",
        "    # path de salida basado en m√©todo y primer latitud del chunk\n",
        "    out_fname = f\"features_{method.lower()}_chunk_{chunk_latitudes[0]:.6f}.nc\"\n",
        "    out_path = os.path.join(model_output_dir, out_fname)\n",
        "\n",
        "    # Si ya existe, lo omitimos (resume)\n",
        "    if os.path.exists(out_path):\n",
        "        print(f\"üëÄ Ya existe {out_fname}, se omite chunk {chunk_latitudes[0]:.6f}-{chunk_latitudes[-1]:.6f}\")\n",
        "        return out_path\n",
        "\n",
        "    records = []\n",
        "    for lat in tqdm(chunk_latitudes, desc=f\"{method}\"):\n",
        "        for lon in longitudes:\n",
        "            ts = monthly_sum.sel(latitude=lat, longitude=lon).values\n",
        "            if np.any(np.isnan(ts)):\n",
        "                continue\n",
        "            imfs, energy, n_modes = compute_imfs_and_energy(ts, method=method)\n",
        "            for idx, e in enumerate(energy):\n",
        "                if len(e) < 5:\n",
        "                    continue\n",
        "                records.append({\n",
        "                    'lat': lat, 'lon': lon, 'imf': idx + 1,\n",
        "                    'L1': e[0], 'L2': e[1], 'L3': e[2], 'L4': e[3], 'Approx': e[4],\n",
        "                    'n_imfs': n_modes, 'method': method\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    df['frecuencia'] = df.apply(classify_frequency, axis=1)\n",
        "\n",
        "    df_original = ds.to_dataframe().reset_index()\n",
        "    df_merge = pd.merge(\n",
        "        df, df_original,\n",
        "        how='left',\n",
        "        left_on=['lat', 'lon'],\n",
        "        right_on=['latitude', 'longitude']\n",
        "    )\n",
        "\n",
        "    # Eliminar duplicados exactos\n",
        "    df_merge = df_merge.drop_duplicates(subset=[\"lat\", \"lon\", \"imf\"])\n",
        "    # Verificar √≠ndice √∫nico\n",
        "    if df_merge.duplicated(subset=[\"lat\", \"lon\", \"imf\"]).any():\n",
        "        raise ValueError(\"Hay duplicados en el √≠ndice (lat, lon, imf) incluso despu√©s de deduplicar.\")\n",
        "\n",
        "    df_merge[\"imf\"] = df_merge[\"imf\"].astype(int)\n",
        "    ds_out = df_merge.set_index([\"lat\", \"lon\", \"imf\"]).to_xarray()\n",
        "\n",
        "    ds_out.to_netcdf(out_path)\n",
        "    print(f\"‚úÖ Chunk {chunk_latitudes[0]:.6f}-{chunk_latitudes[-1]:.6f} guardado en: {out_fname}\")\n",
        "    return out_path\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Ejecutar procesamiento secuencial por chunks\n",
        "# ----------------------------\n",
        "from numpy import array_split\n",
        "\n",
        "# Divide las latitudes en 8 partes (ajustable)\n",
        "chunks = array_split(latitudes, 8)\n",
        "\n",
        "# Ahora s√≠ puedes lanzar cada chunk en un proceso hijo:\n",
        "from multiprocessing import Process\n",
        "from tqdm import tqdm\n",
        "\n",
        "def _run_chunk(method, chunk):\n",
        "    \"\"\"\n",
        "    Wrapper que ejecuta el chunk y captura excepciones normales.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        process_method_chunked(method, chunk)\n",
        "    except Exception as e:\n",
        "        start, end = chunk[0], chunk[-1]\n",
        "        print(f\"‚ùå Error en chunk {start:.6f}-{end:.6f} ({method}): {e}\")\n",
        "\n",
        "# Bucle principal que lanza cada chunk en un subproceso\n",
        "import gc\n",
        "from multiprocessing import Process\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 5.1. Ejecutar procesamiento secuencial por chunks, adaptado por m√©todo\n",
        "for method in ['CEEMDAN', 'TVF-EMD']:\n",
        "    print(f\"\\n‚ñ∂Ô∏è Iniciando m√©todo {method}\")\n",
        "\n",
        "    # Si es TVF-EMD, hacemos chunks ultra-peque√±os (1 lat cada uno)\n",
        "    if method == 'TVF-EMD':\n",
        "        method_chunks = [[lat] for lat in latitudes]\n",
        "    else:\n",
        "        method_chunks = array_split(latitudes, 8)  # tus 8 trozos originales\n",
        "\n",
        "    for chunk in method_chunks:\n",
        "        start, end = chunk[0], chunk[-1]\n",
        "        print(f\"   ‚Ä¢ Lanzando chunk {start:.6f}-{end:.6f}...\", end=\" \")\n",
        "\n",
        "        def _run_chunk(method, chunk):\n",
        "            try:\n",
        "                process_method_chunked(method, chunk)\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ùå Error en chunk {chunk[0]:.6f}-{chunk[-1]:.6f} ({method}): {e}\")\n",
        "\n",
        "        p = Process(target=_run_chunk, args=(method, chunk))\n",
        "        p.start()\n",
        "        p.join()\n",
        "\n",
        "        if p.exitcode == 0:\n",
        "            print(\"‚úÖ OK\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Fall√≥ con c√≥digo {p.exitcode}, se omite\")\n",
        "\n",
        "        # Liberar memoria del padre\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c1a82a",
      "metadata": {
        "id": "97c1a82a"
      },
      "outputs": [],
      "source": [
        "# Celda para cargar, comparar y graficar los resultados desde los archivos .nc generados\n",
        "\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "\n",
        "# Archivos generados previamente\n",
        "nc_ceemdan = os.path.join(model_output_dir, \"features_ceemdan.nc\")\n",
        "nc_tvfemd = os.path.join(model_output_dir, \"features_tvf-emd.nc\")\n",
        "\n",
        "# Cargar datasets\n",
        "ds_ceemdan = xr.open_dataset(nc_ceemdan)\n",
        "ds_tvfemd = xr.open_dataset(nc_tvfemd)\n",
        "\n",
        "# Convertir a DataFrame\n",
        "df_ceemdan = ds_ceemdan.to_dataframe().reset_index()\n",
        "df_tvfemd = ds_tvfemd.to_dataframe().reset_index()\n",
        "\n",
        "# Visualizaci√≥n comparativa\n",
        "sns.set(style='whitegrid')\n",
        "for df, method in zip([df_ceemdan, df_tvfemd], ['CEEMDAN', 'TVF-EMD']):\n",
        "    for tipo in ['Alta', 'Media', 'Baja']:\n",
        "        df_filtered = df[df['frecuencia'] == tipo]\n",
        "        df_map = df_filtered.groupby(['lat', 'lon']).size().reset_index(name='conteo')\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(df_map['lon'], df_map['lat'],\n",
        "                    c=df_map['conteo'], cmap='viridis', s=80, edgecolor='k')\n",
        "        plt.colorbar(label=f'N¬∞ de IMFs {tipo}')\n",
        "        plt.title(f\"{method}: Distribuci√≥n espacial de IMFs {tipo}\")\n",
        "        plt.xlabel(\"Longitud\")\n",
        "        plt.ylabel(\"Latitud\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292794e0",
      "metadata": {
        "id": "292794e0"
      },
      "outputs": [],
      "source": [
        "# CELDA 2: Fusi√≥n final en un ensemble NetCDF con CEEMDAN + TVF-EMD\n",
        "\n",
        "import xarray as xr\n",
        "import os\n",
        "\n",
        "# Archivos generados previamente\n",
        "nc_ceemdan = os.path.join(model_output_dir, \"features_ceemdan.nc\")\n",
        "nc_tvfemd = os.path.join(model_output_dir, \"features_tvf-emd.nc\")\n",
        "\n",
        "# Cargar datasets independientes\n",
        "ds_ceemdan = xr.open_dataset(nc_ceemdan)\n",
        "ds_tvfemd = xr.open_dataset(nc_tvfemd)\n",
        "\n",
        "# Fusi√≥n en ensemble\n",
        "ds_ensemble = xr.merge([ds_ceemdan, ds_tvfemd])\n",
        "\n",
        "# Guardar ensemble final\n",
        "ensemble_path = os.path.join(model_output_dir, \"ensemble_ceemdan_tvfemd.nc\")\n",
        "ds_ensemble.to_netcdf(ensemble_path)\n",
        "\n",
        "ensemble_path"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "precipitation_prediction",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}