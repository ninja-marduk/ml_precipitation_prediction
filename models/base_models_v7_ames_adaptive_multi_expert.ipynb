{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V7-AMES: Adaptive Multi-Expert Ensemble System\n",
    "\n",
    "**Complete Training Pipeline for Monthly Precipitation Prediction**\n",
    "\n",
    "This notebook implements a physics-guided multi-expert ensemble with adaptive routing for spatiotemporal precipitation forecasting in mountainous regions.\n",
    "\n",
    "**Key Components:**\n",
    "- 3 specialized experts (high/medium/low elevation)\n",
    "- Physics-guided gating network\n",
    "- Physics-informed meta-learner\n",
    "- 3-stage hierarchical training protocol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Model Architecture\n\nV7-AMES consists of 5 main components:\n\n1. **Expert1_HighElevation**: GNN-TAT for elevation >2264m (data-driven cluster)\n2. **Expert2_LowElevation**: ConvLSTM for elevation <956m (data-driven cluster)\n3. **Expert3_Transition**: Hybrid GNN-ConvLSTM for 957-2263m (data-driven cluster)\n4. **PhysicsGuidedGating**: Adaptive routing with orographic priors\n5. **PhysicsInformedMetaLearner**: Final ensemble with physics constraints\n\n**Note:** Elevation thresholds are based on K-means clustering from the actual data distribution.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# =============================================================================\n# SECTION 1: V7-AMES MODEL ARCHITECTURE\n# =============================================================================\n\n# V7-AMES: Adaptive Multi-Expert Ensemble System - Complete Architecture\n# This file contains all 5 main components for the V7-AMES model\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GATConv\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Optional, Tuple\n\n@dataclass\nclass V7Config:\n    \"\"\"Configuration for V7-AMES model.\"\"\"\n    \n    # Data dimensions\n    n_lat: int = 61\n    n_lon: int = 65\n    n_nodes: int = 61 * 65  # 3965\n    horizon: int = 12\n    \n    # Expert 1: High Elevation (GNN-TAT)\n    expert1_gnn_hidden: int = 64\n    expert1_gnn_layers: int = 3\n    expert1_gat_heads: int = 4\n    expert1_lstm_hidden: int = 64\n    expert1_lstm_layers: int = 2\n    expert1_attn_heads: int = 4\n    expert1_dropout: float = 0.2\n    \n    # Expert 2: Low Elevation (ConvLSTM)\n    expert2_convlstm_hidden: int = 64\n    expert2_convlstm_layers: int = 2\n    expert2_kernel_size: int = 3\n    expert2_dropout: float = 0.2\n    \n    # Expert 3: Transition (Hybrid)\n    expert3_gnn_hidden: int = 32\n    expert3_gnn_layers: int = 1\n    expert3_conv_hidden: int = 32\n    expert3_conv_layers: int = 2\n    expert3_dropout: float = 0.2\n    \n    # Gating network\n    gating_hidden: int = 32\n    gating_dropout: float = 0.2\n    physics_prior_weight_init: float = 0.3\n    \n    # Meta-learner\n    meta_hidden_1: int = 64\n    meta_hidden_2: int = 32\n    meta_dropout: float = 0.2\n    \n    # Training\n    batch_size: int = 8\n    lr_stage1: float = 0.001\n    lr_stage2: float = 0.001\n    lr_stage3: float = 0.0001\n    epochs_stage1: int = 50\n    epochs_stage2: int = 30\n    epochs_stage3: int = 50\n    patience: int = 15\n    \n    # Physics loss weights\n    lambda_mass_conservation: float = 0.05\n    lambda_orographic: float = 0.1\n    \n    # Elevation thresholds (data-driven from K-means clustering)\n    # Cluster 1 (Low):    58-956m   (mean: 344m)\n    # Cluster 2 (Medium): 959-2263m  (mean: 1579m)\n    # Cluster 3 (High):  2264-4728m  (mean: 2961m)\n    low_elev_threshold: float = 956.0      # Upper bound for low elevation\n    medium_elev_min: float = 957.0         # Lower bound for medium (959 with margin)\n    medium_elev_max: float = 2263.0        # Upper bound for medium\n    high_elev_threshold: float = 2264.0    # Lower bound for high elevation\n    max_elev_bound: float = 5000.0         # Upper safety bound for outliers\n    \n    # Paths\n    data_dir: Path = Path(\"output/V7_AMES_Data\")\n    model_dir: Path = Path(\"output/V7_AMES_Models\")\n    v2_path: Optional[Path] = None\n    v4_path: Optional[Path] = None\n\n# ============================================================================\n# COMPONENT 1: Expert1_HighElevation (GNN-TAT for >2264m)\n# ============================================================================\n\nclass Expert1_HighElevation(nn.Module):\n    \"\"\"Expert 1: GNN-TAT for high elevation (>2264m).\n    \n    Architecture based on V4 GNN-TAT:\n    - Temporal encoder: 2-layer LSTM\n    - Graph attention: 3 GAT layers\n    - Temporal attention: Multi-head attention\n    \"\"\"\n    \n    def __init__(self, config: V7Config, n_features: int):\n        super().__init__()\n        self.config = config\n        self.n_features = n_features\n        self.horizon = config.horizon\n        \n        # Temporal encoder (LSTM)\n        self.temporal_encoder = nn.LSTM(\n            input_size=n_features,\n            hidden_size=config.expert1_lstm_hidden,\n            num_layers=config.expert1_lstm_layers,\n            batch_first=True,\n            dropout=config.expert1_dropout if config.expert1_lstm_layers > 1 else 0.0\n        )\n        \n        # Graph attention layers (GAT)\n        self.gat_layers = nn.ModuleList()\n        in_dim = config.expert1_lstm_hidden\n        \n        for i in range(config.expert1_gnn_layers):\n            self.gat_layers.append(\n                GATConv(\n                    in_channels=in_dim,\n                    out_channels=config.expert1_gnn_hidden,\n                    heads=config.expert1_gat_heads,\n                    dropout=config.expert1_dropout,\n                    concat=True if i < config.expert1_gnn_layers - 1 else False\n                )\n            )\n            if i < config.expert1_gnn_layers - 1:\n                in_dim = config.expert1_gnn_hidden * config.expert1_gat_heads\n            else:\n                in_dim = config.expert1_gnn_hidden\n        \n        # Temporal attention\n        self.temporal_attention = nn.MultiheadAttention(\n            embed_dim=config.expert1_gnn_hidden,\n            num_heads=config.expert1_attn_heads,\n            dropout=config.expert1_dropout,\n            batch_first=True\n        )\n        \n        # Output projection\n        self.output_projection = nn.Sequential(\n            nn.Linear(config.expert1_gnn_hidden, config.expert1_gnn_hidden // 2),\n            nn.ReLU(),\n            nn.Dropout(config.expert1_dropout),\n            nn.Linear(config.expert1_gnn_hidden // 2, config.horizon)\n        )\n    \n    def forward(self, x, edge_index, edge_weight=None):\n        \"\"\"Forward pass.\n        \n        Args:\n            x: [batch, nodes, time, features]\n            edge_index: [2, num_edges]\n            edge_weight: [num_edges] (optional)\n            \n        Returns:\n            predictions: [batch, nodes, horizon]\n        \"\"\"\n        batch_size, n_nodes, time_steps, n_features = x.shape\n        \n        # Temporal encoding per node\n        x_reshaped = x.view(batch_size * n_nodes, time_steps, n_features)\n        temporal_features, _ = self.temporal_encoder(x_reshaped)\n        temporal_features = temporal_features[:, -1, :]  # Take last timestep\n        temporal_features = temporal_features.view(batch_size, n_nodes, -1)\n        \n        # Graph attention layers\n        graph_features = temporal_features\n        for i, gat_layer in enumerate(self.gat_layers):\n            # Flatten batch and nodes for GAT\n            x_flat = graph_features.view(batch_size * n_nodes, -1)\n            x_flat = gat_layer(x_flat, edge_index, edge_weight)\n            x_flat = F.elu(x_flat)\n            graph_features = x_flat.view(batch_size, n_nodes, -1)\n        \n        # Temporal attention over nodes\n        attended_features, _ = self.temporal_attention(\n            graph_features, graph_features, graph_features\n        )\n        \n        # Output projection\n        predictions = self.output_projection(attended_features)\n        \n        return predictions  # [batch, nodes, horizon]\n\n# ============================================================================\n# COMPONENT 2: Expert2_LowElevation (ConvLSTM for <956m)\n# ============================================================================\n\nclass ConvLSTMCell(nn.Module):\n    \"\"\"ConvLSTM cell for spatial-temporal processing.\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, kernel_size):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        padding = kernel_size // 2\n        \n        self.conv = nn.Conv2d(\n            in_channels=input_dim + hidden_dim,\n            out_channels=4 * hidden_dim,\n            kernel_size=kernel_size,\n            padding=padding\n        )\n    \n    def forward(self, x, h, c):\n        \"\"\"Forward pass.\n        \n        Args:\n            x: [batch, channels, height, width]\n            h: [batch, hidden_dim, height, width]\n            c: [batch, hidden_dim, height, width]\n            \n        Returns:\n            h_next, c_next\n        \"\"\"\n        combined = torch.cat([x, h], dim=1)\n        gates = self.conv(combined)\n        \n        i, f, o, g = gates.chunk(4, dim=1)\n        i = torch.sigmoid(i)\n        f = torch.sigmoid(f)\n        o = torch.sigmoid(o)\n        g = torch.tanh(g)\n        \n        c_next = f * c + i * g\n        h_next = o * torch.tanh(c_next)\n        \n        return h_next, c_next\n\nclass Expert2_LowElevation(nn.Module):\n    \"\"\"Expert 2: ConvLSTM for low elevation (<956m).\n    \n    Architecture based on V2 Enhanced ConvLSTM:\n    - 2 ConvLSTM layers for spatial-temporal processing\n    - Output projection for multi-horizon prediction\n    \"\"\"\n    \n    def __init__(self, config: V7Config, n_features: int):\n        super().__init__()\n        self.config = config\n        self.n_features = n_features\n        self.horizon = config.horizon\n        self.n_lat = config.n_lat\n        self.n_lon = config.n_lon\n        \n        # ConvLSTM layers\n        self.convlstm_layers = nn.ModuleList()\n        in_dim = n_features\n        for i in range(config.expert2_convlstm_layers):\n            self.convlstm_layers.append(\n                ConvLSTMCell(\n                    input_dim=in_dim,\n                    hidden_dim=config.expert2_convlstm_hidden,\n                    kernel_size=config.expert2_kernel_size\n                )\n            )\n            in_dim = config.expert2_convlstm_hidden\n        \n        self.dropout = nn.Dropout(config.expert2_dropout)\n        \n        # Output projection\n        self.output_projection = nn.Sequential(\n            nn.Conv2d(\n                config.expert2_convlstm_hidden,\n                config.expert2_convlstm_hidden // 2,\n                kernel_size=1\n            ),\n            nn.ReLU(),\n            nn.Dropout(config.expert2_dropout),\n            nn.Conv2d(\n                config.expert2_convlstm_hidden // 2,\n                config.horizon,\n                kernel_size=1\n            )\n        )\n    \n    def forward(self, x, edge_index=None, edge_weight=None):\n        \"\"\"Forward pass.\n        \n        Args:\n            x: [batch, nodes, time, features]\n            edge_index: Ignored (for API compatibility)\n            edge_weight: Ignored (for API compatibility)\n            \n        Returns:\n            predictions: [batch, nodes, horizon]\n        \"\"\"\n        batch_size, n_nodes, time_steps, n_features = x.shape\n        \n        # Reshape to grid\n        x_grid = x.view(batch_size, self.n_lat, self.n_lon, time_steps, n_features)\n        x_grid = x_grid.permute(0, 3, 4, 1, 2)  # [batch, time, features, lat, lon]\n        \n        # Process through time with ConvLSTM\n        h = [torch.zeros(batch_size, self.config.expert2_convlstm_hidden,\n                        self.n_lat, self.n_lon, device=x.device)\n             for _ in range(self.config.expert2_convlstm_layers)]\n        c = [torch.zeros(batch_size, self.config.expert2_convlstm_hidden,\n                        self.n_lat, self.n_lon, device=x.device)\n             for _ in range(self.config.expert2_convlstm_layers)]\n        \n        for t in range(time_steps):\n            x_t = x_grid[:, t]  # [batch, features, lat, lon]\n            \n            for layer_idx, convlstm_cell in enumerate(self.convlstm_layers):\n                h[layer_idx], c[layer_idx] = convlstm_cell(\n                    x_t if layer_idx == 0 else h[layer_idx - 1],\n                    h[layer_idx],\n                    c[layer_idx]\n                )\n                if layer_idx < len(self.convlstm_layers) - 1:\n                    h[layer_idx] = self.dropout(h[layer_idx])\n        \n        # Final hidden state\n        final_h = h[-1]  # [batch, hidden, lat, lon]\n        \n        # Output projection\n        predictions = self.output_projection(final_h)  # [batch, horizon, lat, lon]\n        \n        # Reshape to nodes\n        predictions = predictions.permute(0, 2, 3, 1)  # [batch, lat, lon, horizon]\n        predictions = predictions.reshape(batch_size, n_nodes, self.horizon)\n        \n        return predictions  # [batch, nodes, horizon]\n\n# ============================================================================\n# COMPONENT 3: Expert3_Transition (Hybrid for 957-2263m)\n# ============================================================================\n\nclass Expert3_Transition(nn.Module):\n    \"\"\"Expert 3: Hybrid GNN + Conv for transition zone (957-2263m).\n    \n    Lightweight hybrid combining:\n    - 1 GAT layer for graph structure\n    - 2 Conv layers for grid patterns\n    - Linear fusion\n    \"\"\"\n    \n    def __init__(self, config: V7Config, n_features: int):\n        super().__init__()\n        self.config = config\n        self.n_features = n_features\n        self.horizon = config.horizon\n        self.n_lat = config.n_lat\n        self.n_lon = config.n_lon\n        \n        # GNN branch (lightweight)\n        self.gat = GATConv(\n            in_channels=n_features,\n            out_channels=config.expert3_gnn_hidden,\n            heads=4,\n            dropout=config.expert3_dropout,\n            concat=False\n        )\n        \n        # Conv branch (lightweight)\n        self.conv1 = nn.Conv2d(\n            in_channels=n_features,\n            out_channels=config.expert3_conv_hidden,\n            kernel_size=3,\n            padding=1\n        )\n        self.conv2 = nn.Conv2d(\n            in_channels=config.expert3_conv_hidden,\n            out_channels=config.expert3_conv_hidden,\n            kernel_size=3,\n            padding=1\n        )\n        \n        self.dropout = nn.Dropout(config.expert3_dropout)\n        \n        # Fusion\n        self.fusion = nn.Linear(\n            config.expert3_gnn_hidden + config.expert3_conv_hidden,\n            config.expert3_gnn_hidden\n        )\n        \n        # Output projection\n        self.output_projection = nn.Sequential(\n            nn.Linear(config.expert3_gnn_hidden, config.expert3_gnn_hidden // 2),\n            nn.ReLU(),\n            nn.Dropout(config.expert3_dropout),\n            nn.Linear(config.expert3_gnn_hidden // 2, config.horizon)\n        )\n    \n    def forward(self, x, edge_index, edge_weight=None):\n        \"\"\"Forward pass.\n        \n        Args:\n            x: [batch, nodes, time, features]\n            edge_index: [2, num_edges]\n            edge_weight: [num_edges] (optional)\n            \n        Returns:\n            predictions: [batch, nodes, horizon]\n        \"\"\"\n        batch_size, n_nodes, time_steps, n_features = x.shape\n        \n        # Take last timestep for simplicity\n        x_last = x[:, :, -1, :]  # [batch, nodes, features]\n        \n        # GNN branch\n        x_flat = x_last.view(batch_size * n_nodes, n_features)\n        gnn_out = self.gat(x_flat, edge_index, edge_weight)\n        gnn_out = F.elu(gnn_out)\n        gnn_out = gnn_out.view(batch_size, n_nodes, -1)\n        \n        # Conv branch\n        x_grid = x_last.view(batch_size, self.n_lat, self.n_lon, n_features)\n        x_grid = x_grid.permute(0, 3, 1, 2)  # [batch, features, lat, lon]\n        \n        conv_out = F.relu(self.conv1(x_grid))\n        conv_out = self.dropout(conv_out)\n        conv_out = F.relu(self.conv2(conv_out))\n        conv_out = conv_out.permute(0, 2, 3, 1)  # [batch, lat, lon, hidden]\n        conv_out = conv_out.reshape(batch_size, n_nodes, -1)\n        \n        # Fusion\n        fused = torch.cat([gnn_out, conv_out], dim=-1)\n        fused = self.fusion(fused)\n        fused = F.relu(fused)\n        \n        # Output projection\n        predictions = self.output_projection(fused)\n        \n        return predictions  # [batch, nodes, horizon]\n\n# ============================================================================\n# COMPONENT 4: PhysicsGuidedGating (Physics-informed routing)\n# ============================================================================\n\nclass PhysicsGuidedGating(nn.Module):\n    \"\"\"Physics-guided gating network for expert routing.\n    \n    Combines:\n    - Physics priors (rule-based from elevation)\n    - Data-driven weights (learned from context)\n    - Learnable balance parameter alpha\n    \"\"\"\n    \n    def __init__(self, config: V7Config, n_context_features: int):\n        super().__init__()\n        self.config = config\n        \n        # Learnable balance between physics and data\n        self.alpha_logit = nn.Parameter(\n            torch.tensor(self._inverse_sigmoid(config.physics_prior_weight_init))\n        )\n        \n        # Data-driven gating network\n        self.context_encoder = nn.Sequential(\n            nn.Linear(n_context_features, config.gating_hidden),\n            nn.ReLU(),\n            nn.Dropout(config.gating_dropout),\n            nn.Linear(config.gating_hidden, 3)  # 3 experts\n        )\n    \n    @staticmethod\n    def _inverse_sigmoid(y):\n        \"\"\"Inverse sigmoid for initialization.\"\"\"\n        y = max(min(y, 0.999), 0.001)\n        return torch.log(torch.tensor(y / (1 - y)))\n    \n    def compute_physics_priors(self, elevation):\n        \"\"\"Compute physics-based routing weights using data-driven thresholds.\n        \n        Args:\n            elevation: [batch, nodes] or [batch, nodes, 1]\n            \n        Returns:\n            priors: [batch, nodes, 3] (weights for 3 experts)\n        \"\"\"\n        if elevation.dim() == 3:\n            elevation = elevation.squeeze(-1)\n        \n        # Expert 1: High elevation (>2264m)\n        w1 = torch.sigmoid((elevation - self.config.high_elev_threshold) / 300.0)\n        \n        # Expert 2: Low elevation (<956m)\n        w2 = torch.sigmoid((self.config.low_elev_threshold - elevation) / 300.0)\n        \n        # Expert 3: Transition zone (957-2263m) - Gaussian peak at center\n        center = (self.config.medium_elev_min + self.config.medium_elev_max) / 2  # ~1610m\n        sigma = (self.config.medium_elev_max - self.config.medium_elev_min) / 4   # ~326m\n        w3 = torch.exp(-((elevation - center) ** 2) / (2 * sigma ** 2))\n        \n        # Stack and normalize\n        priors = torch.stack([w1, w2, w3], dim=-1)  # [batch, nodes, 3]\n        priors = F.softmax(priors, dim=-1)\n        \n        return priors\n    \n    def forward(self, context_features):\n        \"\"\"Forward pass.\n        \n        Args:\n            context_features: Dict with keys:\n                - 'elevation': [batch, nodes] or [batch, nodes, 1]\n                - 'context': [batch, nodes, n_context] (slope, aspect, lat, lon, season)\n                \n        Returns:\n            weights: [batch, nodes, 3] (routing weights for 3 experts)\n            alpha: scalar (physics prior weight)\n        \"\"\"\n        elevation = context_features['elevation']\n        context = context_features['context']\n        \n        # Physics priors\n        physics_priors = self.compute_physics_priors(elevation)\n        \n        # Data-driven weights\n        logits = self.context_encoder(context)\n        data_weights = F.softmax(logits, dim=-1)\n        \n        # Learnable balance\n        alpha = torch.sigmoid(self.alpha_logit)\n        \n        # Combine\n        weights = alpha * physics_priors + (1 - alpha) * data_weights\n        \n        # Ensure normalization (might have small numerical errors)\n        weights = weights / weights.sum(dim=-1, keepdim=True)\n        \n        return weights, alpha\n\n# ============================================================================\n# COMPONENT 5: PhysicsInformedMetaLearner (Final ensemble)\n# ============================================================================\n\nclass PhysicsInformedMetaLearner(nn.Module):\n    \"\"\"Physics-informed meta-learner for combining expert predictions.\n    \n    Components:\n    - Weighted combination of expert predictions\n    - Meta-residual MLP\n    - Physics correction (orographic enhancement + rain shadow)\n    \"\"\"\n    \n    def __init__(self, config: V7Config, n_context_features: int):\n        super().__init__()\n        self.config = config\n        \n        # Meta-residual network\n        meta_input_dim = 3 * config.horizon + n_context_features\n        self.meta_residual = nn.Sequential(\n            nn.Linear(meta_input_dim, config.meta_hidden_1),\n            nn.ReLU(),\n            nn.Dropout(config.meta_dropout),\n            nn.Linear(config.meta_hidden_1, config.meta_hidden_2),\n            nn.ReLU(),\n            nn.Dropout(config.meta_dropout),\n            nn.Linear(config.meta_hidden_2, config.horizon)\n        )\n        \n        # Physics correction parameters (learnable)\n        self.orographic_enhancement = nn.Parameter(torch.tensor(0.1))\n        self.rain_shadow_suppression = nn.Parameter(torch.tensor(0.05))\n    \n    def compute_physics_correction(self, context_features):\n        \"\"\"Compute physics-based correction term.\n        \n        Args:\n            context_features: Dict with keys:\n                - 'elevation': [batch, nodes]\n                - 'slope': [batch, nodes]\n                - 'aspect': [batch, nodes] (optional)\n                \n        Returns:\n            correction: [batch, nodes, 1] (additive correction factor)\n        \"\"\"\n        elevation = context_features['elevation']\n        slope = context_features.get('slope', torch.zeros_like(elevation))\n        \n        if elevation.dim() == 3:\n            elevation = elevation.squeeze(-1)\n        if slope.dim() == 3:\n            slope = slope.squeeze(-1)\n        \n        # Orographic enhancement (high elevation + high slope)\n        # Normalize by max_elev_bound instead of hardcoded 5000\n        oro_factor = (elevation / self.config.max_elev_bound) * (slope / 90.0)\n        oro_enhancement = self.orographic_enhancement * oro_factor\n        \n        # Rain shadow (leeward side - simplified, could use aspect)\n        rain_shadow = -self.rain_shadow_suppression * (slope / 90.0)\n        \n        # Combine\n        correction = oro_enhancement + rain_shadow\n        correction = correction.unsqueeze(-1)  # [batch, nodes, 1]\n        \n        return correction\n    \n    def forward(self, expert_predictions, gating_weights, context_features):\n        \"\"\"Forward pass.\n        \n        Args:\n            expert_predictions: List of 3 tensors, each [batch, nodes, horizon]\n            gating_weights: [batch, nodes, 3]\n            context_features: Dict with context information\n            \n        Returns:\n            final_prediction: [batch, nodes, horizon]\n        \"\"\"\n        batch_size, n_nodes, horizon = expert_predictions[0].shape\n        \n        # Weighted combination\n        stacked_preds = torch.stack(expert_predictions, dim=-1)  # [batch, nodes, horizon, 3]\n        weights_expanded = gating_weights.unsqueeze(2)  # [batch, nodes, 1, 3]\n        weighted_pred = (stacked_preds * weights_expanded).sum(dim=-1)  # [batch, nodes, horizon]\n        \n        # Meta-residual\n        preds_flat = torch.cat([p for p in expert_predictions], dim=-1)  # [batch, nodes, 3*horizon]\n        context = context_features['context']\n        meta_input = torch.cat([preds_flat, context], dim=-1)\n        meta_residual = self.meta_residual(meta_input)  # [batch, nodes, horizon]\n        \n        # Physics correction (broadcast to all horizons)\n        physics_correction = self.compute_physics_correction(context_features)  # [batch, nodes, 1]\n        physics_correction = physics_correction.expand(-1, -1, horizon)\n        \n        # Final prediction\n        final_prediction = weighted_pred + meta_residual + physics_correction\n        \n        return final_prediction\n\n# ============================================================================\n# COMPLETE V7-AMES MODEL\n# ============================================================================\n\nclass V7_AMES(nn.Module):\n    \"\"\"Complete V7-AMES model: Adaptive Multi-Expert Ensemble System.\n    \n    Combines:\n    - 3 specialized experts (high/medium/low elevation)\n    - Physics-guided gating network\n    - Physics-informed meta-learner\n    \"\"\"\n    \n    def __init__(self, config: V7Config, n_features: int, n_context_features: int):\n        super().__init__()\n        self.config = config\n        \n        # Experts\n        self.expert1 = Expert1_HighElevation(config, n_features)\n        self.expert2 = Expert2_LowElevation(config, n_features)\n        self.expert3 = Expert3_Transition(config, n_features)\n        \n        # Gating\n        self.gating = PhysicsGuidedGating(config, n_context_features)\n        \n        # Meta-learner\n        self.meta_learner = PhysicsInformedMetaLearner(config, n_context_features)\n    \n    def forward(self, x, edge_index, edge_weight, context_features, stage='full'):\n        \"\"\"Forward pass.\n        \n        Args:\n            x: [batch, nodes, time, features]\n            edge_index: [2, num_edges]\n            edge_weight: [num_edges]\n            context_features: Dict with context information\n            stage: 'stage1', 'stage2', or 'full'\n            \n        Returns:\n            predictions: [batch, nodes, horizon]\n            aux_outputs: Dict with auxiliary outputs (gating weights, etc.)\n        \"\"\"\n        # Expert predictions\n        pred1 = self.expert1(x, edge_index, edge_weight)\n        pred2 = self.expert2(x, edge_index, edge_weight)\n        pred3 = self.expert3(x, edge_index, edge_weight)\n        \n        if stage == 'stage1':\n            # Stage 1: Return individual expert predictions\n            return {\n                'expert1': pred1,\n                'expert2': pred2,\n                'expert3': pred3\n            }\n        \n        # Gating weights\n        gating_weights, alpha = self.gating(context_features)\n        \n        if stage == 'stage2':\n            # Stage 2: Return gated combination (no meta-learner yet)\n            stacked_preds = torch.stack([pred1, pred2, pred3], dim=-1)\n            weights_expanded = gating_weights.unsqueeze(2)\n            weighted_pred = (stacked_preds * weights_expanded).sum(dim=-1)\n            return weighted_pred, {'gating_weights': gating_weights, 'alpha': alpha}\n        \n        # Stage 3 / Full: Complete pipeline with meta-learner\n        final_pred = self.meta_learner(\n            [pred1, pred2, pred3],\n            gating_weights,\n            context_features\n        )\n        \n        aux_outputs = {\n            'expert1_pred': pred1,\n            'expert2_pred': pred2,\n            'expert3_pred': pred3,\n            'gating_weights': gating_weights,\n            'alpha': alpha\n        }\n        \n        return final_pred, aux_outputs\n    \n    def freeze_experts(self):\n        \"\"\"Freeze expert parameters (for Stage 2).\"\"\"\n        for param in self.expert1.parameters():\n            param.requires_grad = False\n        for param in self.expert2.parameters():\n            param.requires_grad = False\n        for param in self.expert3.parameters():\n            param.requires_grad = False\n    \n    def unfreeze_all(self):\n        \"\"\"Unfreeze all parameters (for Stage 3).\"\"\"\n        for param in self.parameters():\n            param.requires_grad = True\n\n# ============================================================================\n# PHYSICS-INFORMED LOSS FUNCTIONS\n# ============================================================================\n\ndef physics_informed_loss(predictions, targets, context_features, config):\n    \"\"\"Compute physics-informed loss.\n    \n    Args:\n        predictions: [batch, nodes, horizon]\n        targets: [batch, nodes, horizon]\n        context_features: Dict with elevation info\n        config: V7Config\n        \n    Returns:\n        total_loss: scalar\n        loss_components: Dict with individual loss terms\n    \"\"\"\n    # MSE loss\n    mse_loss = F.mse_loss(predictions, targets)\n    \n    # Mass conservation constraint\n    pred_sum = predictions.sum(dim=(1, 2))\n    target_sum = targets.sum(dim=(1, 2))\n    mass_conservation_loss = torch.abs(pred_sum - target_sum) / (target_sum + 1e-6)\n    mass_conservation_loss = mass_conservation_loss.mean()\n    \n    # Orographic enhancement constraint\n    elevation = context_features['elevation']\n    if elevation.dim() == 3:\n        elevation = elevation.squeeze(-1)\n    \n    high_elev_mask = (elevation > config.high_elev_threshold).unsqueeze(-1)\n    \n    if high_elev_mask.any():\n        preds_high = predictions[high_elev_mask.expand_as(predictions)]\n        targets_high = targets[high_elev_mask.expand_as(targets)]\n        orographic_loss = F.relu(targets_high - preds_high).mean()\n    else:\n        orographic_loss = torch.tensor(0.0, device=predictions.device)\n    \n    # Total loss\n    total_loss = (\n        mse_loss +\n        config.lambda_mass_conservation * mass_conservation_loss +\n        config.lambda_orographic * orographic_loss\n    )\n    \n    loss_components = {\n        'mse': mse_loss.item(),\n        'mass_conservation': mass_conservation_loss.item(),\n        'orographic': orographic_loss.item(),\n        'total': total_loss.item()\n    }\n    \n    return total_loss, loss_components\n\n# ============================================================================\n# UTILITY FUNCTIONS\n# ============================================================================\n\ndef set_random_seed(seed=42):\n    \"\"\"Set random seed for reproducibility.\"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    import numpy as np\n    import random\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef count_parameters(model):\n    \"\"\"Count trainable parameters in model.\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef print_model_summary(model, config):\n    \"\"\"Print model architecture summary.\"\"\"\n    print(\"=\" * 80)\n    print(\"V7-AMES Model Summary\")\n    print(\"=\" * 80)\n    print(f\"Expert 1 (High Elevation) parameters: {count_parameters(model.expert1):,}\")\n    print(f\"Expert 2 (Low Elevation) parameters: {count_parameters(model.expert2):,}\")\n    print(f\"Expert 3 (Transition) parameters: {count_parameters(model.expert3):,}\")\n    print(f\"Gating Network parameters: {count_parameters(model.gating):,}\")\n    print(f\"Meta-Learner parameters: {count_parameters(model.meta_learner):,}\")\n    print(\"-\" * 80)\n    print(f\"Total parameters: {count_parameters(model):,}\")\n    print(\"=\" * 80)\n\n# End of V7-AMES architecture\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data Preparation\n\nPrepare elevation-stratified datasets and context features for training:\n\n- **Elevation masks**: High (>2264m), Medium (957-2263m), Low (<956m) - **Data-driven clusters**\n- **Context features**: Elevation, slope, aspect, latitude, longitude\n- **Output**: Saved to `output/V7_AMES_Data/`\n\n**Note:** Thresholds are based on K-means clustering analysis of the actual elevation distribution, not arbitrary values.\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 2: DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "# V7-AMES Data Preparation",
    "# Creates elevation masks and context features for V7-AMES training",
    "",
    "import numpy as np",
    "from pathlib import Path",
    "import torch",
    "",
    "def prepare_v7_data(elevation_path, output_dir, config):",
    "    \"\"\"",
    "    Prepare elevation-stratified data for V7-AMES.",
    "",
    "    Creates:",
    "    - mask_high.npy: High elevation mask (>3000m)",
    "    - mask_medium.npy: Medium elevation mask (2000-3000m)",
    "    - mask_low.npy: Low elevation mask (<2000m)",
    "    - context_features_spatial.npy: Spatial context features",
    "",
    "    Args:",
    "        elevation_path: Path to elevation data file",
    "        output_dir: Directory to save output files",
    "        config: V7Config instance",
    "    \"\"\"",
    "    print(\"=\"*80)",
    "    print(\"V7-AMES DATA PREPARATION\")",
    "    print(\"=\"*80)",
    "",
    "    # Create output directory",
    "    output_dir = Path(output_dir)",
    "    output_dir.mkdir(parents=True, exist_ok=True)",
    "",
    "    # Load elevation data",
    "    print(\"\\n1. Loading elevation data...\")",
    "    if isinstance(elevation_path, str):",
    "        elevation_path = Path(elevation_path)",
    "",
    "    if elevation_path.suffix == '.npy':",
    "        elevation = np.load(elevation_path)",
    "    elif elevation_path.suffix == '.nc':",
    "        import xarray as xr",
    "        ds = xr.open_dataset(elevation_path)",
    "        elevation = ds['elevation'].values",
    "    else:",
    "        raise ValueError(f\"Unsupported elevation file format: {elevation_path.suffix}\")",
    "",
    "    print(f\"   Elevation shape: {elevation.shape}\")",
    "    print(f\"   Elevation range: [{elevation.min():.1f}, {elevation.max():.1f}] meters\")",
    "",
    "    # Create elevation masks",
    "    print(\"\\n2. Creating elevation masks...\")",
    "",
    "    # High elevation mask (>3000m)",
    "    mask_high = (elevation > config.high_elev_threshold).astype(np.float32)",
    "    n_high = mask_high.sum()",
    "    pct_high = (n_high / mask_high.size) * 100",
    "    print(f\"   High elevation (>{config.high_elev_threshold}m): {int(n_high)} cells ({pct_high:.1f}%)\")",
    "",
    "    # Low elevation mask (<2000m)",
    "    mask_low = (elevation < config.low_elev_threshold).astype(np.float32)",
    "    n_low = mask_low.sum()",
    "    pct_low = (n_low / mask_low.size) * 100",
    "    print(f\"   Low elevation (<{config.low_elev_threshold}m): {int(n_low)} cells ({pct_low:.1f}%)\")",
    "",
    "    # Medium elevation mask (2000-3000m)",
    "    mask_medium = ((elevation >= config.medium_elev_min) &",
    "                   (elevation <= config.medium_elev_max)).astype(np.float32)",
    "    n_medium = mask_medium.sum()",
    "    pct_medium = (n_medium / mask_medium.size) * 100",
    "    print(f\"   Medium elevation ({config.medium_elev_min}-{config.medium_elev_max}m): {int(n_medium)} cells ({pct_medium:.1f}%)\")",
    "",
    "    # Save masks",
    "    print(\"\\n3. Saving masks...\")",
    "    np.save(output_dir / 'mask_high.npy', mask_high)",
    "    np.save(output_dir / 'mask_medium.npy', mask_medium)",
    "    np.save(output_dir / 'mask_low.npy', mask_low)",
    "    print(f\"   Saved to: {output_dir}\")",
    "",
    "    # Create context features",
    "    print(\"\\n4. Creating spatial context features...\")",
    "    lat, lon = elevation.shape",
    "",
    "    # Elevation (normalized)",
    "    elev_norm = (elevation - elevation.mean()) / elevation.std()",
    "",
    "    # Slope (approximate from elevation gradient)",
    "    grad_y, grad_x = np.gradient(elevation)",
    "    slope = np.sqrt(grad_x**2 + grad_y**2)",
    "    slope_norm = (slope - slope.mean()) / slope.std()",
    "",
    "    # Aspect (direction of slope)",
    "    aspect = np.arctan2(grad_y, grad_x)",
    "    aspect_sin = np.sin(aspect)",
    "    aspect_cos = np.cos(aspect)",
    "",
    "    # Latitude (normalized)",
    "    lat_coords = np.arange(lat)",
    "    lat_grid = np.tile(lat_coords[:, np.newaxis], (1, lon))",
    "    lat_norm = (lat_grid - lat_grid.mean()) / lat_grid.std()",
    "",
    "    # Longitude (normalized)",
    "    lon_coords = np.arange(lon)",
    "    lon_grid = np.tile(lon_coords[np.newaxis, :], (lat, 1))",
    "    lon_norm = (lon_grid - lon_grid.mean()) / lon_grid.std()",
    "",
    "    # Stack context features: [lat, lon, 5]",
    "    # Features: elevation, slope, aspect_sin, aspect_cos, lat, lon",
    "    context_features = np.stack([",
    "        elev_norm,",
    "        slope_norm,",
    "        aspect_sin,",
    "        aspect_cos,",
    "        lat_norm,",
    "        lon_norm",
    "    ], axis=-1).astype(np.float32)",
    "",
    "    print(f\"   Context features shape: {context_features.shape}\")",
    "    print(f\"   Features: elevation, slope, aspect_sin, aspect_cos, lat, lon\")",
    "",
    "    # Save context features",
    "    np.save(output_dir / 'context_features_spatial.npy', context_features)",
    "    print(f\"   Saved context features to: {output_dir / 'context_features_spatial.npy'}\")",
    "",
    "    print(\"\\n\" + \"=\"*80)",
    "    print(\"DATA PREPARATION COMPLETE\")",
    "    print(\"=\"*80)",
    "    print(f\"\\nOutput files in {output_dir}:\")",
    "    print(\"  - mask_high.npy\")",
    "    print(\"  - mask_medium.npy\")",
    "    print(\"  - mask_low.npy\")",
    "    print(\"  - context_features_spatial.npy\")",
    "    print(\"\\nReady for V7-AMES training!\")",
    "",
    "    return {",
    "        'mask_high': mask_high,",
    "        'mask_medium': mask_medium,",
    "        'mask_low': mask_low,",
    "        'context_features': context_features",
    "    }",
    "",
    "",
    "# Standalone execution for testing",
    "if __name__ == \"__main__\":",
    "    from v7_architecture_temp import V7Config",
    "",
    "    config = V7Config()",
    "",
    "    # Example: Prepare data (adjust path as needed)",
    "    elevation_path = \"data/processed/elevation.npy\"  # Update this path",
    "    output_dir = \"output/V7_AMES_Data\"",
    "",
    "    try:",
    "        results = prepare_v7_data(elevation_path, output_dir, config)",
    "        print(\"\\nData preparation successful!\")",
    "    except FileNotFoundError:",
    "        print(f\"\\nERROR: Elevation file not found at {elevation_path}\")",
    "        print(\"Please update the elevation_path variable to point to your elevation data.\")",
    "        print(\"\\nFor Colab/testing, you can create dummy data:\")",
    "        print(\"```python\")",
    "        print(\"import numpy as np\")",
    "        print(\"elevation = np.random.uniform(1000, 4000, (61, 65))\")",
    "        print(\"np.save('elevation_dummy.npy', elevation)\")",
    "        print(\"```\")",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Pipeline\n",
    "\n",
    "3-stage hierarchical training protocol:\n",
    "\n",
    "- **Stage 1**: Pre-train each expert independently on filtered data\n",
    "- **Stage 2**: Train gating network with frozen experts\n",
    "- **Stage 3**: Joint fine-tuning with physics-informed loss\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 3: TRAINING SETUP - IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Time tracking\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Try to import PyTorch Geometric\n",
    "try:\n",
    "    from torch_geometric.nn import GATConv, global_mean_pool\n",
    "    GNN_AVAILABLE = True\n",
    "    print(\"PyTorch Geometric available\")\n",
    "except ImportError:\n",
    "    print(\"Warning: PyTorch Geometric not available. GNN experts will be skipped.\")\n",
    "    GNN_AVAILABLE = False\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_random_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "\n",
    "set_random_seed(42)\n",
    "print(\"Random seed set to 42\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER FUNCTIONS: TRAINING VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def plot_training_history(train_losses, val_losses, title, save_path=None):\n",
    "    \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "    plt.plot(val_losses, label='Val Loss', linewidth=2)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def print_metrics_table(metrics_dict):\n",
    "    \"\"\"Print metrics in a formatted table.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"{'Metric':<20} {'Value':>15}\")\n",
    "    print(\"-\"*60)\n",
    "    for key, value in metrics_dict.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key:<20} {value:>15.4f}\")\n",
    "        else:\n",
    "            print(f\"{key:<20} {value:>15}\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 3.1: V7-AMES CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class V7Config:",
    "    \"\"\"Complete configuration for V7-AMES\"\"\"",
    "",
    "    # Paths",
    "    data_dir = Path('output/V7_AMES_Data')",
    "    v2_path = Path('output/V2_Enhanced_Models/map_exports/H12/BASIC/ConvLSTM_Enhanced')",
    "    v4_path = Path('output/V4_GNN_TAT_Models/map_exports/H12/BASIC/GNN_TAT_GAT')",
    "    output_dir = Path('output/V7_AMES_Models')",
    "",
    "    # Data",
    "    input_window = 60",
    "    horizon = 12",
    "    grid_shape = (61, 65)",
    "    n_nodes = 61 * 65  # 3965 nodes",
    "    n_features_basic = 12",
    "    n_features_kce = 15",
    "    n_features_context = 5  # elevation, slope, aspect, lat_norm, lon_norm",
    "",
    "    # Expert 1: High Elevation Specialist (GNN-TAT)",
    "    expert1_hidden_dim = 64",
    "    expert1_num_layers = 3",
    "    expert1_dropout = 0.2",
    "    expert1_heads = 4",
    "",
    "    # Expert 2: Low Elevation Specialist (ConvLSTM)",
    "    expert2_hidden_channels = 64",
    "    expert2_num_layers = 2",
    "    expert2_kernel_size = 3",
    "",
    "    # Expert 3: Transition Zone Specialist (Hybrid)",
    "    expert3_gnn_dim = 32",
    "    expert3_conv_dim = 32",
    "",
    "    # Gating Network",
    "    gating_hidden_dim = 32",
    "    gating_num_experts = 3",
    "    physics_prior_weight = 0.3  # Initial balance: 30% physics, 70% data",
    "",
    "    # Meta-Learner",
    "    meta_hidden_dim = 64",
    "",
    "    # Training",
    "    batch_size = 8",
    "    learning_rate = 0.001",
    "    weight_decay = 1e-5",
    "",
    "    # Stage 1: Pre-train experts",
    "    epochs_stage1 = 50",
    "    patience_stage1 = 10",
    "",
    "    # Stage 2: Train gating",
    "    epochs_stage2 = 30",
    "    patience_stage2 = 8",
    "",
    "    # Stage 3: Joint fine-tuning",
    "    epochs_stage3 = 50",
    "    patience_stage3 = 10",
    "",
    "    # Physics loss weights",
    "    lambda_physics = 0.1",
    "    lambda_mass_conservation = 0.05",
    "    lambda_orographic = 0.1",
    "",
    "    # Device",
    "    device = device",
    "",
    "    def __post_init__(self):",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)",
    "",
    "config = V7Config()",
    "print(\"Configuration loaded:\")",
    "print(f\"  Grid shape: {config.grid_shape}\")",
    "print(f\"  Horizon: {config.horizon}\")",
    "print(f\"  Experts: {config.gating_num_experts}\")",
    "print(f\"  Device: {config.device}\")",
    "",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class V7Dataset(Dataset):",
    "    \"\"\"",
    "    Dataset for V7-AMES training",
    "",
    "    Loads predictions from existing models (V2/V4) and filters by elevation zone",
    "    \"\"\"",
    "",
    "    def __init__(self, predictions_path, targets_path, context_path,",
    "                 mask_path=None, config=None, use_actual_data=False):",
    "        \"\"\"",
    "        Args:",
    "            predictions_path: Path to .npy predictions [samples, horizons, lat, lon, 1]",
    "            targets_path: Path to targets .npy (same shape)",
    "            context_path: Path to context features [lat, lon, n_features]",
    "            mask_path: Optional elevation mask [lat, lon] (bool)",
    "            config: V7Config object",
    "            use_actual_data: If False, uses dummy data for testing",
    "        \"\"\"",
    "        self.config = config or V7Config()",
    "        self.use_actual_data = use_actual_data",
    "",
    "        if use_actual_data and Path(predictions_path).exists():",
    "            # Load real data",
    "            print(f\"Loading data from {predictions_path}...\")",
    "            self.data = np.load(predictions_path)",
    "            self.targets = np.load(targets_path)",
    "            self.context_spatial = np.load(context_path)",
    "",
    "            # Load mask if provided",
    "            if mask_path and Path(mask_path).exists():",
    "                self.mask = np.load(mask_path)",
    "                print(f\"  Mask loaded: {self.mask.sum()} cells\")",
    "            else:",
    "                self.mask = None",
    "",
    "            self.n_samples = self.data.shape[0]",
    "            self.lat, self.lon = self.data.shape[2:4]",
    "",
    "            # Flatten spatial dims: [samples, horizons, n_nodes, 1]",
    "            self.data_flat = self.data.reshape(self.n_samples, self.config.horizon, -1, 1)",
    "            self.targets_flat = self.targets.reshape(self.n_samples, self.config.horizon, -1, 1)",
    "            self.context_nodes = self.context_spatial.reshape(-1, self.context_spatial.shape[-1])",
    "",
    "            # Apply mask if provided",
    "            if self.mask is not None:",
    "                mask_flat = self.mask.flatten()",
    "                self.data_flat = self.data_flat[:, :, mask_flat, :]",
    "                self.targets_flat = self.targets_flat[:, :, mask_flat, :]",
    "                self.context_nodes = self.context_nodes[mask_flat, :]",
    "",
    "            self.n_nodes = self.data_flat.shape[2]",
    "            print(f\"  Loaded: {self.n_samples} samples, {self.n_nodes} nodes\")",
    "",
    "        else:",
    "            # Dummy data for testing",
    "            print(\"Using dummy data for testing...\")",
    "            self.n_samples = 100",
    "            self.n_nodes = config.n_nodes if mask_path is None else 500",
    "            self.data_flat = np.random.randn(self.n_samples, config.horizon, self.n_nodes, 1)",
    "            self.targets_flat = np.random.randn(self.n_samples, config.horizon, self.n_nodes, 1)",
    "            self.context_nodes = np.random.randn(self.n_nodes, config.n_features_context)",
    "            print(f\"  Generated: {self.n_samples} samples, {self.n_nodes} nodes\")",
    "",
    "    def __len__(self):",
    "        return self.n_samples",
    "",
    "    def __getitem__(self, idx):",
    "        \"\"\"",
    "        Returns:",
    "            x_grid: [horizon, lat, lon, 1] grid format for ConvLSTM",
    "            x_graph: [n_nodes, horizon] graph format for GNN",
    "            context: [n_nodes, context_dim] physical context",
    "            y: [horizon, n_nodes, 1] targets",
    "        \"\"\"",
    "        # Get data",
    "        data = self.data_flat[idx]  # [horizon, n_nodes, 1]",
    "        y = self.targets_flat[idx]  # [horizon, n_nodes, 1]",
    "",
    "        # Convert to grid format (reshape back to grid)",
    "        if self.use_actual_data:",
    "            # Real grid shape",
    "            x_grid = data.reshape(self.config.horizon, self.config.grid_shape[0],",
    "                                 self.config.grid_shape[1], 1)",
    "        else:",
    "            # Dummy grid (just use subset of nodes)",
    "            lat, lon = self.config.grid_shape",
    "            x_grid = data[:, :lat*lon, :].reshape(self.config.horizon, lat, lon, 1)",
    "",
    "        # Graph format: [n_nodes, horizon]",
    "        x_graph = data.squeeze(-1).transpose(0, 1)  # [n_nodes, horizon]",
    "",
    "        # Context (static per sample)",
    "        context = torch.from_numpy(self.context_nodes).float()",
    "",
    "        # Convert to tensors",
    "        x_grid = torch.from_numpy(x_grid).float()",
    "        x_graph = torch.from_numpy(x_graph).float()",
    "        y = torch.from_numpy(y).float()",
    "",
    "        return x_grid, x_graph, context, y",
    "",
    "# Test dataset",
    "print(\"\\nTesting dataset...\")",
    "test_dataset = V7Dataset(",
    "    predictions_path=config.v4_path / 'predictions.npy',",
    "    targets_path=config.v4_path / 'targets.npy',",
    "    context_path=config.data_dir / 'context_features_spatial.npy',",
    "    mask_path=None,  # No mask for initial test",
    "    config=config,",
    "    use_actual_data=False  # Use dummy data for now",
    ")",
    "",
    "x_grid, x_graph, context, y = test_dataset[0]",
    "print(f\"Sample batch shapes:\")",
    "print(f\"  x_grid: {x_grid.shape}\")",
    "print(f\"  x_graph: {x_graph.shape}\")",
    "print(f\"  context: {context.shape}\")",
    "print(f\"  y: {y.shape}\")",
    "",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Import Model Components\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import V7-AMES model from the architecture file",
    "import sys",
    "sys.path.insert(0, str(Path('models').absolute()))",
    "",
    "# Load the architecture",
    "exec(open('base_models_v7_ames_adaptive_multi_expert.py').read())",
    "",
    "print(\" V7-AMES architecture imported\")",
    "",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Graph Construction for GNN Experts\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_grid_graph(lat, lon, k_neighbors=8):",
    "    \"\"\"",
    "    Create k-NN graph from 2D grid",
    "",
    "    Args:",
    "        lat, lon: Grid dimensions",
    "        k_neighbors: Number of nearest neighbors",
    "",
    "    Returns:",
    "        edge_index: [2, num_edges]",
    "        edge_weight: [num_edges]",
    "    \"\"\"",
    "    n_nodes = lat * lon",
    "",
    "    # Node positions",
    "    positions = []",
    "    for i in range(lat):",
    "        for j in range(lon):",
    "            positions.append([i, j])",
    "    positions = np.array(positions)",
    "",
    "    # Build k-NN graph",
    "    from scipy.spatial import distance_matrix",
    "    dist_matrix = distance_matrix(positions, positions)",
    "",
    "    edge_list = []",
    "    edge_weights = []",
    "",
    "    for i in range(n_nodes):",
    "        # Get k nearest neighbors (excluding self)",
    "        neighbors = np.argsort(dist_matrix[i])[1:k_neighbors+1]",
    "        for j in neighbors:",
    "            edge_list.append([i, j])",
    "            edge_weights.append(1.0 / (dist_matrix[i, j] + 1e-6))",
    "",
    "    edge_index = torch.tensor(edge_list, dtype=torch.long).t()",
    "    edge_weight = torch.tensor(edge_weights, dtype=torch.float)",
    "",
    "    print(f\"Graph created: {n_nodes} nodes, {edge_index.size(1)} edges\")",
    "    return edge_index, edge_weight",
    "",
    "# Create graph",
    "lat, lon = config.grid_shape",
    "edge_index, edge_weight = create_grid_graph(lat, lon, k_neighbors=8)",
    "edge_index = edge_index.to(config.device)",
    "edge_weight = edge_weight.to(config.device)",
    "",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 1: Pre-train Experts\n",
    "\n",
    "Train each expert independently on filtered elevation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 4.1: STAGE 1 - EXPERT 1 (HIGH ELEVATION)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"STAGE 1: PRE-TRAINING EXPERTS\")\n",
    "print()\n",
    "\n",
    "# Split dataset into train/val\n",
    "print(\"Stage 1.1: Training Expert 1 (High Elevation Specialist)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Load high elevation dataset\n",
    "expert1_dataset = V7Dataset(\n",
    "    predictions_path=config.v4_path / 'predictions.npy',\n",
    "    targets_path=config.v4_path / 'targets.npy',\n",
    "    context_path=config.data_dir / 'context_features_spatial.npy',\n",
    "    mask_path=config.data_dir / 'mask_high.npy',\n",
    "    config=config,\n",
    "    use_actual_data=Path(config.data_dir / 'mask_high.npy').exists()\n",
    ")\n",
    "\n",
    "# Train/val split\n",
    "train_size = int(0.8 * len(expert1_dataset))\n",
    "val_size = len(expert1_dataset) - train_size\n",
    "expert1_train, expert1_val = torch.utils.data.random_split(\n",
    "    expert1_dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "expert1_train_loader = DataLoader(expert1_train, batch_size=config.batch_size, shuffle=True)\n",
    "expert1_val_loader = DataLoader(expert1_val, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(expert1_train)}\")\n",
    "print(f\"Val samples: {len(expert1_val)}\")\n",
    "\n",
    "# Initialize Expert 1\n",
    "if GNN_AVAILABLE:\n",
    "    expert1 = Expert1_HighElevation(config, n_features=16).to(config.device)\n",
    "    optimizer1 = torch.optim.Adam(\n",
    "        expert1.parameters(),\n",
    "        lr=config.lr_stage1,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    print(f\"Expert 1 parameters: {sum(p.numel() for p in expert1.parameters()):,}\")\n",
    "    print()\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(config.epochs_stage1):\n",
    "        # Training\n",
    "        expert1.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "        \n",
    "        pbar = tqdm(expert1_train_loader, desc=f\"Epoch {epoch+1}/{config.epochs_stage1}\")\n",
    "        for batch in pbar:\n",
    "            x_grid, x_graph, context, y = batch\n",
    "            x_graph = x_graph.to(config.device)\n",
    "            y = y.to(config.device)\n",
    "            \n",
    "            optimizer1.zero_grad()\n",
    "            predictions = expert1(x_graph, edge_index)\n",
    "            loss = criterion(predictions, y.mean(dim=2))\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(expert1.parameters(), max_norm=1.0)\n",
    "            optimizer1.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            pbar.set_postfix({'train_loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        expert1.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in expert1_val_loader:\n",
    "                x_grid, x_graph, context, y = batch\n",
    "                x_graph = x_graph.to(config.device)\n",
    "                y = y.to(config.device)\n",
    "                \n",
    "                predictions = expert1(x_graph, edge_index)\n",
    "                loss = criterion(predictions, y.mean(dim=2))\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': expert1.state_dict(),\n",
    "                'optimizer_state_dict': optimizer1.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, config.model_dir / 'expert1_best.pt')\n",
    "            print(f\"  -> Best model saved (val_loss: {best_val_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config.patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(\n",
    "        train_losses, val_losses,\n",
    "        'Expert 1 (High Elevation) Training',\n",
    "        save_path=config.model_dir / 'expert1_training.png'\n",
    "    )\n",
    "    \n",
    "    # Print final metrics\n",
    "    print_metrics_table({\n",
    "        'Best Val Loss': best_val_loss,\n",
    "        'Final Train Loss': train_losses[-1],\n",
    "        'Total Epochs': len(train_losses),\n",
    "        'Parameters': sum(p.numel() for p in expert1.parameters())\n",
    "    })\n",
    "\n",
    "else:\n",
    "    print(\"GNN not available. Skipping Expert 1 training.\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Expert 2: Low Elevation Specialist\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 4.2: STAGE 1 - EXPERT 2 (LOW ELEVATION)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)",
    "print(\"Stage 1.2: Training Expert 2 (Low Elevation Specialist)\")",
    "print(\"-\" * 60)",
    "",
    "expert2_dataset = V7Dataset(",
    "    predictions_path=config.v2_path / 'predictions.npy',",
    "    targets_path=config.v2_path / 'targets.npy',",
    "    context_path=config.data_dir / 'context_features_spatial.npy',",
    "    mask_path=config.data_dir / 'mask_low.npy',  # Low elevation only",
    "    config=config,",
    "    use_actual_data=Path(config.data_dir / 'mask_low.npy').exists()",
    ")",
    "",
    "expert2_loader = DataLoader(",
    "    expert2_dataset,",
    "    batch_size=config.batch_size,",
    "    shuffle=True,",
    "    num_workers=0",
    ")",
    "",
    "# Initialize Expert 2",
    "expert2 = Expert2_LowElevation(config).to(config.device)",
    "optimizer2 = torch.optim.Adam(expert2.parameters(),",
    "                               lr=config.learning_rate,",
    "                               weight_decay=config.weight_decay)",
    "",
    "print(f\"Expert 2 initialized: {sum(p.numel() for p in expert2.parameters())} parameters\")",
    "",
    "# Training loop for Expert 2",
    "best_loss = float('inf')",
    "patience_counter = 0",
    "",
    "for epoch in range(config.epochs_stage1):",
    "    expert2.train()",
    "    total_loss = 0",
    "    num_batches = 0",
    "",
    "    pbar = tqdm(expert2_loader, desc=f\"Epoch {epoch+1}/{config.epochs_stage1}\")",
    "    for x_grid, x_graph, context, y in pbar:",
    "        # Move to device",
    "        x_grid = x_grid.to(config.device)",
    "        context = context.to(config.device)",
    "        y = y.to(config.device)",
    "",
    "        # Forward pass (Expert 2 uses grid format)",
    "        optimizer2.zero_grad()",
    "        predictions = expert2(x_grid)  # [batch, horizon, 1]",
    "",
    "        # Compute loss",
    "        loss = criterion(predictions, y.mean(dim=2))",
    "",
    "        # Backward",
    "        loss.backward()",
    "        torch.nn.utils.clip_grad_norm_(expert2.parameters(), max_norm=1.0)",
    "        optimizer2.step()",
    "",
    "        total_loss += loss.item()",
    "        num_batches += 1",
    "",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})",
    "",
    "    avg_loss = total_loss / num_batches",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")",
    "",
    "    # Early stopping",
    "    if avg_loss < best_loss:",
    "        best_loss = avg_loss",
    "        patience_counter = 0",
    "        torch.save({",
    "            'epoch': epoch,",
    "            'model_state_dict': expert2.state_dict(),",
    "            'optimizer_state_dict': optimizer2.state_dict(),",
    "            'loss': best_loss,",
    "        }, config.output_dir / 'expert2_best.pt')",
    "        print(f\"   Best model saved (loss: {best_loss:.4f})\")",
    "    else:",
    "        patience_counter += 1",
    "        if patience_counter >= config.patience_stage1:",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")",
    "            break",
    "",
    "print(f\"\\nExpert 2 training complete. Best loss: {best_loss:.4f}\")",
    "",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Expert 3: Transition Zone Specialist\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 4.3: STAGE 1 - EXPERT 3 (TRANSITION)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)",
    "print(\"Stage 1.3: Training Expert 3 (Transition Zone Specialist)\")",
    "print(\"-\" * 60)",
    "",
    "expert3_dataset = V7Dataset(",
    "    predictions_path=config.v4_path / 'predictions.npy',",
    "    targets_path=config.v4_path / 'targets.npy',",
    "    context_path=config.data_dir / 'context_features_spatial.npy',",
    "    mask_path=config.data_dir / 'mask_medium.npy',  # Medium elevation",
    "    config=config,",
    "    use_actual_data=Path(config.data_dir / 'mask_medium.npy').exists()",
    ")",
    "",
    "expert3_loader = DataLoader(",
    "    expert3_dataset,",
    "    batch_size=config.batch_size,",
    "    shuffle=True,",
    "    num_workers=0",
    ")",
    "",
    "# Initialize Expert 3",
    "if GNN_AVAILABLE:",
    "    expert3 = Expert3_Transition(config).to(config.device)",
    "    optimizer3 = torch.optim.Adam(expert3.parameters(),",
    "                                   lr=config.learning_rate,",
    "                                   weight_decay=config.weight_decay)",
    "",
    "    print(f\"Expert 3 initialized: {sum(p.numel() for p in expert3.parameters())} parameters\")",
    "",
    "    # Training loop (simplified - hybrid needs both graph and grid)",
    "    best_loss = float('inf')",
    "    patience_counter = 0",
    "",
    "    for epoch in range(config.epochs_stage1):",
    "        expert3.train()",
    "        total_loss = 0",
    "        num_batches = 0",
    "",
    "        pbar = tqdm(expert3_loader, desc=f\"Epoch {epoch+1}/{config.epochs_stage1}\")",
    "        for x_grid, x_graph, context, y in pbar:",
    "            x_grid = x_grid.to(config.device)",
    "            x_graph = x_graph.to(config.device)",
    "            context = context.to(config.device)",
    "            y = y.to(config.device)",
    "",
    "            optimizer3.zero_grad()",
    "            predictions = expert3(x_graph, x_grid, edge_index)",
    "",
    "            loss = criterion(predictions, y.mean(dim=2))",
    "            loss.backward()",
    "            torch.nn.utils.clip_grad_norm_(expert3.parameters(), max_norm=1.0)",
    "            optimizer3.step()",
    "",
    "            total_loss += loss.item()",
    "            num_batches += 1",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})",
    "",
    "        avg_loss = total_loss / num_batches",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")",
    "",
    "        if avg_loss < best_loss:",
    "            best_loss = avg_loss",
    "            patience_counter = 0",
    "            torch.save({",
    "                'epoch': epoch,",
    "                'model_state_dict': expert3.state_dict(),",
    "                'optimizer_state_dict': optimizer3.state_dict(),",
    "                'loss': best_loss,",
    "            }, config.output_dir / 'expert3_best.pt')",
    "            print(f\"   Best model saved (loss: {best_loss:.4f})\")",
    "        else:",
    "            patience_counter += 1",
    "            if patience_counter >= config.patience_stage1:",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")",
    "                break",
    "",
    "    print(f\"\\nExpert 3 training complete. Best loss: {best_loss:.4f}\")",
    "else:",
    "    print(\"GNN not available, skipping Expert 3\")",
    "",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Stage 1 Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)",
    "print(\"STAGE 1 COMPLETE: All Experts Pre-trained\")",
    "print()",
    "print(\"Checkpoints saved:\")",
    "if GNN_AVAILABLE:",
    "    print(f\"   Expert 1 (High Elev): {config.output_dir / 'expert1_best.pt'}\")",
    "print(f\"   Expert 2 (Low Elev): {config.output_dir / 'expert2_best.pt'}\")",
    "if GNN_AVAILABLE:",
    "    print(f\"   Expert 3 (Transition): {config.output_dir / 'expert3_best.pt'}\")",
    "print()",
    "",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 2: Train Gating Network\n",
    "\n",
    "Train physics-guided gating network with frozen experts.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 5: STAGE 2 - GATING NETWORK TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)",
    "print(\"STAGE 2: TRAINING GATING NETWORK\")",
    "print()",
    "",
    "# Load full dataset (all elevation zones)",
    "full_dataset = V7Dataset(",
    "    predictions_path=config.v4_path / 'predictions.npy',",
    "    targets_path=config.v4_path / 'targets.npy',",
    "    context_path=config.data_dir / 'context_features_spatial.npy',",
    "    mask_path=None,  # No mask - use all data",
    "    config=config,",
    "    use_actual_data=Path(config.v4_path / 'predictions.npy').exists()",
    ")",
    "",
    "full_loader = DataLoader(",
    "    full_dataset,",
    "    batch_size=config.batch_size,",
    "    shuffle=True,",
    "    num_workers=0",
    ")",
    "",
    "# Initialize complete V7-AMES model",
    "v7_model = V7_AMES(config).to(config.device)",
    "",
    "# Load expert checkpoints",
    "if GNN_AVAILABLE and Path(config.output_dir / 'expert1_best.pt').exists():",
    "    checkpoint = torch.load(config.output_dir / 'expert1_best.pt')",
    "    v7_model.expert1.load_state_dict(checkpoint['model_state_dict'])",
    "    print(\" Expert 1 loaded\")",
    "",
    "if Path(config.output_dir / 'expert2_best.pt').exists():",
    "    checkpoint = torch.load(config.output_dir / 'expert2_best.pt')",
    "    v7_model.expert2.load_state_dict(checkpoint['model_state_dict'])",
    "    print(\" Expert 2 loaded\")",
    "",
    "if GNN_AVAILABLE and Path(config.output_dir / 'expert3_best.pt').exists():",
    "    checkpoint = torch.load(config.output_dir / 'expert3_best.pt')",
    "    v7_model.expert3.load_state_dict(checkpoint['model_state_dict'])",
    "    print(\" Expert 3 loaded\")",
    "",
    "# Freeze experts",
    "for param in v7_model.expert1.parameters() if v7_model.expert1 else []:",
    "    param.requires_grad = False",
    "for param in v7_model.expert2.parameters():",
    "    param.requires_grad = False",
    "for param in v7_model.expert3.parameters() if v7_model.expert3 else []:",
    "    param.requires_grad = False",
    "",
    "print(\"Experts frozen. Training only gating network...\")",
    "",
    "# Optimizer for gating network only",
    "gating_optimizer = torch.optim.Adam(",
    "    v7_model.gating_network.parameters(),",
    "    lr=config.learning_rate,",
    "    weight_decay=config.weight_decay",
    ")",
    "",
    "# Training loop Stage 2",
    "best_loss = float('inf')",
    "patience_counter = 0",
    "",
    "for epoch in range(config.epochs_stage2):",
    "    v7_model.train()",
    "    total_loss = 0",
    "    num_batches = 0",
    "",
    "    pbar = tqdm(full_loader, desc=f\"Stage 2 Epoch {epoch+1}/{config.epochs_stage2}\")",
    "    for x_grid, x_graph, context, y in pbar:",
    "        x_grid = x_grid.to(config.device)",
    "        x_graph = x_graph.to(config.device)",
    "        context = context.to(config.device)",
    "        y = y.to(config.device)",
    "",
    "        gating_optimizer.zero_grad()",
    "",
    "        # Forward through complete model (with correct inputs)",
    "        predictions, gating_weights = v7_model(x_grid, x_graph, context, edge_index)",
    "",
    "        # Loss",
    "        loss = criterion(predictions, y.mean(dim=2))",
    "",
    "        # Backward",
    "        loss.backward()",
    "        torch.nn.utils.clip_grad_norm_(v7_model.gating_network.parameters(), max_norm=1.0)",
    "        gating_optimizer.step()",
    "",
    "        total_loss += loss.item()",
    "        num_batches += 1",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})",
    "",
    "    avg_loss = total_loss / num_batches",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")",
    "",
    "    if avg_loss < best_loss:",
    "        best_loss = avg_loss",
    "        patience_counter = 0",
    "        torch.save({",
    "            'epoch': epoch,",
    "            'model_state_dict': v7_model.state_dict(),",
    "            'optimizer_state_dict': gating_optimizer.state_dict(),",
    "            'loss': best_loss,",
    "        }, config.output_dir / 'v7_ames_stage2_best.pt')",
    "        print(f\"   Best model saved (loss: {best_loss:.4f})\")",
    "    else:",
    "        patience_counter += 1",
    "        if patience_counter >= config.patience_stage2:",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")",
    "            break",
    "",
    "print(f\"\\nStage 2 complete. Best loss: {best_loss:.4f}\")",
    "",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 3: Joint Fine-Tuning\n",
    "\n",
    "Fine-tune all components with physics-informed loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# SECTION 6: STAGE 3 - JOINT FINE-TUNING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)",
    "print(\"STAGE 3: JOINT FINE-TUNING\")",
    "print()",
    "",
    "# Unfreeze all parameters",
    "for param in v7_model.parameters():",
    "    param.requires_grad = True",
    "",
    "print(\"All parameters unfrozen for joint training\")",
    "",
    "# Optimizer for full model",
    "full_optimizer = torch.optim.Adam(",
    "    v7_model.parameters(),",
    "    lr=config.learning_rate * 0.1,  # Lower LR for fine-tuning",
    "    weight_decay=config.weight_decay",
    ")",
    "",
    "# Training loop Stage 3 with physics-informed loss",
    "best_loss = float('inf')",
    "patience_counter = 0",
    "",
    "for epoch in range(config.epochs_stage3):",
    "    v7_model.train()",
    "    total_loss = 0",
    "    total_mse = 0",
    "    total_mass = 0",
    "    total_oro = 0",
    "    num_batches = 0",
    "",
    "    pbar = tqdm(full_loader, desc=f\"Stage 3 Epoch {epoch+1}/{config.epochs_stage3}\")",
    "    for x_grid, x_graph, context, y in pbar:",
    "        x_grid = x_grid.to(config.device)",
    "        x_graph = x_graph.to(config.device)",
    "        context = context.to(config.device)",
    "        y = y.to(config.device)",
    "",
    "        full_optimizer.zero_grad()",
    "",
    "        # Forward (with correct inputs)",
    "        predictions, gating_weights = v7_model(x_grid, x_graph, context, edge_index)",
    "",
    "        # Physics-informed loss (context aggregation handled inside function)",
    "        loss, loss_components = v7_model.physics_informed_loss(",
    "            predictions, y.mean(dim=2), context",
    "        )",
    "",
    "        # Backward",
    "        loss.backward()",
    "        torch.nn.utils.clip_grad_norm_(v7_model.parameters(), max_norm=1.0)",
    "        full_optimizer.step()",
    "",
    "        total_loss += loss.item()",
    "        total_mse += loss_components['mse']",
    "        total_mass += loss_components['mass']",
    "        total_oro += loss_components['orographic']",
    "        num_batches += 1",
    "",
    "        pbar.set_postfix({",
    "            'loss': f'{loss.item():.4f}',",
    "            'mse': f'{loss_components[\"mse\"]:.4f}'",
    "        })",
    "",
    "    avg_loss = total_loss / num_batches",
    "    avg_mse = total_mse / num_batches",
    "    avg_mass = total_mass / num_batches",
    "    avg_oro = total_oro / num_batches",
    "",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f} \"",
    "          f\"(MSE: {avg_mse:.4f}, Mass: {avg_mass:.4f}, Oro: {avg_oro:.4f})\")",
    "",
    "    if avg_loss < best_loss:",
    "        best_loss = avg_loss",
    "        patience_counter = 0",
    "        torch.save({",
    "            'epoch': epoch,",
    "            'model_state_dict': v7_model.state_dict(),",
    "            'optimizer_state_dict': full_optimizer.state_dict(),",
    "            'loss': best_loss,",
    "        }, config.output_dir / 'v7_ames_final_best.pt')",
    "        print(f\"   Best model saved (loss: {best_loss:.4f})\")",
    "    else:",
    "        patience_counter += 1",
    "        if patience_counter >= config.patience_stage3:",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")",
    "            break",
    "",
    "print(f\"\\nStage 3 complete. Best loss: {best_loss:.4f}\")",
    "",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Complete - Results Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)",
    "print(\"V7-AMES TRAINING COMPLETE\")",
    "print()",
    "print(\"Final model saved: v7_ames_final_best.pt\")",
    "print()",
    "print(\"Training Summary:\")",
    "print(\"  Stage 1: Expert pre-training - COMPLETE\")",
    "print(\"  Stage 2: Gating network - COMPLETE\")",
    "print(\"  Stage 3: Joint fine-tuning - COMPLETE\")",
    "print()",
    "print(\"  1. Evaluate on validation set\")",
    "print(\"  2. Ablation studies (disable physics loss, compare with V4)\")",
    "print(\"  3. Generate visualizations\")",
    "print(\"  4. Create predictions for test set\")",
    "",
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}