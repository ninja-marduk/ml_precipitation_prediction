{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# V8: GNN-Mamba for Monthly Precipitation Prediction\n",
    "\n",
    "## State Space Models + Graph Neural Networks\n",
    "\n",
    "**Innovation**: First application of Mamba SSM to regional precipitation prediction\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "| Component | V4 (Baseline) | V8 (This Model) |\n",
    "|-----------|---------------|------------------|\n",
    "| Spatial Encoder | GNN (GAT) | GNN (GAT) |\n",
    "| Temporal Encoder | LSTM + Attention | **Mamba SSM** |\n",
    "| Complexity | O(T) LSTM | O(T) Mamba |\n",
    "| Memory | Fixed cells | **Selective** |\n",
    "| Fusion | Concatenation | **Cross-Attention** |\n",
    "\n",
    "### Expected Improvements\n",
    "- Better long-range dependencies (60 months input)\n",
    "- Automatic seasonal pattern capture\n",
    "- More efficient memory usage\n",
    "\n",
    "### Target Metrics\n",
    "| Metric | V4 Baseline | V8 Target |\n",
    "|--------|-------------|----------|\n",
    "| RÂ² | 0.596 | > 0.62 |\n",
    "| RMSE | 84.37 mm | < 82 mm |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colab_setup"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1: ENVIRONMENT SETUP\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "SEED = 42\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    \n",
    "    # Install dependencies\n",
    "    import torch\n",
    "    TORCH_VERSION = torch.__version__.split('+')[0]\n",
    "    CUDA_VERSION = torch.version.cuda\n",
    "    \n",
    "    print(f\"PyTorch: {TORCH_VERSION}\")\n",
    "    print(f\"CUDA: {CUDA_VERSION}\")\n",
    "    \n",
    "    # Install torch_geometric\n",
    "    try:\n",
    "        import torch_geometric\n",
    "        print(f\"PyG already installed: {torch_geometric.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"Installing PyTorch Geometric...\")\n",
    "        !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH_VERSION}+{CUDA_VERSION}.html\n",
    "        !pip install torch-geometric\n",
    "    \n",
    "    # Install Mamba\n",
    "    print(\"\\nInstalling Mamba SSM...\")\n",
    "    !pip install mamba-ssm causal-conv1d>=1.1.0 --quiet\n",
    "    \n",
    "    # Other dependencies\n",
    "    !pip install netCDF4 xarray dask h5netcdf --quiet\n",
    "    \n",
    "else:\n",
    "    BASE_PATH = Path(r'd:\\github.com\\ninja-marduk\\ml_precipitation_prediction')\n",
    "\n",
    "print(f\"\\nBase path: {BASE_PATH}\")\n",
    "print(f\"Running in: {'Google Colab' if IN_COLAB else 'Local Environment'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 1.1: IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import gc\n",
    "import copy\n",
    "import warnings\n",
    "import math\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict, List, Tuple, Optional, Any, Literal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Matplotlib setup\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "\n",
    "# Mamba SSM\n",
    "USE_OFFICIAL_MAMBA = False\n",
    "try:\n",
    "    from mamba_ssm import Mamba\n",
    "    USE_OFFICIAL_MAMBA = True\n",
    "    print(\"Using official mamba-ssm package\")\n",
    "except ImportError:\n",
    "    print(\"Official mamba-ssm not available, using custom implementation\")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_section"
   },
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 2: CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class V8Config:\n",
    "    \"\"\"V8 GNN-Mamba Configuration.\"\"\"\n",
    "    \n",
    "    # === Data Configuration ===\n",
    "    input_window: int = 60          # 5 years of monthly data\n",
    "    horizon: int = 12               # Predict 12 months ahead\n",
    "    train_val_split: float = 0.8\n",
    "    \n",
    "    # Light mode for testing\n",
    "    light_mode: bool = False\n",
    "    light_grid_size: int = 10\n",
    "    \n",
    "    # Enabled horizons\n",
    "    enabled_horizons: List[int] = field(default_factory=lambda: [12])\n",
    "    \n",
    "    # Grid dimensions (updated after loading)\n",
    "    n_lat: int = 61\n",
    "    n_lon: int = 65\n",
    "    n_nodes: int = 61 * 65\n",
    "    \n",
    "    # === Model Dimensions ===\n",
    "    hidden_dim: int = 64            # Main hidden dimension\n",
    "    n_features: int = 5             # Input features\n",
    "    \n",
    "    # === GNN Configuration (Spatial) ===\n",
    "    gnn_type: str = 'GAT'           # GAT, SAGE, or GCN\n",
    "    gnn_num_layers: int = 3\n",
    "    gnn_num_heads: int = 4\n",
    "    gnn_dropout: float = 0.1\n",
    "    \n",
    "    # === MAMBA Configuration (Temporal) ===\n",
    "    mamba_d_state: int = 16         # SSM state dimension\n",
    "    mamba_d_conv: int = 4           # Causal conv kernel size\n",
    "    mamba_expand: int = 2           # Inner dimension expansion\n",
    "    mamba_num_layers: int = 2       # Number of Mamba blocks\n",
    "    mamba_bidirectional: bool = True  # Process forward + backward\n",
    "    mamba_dropout: float = 0.1\n",
    "    \n",
    "    # === Cross-Modal Attention ===\n",
    "    cross_attn_heads: int = 4\n",
    "    cross_attn_dropout: float = 0.1\n",
    "    \n",
    "    # === Training ===\n",
    "    epochs: int = 100\n",
    "    batch_size: int = 4\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-5\n",
    "    patience: int = 15\n",
    "    gradient_clip: float = 1.0\n",
    "    \n",
    "    # === Physics Loss (from V7) ===\n",
    "    use_physics_loss: bool = True\n",
    "    lambda_mass_conservation: float = 0.05\n",
    "    lambda_orographic: float = 0.1\n",
    "    high_elev_threshold: float = 3000.0\n",
    "    \n",
    "    # === Scheduler ===\n",
    "    scheduler_type: str = 'cosine'  # 'plateau' or 'cosine'\n",
    "    cosine_T0: int = 20\n",
    "    cosine_T_mult: int = 2\n",
    "    \n",
    "    # === Output ===\n",
    "    output_dir: str = 'V8_GNN_Mamba'\n",
    "    save_checkpoints: bool = True\n",
    "    \n",
    "    # === Data handling ===\n",
    "    allow_missing_features: bool = True\n",
    "\n",
    "\n",
    "# Feature sets\n",
    "FEATURE_SETS = {\n",
    "    'BASIC': ['total_precipitation'],\n",
    "    'KCE': ['total_precipitation', 'elev_low', 'elev_med', 'elev_high'],\n",
    "    'FULL': ['total_precipitation', 'elevation', 'slope', 'aspect',\n",
    "             'elev_low', 'elev_med', 'elev_high']\n",
    "}\n",
    "\n",
    "# Initialize config\n",
    "CONFIG = V8Config()\n",
    "\n",
    "# Adjust for Colab memory constraints\n",
    "if IN_COLAB:\n",
    "    CONFIG.batch_size = 2\n",
    "    CONFIG.hidden_dim = 48\n",
    "    CONFIG.mamba_d_state = 12\n",
    "    CONFIG.gnn_num_heads = 2\n",
    "    print(\"Adjusted config for Colab memory constraints\")\n",
    "\n",
    "print(\"\\n=== V8 GNN-Mamba Configuration ===\")\n",
    "print(f\"  Input window: {CONFIG.input_window} months\")\n",
    "print(f\"  Horizon: {CONFIG.horizon} months\")\n",
    "print(f\"  Hidden dim: {CONFIG.hidden_dim}\")\n",
    "print(f\"  Mamba d_state: {CONFIG.mamba_d_state}\")\n",
    "print(f\"  Mamba bidirectional: {CONFIG.mamba_bidirectional}\")\n",
    "print(f\"  GNN type: {CONFIG.gnn_type}\")\n",
    "print(f\"  Batch size: {CONFIG.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mamba_implementation"
   },
   "source": [
    "## 3. Mamba Implementation\n",
    "\n",
    "### State Space Model Basics\n",
    "\n",
    "Mamba is based on the continuous-time state space model:\n",
    "\n",
    "$$h'(t) = Ah(t) + Bx(t)$$\n",
    "$$y(t) = Ch(t) + Dx(t)$$\n",
    "\n",
    "**Key innovation**: A, B, C are **input-dependent** (selective), allowing the model to:\n",
    "- Focus on relevant information\n",
    "- Forget irrelevant context\n",
    "- Maintain O(n) complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mamba_block"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 3: MAMBA IMPLEMENTATION\n",
    "# ============================================================\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Selective State Space Model Block.\n",
    "    \n",
    "    Based on: \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\"\n",
    "    \n",
    "    Key components:\n",
    "    - Input-dependent (selective) SSM parameters\n",
    "    - Gated architecture with SiLU activation\n",
    "    - Causal 1D convolution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 4,\n",
    "        expand: int = 2,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.d_inner = d_model * expand\n",
    "        \n",
    "        # Input projection (splits into x and z for gating)\n",
    "        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)\n",
    "        \n",
    "        # Causal 1D convolution (depthwise)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=self.d_inner,\n",
    "            out_channels=self.d_inner,\n",
    "            kernel_size=d_conv,\n",
    "            padding=d_conv - 1,\n",
    "            groups=self.d_inner  # Depthwise separable\n",
    "        )\n",
    "        \n",
    "        # SSM parameters projection (input-dependent B, C, dt)\n",
    "        self.x_proj = nn.Linear(self.d_inner, d_state * 2 + 1, bias=False)\n",
    "        \n",
    "        # A is log-parametrized for numerical stability\n",
    "        A = torch.arange(1, d_state + 1, dtype=torch.float32)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        \n",
    "        # D is the skip connection coefficient\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)\n",
    "        \n",
    "        # Normalization and dropout\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            y: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch, seq_len, _ = x.shape\n",
    "        residual = x\n",
    "        \n",
    "        # Project and split for gating\n",
    "        xz = self.in_proj(x)  # (B, L, 2*d_inner)\n",
    "        x_branch, z = xz.chunk(2, dim=-1)  # Each: (B, L, d_inner)\n",
    "        \n",
    "        # Causal 1D convolution\n",
    "        x_conv = x_branch.transpose(1, 2)  # (B, d_inner, L)\n",
    "        x_conv = self.conv1d(x_conv)[:, :, :seq_len]  # Truncate to maintain causality\n",
    "        x_conv = x_conv.transpose(1, 2)  # (B, L, d_inner)\n",
    "        x_conv = F.silu(x_conv)\n",
    "        \n",
    "        # Selective SSM\n",
    "        y = self.ssm(x_conv)\n",
    "        \n",
    "        # Gating with z\n",
    "        z = F.silu(z)\n",
    "        output = y * z\n",
    "        \n",
    "        # Output projection + residual\n",
    "        output = self.out_proj(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.norm(output + residual)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def ssm(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Selective State Space Model computation.\n",
    "        \n",
    "        Discretized SSM:\n",
    "            h_t = A_bar * h_{t-1} + B_bar * x_t\n",
    "            y_t = C * h_t + D * x_t\n",
    "        \"\"\"\n",
    "        batch, seq_len, d_inner = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Project to get input-dependent B, C, dt\n",
    "        x_proj = self.x_proj(x)  # (B, L, d_state*2 + 1)\n",
    "        \n",
    "        # Split projections\n",
    "        B = x_proj[:, :, :self.d_state]  # (B, L, d_state)\n",
    "        C = x_proj[:, :, self.d_state:2*self.d_state]  # (B, L, d_state)\n",
    "        dt = F.softplus(x_proj[:, :, -1:])  # (B, L, 1) - positive timestep\n",
    "        \n",
    "        # A from log-parametrization (negative for stability)\n",
    "        A = -torch.exp(self.A_log.float())  # (d_state,)\n",
    "        \n",
    "        # Discretize: A_bar = exp(A * dt)\n",
    "        A_bar = torch.exp(\n",
    "            A.view(1, 1, self.d_state) * dt\n",
    "        )  # (B, L, d_state)\n",
    "        \n",
    "        # B_bar = B * dt (simplified discretization)\n",
    "        B_bar = B * dt  # (B, L, d_state)\n",
    "        \n",
    "        # Sequential scan (can be parallelized with associative scan)\n",
    "        h = torch.zeros(batch, d_inner, self.d_state, device=device)\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # State update: h_t = A_bar * h_{t-1} + B_bar * x_t\n",
    "            h = h * A_bar[:, t:t+1, :].unsqueeze(1) + \\\n",
    "                x[:, t:t+1, :].unsqueeze(-1) * B_bar[:, t:t+1, :].unsqueeze(1)\n",
    "            # h: (B, d_inner, d_state)\n",
    "            \n",
    "            # Output: y_t = C * h_t\n",
    "            y_t = (h * C[:, t:t+1, :].unsqueeze(1)).sum(dim=-1)  # (B, d_inner)\n",
    "            outputs.append(y_t)\n",
    "        \n",
    "        y = torch.stack(outputs, dim=1)  # (B, L, d_inner)\n",
    "        \n",
    "        # Skip connection with D\n",
    "        y = y + x * self.D.view(1, 1, -1)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class MambaTemporalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer bidirectional Mamba encoder for temporal sequences.\n",
    "    \n",
    "    Replaces LSTM + Temporal Attention from V4.\n",
    "    \n",
    "    Advantages:\n",
    "    - O(n) complexity (same as LSTM)\n",
    "    - Selective memory (learns what to remember)\n",
    "    - Parallelizable during training\n",
    "    - Better long-range dependencies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_state: int = 16,\n",
    "        d_conv: int = 4,\n",
    "        expand: int = 2,\n",
    "        n_layers: int = 2,\n",
    "        bidirectional: bool = True,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Forward Mamba layers\n",
    "        self.forward_layers = nn.ModuleList([\n",
    "            MambaBlock(\n",
    "                d_model=d_model,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                dropout=dropout\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Backward Mamba layers (if bidirectional)\n",
    "        if bidirectional:\n",
    "            self.backward_layers = nn.ModuleList([\n",
    "                MambaBlock(\n",
    "                    d_model=d_model,\n",
    "                    d_state=d_state,\n",
    "                    d_conv=d_conv,\n",
    "                    expand=expand,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ])\n",
    "            # Merge forward and backward\n",
    "            self.merge = nn.Linear(d_model * 2, d_model)\n",
    "            self.merge_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.output_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            h: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        h_fwd = x\n",
    "        for layer in self.forward_layers:\n",
    "            h_fwd = layer(h_fwd)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            # Backward pass (flip, process, flip back)\n",
    "            h_bwd = torch.flip(x, dims=[1])\n",
    "            for layer in self.backward_layers:\n",
    "                h_bwd = layer(h_bwd)\n",
    "            h_bwd = torch.flip(h_bwd, dims=[1])\n",
    "            \n",
    "            # Merge forward and backward\n",
    "            h = self.merge(torch.cat([h_fwd, h_bwd], dim=-1))\n",
    "            h = self.merge_norm(h)\n",
    "        else:\n",
    "            h = h_fwd\n",
    "        \n",
    "        return self.output_norm(h)\n",
    "\n",
    "\n",
    "# Test Mamba implementation\n",
    "print(\"Testing Mamba implementation...\")\n",
    "test_mamba = MambaTemporalEncoder(\n",
    "    d_model=CONFIG.hidden_dim,\n",
    "    d_state=CONFIG.mamba_d_state,\n",
    "    d_conv=CONFIG.mamba_d_conv,\n",
    "    expand=CONFIG.mamba_expand,\n",
    "    n_layers=CONFIG.mamba_num_layers,\n",
    "    bidirectional=CONFIG.mamba_bidirectional\n",
    ").to(device)\n",
    "\n",
    "test_input = torch.randn(2, CONFIG.input_window, CONFIG.hidden_dim).to(device)\n",
    "test_output = test_mamba(test_input)\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in test_mamba.parameters()):,}\")\n",
    "del test_mamba, test_input, test_output\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnn_section"
   },
   "source": [
    "## 4. Spatial GNN Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnn_encoder"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 4: SPATIAL GNN ENCODER\n",
    "# ============================================================\n",
    "\n",
    "class SpatialGNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Neural Network for spatial encoding.\n",
    "    \n",
    "    Supports GAT, GraphSAGE, and GCN.\n",
    "    Based on V4 architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_layers: int = 3,\n",
    "        gnn_type: str = 'GAT',\n",
    "        num_heads: int = 4,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gnn_type = gnn_type\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # GNN layers\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if gnn_type == 'GAT':\n",
    "                # GAT: multi-head attention\n",
    "                self.gnn_layers.append(\n",
    "                    GATConv(\n",
    "                        in_channels=hidden_dim,\n",
    "                        out_channels=hidden_dim // num_heads,\n",
    "                        heads=num_heads,\n",
    "                        dropout=dropout,\n",
    "                        concat=True\n",
    "                    )\n",
    "                )\n",
    "            elif gnn_type == 'SAGE':\n",
    "                self.gnn_layers.append(\n",
    "                    SAGEConv(hidden_dim, hidden_dim)\n",
    "                )\n",
    "            else:  # GCN\n",
    "                self.gnn_layers.append(\n",
    "                    GCNConv(hidden_dim, hidden_dim)\n",
    "                )\n",
    "            \n",
    "            self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_weight: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (num_nodes, input_dim) or (batch*num_nodes, input_dim)\n",
    "            edge_index: (2, num_edges)\n",
    "            edge_weight: (num_edges,) optional\n",
    "        Returns:\n",
    "            h: (num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Input projection\n",
    "        h = self.input_proj(x)\n",
    "        \n",
    "        # GNN layers with residual connections\n",
    "        for i, (gnn, norm) in enumerate(zip(self.gnn_layers, self.norms)):\n",
    "            h_res = h\n",
    "            \n",
    "            # GNN forward\n",
    "            if self.gnn_type == 'GAT':\n",
    "                h = gnn(h, edge_index)\n",
    "            elif edge_weight is not None and self.gnn_type == 'GCN':\n",
    "                h = gnn(h, edge_index, edge_weight)\n",
    "            else:\n",
    "                h = gnn(h, edge_index)\n",
    "            \n",
    "            # Activation + Normalization + Residual\n",
    "            h = F.gelu(h)\n",
    "            h = norm(h)\n",
    "            h = self.dropout(h)\n",
    "            h = h + h_res  # Residual\n",
    "        \n",
    "        return self.output_norm(h)\n",
    "\n",
    "\n",
    "print(\"SpatialGNNEncoder defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "main_model"
   },
   "source": [
    "## 5. GNN-Mamba V8 Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnn_mamba_v8"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 5: GNN-MAMBA V8 MAIN MODEL\n",
    "# ============================================================\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention between spatial (GNN) and temporal (Mamba) representations.\n",
    "    \n",
    "    Allows spatial features to attend to temporal context and vice versa.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int = 4,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key_value: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch, n_nodes, d_model) - spatial features\n",
    "            key_value: (batch, n_nodes, d_model) - temporal features\n",
    "        Returns:\n",
    "            output: (batch, n_nodes, d_model)\n",
    "        \"\"\"\n",
    "        # Cross-attention\n",
    "        attn_out, _ = self.attn(query, key_value, key_value)\n",
    "        x = self.norm1(query + attn_out)\n",
    "        \n",
    "        # Feed-forward\n",
    "        x = self.norm2(x + self.ffn(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class GNN_Mamba_V8(nn.Module):\n",
    "    \"\"\"\n",
    "    V8: Graph Neural Network + Mamba State Space Model\n",
    "    \n",
    "    Architecture:\n",
    "    1. Input projection\n",
    "    2. Parallel branches:\n",
    "       - GNN: spatial encoding per timestep\n",
    "       - Mamba: temporal encoding per node\n",
    "    3. Cross-modal attention fusion\n",
    "    4. Prediction head\n",
    "    \n",
    "    Improvements over V4:\n",
    "    - Mamba replaces LSTM + Attention\n",
    "    - Cross-modal attention instead of concatenation\n",
    "    - Better handling of long sequences (60 months)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: V8Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # === Input Projection ===\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(config.n_features, config.hidden_dim),\n",
    "            nn.LayerNorm(config.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.gnn_dropout)\n",
    "        )\n",
    "        \n",
    "        # === Spatial Branch: GNN ===\n",
    "        self.gnn_encoder = SpatialGNNEncoder(\n",
    "            input_dim=config.hidden_dim,\n",
    "            hidden_dim=config.hidden_dim,\n",
    "            num_layers=config.gnn_num_layers,\n",
    "            gnn_type=config.gnn_type,\n",
    "            num_heads=config.gnn_num_heads,\n",
    "            dropout=config.gnn_dropout\n",
    "        )\n",
    "        \n",
    "        # === Temporal Branch: Mamba ===\n",
    "        self.mamba_encoder = MambaTemporalEncoder(\n",
    "            d_model=config.hidden_dim,\n",
    "            d_state=config.mamba_d_state,\n",
    "            d_conv=config.mamba_d_conv,\n",
    "            expand=config.mamba_expand,\n",
    "            n_layers=config.mamba_num_layers,\n",
    "            bidirectional=config.mamba_bidirectional,\n",
    "            dropout=config.mamba_dropout\n",
    "        )\n",
    "        \n",
    "        # === Cross-Modal Fusion ===\n",
    "        self.cross_attention = CrossModalAttention(\n",
    "            d_model=config.hidden_dim,\n",
    "            num_heads=config.cross_attn_heads,\n",
    "            dropout=config.cross_attn_dropout\n",
    "        )\n",
    "        \n",
    "        # === Prediction Head ===\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.gnn_dropout),\n",
    "            nn.Linear(config.hidden_dim, config.hidden_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.gnn_dropout),\n",
    "            nn.Linear(config.hidden_dim // 2, config.horizon)\n",
    "        )\n",
    "        \n",
    "        # Store dimensions for reshaping\n",
    "        self.n_lat = config.n_lat\n",
    "        self.n_lon = config.n_lon\n",
    "        self.n_nodes = config.n_nodes\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with Xavier/Kaiming.\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name and param.dim() >= 2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "                \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_weight: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, seq_len, n_nodes, n_features) or\n",
    "               (batch, seq_len, lat, lon, n_features)\n",
    "            edge_index: (2, num_edges)\n",
    "            edge_weight: (num_edges,) optional\n",
    "            \n",
    "        Returns:\n",
    "            pred: (batch, n_nodes, horizon)\n",
    "            info: dict with intermediate representations\n",
    "        \"\"\"\n",
    "        # Handle different input shapes\n",
    "        if x.dim() == 5:  # (B, T, lat, lon, F)\n",
    "            batch_size, seq_len, n_lat, n_lon, n_features = x.shape\n",
    "            n_nodes = n_lat * n_lon\n",
    "            x = x.view(batch_size, seq_len, n_nodes, n_features)\n",
    "        else:  # (B, T, N, F)\n",
    "            batch_size, seq_len, n_nodes, n_features = x.shape\n",
    "        \n",
    "        # 1. Input projection\n",
    "        x = self.input_proj(x)  # (B, T, N, H)\n",
    "        \n",
    "        # 2. Spatial encoding (GNN per timestep)\n",
    "        h_spatial = self._encode_spatial(x, edge_index, edge_weight)\n",
    "        # (B, T, N, H)\n",
    "        \n",
    "        # 3. Temporal encoding (Mamba per node)\n",
    "        h_temporal = self._encode_temporal(x)\n",
    "        # (B, T, N, H)\n",
    "        \n",
    "        # 4. Cross-modal fusion (at final timestep)\n",
    "        h_fused = self._fuse_representations(h_spatial, h_temporal)\n",
    "        # (B, N, H)\n",
    "        \n",
    "        # 5. Prediction\n",
    "        pred = self.predictor(h_fused)  # (B, N, horizon)\n",
    "        \n",
    "        info = {\n",
    "            'h_spatial': h_spatial[:, -1, :, :],  # Last timestep\n",
    "            'h_temporal': h_temporal[:, -1, :, :],\n",
    "            'h_fused': h_fused\n",
    "        }\n",
    "        \n",
    "        return pred, info\n",
    "    \n",
    "    def _encode_spatial(self, x, edge_index, edge_weight):\n",
    "        \"\"\"Apply GNN to each timestep.\"\"\"\n",
    "        batch_size, seq_len, n_nodes, hidden_dim = x.shape\n",
    "        \n",
    "        # Process in chunks to save memory\n",
    "        chunk_size = 10  # timesteps per chunk\n",
    "        outputs = []\n",
    "        \n",
    "        for t_start in range(0, seq_len, chunk_size):\n",
    "            t_end = min(t_start + chunk_size, seq_len)\n",
    "            x_chunk = x[:, t_start:t_end, :, :]  # (B, chunk, N, H)\n",
    "            \n",
    "            chunk_len = t_end - t_start\n",
    "            x_flat = x_chunk.reshape(batch_size * chunk_len, n_nodes, hidden_dim)\n",
    "            \n",
    "            # Apply GNN to each graph in batch\n",
    "            h_list = []\n",
    "            for i in range(x_flat.shape[0]):\n",
    "                h_i = self.gnn_encoder(x_flat[i], edge_index, edge_weight)\n",
    "                h_list.append(h_i)\n",
    "            \n",
    "            h_chunk = torch.stack(h_list, dim=0)  # (B*chunk, N, H)\n",
    "            h_chunk = h_chunk.view(batch_size, chunk_len, n_nodes, hidden_dim)\n",
    "            outputs.append(h_chunk)\n",
    "        \n",
    "        return torch.cat(outputs, dim=1)  # (B, T, N, H)\n",
    "    \n",
    "    def _encode_temporal(self, x):\n",
    "        \"\"\"Apply Mamba to each node's temporal sequence.\"\"\"\n",
    "        batch_size, seq_len, n_nodes, hidden_dim = x.shape\n",
    "        \n",
    "        # Reshape: (B, T, N, H) -> (B*N, T, H)\n",
    "        x_temporal = x.permute(0, 2, 1, 3)  # (B, N, T, H)\n",
    "        x_temporal = x_temporal.reshape(batch_size * n_nodes, seq_len, hidden_dim)\n",
    "        \n",
    "        # Apply Mamba\n",
    "        h_temporal = self.mamba_encoder(x_temporal)  # (B*N, T, H)\n",
    "        \n",
    "        # Reshape back: (B*N, T, H) -> (B, T, N, H)\n",
    "        h_temporal = h_temporal.view(batch_size, n_nodes, seq_len, hidden_dim)\n",
    "        h_temporal = h_temporal.permute(0, 2, 1, 3)  # (B, T, N, H)\n",
    "        \n",
    "        return h_temporal\n",
    "    \n",
    "    def _fuse_representations(self, h_spatial, h_temporal):\n",
    "        \"\"\"Fuse spatial and temporal representations at final timestep.\"\"\"\n",
    "        # Take final timestep\n",
    "        h_spatial_final = h_spatial[:, -1, :, :]  # (B, N, H)\n",
    "        h_temporal_final = h_temporal[:, -1, :, :]  # (B, N, H)\n",
    "        \n",
    "        # Cross-attention: spatial queries temporal\n",
    "        h_fused = self.cross_attention(h_spatial_final, h_temporal_final)\n",
    "        \n",
    "        return h_fused  # (B, N, H)\n",
    "\n",
    "\n",
    "# Test model\n",
    "print(\"\\nTesting GNN_Mamba_V8...\")\n",
    "test_config = V8Config(\n",
    "    n_features=4,\n",
    "    n_lat=10,\n",
    "    n_lon=10,\n",
    "    n_nodes=100,\n",
    "    hidden_dim=32,\n",
    "    mamba_d_state=8,\n",
    "    input_window=12,\n",
    "    horizon=6\n",
    ")\n",
    "\n",
    "test_model = GNN_Mamba_V8(test_config).to(device)\n",
    "test_x = torch.randn(2, 12, 100, 4).to(device)\n",
    "test_edge_index = torch.randint(0, 100, (2, 500)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred, test_info = test_model(test_x, test_edge_index)\n",
    "\n",
    "print(f\"  Input shape: {test_x.shape}\")\n",
    "print(f\"  Output shape: {test_pred.shape}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "\n",
    "del test_model, test_x, test_pred, test_info\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "print(\"\\nGNN_Mamba_V8 test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_section"
   },
   "source": [
    "## 6. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_loading"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 6: DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = BASE_PATH / 'data' / 'processed' / 'dataset_monthly_precipitation_boyaca_combined.nc'\n",
    "OUTPUT_ROOT = BASE_PATH / 'models' / 'output' / CONFIG.output_dir\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_ROOT}\")\n",
    "print(f\"Data exists: {DATA_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_functions"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 6.1: DATA FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def validate_dataset(ds: xr.Dataset, config: V8Config) -> Tuple[str, str]:\n",
    "    \"\"\"Validate dataset dimensions.\"\"\"\n",
    "    lat_candidates = ['latitude', 'lat', 'y']\n",
    "    lon_candidates = ['longitude', 'lon', 'x']\n",
    "    \n",
    "    lat_dim = next((d for d in lat_candidates if d in ds.dims), None)\n",
    "    lon_dim = next((d for d in lon_candidates if d in ds.dims), None)\n",
    "    \n",
    "    if lat_dim is None or lon_dim is None:\n",
    "        raise ValueError(f\"Cannot find lat/lon dims in {list(ds.dims.keys())}\")\n",
    "    \n",
    "    if 'time' not in ds.dims:\n",
    "        raise ValueError(\"Dataset must have 'time' dimension\")\n",
    "    \n",
    "    # Update config\n",
    "    config.n_lat = ds.dims[lat_dim]\n",
    "    config.n_lon = ds.dims[lon_dim]\n",
    "    config.n_nodes = config.n_lat * config.n_lon\n",
    "    \n",
    "    return lat_dim, lon_dim\n",
    "\n",
    "\n",
    "def load_dataset(data_path: Path, config: V8Config) -> xr.Dataset:\n",
    "    \"\"\"Load and validate NetCDF dataset.\"\"\"\n",
    "    print(f\"Loading dataset from: {data_path}\")\n",
    "    ds = xr.open_dataset(data_path)\n",
    "    \n",
    "    lat_dim, lon_dim = validate_dataset(ds, config)\n",
    "    \n",
    "    print(f\"Dataset dimensions:\")\n",
    "    for dim, size in ds.dims.items():\n",
    "        print(f\"  - {dim}: {size}\")\n",
    "    \n",
    "    print(f\"Available variables: {list(ds.data_vars)}\")\n",
    "    \n",
    "    if config.light_mode:\n",
    "        ds = ds.isel({\n",
    "            lat_dim: slice(0, config.light_grid_size),\n",
    "            lon_dim: slice(0, config.light_grid_size)\n",
    "        })\n",
    "        config.n_lat = config.light_grid_size\n",
    "        config.n_lon = config.light_grid_size\n",
    "        config.n_nodes = config.n_lat * config.n_lon\n",
    "        print(f\"Light mode: using {config.light_grid_size}x{config.light_grid_size} grid\")\n",
    "    \n",
    "    return ds, lat_dim, lon_dim\n",
    "\n",
    "\n",
    "def create_elevation_clusters(ds: xr.Dataset, n_clusters: int = 3) -> xr.Dataset:\n",
    "    \"\"\"Add elevation cluster features (KCE).\"\"\"\n",
    "    if 'elevation' not in ds:\n",
    "        print(\"Warning: No elevation data, skipping clustering\")\n",
    "        return ds\n",
    "    \n",
    "    elevation = ds['elevation'].values\n",
    "    elev_dims = ds['elevation'].dims\n",
    "    \n",
    "    if elevation.ndim == 3:\n",
    "        elevation = elevation[0]\n",
    "        elev_dims = elev_dims[-2:]\n",
    "    \n",
    "    valid_mask = ~np.isnan(elevation)\n",
    "    elev_flat = elevation[valid_mask].reshape(-1, 1)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=10)\n",
    "    labels = np.full(elevation.shape, -1)\n",
    "    labels[valid_mask] = kmeans.fit_predict(elev_flat)\n",
    "    \n",
    "    for i, name in enumerate(['elev_low', 'elev_med', 'elev_high']):\n",
    "        cluster_data = np.zeros_like(elevation)\n",
    "        cluster_data[labels == i] = 1.0\n",
    "        ds[name] = xr.DataArray(data=cluster_data, dims=elev_dims)\n",
    "    \n",
    "    print(\"Added elevation clusters: elev_low, elev_med, elev_high\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def build_spatial_graph(\n",
    "    ds: xr.Dataset,\n",
    "    lat_dim: str,\n",
    "    lon_dim: str,\n",
    "    k_neighbors: int = 8,\n",
    "    max_edges: int = 500000\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Build spatial graph based on geographic proximity.\"\"\"\n",
    "    lat_vals = ds[lat_dim].values\n",
    "    lon_vals = ds[lon_dim].values\n",
    "    \n",
    "    n_lat, n_lon = len(lat_vals), len(lon_vals)\n",
    "    n_nodes = n_lat * n_lon\n",
    "    \n",
    "    # Create node positions\n",
    "    lat_grid, lon_grid = np.meshgrid(lat_vals, lon_vals, indexing='ij')\n",
    "    positions = np.stack([lat_grid.flatten(), lon_grid.flatten()], axis=1)\n",
    "    \n",
    "    # Build edges based on grid connectivity (8-connectivity)\n",
    "    edges = []\n",
    "    weights = []\n",
    "    \n",
    "    for i in range(n_lat):\n",
    "        for j in range(n_lon):\n",
    "            node_idx = i * n_lon + j\n",
    "            \n",
    "            # 8-connectivity neighbors\n",
    "            neighbors = [\n",
    "                (i-1, j-1), (i-1, j), (i-1, j+1),\n",
    "                (i, j-1),             (i, j+1),\n",
    "                (i+1, j-1), (i+1, j), (i+1, j+1)\n",
    "            ]\n",
    "            \n",
    "            for ni, nj in neighbors:\n",
    "                if 0 <= ni < n_lat and 0 <= nj < n_lon:\n",
    "                    neighbor_idx = ni * n_lon + nj\n",
    "                    \n",
    "                    # Distance-based weight\n",
    "                    dist = np.sqrt(\n",
    "                        (lat_vals[i] - lat_vals[ni])**2 +\n",
    "                        (lon_vals[j] - lon_vals[nj])**2\n",
    "                    )\n",
    "                    weight = 1.0 / (dist + 1e-6)\n",
    "                    \n",
    "                    edges.append([node_idx, neighbor_idx])\n",
    "                    weights.append(weight)\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_weight = torch.tensor(weights, dtype=torch.float32)\n",
    "    \n",
    "    # Normalize weights\n",
    "    edge_weight = edge_weight / edge_weight.max()\n",
    "    \n",
    "    # Limit edges if needed\n",
    "    if edge_index.shape[1] > max_edges:\n",
    "        top_k = torch.topk(edge_weight, max_edges).indices\n",
    "        edge_index = edge_index[:, top_k]\n",
    "        edge_weight = edge_weight[top_k]\n",
    "    \n",
    "    print(f\"Graph built: {n_nodes} nodes, {edge_index.shape[1]} edges\")\n",
    "    \n",
    "    return edge_index, edge_weight\n",
    "\n",
    "\n",
    "def extract_features(\n",
    "    ds: xr.Dataset,\n",
    "    feature_names: List[str],\n",
    "    config: V8Config\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract features from dataset.\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for name in feature_names:\n",
    "        if name in ds.data_vars:\n",
    "            data = ds[name].values\n",
    "            if data.ndim == 2:  # Static feature\n",
    "                data = np.broadcast_to(data, (ds.dims['time'], *data.shape))\n",
    "            features.append(data)\n",
    "        elif config.allow_missing_features:\n",
    "            print(f\"Warning: Missing feature {name}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Missing feature: {name}\")\n",
    "    \n",
    "    if not features:\n",
    "        raise ValueError(\"No features extracted\")\n",
    "    \n",
    "    features = np.stack(features, axis=-1)\n",
    "    features = np.nan_to_num(features, nan=0.0)\n",
    "    \n",
    "    config.n_features = features.shape[-1]\n",
    "    print(f\"Extracted features shape: {features.shape}\")\n",
    "    \n",
    "    return features.astype(np.float32)\n",
    "\n",
    "\n",
    "print(\"Data functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset_class"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 6.2: DATASET CLASS\n",
    "# ============================================================\n",
    "\n",
    "class V8Dataset(Dataset):\n",
    "    \"\"\"Dataset for V8 GNN-Mamba model.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        features: torch.Tensor,      # (time, lat, lon, n_features)\n",
    "        target: torch.Tensor,         # (time, lat, lon)\n",
    "        input_window: int,\n",
    "        horizon: int,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_weight: torch.Tensor,\n",
    "        start_idx: int,\n",
    "        end_idx: int\n",
    "    ):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.input_window = input_window\n",
    "        self.horizon = horizon\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_weight = edge_weight\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx\n",
    "        \n",
    "        self.n_lat = features.shape[1]\n",
    "        self.n_lon = features.shape[2]\n",
    "        self.n_nodes = self.n_lat * self.n_lon\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.end_idx - self.start_idx\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = self.start_idx + idx\n",
    "        \n",
    "        # Input sequence\n",
    "        x = self.features[i:i+self.input_window]  # (T, lat, lon, F)\n",
    "        x = x.reshape(self.input_window, self.n_nodes, -1)  # (T, N, F)\n",
    "        \n",
    "        # Target\n",
    "        y = self.target[i+self.input_window:i+self.input_window+self.horizon]\n",
    "        y = y.reshape(self.horizon, self.n_nodes)  # (H, N)\n",
    "        y = y.permute(1, 0)  # (N, H)\n",
    "        \n",
    "        return {\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'edge_index': self.edge_index,\n",
    "            'edge_weight': self.edge_weight\n",
    "        }\n",
    "\n",
    "\n",
    "def prepare_data(ds: xr.Dataset, config: V8Config, lat_dim: str, lon_dim: str):\n",
    "    \"\"\"Prepare train/val datasets.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Preparing data for V8 GNN-Mamba\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Add elevation clusters\n",
    "    ds = create_elevation_clusters(ds)\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(ds, FEATURE_SETS['KCE'], config)\n",
    "    \n",
    "    # Target\n",
    "    target = ds['total_precipitation'].values.astype(np.float32)\n",
    "    target = np.nan_to_num(target, nan=0.0)\n",
    "    print(f\"Target shape: {target.shape}\")\n",
    "    \n",
    "    # Build graph\n",
    "    edge_index, edge_weight = build_spatial_graph(ds, lat_dim, lon_dim)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    features = torch.from_numpy(features)\n",
    "    target = torch.from_numpy(target)\n",
    "    \n",
    "    # Time split\n",
    "    n_time = features.shape[0]\n",
    "    max_start = n_time - config.input_window - config.horizon\n",
    "    \n",
    "    split_idx = int(max_start * config.train_val_split)\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"  Total samples: {max_start + 1}\")\n",
    "    print(f\"  Train: 0 to {split_idx}\")\n",
    "    print(f\"  Val: {split_idx} to {max_start + 1}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = V8Dataset(\n",
    "        features, target,\n",
    "        config.input_window, config.horizon,\n",
    "        edge_index, edge_weight,\n",
    "        start_idx=0, end_idx=split_idx\n",
    "    )\n",
    "    \n",
    "    val_dataset = V8Dataset(\n",
    "        features, target,\n",
    "        config.input_window, config.horizon,\n",
    "        edge_index, edge_weight,\n",
    "        start_idx=split_idx, end_idx=max_start + 1\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, edge_index, edge_weight\n",
    "\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 6.3: LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "# Load dataset\n",
    "ds, lat_dim, lon_dim = load_dataset(DATA_PATH, CONFIG)\n",
    "\n",
    "# Prepare data\n",
    "train_dataset, val_dataset, edge_index, edge_weight = prepare_data(\n",
    "    ds, CONFIG, lat_dim, lon_dim\n",
    ")\n",
    "\n",
    "# Move graph to device\n",
    "edge_index = edge_index.to(device)\n",
    "edge_weight = edge_weight.to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nData loaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Test batch\n",
    "test_batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  x: {test_batch['x'].shape}\")\n",
    "print(f\"  y: {test_batch['y'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loss_functions"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 7: LOSS FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def physics_informed_loss(\n",
    "    pred: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    elevation: Optional[torch.Tensor] = None,\n",
    "    config: V8Config = None\n",
    ") -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Physics-informed loss with mass conservation and orographic constraints.\n",
    "    \n",
    "    Args:\n",
    "        pred: (batch, n_nodes, horizon)\n",
    "        target: (batch, n_nodes, horizon)\n",
    "        elevation: (n_nodes,) optional\n",
    "        config: V8Config\n",
    "    \"\"\"\n",
    "    # Base MSE loss\n",
    "    mse_loss = F.mse_loss(pred, target)\n",
    "    \n",
    "    components = {'mse': mse_loss.item()}\n",
    "    total_loss = mse_loss\n",
    "    \n",
    "    if config and config.use_physics_loss:\n",
    "        # Mass conservation: total precipitation should be preserved\n",
    "        pred_sum = pred.sum(dim=1)  # (batch, horizon)\n",
    "        target_sum = target.sum(dim=1)\n",
    "        mass_loss = torch.abs(pred_sum - target_sum) / (target_sum.abs() + 1e-6)\n",
    "        mass_loss = mass_loss.mean()\n",
    "        \n",
    "        total_loss = total_loss + config.lambda_mass_conservation * mass_loss\n",
    "        components['mass'] = mass_loss.item()\n",
    "        \n",
    "        # Orographic constraint: high elevation should have higher precipitation\n",
    "        if elevation is not None:\n",
    "            high_elev_mask = elevation > config.high_elev_threshold\n",
    "            if high_elev_mask.sum() > 0:\n",
    "                pred_high = pred[:, high_elev_mask, :].mean()\n",
    "                target_high = target[:, high_elev_mask, :].mean()\n",
    "                oro_loss = F.relu(target_high - pred_high)\n",
    "                \n",
    "                total_loss = total_loss + config.lambda_orographic * oro_loss\n",
    "                components['orographic'] = oro_loss.item()\n",
    "    \n",
    "    return total_loss, components\n",
    "\n",
    "\n",
    "print(\"Loss functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_loop"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 7.1: TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "def train_v8(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    config: V8Config,\n",
    "    output_dir: Path\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Training loop for V8 GNN-Mamba.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    if config.scheduler_type == 'cosine':\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=config.cosine_T0,\n",
    "            T_mult=config.cosine_T_mult\n",
    "        )\n",
    "    else:\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=5\n",
    "        )\n",
    "    \n",
    "    # History\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_mse': [],\n",
    "        'val_mse': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting V8 GNN-Mamba Training\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        # === Training ===\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "        train_mse_sum = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x = batch['x'].to(device)  # (B, T, N, F)\n",
    "            y = batch['y'].to(device)  # (B, N, H)\n",
    "            edge_idx = batch['edge_index'][0].to(device)\n",
    "            edge_wt = batch['edge_weight'][0].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            pred, info = model(x, edge_idx, edge_wt)\n",
    "            \n",
    "            # Loss\n",
    "            loss, components = physics_informed_loss(pred, y, config=config)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if config.gradient_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(),\n",
    "                    config.gradient_clip\n",
    "                )\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss_sum += loss.item()\n",
    "            train_mse_sum += components['mse']\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_train_loss = train_loss_sum / n_batches\n",
    "        avg_train_mse = train_mse_sum / n_batches\n",
    "        \n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        val_mse_sum = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x = batch['x'].to(device)\n",
    "                y = batch['y'].to(device)\n",
    "                edge_idx = batch['edge_index'][0].to(device)\n",
    "                edge_wt = batch['edge_weight'][0].to(device)\n",
    "                \n",
    "                pred, info = model(x, edge_idx, edge_wt)\n",
    "                loss, components = physics_informed_loss(pred, y, config=config)\n",
    "                \n",
    "                val_loss_sum += loss.item()\n",
    "                val_mse_sum += components['mse']\n",
    "                val_batches += 1\n",
    "        \n",
    "        avg_val_loss = val_loss_sum / val_batches\n",
    "        avg_val_mse = val_mse_sum / val_batches\n",
    "        \n",
    "        # Update scheduler\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if config.scheduler_type == 'cosine':\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_mse'].append(avg_train_mse)\n",
    "        history['val_mse'].append(avg_val_mse)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1:3d}/{config.epochs}: \"\n",
    "                f\"Train={avg_train_loss:.4f} Val={avg_val_loss:.4f} \"\n",
    "                f\"MSE={avg_val_mse:.4f} LR={current_lr:.2e}\"\n",
    "            )\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            if config.save_checkpoints:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'val_loss': avg_val_loss,\n",
    "                    'config': asdict(config)\n",
    "                }\n",
    "                torch.save(checkpoint, output_dir / 'v8_best.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config.patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    print(f\"\\nTraining complete. Best val loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"Training loop defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_training"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 7.2: RUN TRAINING\n",
    "# ============================================================\n",
    "\n",
    "# Initialize model\n",
    "model = GNN_Mamba_V8(CONFIG)\n",
    "print(f\"\\nModel initialized:\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Train\n",
    "history = train_v8(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=CONFIG,\n",
    "    output_dir=OUTPUT_ROOT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation_section"
   },
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluation"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 8: EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    config: V8Config\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate model and compute metrics per horizon.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x = batch['x'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            edge_idx = batch['edge_index'][0].to(device)\n",
    "            edge_wt = batch['edge_weight'][0].to(device)\n",
    "            \n",
    "            pred, _ = model(x, edge_idx, edge_wt)\n",
    "            \n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(all_preds, axis=0)  # (N_samples, N_nodes, H)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    # Compute metrics per horizon\n",
    "    results = []\n",
    "    for h in range(config.horizon):\n",
    "        pred_h = preds[:, :, h].flatten()\n",
    "        target_h = targets[:, :, h].flatten()\n",
    "        \n",
    "        # RMSE\n",
    "        rmse = np.sqrt(np.mean((pred_h - target_h) ** 2))\n",
    "        \n",
    "        # MAE\n",
    "        mae = np.mean(np.abs(pred_h - target_h))\n",
    "        \n",
    "        # RÂ²\n",
    "        ss_res = np.sum((target_h - pred_h) ** 2)\n",
    "        ss_tot = np.sum((target_h - target_h.mean()) ** 2)\n",
    "        r2 = 1 - (ss_res / (ss_tot + 1e-8))\n",
    "        \n",
    "        # Bias\n",
    "        bias = np.mean(pred_h - target_h)\n",
    "        \n",
    "        results.append({\n",
    "            'H': h + 1,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R^2': r2,\n",
    "            'Bias': bias\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'metrics': results,\n",
    "        'predictions': preds,\n",
    "        'targets': targets\n",
    "    }\n",
    "\n",
    "\n",
    "# Load best model\n",
    "best_checkpoint = OUTPUT_ROOT / 'v8_best.pt'\n",
    "if best_checkpoint.exists():\n",
    "    checkpoint = torch.load(best_checkpoint, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"V8 GNN-Mamba Evaluation Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "eval_results = evaluate_model(model, val_loader, CONFIG)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics by Horizon:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'H':>3} {'RMSE':>10} {'MAE':>10} {'RÂ²':>10} {'Bias':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for m in eval_results['metrics']:\n",
    "    print(\n",
    "        f\"{m['H']:>3} \"\n",
    "        f\"{m['RMSE']:>10.2f} \"\n",
    "        f\"{m['MAE']:>10.2f} \"\n",
    "        f\"{m['R^2']:>10.4f} \"\n",
    "        f\"{m['Bias']:>10.2f}\"\n",
    "    )\n",
    "\n",
    "# Summary\n",
    "avg_rmse = np.mean([m['RMSE'] for m in eval_results['metrics']])\n",
    "avg_mae = np.mean([m['MAE'] for m in eval_results['metrics']])\n",
    "avg_r2 = np.mean([m['R^2'] for m in eval_results['metrics']])\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'AVG':>3} {avg_rmse:>10.2f} {avg_mae:>10.2f} {avg_r2:>10.4f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Compare with V4 baseline\n",
    "print(\"=\" * 50)\n",
    "print(\"Comparison with V4 Baseline\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  V4 Baseline: RÂ²=0.596, RMSE=84.37mm\")\n",
    "print(f\"  V8 Mamba:    RÂ²={avg_r2:.3f}, RMSE={avg_rmse:.2f}mm\")\n",
    "\n",
    "r2_change = (avg_r2 - 0.596) / 0.596 * 100\n",
    "rmse_change = (84.37 - avg_rmse) / 84.37 * 100\n",
    "\n",
    "print(f\"\\n  RÂ² change:   {r2_change:+.2f}%\")\n",
    "print(f\"  RMSE change: {rmse_change:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualization"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 8.1: VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Training curves\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['train_loss'], label='Train Loss', alpha=0.8)\n",
    "ax1.plot(history['val_loss'], label='Val Loss', alpha=0.8)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Curves')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. RMSE by horizon\n",
    "ax2 = axes[0, 1]\n",
    "horizons = [m['H'] for m in eval_results['metrics']]\n",
    "rmses = [m['RMSE'] for m in eval_results['metrics']]\n",
    "ax2.bar(horizons, rmses, color='steelblue', edgecolor='black')\n",
    "ax2.axhline(y=84.37, color='red', linestyle='--', label='V4 Baseline')\n",
    "ax2.set_xlabel('Forecast Horizon (months)')\n",
    "ax2.set_ylabel('RMSE (mm)')\n",
    "ax2.set_title('RMSE by Forecast Horizon')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. RÂ² by horizon\n",
    "ax3 = axes[1, 0]\n",
    "r2s = [m['R^2'] for m in eval_results['metrics']]\n",
    "ax3.bar(horizons, r2s, color='forestgreen', edgecolor='black')\n",
    "ax3.axhline(y=0.596, color='red', linestyle='--', label='V4 Baseline')\n",
    "ax3.set_xlabel('Forecast Horizon (months)')\n",
    "ax3.set_ylabel('RÂ²')\n",
    "ax3.set_title('RÂ² by Forecast Horizon')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Learning rate schedule\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(history['lr'], color='orange')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Learning Rate')\n",
    "ax4.set_title('Learning Rate Schedule')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('V8 GNN-Mamba Training Results', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "fig_path = OUTPUT_ROOT / 'v8_training_results.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"Figure saved to: {fig_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_results"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 8.2: SAVE RESULTS\n",
    "# ============================================================\n",
    "\n",
    "# Save metrics\n",
    "metrics_df = pd.DataFrame(eval_results['metrics'])\n",
    "metrics_df.to_csv(OUTPUT_ROOT / 'v8_metrics.csv', index=False)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'model': 'V8 GNN-Mamba',\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'config': asdict(CONFIG),\n",
    "    'results': {\n",
    "        'avg_rmse': float(avg_rmse),\n",
    "        'avg_mae': float(avg_mae),\n",
    "        'avg_r2': float(avg_r2),\n",
    "        'metrics_by_horizon': eval_results['metrics']\n",
    "    },\n",
    "    'comparison': {\n",
    "        'v4_baseline': {'r2': 0.596, 'rmse': 84.37},\n",
    "        'v8_mamba': {'r2': float(avg_r2), 'rmse': float(avg_rmse)},\n",
    "        'improvement': {\n",
    "            'r2_percent': float(r2_change),\n",
    "            'rmse_percent': float(rmse_change)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_ROOT / 'v8_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nResults saved to: {OUTPUT_ROOT}\")\n",
    "print(f\"  - v8_metrics.csv\")\n",
    "print(f\"  - v8_summary.json\")\n",
    "print(f\"  - v8_training_results.png\")\n",
    "print(f\"  - v8_best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "### V8 GNN-Mamba Summary\n",
    "\n",
    "This notebook implemented the V8 architecture combining:\n",
    "- **Graph Neural Networks** for spatial encoding\n",
    "- **Mamba State Space Models** for temporal encoding\n",
    "- **Cross-Modal Attention** for feature fusion\n",
    "\n",
    "### Key Innovations\n",
    "1. First application of Mamba SSM to regional precipitation prediction\n",
    "2. Selective memory mechanism for capturing seasonal patterns\n",
    "3. Efficient O(n) complexity for 60-month input sequences\n",
    "\n",
    "### Next Steps\n",
    "1. Ablation studies (Mamba vs LSTM, bidirectional vs unidirectional)\n",
    "2. Physics loss refinement\n",
    "3. Ensemble with V4 baseline"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
