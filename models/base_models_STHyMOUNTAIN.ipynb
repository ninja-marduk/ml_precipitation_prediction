{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f4e7925",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/main/models/base_models_STHyMOUNTAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af3744f4",
      "metadata": {
        "id": "af3744f4"
      },
      "source": [
        "# üìò Entrenamiento de Modelos Baseline para Predicci√≥n Espaciotemporal de Precipitaci√≥n Mensual STHyMOUNTAIN\n",
        "\n",
        "Este notebook implementa modelos baseline para la predicci√≥n de precipitaciones usando datos espaciotemporales."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a994be36",
      "metadata": {},
      "source": [
        "## üîç Implementaci√≥n de Modelos Avanzados y T√©cnicas de Validaci√≥n\n",
        "\n",
        "Adem√°s de los modelos tabulares baseline, implementaremos:\n",
        "\n",
        "1. **Optimizaci√≥n avanzada con Optuna** para los modelos tabulares XGBoost y LightGBM\n",
        "2. **Validaci√≥n robusta** mediante:\n",
        "   - Hold-Out Validation (ya implementada)\n",
        "   - Cross-Validation (k=5)\n",
        "   - Bootstrapping (100 muestras)\n",
        "3. **Modelos de Deep Learning** para capturar patrones espaciales y temporales:\n",
        "   - Redes CNN para patrones espaciales\n",
        "   - Redes ConvLSTM para patrones espaciotemporales\n",
        "\n",
        "El objetivo es proporcionar una evaluaci√≥n completa de diferentes enfoques de modelado para la predicci√≥n de precipitaci√≥n en regiones monta√±osas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "06416284",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06416284",
        "outputId": "a8e4c864-34e9-41b2-d5c3-e6ccaaf3699e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entorno configurado. Usando ruta base: ..\n",
            "Directorio para salida de modelos creado: ../models/output\n",
            "Respaldo del DataFrame guardado en: ../data/output/temp/dataframe_backup.parquet\n",
            "Error al cargar el modelo RandomForest: [Errno 2] No such file or directory: '../models/output/RandomForest.pkl'\n",
            "Error al cargar el modelo XGBoost: [Errno 2] No such file or directory: '../models/output/XGBoost.pkl'\n",
            "Error al cargar el modelo LightGBM: [Errno 2] No such file or directory: '../models/output/LightGBM.pkl'\n",
            "No se encontr√≥ el archivo de respaldo en: ../models/output/cnn_model.h5\n",
            "No se encontr√≥ el archivo de respaldo en: ../models/output/convlstm_model.h5\n",
            "Respaldo del DataFrame guardado en: ../data/output/temp/dataframe_backup.parquet\n",
            "Error al cargar el modelo RandomForest: [Errno 2] No such file or directory: '../models/output/RandomForest.pkl'\n",
            "Error al cargar el modelo XGBoost: [Errno 2] No such file or directory: '../models/output/XGBoost.pkl'\n",
            "Error al cargar el modelo LightGBM: [Errno 2] No such file or directory: '../models/output/LightGBM.pkl'\n",
            "No se encontr√≥ el archivo de respaldo en: ../models/output/cnn_model.h5\n",
            "No se encontr√≥ el archivo de respaldo en: ../models/output/convlstm_model.h5\n"
          ]
        }
      ],
      "source": [
        "# Configuraci√≥n del entorno (compatible con Colab y local)\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# Detectar si estamos en Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')   \n",
        "    # Si estamos en Colab, clonar el repositorio\n",
        "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
        "    %cd ml_precipitation_prediction\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn\n",
        "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
        "else:\n",
        "    # Si estamos en local, usar la ruta actual\n",
        "    if '/models' in os.getcwd():\n",
        "        BASE_PATH = Path('..')\n",
        "    else:\n",
        "        BASE_PATH = Path('.')\n",
        "\n",
        "print(f\"Entorno configurado. Usando ruta base: {BASE_PATH}\")\n",
        "\n",
        "# Si BASE_PATH viene como string, lo convertimos\n",
        "BASE_PATH = Path(BASE_PATH)\n",
        "\n",
        "# Ahora puedes concatenar correctamente\n",
        "model_output_dir = BASE_PATH / 'models' / 'output'\n",
        "model_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Directorio para salida de modelos creado: {model_output_dir}\")\n",
        "\n",
        "# Implementaci√≥n de resiliencia para interacci√≥n con Google Drive y restauraci√≥n de datos\n",
        "def backup_dataframe(df, backup_path):\n",
        "    \"\"\"Guarda un DataFrame como respaldo en formato Parquet.\"\"\"\n",
        "    try:\n",
        "        df.to_parquet(backup_path, index=False)\n",
        "        print(f\"Respaldo del DataFrame guardado en: {backup_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar respaldo del DataFrame: {e}\")\n",
        "\n",
        "def restore_dataframe(backup_path):\n",
        "    \"\"\"Restaura un DataFrame desde un archivo de respaldo en formato Parquet.\"\"\"\n",
        "    try:\n",
        "        if backup_path.exists():\n",
        "            df_restored = pd.read_parquet(backup_path)\n",
        "            print(f\"DataFrame restaurado desde: {backup_path}\")\n",
        "            return df_restored\n",
        "        else:\n",
        "            print(f\"No se encontr√≥ el archivo de respaldo en: {backup_path}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error al restaurar el DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "# Ruta para respaldo temporal del DataFrame\n",
        "temp_dir = BASE_PATH / 'data' / 'output' / 'temp'\n",
        "temp_dir.mkdir(parents=True, exist_ok=True)\n",
        "temp_file_path = temp_dir / 'dataframe_backup.parquet'\n",
        "\n",
        "# Respaldo inicial del DataFrame principal\n",
        "if 'df' in locals() and df is not None:\n",
        "    backup_dataframe(df, temp_file_path)\n",
        "\n",
        "# Modificar interacci√≥n con Google Drive para reintentos\n",
        "max_retries = 3\n",
        "retry_delay = 5  # segundos\n",
        "\n",
        "def mount_google_drive():\n",
        "    \"\"\"Intenta montar Google Drive con reintentos.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Google Drive montado exitosamente.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error al montar Google Drive (intento {attempt + 1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "    print(\"No se pudo montar Google Drive despu√©s de varios intentos.\")\n",
        "    return False\n",
        "\n",
        "if IN_COLAB:\n",
        "    if not mount_google_drive():\n",
        "        print(\"Usando datos en memoria o restaurando desde respaldo local.\")\n",
        "        df = restore_dataframe(temp_file_path)\n",
        "\n",
        "# Restaurar modelos guardados en caso de fallo\n",
        "model_files = {\n",
        "    'RandomForest': model_output_dir / 'RandomForest.pkl',\n",
        "    'XGBoost': model_output_dir / 'XGBoost.pkl',\n",
        "    'LightGBM': model_output_dir / 'LightGBM.pkl'\n",
        "}\n",
        "\n",
        "def load_saved_model(model_name, model_path):\n",
        "    \"\"\"Carga un modelo guardado desde disco.\"\"\"\n",
        "    try:\n",
        "        with open(model_path, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "            print(f\"Modelo {model_name} cargado desde: {model_path}\")\n",
        "            return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el modelo {model_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Intentar cargar modelos guardados\n",
        "for model_name, model_path in model_files.items():\n",
        "    if model_name not in modelos_base:\n",
        "        modelos_base[model_name] = load_saved_model(model_name, model_path)\n",
        "\n",
        "# Implementaci√≥n de resiliencia para modelos CNN y ConvLSTM\n",
        "\n",
        "# Respaldo y restauraci√≥n de modelos CNN y ConvLSTM\n",
        "cnn_model_path = model_output_dir / 'cnn_model.h5'\n",
        "convlstm_model_path = model_output_dir / 'convlstm_model.h5'\n",
        "\n",
        "def backup_model(model, model_path):\n",
        "    \"\"\"Guarda un modelo de Keras como respaldo.\"\"\"\n",
        "    try:\n",
        "        model.save(model_path)\n",
        "        print(f\"Modelo respaldado en: {model_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar respaldo del modelo: {e}\")\n",
        "\n",
        "def restore_model(model_path):\n",
        "    \"\"\"Restaura un modelo de Keras desde un archivo de respaldo.\"\"\"\n",
        "    try:\n",
        "        if model_path.exists():\n",
        "            model = tf.keras.models.load_model(model_path)\n",
        "            print(f\"Modelo restaurado desde: {model_path}\")\n",
        "            return model\n",
        "        else:\n",
        "            print(f\"No se encontr√≥ el archivo de respaldo en: {model_path}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error al restaurar el modelo: {e}\")\n",
        "        return None\n",
        "\n",
        "# Respaldo inicial de modelos si existen\n",
        "if 'cnn_model' in locals() and cnn_model is not None:\n",
        "    backup_model(cnn_model, cnn_model_path)\n",
        "if 'convlstm_model' in locals() and convlstm_model is not None:\n",
        "    backup_model(convlstm_model, convlstm_model_path)\n",
        "\n",
        "# Restaurar modelos en caso de fallo\n",
        "if 'cnn_model' not in locals() or cnn_model is None:\n",
        "    cnn_model = restore_model(cnn_model_path)\n",
        "if 'convlstm_model' not in locals() or convlstm_model is None:\n",
        "    convlstm_model = restore_model(convlstm_model_path)\n",
        "\n",
        "# Modificar interacci√≥n con Google Drive para reintentos\n",
        "max_retries = 3\n",
        "retry_delay = 5  # segundos\n",
        "\n",
        "def mount_google_drive():\n",
        "    \"\"\"Intenta montar Google Drive con reintentos.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Google Drive montado exitosamente.\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error al montar Google Drive (intento {attempt + 1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(retry_delay)\n",
        "    print(\"No se pudo montar Google Drive despu√©s de varios intentos.\")\n",
        "    return False\n",
        "\n",
        "if IN_COLAB:\n",
        "    if not mount_google_drive():\n",
        "        print(\"Usando datos en memoria o restaurando desde respaldo local para modelos CNN y ConvLSTM.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e47fb555",
      "metadata": {
        "id": "e47fb555"
      },
      "outputs": [],
      "source": [
        "# 1. Importaciones necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import optuna\n",
        "import pickle\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importaciones para barras de progreso y mejora de visualizaci√≥n\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import time\n",
        "\n",
        "# Configurar visualizaci√≥n m√°s atractiva\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"notebook\", font_scale=1.2)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "313434be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow versi√≥n: 2.18.0\n",
            "No se detect√≥ GPU. Usando CPU.\n"
          ]
        }
      ],
      "source": [
        "# Importaciones adicionales para Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, save_model, load_model\n",
        "from tensorflow.keras.layers import (Dense, Dropout, Conv2D, Conv3D, ConvLSTM2D, BatchNormalization, \n",
        "                                   MaxPooling2D, Flatten, Input, concatenate, Reshape, TimeDistributed, UpSampling2D)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(\"TensorFlow versi√≥n:\", tf.__version__)\n",
        "\n",
        "# Configurar GPU si est√° disponible\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    print(f\"GPU disponible: {physical_devices}\")\n",
        "    # Permitir crecimiento de memoria seg√∫n sea necesario\n",
        "    for device in physical_devices:\n",
        "        tf.config.experimental.set_memory_growth(device, True)\n",
        "else:\n",
        "    print(\"No se detect√≥ GPU. Usando CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "26215d90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "26215d90",
        "outputId": "ccb06926-e151-453f-a306-999f15566bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buscando archivo en: ../data/output/complete_dataset_with_features.nc\n",
            "Intentando cargar el archivo: ../data/output/complete_dataset_with_features.nc\n",
            "Archivo cargado exitosamente con xarray\n",
            "\n",
            "Informaci√≥n del dataset:\n",
            "xarray.Dataset {\n",
            "dimensions:\n",
            "\ttime = 530 ;\n",
            "\tlatitude = 62 ;\n",
            "\tlongitude = 66 ;\n",
            "\n",
            "variables:\n",
            "\tdatetime64[ns] time(time) ;\n",
            "\tfloat32 latitude(latitude) ;\n",
            "\tfloat32 longitude(longitude) ;\n",
            "\tfloat32 total_precipitation(time, latitude, longitude) ;\n",
            "\tfloat32 max_daily_precipitation(time, latitude, longitude) ;\n",
            "\tfloat32 min_daily_precipitation(time, latitude, longitude) ;\n",
            "\tfloat32 daily_precipitation_std(time, latitude, longitude) ;\n",
            "\tfloat32 month_sin(time, latitude, longitude) ;\n",
            "\tfloat32 month_cos(time, latitude, longitude) ;\n",
            "\tfloat32 doy_sin(time, latitude, longitude) ;\n",
            "\tfloat32 doy_cos(time, latitude, longitude) ;\n",
            "\tfloat64 elevation(latitude, longitude) ;\n",
            "\tfloat32 slope(latitude, longitude) ;\n",
            "\tfloat32 aspect(latitude, longitude) ;\n",
            "\n",
            "// global attributes:\n",
            "\t:description = ST-HyMOUNTAIN-Net ready dataset with CHIRPS monthly precipitation and DEM variables ;\n",
            "\t:source = CHIRPS v2.0 & DEM Boyac√° ;\n",
            "\t:created_at = 2025-04-27 19:02:24 ;\n",
            "}None\n",
            "\n",
            "Variables disponibles:\n",
            "- total_precipitation: (530, 62, 66)\n",
            "- max_daily_precipitation: (530, 62, 66)\n",
            "- min_daily_precipitation: (530, 62, 66)\n",
            "- daily_precipitation_std: (530, 62, 66)\n",
            "- month_sin: (530, 62, 66)\n",
            "- month_cos: (530, 62, 66)\n",
            "- doy_sin: (530, 62, 66)\n",
            "- doy_cos: (530, 62, 66)\n",
            "- elevation: (62, 66)\n",
            "- slope: (62, 66)\n",
            "- aspect: (62, 66)\n",
            "Dataset cargado con √©xito. Dimensiones: (2168760, 14)\n",
            "\n",
            "Primeras filas del DataFrame:\n",
            "Dataset cargado con √©xito. Dimensiones: (2168760, 14)\n",
            "\n",
            "Primeras filas del DataFrame:\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "time",
                  "rawType": "datetime64[ns]",
                  "type": "datetime"
                },
                {
                  "name": "latitude",
                  "rawType": "float32",
                  "type": "float"
                },
                {
                  "name": "longitude",
                  "rawType": "float32",
                  "type": "float"
                },
                {
                  "name": "total_precipitation",
                  "rawType": "float32",
                  "type": "float"
                },
                {
                  "name": "max_daily_precipitation",
                  "rawType": "float32",
                  "type": "float"
                },
                {
                  "name": "min_daily_precipitation",
                  "rawType": "float32",
                  "type": "float"
                },
                {
                  "name": "daily_precipitation_std",
                  "rawType": "float32",
                  "type": "float"
                },
                {
                  "name": "month_sin",
                  "rawType": "float32",
                  "type": "float"
                },
                {
                  "name": "month_cos",
                  "rawType": "float32",
                  "type": "float"
                },
                {
                  "name": "doy_sin",
                  "rawType": "float32",
                  "type": "float"
                },
                {
                  "name": "doy_cos",
                  "rawType": "float32",
                  "type": "float"
                },
                {
                  "name": "elevation",
                  "rawType": "float64",
                  "type": "float"
                },
                {
                  "name": "slope",
                  "rawType": "float32",
                  "type": "float"
                },
                {
                  "name": "aspect",
                  "rawType": "float32",
                  "type": "float"
                }
              ],
              "conversionMethod": "pd.DataFrame",
              "ref": "38705b6d-e788-4678-921e-812f96eb3f75",
              "rows": [
                [
                  "0",
                  "1981-01-01 00:00:00",
                  "4.324997",
                  "-74.975006",
                  "47.38105",
                  "24.706928",
                  "0.0",
                  "5.8257756",
                  "0.5",
                  "0.8660254",
                  "0.017201575",
                  "0.99985206",
                  "493.78455182073014",
                  "89.53955",
                  "102.0445"
                ],
                [
                  "1",
                  "1981-01-01 00:00:00",
                  "4.324997",
                  "-74.925",
                  "40.750824",
                  "21.819195",
                  "0.0",
                  "5.0190454",
                  "0.5",
                  "0.8660254",
                  "0.017201575",
                  "0.99985206",
                  "519.7501066909579",
                  "89.86702",
                  "73.481674"
                ],
                [
                  "2",
                  "1981-01-01 00:00:00",
                  "4.324997",
                  "-74.87501",
                  "46.338623",
                  "26.092327",
                  "0.0",
                  "5.7402234",
                  "0.5",
                  "0.8660254",
                  "0.017201575",
                  "0.99985206",
                  "248.7760453427361",
                  "89.72222",
                  "65.91682"
                ],
                [
                  "3",
                  "1981-01-01 00:00:00",
                  "4.324997",
                  "-74.825005",
                  "48.779938",
                  "29.42145",
                  "0.0",
                  "5.611738",
                  "0.5",
                  "0.8660254",
                  "0.017201575",
                  "0.99985206",
                  "351.4157280671193",
                  "86.98613",
                  "140.916"
                ],
                [
                  "4",
                  "1981-01-01 00:00:00",
                  "4.324997",
                  "-74.775",
                  "38.932945",
                  "18.48306",
                  "0.0",
                  "3.7335742",
                  "0.5",
                  "0.8660254",
                  "0.017201575",
                  "0.99985206",
                  "278.2619223660964",
                  "88.27329",
                  "18.43994"
                ]
              ],
              "shape": {
                "columns": 14,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>total_precipitation</th>\n",
              "      <th>max_daily_precipitation</th>\n",
              "      <th>min_daily_precipitation</th>\n",
              "      <th>daily_precipitation_std</th>\n",
              "      <th>month_sin</th>\n",
              "      <th>month_cos</th>\n",
              "      <th>doy_sin</th>\n",
              "      <th>doy_cos</th>\n",
              "      <th>elevation</th>\n",
              "      <th>slope</th>\n",
              "      <th>aspect</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1981-01-01</td>\n",
              "      <td>4.324997</td>\n",
              "      <td>-74.975006</td>\n",
              "      <td>47.381050</td>\n",
              "      <td>24.706928</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.825776</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.017202</td>\n",
              "      <td>0.999852</td>\n",
              "      <td>493.784552</td>\n",
              "      <td>89.539551</td>\n",
              "      <td>102.044502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1981-01-01</td>\n",
              "      <td>4.324997</td>\n",
              "      <td>-74.925003</td>\n",
              "      <td>40.750824</td>\n",
              "      <td>21.819195</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.019045</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.017202</td>\n",
              "      <td>0.999852</td>\n",
              "      <td>519.750107</td>\n",
              "      <td>89.867020</td>\n",
              "      <td>73.481674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1981-01-01</td>\n",
              "      <td>4.324997</td>\n",
              "      <td>-74.875008</td>\n",
              "      <td>46.338623</td>\n",
              "      <td>26.092327</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.740223</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.017202</td>\n",
              "      <td>0.999852</td>\n",
              "      <td>248.776045</td>\n",
              "      <td>89.722221</td>\n",
              "      <td>65.916817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1981-01-01</td>\n",
              "      <td>4.324997</td>\n",
              "      <td>-74.825005</td>\n",
              "      <td>48.779938</td>\n",
              "      <td>29.421450</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.611738</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.017202</td>\n",
              "      <td>0.999852</td>\n",
              "      <td>351.415728</td>\n",
              "      <td>86.986130</td>\n",
              "      <td>140.916000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1981-01-01</td>\n",
              "      <td>4.324997</td>\n",
              "      <td>-74.775002</td>\n",
              "      <td>38.932945</td>\n",
              "      <td>18.483061</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.733574</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.017202</td>\n",
              "      <td>0.999852</td>\n",
              "      <td>278.261922</td>\n",
              "      <td>88.273293</td>\n",
              "      <td>18.439939</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        time  latitude  longitude  total_precipitation  \\\n",
              "0 1981-01-01  4.324997 -74.975006            47.381050   \n",
              "1 1981-01-01  4.324997 -74.925003            40.750824   \n",
              "2 1981-01-01  4.324997 -74.875008            46.338623   \n",
              "3 1981-01-01  4.324997 -74.825005            48.779938   \n",
              "4 1981-01-01  4.324997 -74.775002            38.932945   \n",
              "\n",
              "   max_daily_precipitation  min_daily_precipitation  daily_precipitation_std  \\\n",
              "0                24.706928                      0.0                 5.825776   \n",
              "1                21.819195                      0.0                 5.019045   \n",
              "2                26.092327                      0.0                 5.740223   \n",
              "3                29.421450                      0.0                 5.611738   \n",
              "4                18.483061                      0.0                 3.733574   \n",
              "\n",
              "   month_sin  month_cos   doy_sin   doy_cos   elevation      slope      aspect  \n",
              "0        0.5   0.866025  0.017202  0.999852  493.784552  89.539551  102.044502  \n",
              "1        0.5   0.866025  0.017202  0.999852  519.750107  89.867020   73.481674  \n",
              "2        0.5   0.866025  0.017202  0.999852  248.776045  89.722221   65.916817  \n",
              "3        0.5   0.866025  0.017202  0.999852  351.415728  86.986130  140.916000  \n",
              "4        0.5   0.866025  0.017202  0.999852  278.261922  88.273293   18.439939  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 2. Cargar el dataset NetCDF\n",
        "def load_dataset(file_path):\n",
        "    \"\"\"Carga un archivo NetCDF y lo convierte a pandas DataFrame\"\"\"\n",
        "    try:\n",
        "        # Cargar el archivo NetCDF con xarray\n",
        "        print(f\"Intentando cargar el archivo: {file_path}\")\n",
        "        ds = xr.open_dataset(file_path)\n",
        "        print(\"Archivo cargado exitosamente con xarray\")\n",
        "\n",
        "        # Mostrar informaci√≥n del dataset cargado\n",
        "        print(\"\\nInformaci√≥n del dataset:\")\n",
        "        print(ds.info())\n",
        "        print(\"\\nVariables disponibles:\")\n",
        "        for var_name in ds.data_vars:\n",
        "            print(f\"- {var_name}: {ds[var_name].shape}\")\n",
        "\n",
        "        # Convertir a DataFrame\n",
        "        df = ds.to_dataframe().reset_index()\n",
        "        return df, ds\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el archivo NetCDF: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Ruta al dataset\n",
        "data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features.nc'\n",
        "print(f\"Buscando archivo en: {data_file}\")\n",
        "\n",
        "# Cargar el dataset\n",
        "df, ds_original = load_dataset(data_file)\n",
        "\n",
        "# Verificar si se carg√≥ correctamente\n",
        "if df is not None:\n",
        "    print(f\"Dataset cargado con √©xito. Dimensiones: {df.shape}\")\n",
        "    print(\"\\nPrimeras filas del DataFrame:\")\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"No se pudo cargar el dataset. Verificar la ruta y el formato del archivo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2f0aebbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f0aebbc",
        "outputId": "48e82987-ff47-41e8-9f40-57e53cf90cf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columna objetivo identificada: total_precipitation\n",
            "Filas antes de eliminar NaN: 2168760\n",
            "Filas despu√©s de eliminar NaN: 2168760\n",
            "\n",
            "Features seleccionadas (12):\n",
            "['latitude', 'longitude', 'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos', 'elevation', 'slope', 'aspect']\n",
            "\n",
            "Variable objetivo: total_precipitation\n"
          ]
        }
      ],
      "source": [
        "# 3. Preparaci√≥n de los datos\n",
        "if df is not None:\n",
        "    # Identificar la columna objetivo (precipitaci√≥n)\n",
        "    target_column = 'total_precipitation'  # Ajustar si tiene otro nombre en tu dataset\n",
        "\n",
        "    # Ver si existe 'precip_target' o usar 'total_precipitation'\n",
        "    if 'total_precipitation' in df.columns:\n",
        "        target_column = 'total_precipitation'\n",
        "\n",
        "    print(f\"Columna objetivo identificada: {target_column}\")\n",
        "\n",
        "    # Separar variables predictoras y variable objetivo\n",
        "    feature_cols = [col for col in df.columns if col != target_column and not pd.isna(df[col]).all()]\n",
        "\n",
        "    # Eliminar columnas no num√©ricas para los modelos (como fechas o coordenadas si no se usan como features)\n",
        "    non_feature_cols = ['time', 'spatial_ref']\n",
        "    feature_cols = [col for col in feature_cols if col not in non_feature_cols]\n",
        "\n",
        "    # Eliminar filas con valores NaN\n",
        "    print(f\"Filas antes de eliminar NaN: {df.shape[0]}\")\n",
        "    df_clean = df.dropna(subset=[target_column] + feature_cols)\n",
        "    print(f\"Filas despu√©s de eliminar NaN: {df_clean.shape[0]}\")\n",
        "\n",
        "    # Separar features y target\n",
        "    X = df_clean[feature_cols]\n",
        "    y = df_clean[target_column]\n",
        "\n",
        "    print(f\"\\nFeatures seleccionadas ({len(feature_cols)}):\\n{feature_cols}\")\n",
        "    print(f\"\\nVariable objetivo: {target_column}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "da222af5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da222af5",
        "outputId": "579fbd57-a790-42dd-807a-e61fc1daacbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensiones del conjunto de entrenamiento: (1735008, 12)\n",
            "Dimensiones del conjunto de prueba: (433752, 12)\n",
            "Escalador guardado en models/output/scaler.pkl\n",
            "Escalador guardado en models/output/scaler.pkl\n"
          ]
        }
      ],
      "source": [
        "# 4. Divisi√≥n del conjunto de datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Dimensiones del conjunto de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Dimensiones del conjunto de prueba: {X_test.shape}\")\n",
        "\n",
        "# 5. Estandarizaci√≥n de variables predictoras\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Guardar el scaler para uso futuro\n",
        "with open(model_output_dir / 'scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(\"Escalador guardado en models/output/scaler.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c5ba053b",
      "metadata": {
        "id": "c5ba053b"
      },
      "outputs": [],
      "source": [
        "# 6. Funciones de evaluaci√≥n y entrenamiento\n",
        "def evaluar_modelo(y_true, y_pred):\n",
        "    \"\"\"Eval√∫a el rendimiento de un modelo usando m√∫ltiples m√©tricas\"\"\"\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, r2\n",
        "\n",
        "def entrenar_y_evaluar_modelo(modelo, nombre, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Entrena un modelo y eval√∫a su rendimiento con visualizaci√≥n del progreso\"\"\"\n",
        "    # Crear widget para mostrar informaci√≥n del proceso\n",
        "    display(HTML(f'<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\">' +\n",
        "                 f'<h3>üîÑ Entrenando modelo: {nombre}</h3>' +\n",
        "                 f'<div id=\"status_{nombre}\">Estado: Iniciando entrenamiento...</div>' +\n",
        "                 f'</div>'))\n",
        "    \n",
        "    # Tiempo de inicio\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Entrenar el modelo con seguimiento visual seg√∫n el tipo\n",
        "    if hasattr(modelo, 'fit_generator') or nombre in ['XGBoost', 'XGBoost_Optuna', 'LightGBM', 'LightGBM_Optuna']:\n",
        "        # Para modelos que soportan entrenamiento por lotes como XGBoost, LightGBM\n",
        "        print(f\"Entrenando {nombre} con visualizaci√≥n de progreso...\")\n",
        "        if hasattr(modelo, 'n_estimators'):\n",
        "            n_estimators = modelo.n_estimators\n",
        "            for i in tqdm(range(n_estimators), desc=f\"Entrenando {nombre}\"):\n",
        "                if i == 0:\n",
        "                    # Primera iteraci√≥n, ajuste inicial\n",
        "                    if nombre.startswith('LightGBM'):\n",
        "                        # LightGBM tiene par√°metro verbose\n",
        "                        temp_modelo = type(modelo)(n_estimators=1, **{k:v for k,v in modelo.get_params().items() \n",
        "                                                                 if k != 'n_estimators' and k != 'verbose'}, verbose=-1)\n",
        "                    else:\n",
        "                        temp_modelo = type(modelo)(n_estimators=1, **{k:v for k,v in modelo.get_params().items() \n",
        "                                                                if k != 'n_estimators'})\n",
        "                    temp_modelo.fit(X_train, y_train)\n",
        "                elif i == n_estimators - 1:\n",
        "                    # √öltima iteraci√≥n, ajuste completo\n",
        "                    modelo.fit(X_train, y_train)\n",
        "                \n",
        "                # Actualizar progreso visual\n",
        "                if i % max(1, n_estimators // 10) == 0:\n",
        "                    clear_output(wait=True)\n",
        "                    display(HTML(f'<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\">' +\n",
        "                                f'<h3>üîÑ Entrenando modelo: {nombre}</h3>' +\n",
        "                                f'<div id=\"status_{nombre}\">Estado: Progreso {i+1}/{n_estimators} estimadores ({((i+1)/n_estimators*100):.1f}%)</div>' +\n",
        "                                f'</div>'))\n",
        "                    time.sleep(0.1)  # Peque√±a pausa para actualizaci√≥n visual\n",
        "        else:\n",
        "            # Si no tiene n_estimators, entrenamiento directo\n",
        "            modelo.fit(X_train, y_train)\n",
        "    else:\n",
        "        # Para modelos est√°ndar como RandomForest\n",
        "        modelo.fit(X_train, y_train)\n",
        "    \n",
        "    # Tiempo de entrenamiento\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Visualizar tiempo de entrenamiento\n",
        "    display(HTML(f'<div style=\"background-color:#e6ffe6; padding:10px; border-radius:5px;\">' +\n",
        "                f'<h3>‚úÖ Entrenamiento completado: {nombre}</h3>' +\n",
        "                f'<div>Tiempo de entrenamiento: {training_time:.2f} segundos</div>' +\n",
        "                f'</div>'))\n",
        "    \n",
        "    print(f\"Evaluando rendimiento de {nombre}...\")\n",
        "    predicciones = modelo.predict(X_test)\n",
        "    rmse, mae, r2 = evaluar_modelo(y_test, predicciones)\n",
        "    \n",
        "    # Visualizar m√©tricas con estilo\n",
        "    display(HTML(f'<div style=\"background-color:#f5f5dc; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
        "                f'<h3>üìä M√©tricas para {nombre}</h3>' +\n",
        "                f'<table style=\"width:100%; text-align:left;\">' +\n",
        "                f'<tr><th>M√©trica</th><th>Valor</th></tr>' +\n",
        "                f'<tr><td>RMSE</td><td>{rmse:.4f}</td></tr>' +\n",
        "                f'<tr><td>MAE</td><td>{mae:.4f}</td></tr>' +\n",
        "                f'<tr><td>R¬≤</td><td>{r2:.4f}</td></tr>' +\n",
        "                f'</table></div>'))\n",
        "    \n",
        "    return modelo, (rmse, mae, r2)\n",
        "\n",
        "def guardar_modelo(modelo, nombre):\n",
        "    \"\"\"Guarda un modelo entrenado en disco\"\"\"\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"{nombre}_{timestamp}.pkl\"\n",
        "    with open(model_output_dir / filename, 'wb') as f:\n",
        "        pickle.dump(modelo, f)\n",
        "    \n",
        "    # Visualizar confirmaci√≥n de guardado\n",
        "    display(HTML(f'<div style=\"background-color:#e6ffee; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
        "                f'<h3>üíæ Modelo guardado</h3>' +\n",
        "                f'<div>Modelo <b>{nombre}</b> guardado como: {filename}</div>' +\n",
        "                f'</div>'))\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e59f9865",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Entrenando modelos baseline sin optimizaci√≥n de hiperpar√°metros...\n",
            "\n",
            "Entrenando RandomForest base...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\"><h3>üîÑ Entrenando modelo: RandomForest</h3><div id=\"status_RandomForest\">Estado: Iniciando entrenamiento...</div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEntrenando RandomForest base...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m rf_model, rf_metrics \u001b[38;5;241m=\u001b[39m entrenar_y_evaluar_modelo(\n\u001b[1;32m     14\u001b[0m     rf_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m'\u001b[39m, X_train_scaled, y_train, X_test_scaled, y_test\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m resultados_base[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m rf_metrics\n\u001b[1;32m     17\u001b[0m modelos_base[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m rf_model\n",
            "Cell \u001b[0;32mIn[18], line 54\u001b[0m, in \u001b[0;36mentrenar_y_evaluar_modelo\u001b[0;34m(modelo, nombre, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     51\u001b[0m         modelo\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Para modelos est√°ndar como RandomForest\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     modelo\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Tiempo de entrenamiento\u001b[39;00m\n\u001b[1;32m     57\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
            "File \u001b[0;32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    479\u001b[0m ]\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    488\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    489\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    490\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    491\u001b[0m )(\n\u001b[1;32m    492\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    493\u001b[0m         t,\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[1;32m    495\u001b[0m         X,\n\u001b[1;32m    496\u001b[0m         y,\n\u001b[1;32m    497\u001b[0m         sample_weight,\n\u001b[1;32m    498\u001b[0m         i,\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[1;32m    500\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    501\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[1;32m    502\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[1;32m    503\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    506\u001b[0m )\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
            "File \u001b[0;32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
            "File \u001b[0;32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[0;32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[0;32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 189\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    190\u001b[0m         X,\n\u001b[1;32m    191\u001b[0m         y,\n\u001b[1;32m    192\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight,\n\u001b[1;32m    193\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    194\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    198\u001b[0m         X,\n\u001b[1;32m    199\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    203\u001b[0m     )\n",
            "File \u001b[0;32m~/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 7. Entrenamiento de modelos base sin optimizaci√≥n\n",
        "\n",
        "# Inicializar diccionarios para almacenar resultados y modelos\n",
        "resultados_base = {}  # Para almacenar m√©tricas (RMSE, MAE, R2)\n",
        "modelos_base = {}     # Para almacenar instancias de modelos\n",
        "modelos_guardados = {} # Para almacenar nombres de archivos guardados\n",
        "\n",
        "print(\"\\nüîç Entrenando modelos baseline sin optimizaci√≥n de hiperpar√°metros...\")\n",
        "\n",
        "# 1. Modelo RandomForest b√°sico\n",
        "print(\"\\nEntrenando RandomForest base...\")\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model, rf_metrics = entrenar_y_evaluar_modelo(\n",
        "    rf_model, 'RandomForest', X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "resultados_base['RandomForest'] = rf_metrics\n",
        "modelos_base['RandomForest'] = rf_model\n",
        "modelo_file = guardar_modelo(rf_model, 'RandomForest')\n",
        "modelos_guardados['RandomForest'] = modelo_file\n",
        "\n",
        "# Visualizar importancia de caracter√≠sticas para RandomForest\n",
        "plt.figure(figsize=(12, 6))\n",
        "feat_importances = rf_model.feature_importances_\n",
        "indices = np.argsort(feat_importances)[::-1]\n",
        "plt.bar(range(len(indices)), feat_importances[indices], color='skyblue')\n",
        "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
        "plt.title('Importancia de Caracter√≠sticas - RandomForest')\n",
        "plt.tight_layout()\n",
        "plt.savefig(model_output_dir / 'randomforest_feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# 2. Modelo XGBoost b√°sico\n",
        "print(\"\\nEntrenando XGBoost base...\")\n",
        "xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
        "xgb_model, xgb_metrics = entrenar_y_evaluar_modelo(\n",
        "    xgb_model, 'XGBoost', X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "resultados_base['XGBoost'] = xgb_metrics\n",
        "modelos_base['XGBoost'] = xgb_model\n",
        "modelo_file = guardar_modelo(xgb_model, 'XGBoost')\n",
        "modelos_guardados['XGBoost'] = modelo_file\n",
        "\n",
        "# Visualizar importancia de caracter√≠sticas para XGBoost\n",
        "plt.figure(figsize=(12, 6))\n",
        "feat_importances = xgb_model.feature_importances_\n",
        "indices = np.argsort(feat_importances)[::-1]\n",
        "plt.bar(range(len(indices)), feat_importances[indices], color='coral')\n",
        "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
        "plt.title('Importancia de Caracter√≠sticas - XGBoost')\n",
        "plt.tight_layout()\n",
        "plt.savefig(model_output_dir / 'xgboost_feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# 3. Modelo LightGBM b√°sico\n",
        "print(\"\\nEntrenando LightGBM base...\")\n",
        "lgbm_model = LGBMRegressor(n_estimators=100, random_state=42)\n",
        "lgbm_model, lgbm_metrics = entrenar_y_evaluar_modelo(\n",
        "    lgbm_model, 'LightGBM', X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "resultados_base['LightGBM'] = lgbm_metrics\n",
        "modelos_base['LightGBM'] = lgbm_model\n",
        "modelo_file = guardar_modelo(lgbm_model, 'LightGBM')\n",
        "modelos_guardados['LightGBM'] = modelo_file\n",
        "\n",
        "# Visualizar importancia de caracter√≠sticas para LightGBM\n",
        "plt.figure(figsize=(12, 6))\n",
        "feat_importances = lgbm_model.feature_importances_\n",
        "indices = np.argsort(feat_importances)[::-1]\n",
        "plt.bar(range(len(indices)), feat_importances[indices], color='lightgreen')\n",
        "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
        "plt.title('Importancia de Caracter√≠sticas - LightGBM')\n",
        "plt.tight_layout()\n",
        "plt.savefig(model_output_dir / 'lightgbm_feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# Comparar resultados de modelos base\n",
        "print(\"\\nüîç Comparaci√≥n de modelos base sin optimizaci√≥n:\")\n",
        "temp_df = pd.DataFrame(resultados_base, index=['RMSE', 'MAE', 'R2']).T\n",
        "print(\"\\nOrdenados por RMSE (menor es mejor):\")\n",
        "display(temp_df.sort_values('RMSE'))\n",
        "\n",
        "# Visualizar comparaci√≥n de RMSE\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=temp_df.index, y=temp_df['RMSE'])\n",
        "plt.title('Comparaci√≥n de RMSE - Modelos Base')\n",
        "plt.ylabel('RMSE (menor es mejor)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(model_output_dir / 'baseline_rmse_comparison.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b730861a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementaci√≥n de funci√≥n para optimizaci√≥n adaptativa de memoria\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import warnings\n",
        "import optuna\n",
        "\n",
        "def run_memory_efficient_optimization(model_type, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Ejecuta optimizaci√≥n adaptativa de hiperpar√°metros considerando uso de memoria.\n",
        "    \n",
        "    Args:\n",
        "        model_type (str): Tipo de modelo a optimizar ('RandomForest', 'XGBoost', 'LightGBM')\n",
        "        X_train, y_train: Datos de entrenamiento\n",
        "        X_test, y_test: Datos de prueba\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (mejores_par√°metros, modelo_optimizado, m√©tricas)\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìä Iniciando optimizaci√≥n adaptativa para {model_type}...\")\n",
        "    \n",
        "    # Verificar memoria disponible\n",
        "    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
        "    print(f\"Memoria RAM disponible: {available_memory_gb:.2f} GB\")\n",
        "    \n",
        "    # Conjunto de referencias de modelos\n",
        "    model_types = {\n",
        "        'RandomForest': RandomForestRegressor,\n",
        "        'XGBoost': XGBRegressor,\n",
        "        'LightGBM': LGBMRegressor\n",
        "    }\n",
        "    \n",
        "    if model_type not in model_types:\n",
        "        raise ValueError(f\"Tipo de modelo no soportado: {model_type}. Debe ser uno de: {list(model_types.keys())}\")\n",
        "    \n",
        "    # Ajustar par√°metros seg√∫n memoria disponible\n",
        "    if available_memory_gb < 2.0:\n",
        "        print(\"‚ö†Ô∏è Memoria baja detectada. Ajustando optimizaci√≥n para uso m√≠nimo de memoria.\")\n",
        "        n_trials = 10\n",
        "        max_estimators = 50\n",
        "        max_depth = 6\n",
        "        subsample = 0.5\n",
        "    elif available_memory_gb < 8.0:\n",
        "        print(\"‚ÑπÔ∏è Memoria moderada detectada. Usando configuraci√≥n balanceada.\")\n",
        "        n_trials = 30\n",
        "        max_estimators = 300\n",
        "        max_depth = 12\n",
        "        subsample = 0.7\n",
        "    else:\n",
        "        print(\"‚úÖ Memoria amplia detectada. Usando configuraci√≥n completa para optimizaci√≥n.\")\n",
        "        n_trials = 50\n",
        "        max_estimators = 500\n",
        "        max_depth = 20\n",
        "        subsample = 0.9\n",
        "        \n",
        "    # Funci√≥n objetivo para Optuna seg√∫n tipo de modelo\n",
        "    def objective(trial):\n",
        "        # Par√°metros comunes\n",
        "        common_params = {\n",
        "            'random_state': 42\n",
        "        }\n",
        "        \n",
        "        # Par√°metros espec√≠ficos por tipo de modelo\n",
        "        if model_type == 'RandomForest':\n",
        "            params = {\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 50, max_estimators),\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, max_depth),\n",
        "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
        "                'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
        "                'n_jobs': -1 if available_memory_gb > 4.0 else 1\n",
        "            }\n",
        "        elif model_type == 'XGBoost':\n",
        "            params = {\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 50, max_estimators),\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, max_depth),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "                'subsample': trial.suggest_float('subsample', 0.5, subsample),\n",
        "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "                'gamma': trial.suggest_float('gamma', 0, 5),\n",
        "                'tree_method': 'hist',  # M√°s eficiente en memoria\n",
        "                'use_label_encoder': False,  # Evitar warning\n",
        "                'n_jobs': -1 if available_memory_gb > 4.0 else 1\n",
        "            }\n",
        "        elif model_type == 'LightGBM':\n",
        "            params = {\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 50, max_estimators),\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, max_depth),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "                'subsample': trial.suggest_float('subsample', 0.5, subsample),\n",
        "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
        "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
        "                'boosting_type': 'gbdt',  # Tipo m√°s eficiente\n",
        "                'n_jobs': -1 if available_memory_gb > 4.0 else 1,\n",
        "                'verbose': -1\n",
        "            }\n",
        "        \n",
        "        # Combinar par√°metros comunes y espec√≠ficos\n",
        "        params.update(common_params)\n",
        "        \n",
        "        # Asegurar liberaci√≥n de memoria antes de crear nuevo modelo\n",
        "        gc.collect()\n",
        "        \n",
        "        # Crear y entrenar modelo\n",
        "        try:\n",
        "            model_class = model_types[model_type]\n",
        "            model = model_class(**params)\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "            return rmse\n",
        "        except Exception as e:\n",
        "            print(f\"Error al entrenar modelo con par√°metros: {params}\")\n",
        "            print(f\"Error: {e}\")\n",
        "            # Retornar un valor alto para que Optuna evite estos par√°metros\n",
        "            return float('inf')\n",
        "    \n",
        "    # Configuraci√≥n de estudio Optuna\n",
        "    pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
        "    sampler = optuna.samplers.TPESampler(seed=42)\n",
        "    \n",
        "    study_name = f\"{model_type}_memory_optimized\"\n",
        "    storage_name = \"sqlite:///{}.db\".format(model_output_dir / study_name)\n",
        "    \n",
        "    # Crear y ejecutar estudio\n",
        "    try:\n",
        "        study = optuna.create_study(\n",
        "            study_name=study_name,\n",
        "            storage=storage_name,\n",
        "            direction='minimize',\n",
        "            sampler=sampler,\n",
        "            pruner=pruner,\n",
        "            load_if_exists=True\n",
        "        )\n",
        "        \n",
        "        print(f\"Ejecutando {n_trials} pruebas para {model_type}...\")\n",
        "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "        \n",
        "        # Obtener mejores par√°metros\n",
        "        best_params = study.best_params\n",
        "        best_params['random_state'] = 42\n",
        "        \n",
        "        # Si es XGBoost, agregar par√°metros adicionales\n",
        "        if model_type == 'XGBoost':\n",
        "            best_params['tree_method'] = 'hist'\n",
        "            best_params['use_label_encoder'] = False\n",
        "        \n",
        "        # Entrenar modelo final con mejores par√°metros\n",
        "        print(f\"\\n‚úÖ Entrenando modelo final {model_type} con mejores par√°metros...\")\n",
        "        model_class = model_types[model_type]\n",
        "        best_model = model_class(**best_params)\n",
        "        \n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            best_model.fit(X_train, y_train)\n",
        "        \n",
        "        # Evaluar modelo final\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        \n",
        "        print(f\"\\nüìä M√©tricas de {model_type} optimizado:\")\n",
        "        print(f\"RMSE: {rmse:.4f}\")\n",
        "        print(f\"MAE: {mae:.4f}\")\n",
        "        print(f\"R¬≤: {r2:.4f}\")\n",
        "        \n",
        "        # A√±adir al diccionario de resultados\n",
        "        resultados_base[f\"{model_type}_Optuna\"] = (rmse, mae, r2)\n",
        "        \n",
        "        # Guardar modelo\n",
        "        filename = f\"{model_type}_optimized.pkl\"\n",
        "        with open(model_output_dir / filename, 'wb') as f:\n",
        "            pickle.dump(best_model, f)\n",
        "        print(f\"Modelo guardado como: {filename}\")\n",
        "        \n",
        "        # Devolver resultados\n",
        "        return best_params, best_model, (rmse, mae, r2)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error durante la optimizaci√≥n: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {}, None, (float('inf'), float('inf'), -float('inf'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b0f81dd",
      "metadata": {},
      "source": [
        "## üß† Implementaci√≥n de Modelos de Deep Learning\n",
        "\n",
        "A continuaci√≥n implementaremos modelos basados en redes neuronales profundas para capturar patrones espaciales y temporales en los datos de precipitaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a09e57e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementaci√≥n de modelo CNN para predicci√≥n espacial\n",
        "\n",
        "# Necesitamos reformatear los datos para CNN\n",
        "# Primero verificamos si hay columnas de coordenadas 'x' y 'y' en nuestros datos\n",
        "print(\"\\nüîç Preparando datos para modelos CNN...\")\n",
        "\n",
        "# Verificar si tenemos columnas de coordenadas en nuestros datos\n",
        "coord_cols = [col for col in feature_cols if col in ['x', 'y', 'latitude', 'longitude', 'lat', 'lon']]\n",
        "\n",
        "if len(coord_cols) >= 2:\n",
        "    print(f\"Columnas de coordenadas encontradas: {coord_cols}\")\n",
        "    \n",
        "    # Mapeo de nombres de columnas comunes\n",
        "    lat_names = ['latitude', 'lat', 'y']\n",
        "    lon_names = ['longitude', 'lon', 'x']\n",
        "    \n",
        "    # Identificar columnas de latitud y longitud\n",
        "    lat_col = next((col for col in coord_cols if col in lat_names), None)\n",
        "    lon_col = next((col for col in coord_cols if col in lon_names), None)\n",
        "    \n",
        "    if lat_col and lon_col:\n",
        "        print(f\"Usando {lat_col} y {lon_col} como coordenadas para CNN\")\n",
        "        \n",
        "        # Convertir datos a formato espacial para CNN\n",
        "        def prepare_spatial_data(X_data, y_data, lat_col, lon_col):\n",
        "            \"\"\"Prepara datos espaciales para CNN\"\"\"\n",
        "            # Extraer coordenadas √∫nicas en orden\n",
        "            lats = sorted(X_data[lat_col].unique())\n",
        "            lons = sorted(X_data[lon_col].unique())\n",
        "            \n",
        "            # Crear diccionarios de mapeo para √≠ndices\n",
        "            lat_to_idx = {lat: idx for idx, lat in enumerate(lats)}\n",
        "            lon_to_idx = {lon: idx for idx, lon in enumerate(lons)}\n",
        "            \n",
        "            # Dimensiones de la grilla\n",
        "            grid_height = len(lats)\n",
        "            grid_width = len(lons)\n",
        "            n_features = X_data.shape[1] - 2  # Restar las dos columnas de coordenadas\n",
        "            \n",
        "            # Inicializar arrays\n",
        "            # Las dimensiones son: [muestras, altura, ancho, canales]\n",
        "            X_grid = np.zeros((len(X_data), grid_height, grid_width, n_features))\n",
        "            y_grid = np.zeros((len(y_data), grid_height, grid_width, 1))\n",
        "            \n",
        "            # Recorrer todos los datos y ubicarlos en la grilla\n",
        "            non_coord_cols = [col for col in X_data.columns if col != lat_col and col != lon_col]\n",
        "            \n",
        "            for idx in range(len(X_data)):\n",
        "                lat = X_data.iloc[idx][lat_col]\n",
        "                lon = X_data.iloc[idx][lon_col]\n",
        "                \n",
        "                lat_idx = lat_to_idx[lat]\n",
        "                lon_idx = lon_to_idx[lon]\n",
        "                \n",
        "                # Colocar caracter√≠sticas en la grilla\n",
        "                for i, col in enumerate(non_coord_cols):\n",
        "                    X_grid[idx, lat_idx, lon_idx, i] = X_data.iloc[idx][col]\n",
        "                \n",
        "                # Colocar valor objetivo\n",
        "                y_grid[idx, lat_idx, lon_idx, 0] = y_data.iloc[idx]\n",
        "            \n",
        "            return X_grid, y_grid\n",
        "        \n",
        "        # Convertir datos de entrenamiento a formato espacial\n",
        "        print(\"Convirtiendo datos a formato espacial...\")\n",
        "        try:\n",
        "            # Reconstruir dataframes a partir de arrays escalados\n",
        "            X_train_df = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
        "            X_test_df = pd.DataFrame(X_test_scaled, columns=feature_cols)\n",
        "            \n",
        "            # Preparar datos espaciales\n",
        "            X_train_spatial, y_train_spatial = prepare_spatial_data(X_train_df, y_train, lat_col, lon_col)\n",
        "            X_test_spatial, y_test_spatial = prepare_spatial_data(X_test_df, y_test, lat_col, lon_col)\n",
        "            \n",
        "            print(f\"Datos espaciales preparados:\")\n",
        "            print(f\"X_train_spatial: {X_train_spatial.shape}\")\n",
        "            print(f\"y_train_spatial: {y_train_spatial.shape}\")\n",
        "            print(f\"X_test_spatial: {X_test_spatial.shape}\")\n",
        "            print(f\"y_test_spatial: {y_test_spatial.shape}\")\n",
        "            \n",
        "            # Si la conversi√≥n fue exitosa, procedemos con el modelo CNN\n",
        "            # Modelo CNN para predicci√≥n de precipitaci√≥n\n",
        "            def create_cnn_model(input_shape):\n",
        "                \"\"\"Crea un modelo CNN para predicci√≥n espacial\"\"\"\n",
        "                # Usar Input como primera capa para evitar el warning\n",
        "                inputs = Input(shape=input_shape)\n",
        "                \n",
        "                # Primera capa convolucional\n",
        "                x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
        "                x = BatchNormalization()(x)\n",
        "                \n",
        "                # Segunda capa convolucional\n",
        "                x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "                x = BatchNormalization()(x)\n",
        "                x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "                x = Dropout(0.25)(x)\n",
        "                \n",
        "                # Tercera capa convolucional\n",
        "                x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "                x = BatchNormalization()(x)\n",
        "                x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "                x = Dropout(0.25)(x)\n",
        "                \n",
        "                # Cuarta capa convolucional\n",
        "                x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "                x = BatchNormalization()(x)\n",
        "                \n",
        "                # Capa de salida\n",
        "                outputs = Conv2D(1, kernel_size=(1, 1), activation='linear', padding='same')(x)\n",
        "                \n",
        "                # Crear modelo usando API funcional\n",
        "                model = Model(inputs=inputs, outputs=outputs)\n",
        "                \n",
        "                # Compilar modelo\n",
        "                model.compile(\n",
        "                    loss='mse',\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    metrics=['mae']\n",
        "                )\n",
        "                \n",
        "                return model\n",
        "            \n",
        "            # Crear y entrenar modelo CNN\n",
        "            print(\"\\nüß† Creando y entrenando modelo CNN...\")\n",
        "            input_shape = X_train_spatial.shape[1:]  # (altura, ancho, canales)\n",
        "            cnn_model = create_cnn_model(input_shape)\n",
        "            \n",
        "            # Mostrar resumen del modelo\n",
        "            cnn_model.summary()\n",
        "            \n",
        "            # Callbacks para entrenamiento\n",
        "            callbacks = [\n",
        "                EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
        "                ModelCheckpoint(filepath=model_output_dir / 'cnn_model_best.h5',\n",
        "                              save_best_only=True, monitor='val_loss')\n",
        "            ]\n",
        "            \n",
        "            # Entrenar modelo\n",
        "            history = cnn_model.fit(\n",
        "                X_train_spatial, y_train_spatial,\n",
        "                validation_split=0.2,\n",
        "                epochs=100,\n",
        "                batch_size=32,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "            \n",
        "            # Evaluar modelo\n",
        "            print(\"\\nüìä Evaluando modelo CNN...\")\n",
        "            cnn_metrics = cnn_model.evaluate(X_test_spatial, y_test_spatial)\n",
        "            print(f\"Loss (MSE): {cnn_metrics[0]:.4f}\")\n",
        "            print(f\"MAE: {cnn_metrics[1]:.4f}\")\n",
        "            \n",
        "            # Predecir con el modelo\n",
        "            y_pred_cnn = cnn_model.predict(X_test_spatial)\n",
        "            \n",
        "            # Aplanar las predicciones para calcular m√©tricas\n",
        "            y_test_flat = y_test_spatial.flatten()\n",
        "            y_pred_flat = y_pred_cnn.flatten()\n",
        "            \n",
        "            # Filtrar valores donde y_test_flat > 0 (presumiblemente donde hay datos)\n",
        "            valid_indices = y_test_flat > 0\n",
        "            y_test_valid = y_test_flat[valid_indices]\n",
        "            y_pred_valid = y_pred_flat[valid_indices]\n",
        "            \n",
        "            # Calcular m√©tricas\n",
        "            cnn_rmse = np.sqrt(mean_squared_error(y_test_valid, y_pred_valid))\n",
        "            cnn_mae = mean_absolute_error(y_test_valid, y_pred_valid)\n",
        "            cnn_r2 = r2_score(y_test_valid, y_pred_valid)\n",
        "            \n",
        "            print(f\"RMSE: {cnn_rmse:.4f}\")\n",
        "            print(f\"MAE: {cnn_mae:.4f}\")\n",
        "            print(f\"R¬≤: {cnn_r2:.4f}\")\n",
        "            \n",
        "            # Guardar modelo\n",
        "            cnn_model.save(model_output_dir / 'cnn_model_final.h5')\n",
        "            print(\"Modelo CNN guardado como 'cnn_model_final.h5'\")\n",
        "            \n",
        "            # Visualizar la historia del entrenamiento\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(history.history['loss'])\n",
        "            plt.plot(history.history['val_loss'])\n",
        "            plt.title('P√©rdida del modelo')\n",
        "            plt.ylabel('P√©rdida')\n",
        "            plt.xlabel('√âpoca')\n",
        "            plt.legend(['Entrenamiento', 'Validaci√≥n'], loc='upper right')\n",
        "            \n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(history.history['mae'])\n",
        "            plt.plot(history.history['val_mae'])\n",
        "            plt.title('Error absoluto medio')\n",
        "            plt.ylabel('MAE')\n",
        "            plt.xlabel('√âpoca')\n",
        "            plt.legend(['Entrenamiento', 'Validaci√≥n'], loc='upper right')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(model_output_dir / 'cnn_training_history.png')\n",
        "            plt.show()\n",
        "            \n",
        "            # A√±adir resultados a nuestro diccionario de resultados\n",
        "            resultados_base['CNN'] = (cnn_rmse, cnn_mae, cnn_r2)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error preparando datos espaciales para CNN: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"No se pudieron identificar columnas de latitud y longitud.\")\n",
        "else:\n",
        "    print(\"No se encontraron suficientes columnas de coordenadas para implementar CNN.\")\n",
        "    print(\"El modelo CNN requiere al menos columnas de latitud y longitud.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1286207f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementaci√≥n de modelo ConvLSTM para predicci√≥n espaciotemporal\n",
        "print(\"\\nüîç Preparando datos para modelo ConvLSTM...\")\n",
        "\n",
        "# Verificar si tenemos el DataFrame disponible\n",
        "if 'df' not in locals() or df is None:\n",
        "    print(\"DataFrame no disponible, intentando recargarlo...\")\n",
        "    try:\n",
        "        # Recargar el dataset si no est√° disponible\n",
        "        data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features.nc'\n",
        "        print(f\"Recargando archivo desde: {data_file}\")\n",
        "        df, ds_original = load_dataset(data_file)\n",
        "        \n",
        "        if df is None:\n",
        "            print(\"Error: No se pudo recargar el DataFrame. Verificar la ruta del archivo.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al recargar el DataFrame: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Verificar si tenemos las variables necesarias\n",
        "if 'feature_cols' not in locals() or 'target_column' not in locals():\n",
        "    print(\"Variables necesarias no definidas, intentando redefinirlas...\")\n",
        "    if df is not None:\n",
        "        # Identificar la columna objetivo (precipitaci√≥n)\n",
        "        target_column = 'total_precipitation'  # Ajustar si tiene otro nombre en tu dataset\n",
        "        \n",
        "        # Ver si existe 'precip_target' o usar 'total_precipitation'\n",
        "        if 'total_precipitation' in df.columns:\n",
        "            target_column = 'total_precipitation'\n",
        "        \n",
        "        # Separar variables predictoras y variable objetivo\n",
        "        feature_cols = [col for col in df.columns if col != target_column and not pd.isna(df[col]).all()]\n",
        "        \n",
        "        # Eliminar columnas no num√©ricas para los modelos\n",
        "        non_feature_cols = ['time', 'spatial_ref']\n",
        "        feature_cols = [col for col in feature_cols if col not in non_feature_cols]\n",
        "\n",
        "# Si el DataFrame est√° disponible, continuar con la preparaci√≥n de datos\n",
        "if df is not None:\n",
        "    # Para ConvLSTM necesitamos datos con dimensi√≥n temporal\n",
        "    time_cols = [col for col in df.columns if col in ['time', 'date', 'month', 'year', 'day']]\n",
        "\n",
        "    if len(time_cols) > 0 and len(coord_cols) >= 2:\n",
        "        print(f\"Columnas temporales encontradas: {time_cols}\")\n",
        "        time_col = time_cols[0]\n",
        "        \n",
        "        # Funci√≥n para preparar datos espaciotemporales\n",
        "        def prepare_spatiotemporal_data(df, feature_cols, target_column, lat_col, lon_col, time_col, \n",
        "                                        sequence_length=3):\n",
        "            \"\"\"Prepara datos para ConvLSTM con dimensi√≥n espaciotemporal\"\"\"\n",
        "            print(\"Preparando datos espaciotemporales para ConvLSTM...\")\n",
        "            try:\n",
        "                # Asegurarnos que la columna temporal est√° ordenada\n",
        "                # Verificar el tipo de la columna temporal\n",
        "                time_dtype = df[time_col].dtype\n",
        "                print(f\"Tipo de dato de columna temporal: {time_dtype}\")\n",
        "                \n",
        "                if pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
        "                    # Ya es datetime, ordenamos\n",
        "                    df_sorted = df.sort_values(by=time_col)\n",
        "                else:\n",
        "                    # Intentar convertir a datetime\n",
        "                    try:\n",
        "                        df[time_col] = pd.to_datetime(df[time_col])\n",
        "                        df_sorted = df.sort_values(by=time_col)\n",
        "                    except Exception as e:\n",
        "                        print(f\"No se pudo convertir columna temporal a datetime: {e}\")\n",
        "                        # Si no podemos convertir, asumimos que ya est√° ordenado\n",
        "                        df_sorted = df\n",
        "                \n",
        "                # Extraer coordenadas √∫nicas\n",
        "                lats = sorted(df_sorted[lat_col].unique())\n",
        "                lons = sorted(df_sorted[lon_col].unique())\n",
        "                time_steps = sorted(df_sorted[time_col].unique())\n",
        "                \n",
        "                print(f\"Dimensiones espaciotemporales:\")\n",
        "                print(f\"- Latitudes (filas): {len(lats)}\")\n",
        "                print(f\"- Longitudes (columnas): {len(lons)}\")\n",
        "                print(f\"- Pasos temporales: {len(time_steps)}\")\n",
        "                \n",
        "                # Crear mapeos para √≠ndices\n",
        "                lat_to_idx = {lat: idx for idx, lat in enumerate(lats)}\n",
        "                lon_to_idx = {lon: idx for idx, lon in enumerate(lons)}\n",
        "                time_to_idx = {time: idx for idx, time in enumerate(time_steps)}\n",
        "                \n",
        "                # Filtrar columnas feature eliminando coordenadas y tiempo\n",
        "                feature_cols_filtered = [col for col in feature_cols if col != lat_col and col != lon_col and col != time_col]\n",
        "                n_features = len(feature_cols_filtered)\n",
        "                \n",
        "                # Dimensiones de la grilla espaciotemporal\n",
        "                grid_height = len(lats)\n",
        "                grid_width = len(lons)\n",
        "                n_timesteps = len(time_steps)\n",
        "                \n",
        "                print(f\"Caracter√≠sticas a usar: {n_features}\")\n",
        "                \n",
        "                # Crear un DataFrame indexado para acceso r√°pido\n",
        "                df_indexed = df_sorted.set_index([time_col, lat_col, lon_col])\n",
        "                \n",
        "                # Crear matrices 3D para cada paso temporal\n",
        "                # Las dimensiones son: [tiempo, altura, ancho, features]\n",
        "                X_spatiotemporal = np.zeros((n_timesteps, grid_height, grid_width, n_features))\n",
        "                y_spatiotemporal = np.zeros((n_timesteps, grid_height, grid_width, 1))\n",
        "                \n",
        "                # Llenar matrices con datos disponibles\n",
        "                for t_idx, t in enumerate(time_steps):\n",
        "                    for lat_idx, lat in enumerate(lats):\n",
        "                        for lon_idx, lon in enumerate(lons):\n",
        "                            try:\n",
        "                                # Obtener datos para esta coordenada y tiempo\n",
        "                                data = df_indexed.loc[(t, lat, lon)]\n",
        "                                \n",
        "                                # Llenar caracter√≠sticas\n",
        "                                for f_idx, feat in enumerate(feature_cols_filtered):\n",
        "                                    X_spatiotemporal[t_idx, lat_idx, lon_idx, f_idx] = data[feat]\n",
        "                                \n",
        "                                # Llenar target\n",
        "                                y_spatiotemporal[t_idx, lat_idx, lon_idx, 0] = data[target_column]\n",
        "                            except KeyError:\n",
        "                                # Este punto espaciotemporal no existe en los datos\n",
        "                                pass\n",
        "                \n",
        "                # Crear secuencias para ConvLSTM\n",
        "                # Para cada paso temporal t, usaremos t-sequence_length hasta t-1 para predecir t\n",
        "                n_sequences = n_timesteps - sequence_length\n",
        "                \n",
        "                if n_sequences <= 0:\n",
        "                    print(\"No hay suficientes pasos temporales para crear secuencias. Ajustando sequence_length.\")\n",
        "                    sequence_length = max(1, n_timesteps // 2)\n",
        "                    n_sequences = n_timesteps - sequence_length\n",
        "                    print(f\"Nuevo sequence_length: {sequence_length}, n_sequences: {n_sequences}\")\n",
        "                \n",
        "                # Crear arrays para secuencias\n",
        "                X_sequences = np.zeros((n_sequences, sequence_length, grid_height, grid_width, n_features))\n",
        "                y_sequences = np.zeros((n_sequences, grid_height, grid_width, 1))\n",
        "                \n",
        "                for i in range(n_sequences):\n",
        "                    X_sequences[i] = X_spatiotemporal[i:i+sequence_length]\n",
        "                    y_sequences[i] = y_spatiotemporal[i+sequence_length]\n",
        "                \n",
        "                print(f\"Secuencias creadas:\")\n",
        "                print(f\"X_sequences: {X_sequences.shape}\")\n",
        "                print(f\"y_sequences: {y_sequences.shape}\")\n",
        "                \n",
        "                # Dividir en train/test\n",
        "                train_size = int(0.8 * n_sequences)\n",
        "                X_train = X_sequences[:train_size]\n",
        "                y_train = y_sequences[:train_size]\n",
        "                X_test = X_sequences[train_size:]\n",
        "                y_test = y_sequences[train_size:]\n",
        "                \n",
        "                return X_train, y_train, X_test, y_test\n",
        "            except Exception as e:\n",
        "                print(f\"Error preparando datos espaciotemporales: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                return None, None, None, None\n",
        "        \n",
        "        # Intentar preparar datos espaciotemporales\n",
        "        try:\n",
        "            X_train_convlstm, y_train_convlstm, X_test_convlstm, y_test_convlstm = prepare_spatiotemporal_data(\n",
        "                df, feature_cols, target_column, lat_col, lon_col, time_col, sequence_length=3\n",
        "            )\n",
        "            \n",
        "            # Si los datos se preparan correctamente, crear y entrenar modelo ConvLSTM\n",
        "            if X_train_convlstm is not None:\n",
        "                print(\"\\nüß† Creando y entrenando modelo ConvLSTM...\")\n",
        "                \n",
        "                def create_convlstm_model(input_shape):\n",
        "                    \"\"\"Crea un modelo ConvLSTM para predicci√≥n espaciotemporal\"\"\"\n",
        "                    model = Sequential([\n",
        "                        # Capa ConvLSTM\n",
        "                        ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same',\n",
        "                                  return_sequences=True, activation='tanh',\n",
        "                                  input_shape=input_shape),\n",
        "                        BatchNormalization(),\n",
        "                        \n",
        "                        # Segunda capa ConvLSTM\n",
        "                        ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same',\n",
        "                                   return_sequences=False, activation='tanh'),\n",
        "                        BatchNormalization(),\n",
        "                        \n",
        "                        # Capa convolucional para reducir mapas de caracter√≠sticas\n",
        "                        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "                        BatchNormalization(),\n",
        "                        MaxPooling2D(pool_size=(2, 2)),\n",
        "                        \n",
        "                        # Capas finales\n",
        "                        Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "                        UpSampling2D(size=(2, 2)),  # Restaurar dimensi√≥n original\n",
        "                        Conv2D(filters=1, kernel_size=(3, 3), activation='linear', padding='same')\n",
        "                    ])\n",
        "                    \n",
        "                    # Compilar modelo\n",
        "                    model.compile(\n",
        "                        loss='mse',\n",
        "                        optimizer=Adam(learning_rate=0.001),\n",
        "                        metrics=['mae']\n",
        "                    )\n",
        "                    \n",
        "                    return model\n",
        "                \n",
        "                # Crear modelo ConvLSTM\n",
        "                input_shape = X_train_convlstm.shape[1:]  # (sequence_length, height, width, features)\n",
        "                convlstm_model = create_convlstm_model(input_shape)\n",
        "                \n",
        "                # Mostrar resumen del modelo\n",
        "                convlstm_model.summary()\n",
        "                \n",
        "                # Callbacks para entrenamiento\n",
        "                callbacks = [\n",
        "                    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "                    ModelCheckpoint(filepath=model_output_dir / 'convlstm_model_best.h5',\n",
        "                                  save_best_only=True, monitor='val_loss')\n",
        "                ]\n",
        "                \n",
        "                # Entrenar modelo\n",
        "                history = convlstm_model.fit(\n",
        "                    X_train_convlstm, y_train_convlstm,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=100,\n",
        "                    batch_size=16,\n",
        "                    callbacks=callbacks,\n",
        "                    verbose=1\n",
        "                )\n",
        "                \n",
        "                # Evaluar modelo\n",
        "                print(\"\\nüìä Evaluando modelo ConvLSTM...\")\n",
        "                convlstm_metrics = convlstm_model.evaluate(X_test_convlstm, y_test_convlstm)\n",
        "                print(f\"Loss (MSE): {convlstm_metrics[0]:.4f}\")\n",
        "                print(f\"MAE: {convlstm_metrics[1]:.4f}\")\n",
        "                \n",
        "                # Predecir con el modelo\n",
        "                y_pred_convlstm = convlstm_model.predict(X_test_convlstm)\n",
        "                \n",
        "                # Aplanar las predicciones para calcular m√©tricas\n",
        "                y_test_flat = y_test_convlstm.flatten()\n",
        "                y_pred_flat = y_pred_convlstm.flatten()\n",
        "                \n",
        "                # Filtrar valores donde y_test_flat > 0 (presumiblemente donde hay datos)\n",
        "                valid_indices = y_test_flat > 0\n",
        "                y_test_valid = y_test_flat[valid_indices]\n",
        "                y_pred_valid = y_pred_flat[valid_indices]\n",
        "                \n",
        "                # Calcular m√©tricas\n",
        "                convlstm_rmse = np.sqrt(mean_squared_error(y_test_valid, y_pred_valid))\n",
        "                convlstm_mae = mean_absolute_error(y_test_valid, y_pred_valid)\n",
        "                convlstm_r2 = r2_score(y_test_valid, y_pred_valid)\n",
        "                \n",
        "                print(f\"RMSE: {convlstm_rmse:.4f}\")\n",
        "                print(f\"MAE: {convlstm_mae:.4f}\")\n",
        "                print(f\"R¬≤: {convlstm_r2:.4f}\")\n",
        "                \n",
        "                # Guardar modelo\n",
        "                convlstm_model.save(model_output_dir / 'convlstm_model_final.h5')\n",
        "                print(\"Modelo ConvLSTM guardado como 'convlstm_model_final.h5'\")\n",
        "                \n",
        "                # Visualizar la historia del entrenamiento\n",
        "                plt.figure(figsize=(12, 5))\n",
        "                plt.subplot(1, 2, 1)\n",
        "                plt.plot(history.history['loss'])\n",
        "                plt.plot(history.history['val_loss'])\n",
        "                plt.title('P√©rdida del modelo ConvLSTM')\n",
        "                plt.ylabel('P√©rdida')\n",
        "                plt.xlabel('√âpoca')\n",
        "                plt.legend(['Entrenamiento', 'Validaci√≥n'], loc='upper right')\n",
        "                \n",
        "                plt.subplot(1, 2, 2)\n",
        "                plt.plot(history.history['mae'])\n",
        "                plt.plot(history.history['val_mae'])\n",
        "                plt.title('Error absoluto medio ConvLSTM')\n",
        "                plt.ylabel('MAE')\n",
        "                plt.xlabel('√âpoca')\n",
        "                plt.legend(['Entrenamiento', 'Validaci√≥n'], loc='upper right')\n",
        "                \n",
        "                plt.tight_layout()\n",
        "                plt.savefig(model_output_dir / 'convlstm_training_history.png')\n",
        "                plt.show()\n",
        "                \n",
        "                # A√±adir resultados a nuestro diccionario de comparaci√≥n\n",
        "                resultados_base['ConvLSTM'] = (convlstm_rmse, convlstm_mae, convlstm_r2)\n",
        "            else:\n",
        "                print(\"No se pudieron preparar datos para ConvLSTM.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al ejecutar preparaci√≥n de datos para ConvLSTM: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"No se encontraron columnas temporales o espaciales suficientes para implementar ConvLSTM.\")\n",
        "        print(\"El modelo ConvLSTM requiere al menos una columna temporal y dos columnas espaciales.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87873507",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecutar optimizaci√≥n adaptativa de memoria RAM para modelos base\n",
        "print(\"\\nüîç Ejecutando optimizaci√≥n adaptativa de memoria RAM para Random Forest...\")\n",
        "rf_params, rf_model_opt, rf_metrics_opt = run_memory_efficient_optimization('RandomForest', X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "\n",
        "print(\"\\nüîç Ejecutando optimizaci√≥n adaptativa de memoria RAM para XGBoost...\")\n",
        "xgb_params, xgb_model_opt, xgb_metrics_opt = run_memory_efficient_optimization('XGBoost', X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "\n",
        "print(\"\\nüîç Ejecutando optimizaci√≥n adaptativa de memoria RAM para LightGBM...\")\n",
        "lgbm_params, lgbm_model_opt, lgbm_metrics_opt = run_memory_efficient_optimization('LightGBM', X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "\n",
        "# Resumen de par√°metros √≥ptimos encontrados\n",
        "print(\"\\nüìä Mejores par√°metros encontrados para cada modelo:\")\n",
        "print(f\"\\nRandom Forest: {rf_params}\")\n",
        "print(f\"\\nXGBoost: {xgb_params}\")\n",
        "print(f\"\\nLightGBM: {lgbm_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4560e380",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mejora de visibilidad en el entrenamiento y errores para modelos CNN y ConvLSTM\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def plot_training_history(history, model_name):\n",
        "    \"\"\"Visualiza la historia del entrenamiento de un modelo.\"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # P√©rdida\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Entrenamiento')\n",
        "    plt.plot(history.history['val_loss'], label='Validaci√≥n')\n",
        "    plt.title(f'P√©rdida del modelo {model_name}')\n",
        "    plt.ylabel('P√©rdida')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    # MAE\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['mae'], label='Entrenamiento')\n",
        "    plt.plot(history.history['val_mae'], label='Validaci√≥n')\n",
        "    plt.title(f'Error absoluto medio (MAE) - {model_name}')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def display_model_metrics(metrics, model_name):\n",
        "    \"\"\"Muestra las m√©tricas de evaluaci√≥n de un modelo de forma visual.\"\"\"\n",
        "    rmse, mae, r2 = metrics\n",
        "    display(HTML(f'<div style=\"background-color:#f5f5dc; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
        "                 f'<h3>üìä M√©tricas para {model_name}</h3>' +\n",
        "                 f'<table style=\"width:100%; text-align:left;\">' +\n",
        "                 f'<tr><th>M√©trica</th><th>Valor</th></tr>' +\n",
        "                 f'<tr><td>RMSE</td><td>{rmse:.4f}</td></tr>' +\n",
        "                 f'<tr><td>MAE</td><td>{mae:.4f}</td></tr>' +\n",
        "                 f'<tr><td>R¬≤</td><td>{r2:.4f}</td></tr>' +\n",
        "                 f'</table></div>'))\n",
        "\n",
        "# Aplicar mejoras de visibilidad al modelo CNN\n",
        "if 'cnn_model' in locals() and 'history' in locals():\n",
        "    print(\"\\nüìà Visualizando historia del entrenamiento para modelo CNN...\")\n",
        "    plot_training_history(history, 'CNN')\n",
        "\n",
        "if 'cnn_metrics' in locals():\n",
        "    print(\"\\nüìä Mostrando m√©tricas para modelo CNN...\")\n",
        "    display_model_metrics((cnn_rmse, cnn_mae, cnn_r2), 'CNN')\n",
        "\n",
        "# Aplicar mejoras de visibilidad al modelo ConvLSTM\n",
        "if 'convlstm_model' in locals() and 'history' in locals():\n",
        "    print(\"\\nüìà Visualizando historia del entrenamiento para modelo ConvLSTM...\")\n",
        "    plot_training_history(history, 'ConvLSTM')\n",
        "\n",
        "if 'convlstm_metrics' in locals():\n",
        "    print(\"\\nüìä Mostrando m√©tricas para modelo ConvLSTM...\")\n",
        "    display_model_metrics((convlstm_rmse, convlstm_mae, convlstm_r2), 'ConvLSTM')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "precipitation_prediction",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
