{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b176146b",
   "metadata": {},
   "source": [
    "\n",
    "# Spatiotemporal Precipitation Prediction\n",
    "**5 √ó 5 Experiments Notebook**  \n",
    "Train & validate five architectures across five temporal folds (48 m train ‚Üí 12 m val).  Designed to run **locally or on Google Colab** ‚Äî auto‚Äëdetects GPU/CPU and adapts parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b7a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚ñ∂Ô∏è Prevent kernel crashes due to CDN and memory issues\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# 1. Disable CDN access to prevent widget errors\n",
    "os.environ['JUPYTER_DISABLE_MATHJAX'] = '1'  # Disable MathJax (uses CDN)\n",
    "os.environ['TQDM_DISABLE'] = '1'  # Avoid tqdm widgets that might use CDN\n",
    "os.environ['MPLBACKEND'] = 'Agg'  # Use non-interactive backend for matplotlib\n",
    "\n",
    "# Ignore warnings related to widgets and CDN\n",
    "warnings.filterwarnings('ignore', message=\".*widget.*|.*CDN.*|.*SSL.*\")\n",
    "\n",
    "# 2. Configure memory limit to avoid OOM\n",
    "try:\n",
    "    import resource\n",
    "    # Soft limit of 12GB (adjust according to available memory)\n",
    "    soft, hard = resource.getrlimit(resource.RLIMIT_AS)\n",
    "    mem_limit = 12 * (1024**3)  # 12GB in bytes\n",
    "    resource.setrlimit(resource.RLIMIT_AS, (mem_limit, hard))\n",
    "    print(f\"‚úÖ Memory limit set: 12GB\")\n",
    "except Exception:\n",
    "    print(\"‚ö†Ô∏è Could not set memory limit\")\n",
    "\n",
    "# 3. Function to free memory (use it when you notice slowdowns)\n",
    "def clean_memory():\n",
    "    \"\"\"Releases memory to prevent kernel crashes\"\"\"\n",
    "    import gc\n",
    "    print(\"üßπ Cleaning memory...\")\n",
    "    \n",
    "    # Garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Release GPU cache if PyTorch is available\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"  ‚úì GPU cache released\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Close matplotlib figures\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.close('all')\n",
    "        print(\"  ‚úì Figures closed\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "        \n",
    "    print(\"‚úÖ Memory released\")\n",
    "\n",
    "print(\"‚úÖ Anti-blocking configuration successfully applied\")\n",
    "print(\"üí° Use clean_memory() if you notice the notebook slowing down\")\n",
    "# ‚ñ∂Ô∏è Memory monitor and safe execution\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directory for checkpoints\n",
    "CHECKPOINT_DIR = Path('./checkpoints')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Define MVP mode - set to True for minimal viable product run (faster execution)\n",
    "MVP_MODE = True\n",
    "\n",
    "print(\"\"\"\n",
    "üîÑ CHECKPOINT SYSTEM ACTIVE\n",
    "\n",
    "The notebook uses a robust checkpoint system that allows recovery from crashes:\n",
    "- Each of the {'5' if not MVP_MODE else '1'} experiments ({len(EXPERIMENTS) if 'EXPERIMENTS' in globals() else '5'} architectures √ó {'5' if not MVP_MODE else '1'} folds) is saved individually\n",
    "- Training automatically resumes from the last saved checkpoint\n",
    "- Perfect for long-running experiments that might be interrupted\n",
    "\"\"\")\n",
    "\n",
    "class SafeExecution:\n",
    "    \"\"\"\n",
    "    Robust execution system to protect against crashes during training\n",
    "    \n",
    "    Features:\n",
    "    - Automatic saving of trained models and metrics\n",
    "    - Recovery from previous checkpoints if training was interrupted\n",
    "    - Memory cleanup before each experiment\n",
    "    \n",
    "    Note: Ideal for long-running notebooks with multiple experiments\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_checkpoint(data, name):\n",
    "        \"\"\"Save data in a checkpoint\"\"\"\n",
    "        try:\n",
    "            path = CHECKPOINT_DIR / f\"{name}.pkl\"\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            print(f\"‚úÖ Checkpoint saved: {path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving checkpoint: {e}\")\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_checkpoint(name):\n",
    "        \"\"\"Load data from a checkpoint\"\"\"\n",
    "        try:\n",
    "            path = CHECKPOINT_DIR / f\"{name}.pkl\"\n",
    "            if not path.exists():\n",
    "                return None\n",
    "            \n",
    "            with open(path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            print(f\"‚úÖ Checkpoint loaded: {path}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading checkpoint: {e}\")\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def run_experiment(exp_name, fold=None):\n",
    "        \"\"\"\n",
    "        Run an experiment safely with automatic checkpoint recovery\n",
    "        \n",
    "        Args:\n",
    "            exp_name: Name of the experiment to run (must be in EXPERIMENTS)\n",
    "            fold: Specific fold to run, or None to run all folds\n",
    "            \n",
    "        Note:\n",
    "            When in MVP_MODE, this will only run fold F1 regardless of what's specified\n",
    "        \"\"\"\n",
    "        if exp_name not in EXPERIMENTS:\n",
    "            print(f\"‚ùå Experiment '{exp_name}' does not exist\")\n",
    "            return\n",
    "        \n",
    "        # Determine folds to run\n",
    "        if fold:\n",
    "            folds_to_run = [fold] if fold in FOLDS else []\n",
    "        else:\n",
    "            folds_to_run = list(FOLDS.keys())\n",
    "        \n",
    "        if not folds_to_run:\n",
    "            if MVP_MODE and fold not in FOLDS:\n",
    "                print(f\"‚ùå Fold '{fold}' not available in MVP_MODE (only {list(FOLDS.keys())} available)\")\n",
    "            else:\n",
    "                print(f\"‚ùå Invalid fold '{fold}'\")\n",
    "            return\n",
    "            \n",
    "        print(f\"üîÑ Running experiment {exp_name} on folds: {', '.join(folds_to_run)}\")\n",
    "        \n",
    "        for current_fold in folds_to_run:\n",
    "            # Checkpoint name for this experiment/fold\n",
    "            checkpoint_name = f\"{exp_name}_{current_fold}_result\"\n",
    "            \n",
    "            # Check if a previous result exists\n",
    "            checkpoint_data = SafeExecution.load_checkpoint(checkpoint_name)\n",
    "            if checkpoint_data:\n",
    "                model, history, best_rmse = checkpoint_data\n",
    "                print(f\"‚úÖ Using previous result: RMSE = {best_rmse:.4f}\")\n",
    "                \n",
    "                # Register global result\n",
    "                if 'RESULTS' in globals():\n",
    "                    RESULTS.append({\n",
    "                        'exp': exp_name,\n",
    "                        'fold': current_fold,\n",
    "                        'rmse': best_rmse\n",
    "                    })\n",
    "                    \n",
    "                # Update global histories\n",
    "                if 'ALL_HISTORIES' in globals():\n",
    "                    if exp_name not in ALL_HISTORIES:\n",
    "                        ALL_HISTORIES[exp_name] = {}\n",
    "                    ALL_HISTORIES[exp_name][current_fold] = history\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            # If no checkpoint, run the training\n",
    "            try:\n",
    "                # Free memory before starting\n",
    "                clean_memory()\n",
    "                \n",
    "                # Get configuration and build dataloaders\n",
    "                print(f\"üîÑ Preparing data for fold {current_fold}\")\n",
    "                cfg = EXPERIMENTS[exp_name]\n",
    "                val_year = FOLDS[current_fold]\n",
    "                \n",
    "                # Use reduced batch size for greater stability\n",
    "                batch_size = max(8, BATCH_SIZE // 2)  # Half the original batch size, minimum 8\n",
    "                train_loader, val_loader, in_dim = build_dataloaders(val_year, cfg['use_lags'], batch_size)\n",
    "                \n",
    "                # Adjust dropout according to documentation\n",
    "                dropout = 0.25 if current_fold in ['F4', 'F5'] else 0.20\n",
    "                \n",
    "                # Create model\n",
    "                model = MODEL_FACTORY[cfg['model']](in_dim, dropout=dropout).to(DEVICE)\n",
    "                \n",
    "                # Train model with error handling\n",
    "                print(f\"üîÑ Training {exp_name} on fold {current_fold}\")\n",
    "                try:\n",
    "                    model, history, best_rmse = train_with_history(\n",
    "                        model, train_loader, val_loader,\n",
    "                        epochs=60, patience=20,\n",
    "                        lr=1e-3, weight_decay=1e-4,\n",
    "                        fold=current_fold, exp_name=exp_name\n",
    "                    )\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    SafeExecution.save_checkpoint(\n",
    "                        (model, history, best_rmse),\n",
    "                        checkpoint_name\n",
    "                    )\n",
    "                    \n",
    "                    # Register global result\n",
    "                    if 'RESULTS' in globals():\n",
    "                        RESULTS.append({\n",
    "                            'exp': exp_name,\n",
    "                            'fold': current_fold,\n",
    "                            'rmse': best_rmse\n",
    "                        })\n",
    "                    \n",
    "                    # Update global histories\n",
    "                    if 'ALL_HISTORIES' in globals():\n",
    "                        if exp_name not in ALL_HISTORIES:\n",
    "                            ALL_HISTORIES[exp_name] = {}\n",
    "                        ALL_HISTORIES[exp_name][current_fold] = history\n",
    "                        \n",
    "                    print(f\"‚úÖ Training completed: RMSE = {best_rmse:.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error in training: {e}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error in experiment {exp_name}, fold {current_fold}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"‚úÖ Experiment {exp_name} completed\")\n",
    "\n",
    "# Function to display saved results\n",
    "def show_results():\n",
    "    \"\"\"Displays a table of results with experiments executed so far\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Search for results in checkpoints\n",
    "    results = []\n",
    "    \n",
    "    for file in CHECKPOINT_DIR.glob(\"*_result.pkl\"):\n",
    "        try:\n",
    "            parts = file.stem.split('_')\n",
    "            exp = parts[0] \n",
    "            fold = parts[1]\n",
    "            \n",
    "            checkpoint = SafeExecution.load_checkpoint(f\"{exp}_{fold}_result\")\n",
    "            if checkpoint:\n",
    "                _, _, rmse = checkpoint\n",
    "                results.append({\n",
    "                    'exp': exp,\n",
    "                    'fold': fold,\n",
    "                    'rmse': rmse\n",
    "                })\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        table = df.pivot(index='exp', columns='fold', values='rmse')\n",
    "        display(table)\n",
    "        \n",
    "        # Show progress\n",
    "        total = len(EXPERIMENTS) * len(FOLDS)\n",
    "        completed = len(results)\n",
    "        \n",
    "        print(f\"\\nüìä Progress: {completed}/{total} ({completed/total:.1%})\")\n",
    "        \n",
    "        if MVP_MODE:\n",
    "            print(f\"\\nüöÄ MVP Mode: Only showing results for fold F1 (most recent data)\")\n",
    "    else:\n",
    "        print(\"‚ùå No saved results found\")\n",
    "\n",
    "\n",
    "print(\"\"\"‚úÖ Safe execution system activated\n",
    "\n",
    "To run experiments safely:\n",
    "\n",
    "  1. SafeExecution.run_experiment('GRU-ED', fold='F1')  # A specific fold\n",
    "  2. SafeExecution.run_experiment('GRU-ED')             # All folds (in MVP mode: only F1)\n",
    "  3. show_results()                                     # View saved results\n",
    "\n",
    "Results are automatically saved and can be recovered\n",
    "if the kernel dies during execution.\n",
    "\n",
    "Current mode: {\"üöÄ MVP (F1 only)\" if MVP_MODE else \"üìä FULL (all folds)\"}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ‚ñ∂Ô∏è Environment setup (PyTorch + TF + XGBoost)\n",
    "import sys, os, logging, warnings, json\n",
    "from pathlib import Path\n",
    "import platform, multiprocessing\n",
    "\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    print(\"Warning: psutil is not installed. Hardware detection will be limited.\")\n",
    "    # Create a simple fallback class for psutil\n",
    "    class PsutilFallback:\n",
    "        @staticmethod\n",
    "        def cpu_count(logical=True):\n",
    "            return multiprocessing.cpu_count()\n",
    "        \n",
    "        @staticmethod\n",
    "        def virtual_memory():\n",
    "            class MemInfo:\n",
    "                total = 8 * (1024**3)  # Assume 8GB\n",
    "                available = 4 * (1024**3)  # Assume 4GB available\n",
    "                percent = 50.0\n",
    "            return MemInfo()\n",
    "        \n",
    "        @staticmethod\n",
    "        def cpu_percent(*args, **kwargs):\n",
    "            return 50.0\n",
    "    \n",
    "    psutil = PsutilFallback()\n",
    "\n",
    "# Import main libraries with exception handling\n",
    "try:\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "except ImportError as e:\n",
    "    print(f\"Critical error: {e}\")\n",
    "    print(\"Please run the robust dependency installation cell first\")\n",
    "\n",
    "# Additional imports are done with try/except to avoid kernel failures\n",
    "try:\n",
    "    import xarray as xr\n",
    "    import pytorch_lightning as pl\n",
    "    import tensorflow as tf\n",
    "    import geopandas as gpd\n",
    "    import cartopy.crs as ccrs\n",
    "    import time\n",
    "    from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from torch.optim.lr_scheduler import OneCycleLR\n",
    "    from torch import nn\n",
    "    from tqdm.auto import tqdm\n",
    "    from IPython.display import display, HTML\n",
    "    # ‚ñ∂Ô∏è Functions for learning curves and prediction visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "    import cartopy.feature as cfeature\n",
    "    from sklearn.metrics import mean_absolute_percentage_error\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Could not import all dependencies: {e}\")\n",
    "    print(\"Some features may not be available\")\n",
    "\n",
    "print(\"Main imports loaded correctly\")\n",
    "\n",
    "# ‚ñ∂Ô∏è Advanced environment detection and automatic resource configuration\n",
    "def detect_environment():\n",
    "    \"\"\"\n",
    "    Detects and configures optimal resources for training based on available hardware.\n",
    "    Returns a dictionary of configurations to optimize performance.\n",
    "    \"\"\"\n",
    "    env_info = {}\n",
    "    \n",
    "    # System information\n",
    "    env_info['system'] = {\n",
    "        'os': platform.system(),\n",
    "        'platform': platform.platform(),\n",
    "        'python': platform.python_version()\n",
    "    }\n",
    "    \n",
    "    # CPU information\n",
    "    cpu_count = multiprocessing.cpu_count()\n",
    "    env_info['cpu'] = {\n",
    "        'cores_logical': cpu_count,\n",
    "        'cores_physical': psutil.cpu_count(logical=False) or cpu_count,\n",
    "        'frequency': psutil.cpu_freq().max if psutil.cpu_freq() else 'Unknown',\n",
    "        'usage_percent': psutil.cpu_percent(interval=0.1)\n",
    "    }\n",
    "    \n",
    "    # RAM memory information\n",
    "    memory = psutil.virtual_memory()\n",
    "    env_info['memory'] = {\n",
    "        'total_gb': round(memory.total / (1024**3), 2),\n",
    "        'available_gb': round(memory.available / (1024**3), 2),\n",
    "        'used_percent': memory.percent\n",
    "    }\n",
    "    \n",
    "    # PyTorch and GPU configuration\n",
    "    env_info['torch'] = {\n",
    "        'version': torch.__version__,\n",
    "        'cuda_available': torch.cuda.is_available(),\n",
    "        'cudnn_enabled': torch.backends.cudnn.enabled,\n",
    "        'gpu_count': torch.cuda.device_count()\n",
    "    }\n",
    "    \n",
    "    # GPU details if available\n",
    "    if torch.cuda.is_available():\n",
    "        gpus = []\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_props = torch.cuda.get_device_properties(i)\n",
    "            gpus.append({\n",
    "                'name': gpu_props.name,\n",
    "                'memory_gb': round(gpu_props.total_memory / (1024**3), 2),\n",
    "                'compute_capability': f\"{gpu_props.major}.{gpu_props.minor}\",\n",
    "                'multi_processor_count': gpu_props.multi_processor_count\n",
    "            })\n",
    "        env_info['gpu'] = gpus\n",
    "    \n",
    "    # Automatically determine optimal configuration\n",
    "    config = auto_configure_resources(env_info)\n",
    "    env_info['optimized_config'] = config\n",
    "    \n",
    "    return env_info\n",
    "\n",
    "def auto_configure_resources(env_info):\n",
    "    \"\"\"\n",
    "    Automatically configures parameters to optimize performance\n",
    "    based on available hardware.\n",
    "    \"\"\"\n",
    "    config = {}\n",
    "    \n",
    "    # Compute device (GPU or CPU)\n",
    "    config['device'] = 'cuda' if env_info['torch']['cuda_available'] else 'cpu'\n",
    "    \n",
    "    # Worker configuration\n",
    "    if config['device'] == 'cuda':\n",
    "        # For GPU: Fewer workers to avoid bottlenecks in data transfer\n",
    "        recommended_workers = min(4, env_info['cpu']['cores_logical'] // 2)\n",
    "    else:\n",
    "        # For CPU: More workers to parallelize data loading\n",
    "        recommended_workers = max(4, env_info['cpu']['cores_logical'] - 2)\n",
    "    \n",
    "    # Adjust workers based on available memory\n",
    "    mem_factor = env_info['memory']['available_gb'] / 16.0  # Normalize to 16GB\n",
    "    recommended_workers = min(recommended_workers, int(recommended_workers * mem_factor) + 1)\n",
    "    config['num_workers'] = max(1, recommended_workers)  # At least 1 worker\n",
    "    \n",
    "    # Configure prefetch_factor based on memory\n",
    "    config['prefetch_factor'] = 2 if env_info['memory']['available_gb'] < 8 else 4\n",
    "    \n",
    "    # Automatic batch size\n",
    "    if config['device'] == 'cuda':\n",
    "        # If GPU available, based on GPU memory\n",
    "        total_gpu_mem = sum(gpu['memory_gb'] for gpu in env_info['gpu'])\n",
    "        if total_gpu_mem > 10:\n",
    "            config['batch_size'] = 128\n",
    "        elif total_gpu_mem > 6:\n",
    "            config['batch_size'] = 64\n",
    "        else:\n",
    "            config['batch_size'] = 32\n",
    "    else:\n",
    "        # If CPU only, smaller batch size\n",
    "        if env_info['memory']['available_gb'] > 12:\n",
    "            config['batch_size'] = 32\n",
    "        else:\n",
    "            config['batch_size'] = 16\n",
    "    \n",
    "    # Automatic optimizations\n",
    "    if config['device'] == 'cuda':\n",
    "        # Enable CUDA optimizations\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        # Enable TF32 on Ampere+ GPUs (compute capability >= 8.0)\n",
    "        has_ampere = any(float(gpu['compute_capability']) >= 8.0 for gpu in env_info['gpu'])\n",
    "        if has_ampere:\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            config['use_tf32'] = True\n",
    "        \n",
    "        # Configure mixed precision usage automatically\n",
    "        config['use_amp'] = True\n",
    "        \n",
    "        # Memory optimizations\n",
    "        config['pin_memory'] = True\n",
    "        config['non_blocking'] = True\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Detect environment and apply optimized configurations\n",
    "ENV_INFO = detect_environment()\n",
    "CONFIG = ENV_INFO['optimized_config']\n",
    "\n",
    "# Update global variables with optimized configuration\n",
    "DEVICE = torch.device(CONFIG['device'])\n",
    "N_GPU = torch.cuda.device_count()\n",
    "CPU_CORES = ENV_INFO['cpu']['cores_logical']\n",
    "NUM_WORKERS = CONFIG['num_workers']\n",
    "BATCH_SIZE = CONFIG['batch_size']\n",
    "\n",
    "# Activate GPU optimizations if available\n",
    "if DEVICE.type == 'cuda':\n",
    "    # Enable asynchronous operations - fixed to use device index\n",
    "    if DEVICE.index is not None:\n",
    "        torch.cuda.set_device(DEVICE.index)\n",
    "    else:\n",
    "        # If no index specified, use device 0\n",
    "        torch.cuda.set_device(0)\n",
    "    \n",
    "    # Automatic mixed precision \n",
    "    if CONFIG.get('use_amp', False):\n",
    "        # Will be activated in training functions\n",
    "        pass\n",
    "\n",
    "    # Enable optimizations for tensor types\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Show system information and optimized configuration\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä ENVIRONMENT DETECTION AND OPTIMIZATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nüìã System Summary:\")\n",
    "print(f\"üñ•Ô∏è  OS: {ENV_INFO['system']['platform']}\")\n",
    "print(f\"üß† CPU: {ENV_INFO['cpu']['cores_physical']} physical cores / {ENV_INFO['cpu']['cores_logical']} logical cores\")\n",
    "print(f\"üíæ Memory: {ENV_INFO['memory']['available_gb']:.1f}GB available of {ENV_INFO['memory']['total_gb']:.1f}GB total\")\n",
    "\n",
    "if ENV_INFO['torch']['cuda_available']:\n",
    "    print(\"\\nüî• Detected GPUs:\")\n",
    "    for i, gpu in enumerate(ENV_INFO['gpu']):\n",
    "        print(f\"   GPU {i}: {gpu['name']} ({gpu['memory_gb']:.1f}GB, {gpu['compute_capability']})\")\n",
    "        \n",
    "    # Show initial memory information\n",
    "    mem_allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "    mem_reserved = torch.cuda.memory_reserved() / (1024**3)\n",
    "    print(f\"   Initial GPU memory: {mem_allocated:.2f}GB used / {mem_reserved:.2f}GB reserved\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No GPUs detected - Using CPU\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Optimized Training Configuration:\")\n",
    "print(f\"üì± Device: {CONFIG['device'].upper()}\")\n",
    "print(f\"üë• Workers: {CONFIG['num_workers']} (of {ENV_INFO['cpu']['cores_logical']} available)\")\n",
    "print(f\"üì¶ Batch Size: {CONFIG['batch_size']}\")\n",
    "if CONFIG['device'] == 'cuda':\n",
    "    print(f\"üöÄ Mixed Precision: {'Enabled' if CONFIG.get('use_amp', False) else 'Disabled'}\")\n",
    "    print(f\"‚ö° TF32: {'Available' if CONFIG.get('use_tf32', False) else 'Not available'}\")\n",
    "    print(f\"üîÑ Pin Memory: {'Enabled' if CONFIG.get('pin_memory', False) else 'Disabled'}\")\n",
    "    print(f\"‚è© Non-blocking Transfers: {'Enabled' if CONFIG.get('non_blocking', False) else 'Disabled'}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Environment automatically configured for optimal performance\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ‚ñ∂Ô∏è Path configuration (Colab vs Local)\n",
    "from pathlib import Path\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    # climb to project root if inside subfolder\n",
    "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
    "        if (p / '.git').exists():\n",
    "            BASE_PATH = p; break\n",
    "    DEBUG_MODE = True\n",
    "    SAFE_LOCAL_MODE = True\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_WORKERS = 0\n",
    "    INPUT_WINDOW = 24  # Reducido\n",
    "    HORIZON = 6        # Reducido\n",
    "print('BASE_PATH =', BASE_PATH)\n",
    "\n",
    "# centralised dataset / model paths\n",
    "DATA_DIR      = BASE_PATH/'data'/'output'\n",
    "MODEL_DIR     = BASE_PATH/'models'/'output'/'trained_models'; MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMAGE_DIR     = MODEL_DIR/'images'; IMAGE_DIR.mkdir(exist_ok=True)\n",
    "FEATURES_NC   = BASE_PATH/'models'/'output'/'features_fusion_branches.nc'\n",
    "FULL_NC       = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_with_windows.nc'\n",
    "print('Using FULL_NC  :', FULL_NC)\n",
    "print('Using FEATURES :', FEATURES_NC)\n",
    "\n",
    "# Actualizar diccionario de experimentos seg√∫n la nueva nomenclatura\n",
    "EXPERIMENTS = {\n",
    "    'GRU-ED': {'model':'gru_ed', 'use_lags':False},\n",
    "    'GRU-ED-PAFC': {'model':'gru_ed', 'use_lags':True},\n",
    "    'AE-FUSION-GRU-ED-PAFC': {'model':'ae_fusion_gru', 'use_lags':True},\n",
    "    'AE-FUSION-GRU-ED-PAFC-T': {'model':'ae_fusion_gru_t', 'use_lags':True},\n",
    "    'AE-FUSION-GRU-ED-PAFC-T-TopoMask': {'model':'ae_fusion_gru_t_mask', 'use_lags':True},\n",
    "}\n",
    "\n",
    "# ‚ñ∂Ô∏è Add variable definitions consistent with documentation\n",
    "FULL_FEATURES = [\n",
    "    'precip_hist','lag_1','lag_2','lag_12',\n",
    "    'month_sin','month_cos','doy_sin','doy_cos',\n",
    "    'elevation','slope','roughness','curvature','aspect',\n",
    "    'alt_cluster','ceemdan_imf1','ceemdan_imf2','ceemdan_imf3',\n",
    "    'tvfemd_imf1','tvfemd_imf2','tvfemd_imf3'\n",
    "]\n",
    "\n",
    "BASE_FEATURES = [\n",
    "    'total_precipitation',  # en lugar de 'precip_hist'\n",
    "    'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag12',\n",
    "    'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "    'elevation', 'slope', 'aspect',\n",
    "    'cluster_elevation'  # en lugar de 'alt_cluster'\n",
    "]\n",
    "\n",
    "\n",
    "# ‚ñ∂Ô∏è Helper functions\n",
    "import pandas as pd, numpy as np\n",
    "def add_time_encodings(ds: xr.Dataset):\n",
    "    '''Add month/day-of-year sinusoidal encodings'''\n",
    "    dates = pd.to_datetime(ds['time'].values)\n",
    "    month = dates.month\n",
    "    doy = dates.dayofyear\n",
    "    ds['month_sin'] = ('time', np.sin(2*np.pi*month/12))\n",
    "    ds['month_cos'] = ('time', np.cos(2*np.pi*month/12))\n",
    "    ds['doy_sin']   = ('time', np.sin(2*np.pi*doy/365.25))\n",
    "    ds['doy_cos']   = ('time', np.cos(2*np.pi*doy/365.25))\n",
    "    return ds\n",
    "\n",
    "# ‚ñ∂Ô∏è Logger & helper prints\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "                    datefmt='%H:%M:%S')\n",
    "logger = logging.getLogger('precip')\n",
    "\n",
    "def print_progress(msg, level=0, is_start=False, is_end=False):\n",
    "    prefix={0:'üîµ ' if is_start else '‚úÖ ' if is_end else '‚û°Ô∏è ',\n",
    "            1:'  ‚ö™ ',2:'    ‚Ä¢ '}.get(level,'')\n",
    "    print(f'{prefix}{msg}')\n",
    "\n",
    "# Enhanced version of print_progress with more features\n",
    "def enhanced_logger(msg, level=0, is_start=False, is_end=False):\n",
    "    \"\"\"\n",
    "    Enhanced logging function with timestamp and styling.\n",
    "    \n",
    "    Args:\n",
    "        msg: Message to log\n",
    "        level: Indentation level (0=main, 1=sub, 2=detail)\n",
    "        is_start: Set to True for start messages (blue)\n",
    "        is_end: Set to True for completion messages (green)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    timestamp = time.strftime('%H:%M:%S')\n",
    "    prefix = {0:'üîµ ' if is_start else '‚úÖ ' if is_end else '‚û°Ô∏è ',\n",
    "              1:'  ‚ö™ ', 2:'    ‚Ä¢ '}.get(level, '')\n",
    "    print(f'[{timestamp}] {prefix}{msg}')\n",
    "\n",
    "# (Reuse code from earlier minimal pipeline, but path variable FULL_NC)\n",
    "DATASET_PATH = str(FULL_NC)\n",
    "INPUT_WINDOW=48; HORIZON=12; BATCH_SIZE=32\n",
    "FOLDS={'F1':2024,'F2':2023,'F3':2022,'F4':2000,'F5':1990}\n",
    "# ... (insert PyTorch dataset, model, training utils from earlier) ...\n",
    "print_progress('‚ö†Ô∏è   PyTorch quick baseline section trimmed for brevity ‚Äî insert from earlier if desired', level=1)\n",
    "\n",
    "# ‚ñ∂Ô∏è Verify precipitation lags utility\n",
    "def verify_precipitation_lags(ds, required_lags=None, min_valid_ratio=0.9):\n",
    "    all_possible = [f\"total_precipitation_lag{i}\" for i in [1,2,3,4,12,24,36]]\n",
    "    lags = required_lags or [l for l in all_possible if l in ds.data_vars]\n",
    "    if not lags: raise ValueError('No lag variables found.')\n",
    "    for lag in lags:\n",
    "        arr = ds[lag].values\n",
    "        valid = np.count_nonzero(~np.isnan(arr))\n",
    "        ratio = valid/arr.size\n",
    "        logger.info(f'{lag}: {ratio:.1%} valid')\n",
    "        if ratio<min_valid_ratio:\n",
    "            raise ValueError(f'{lag} has only {ratio:.1%} valid data (<{min_valid_ratio})')\n",
    "    logger.info('Lag verification ‚úÖ')\n",
    "\n",
    "# ‚ñ∂Ô∏è NaN‚Äërobust scaling utils\n",
    "def check_nans(arr, name='array'):\n",
    "    nan_cnt=np.isnan(arr).sum(); tot=arr.size\n",
    "    return {'name':name,'nan':nan_cnt,'total':tot,'pct':nan_cnt/tot*100,'has':nan_cnt>0}\n",
    "\n",
    "def replace_nans(arr, strategy='mean'):\n",
    "    if not np.isnan(arr).any(): return arr\n",
    "    arr=arr.copy()\n",
    "    if strategy=='mean':\n",
    "        fill=np.nanmean(arr); arr[np.isnan(arr)]=fill\n",
    "    elif strategy=='median':\n",
    "        fill=np.nanmedian(arr); arr[np.isnan(arr)]=fill\n",
    "    else:\n",
    "        arr=np.nan_to_num(arr)\n",
    "    return arr\n",
    "\n",
    "class ScalerNaN:\n",
    "    def fit(self,X):\n",
    "        self.mean_=np.nanmean(X,0); var=np.nanvar(X,0); var[var<1e-9]=1\n",
    "        self.scale_=np.sqrt(var); return self\n",
    "    def transform(self,X):\n",
    "        return (X-self.mean_)/self.scale_\n",
    "    def fit_transform(self,X): self.fit(X); return self.transform(X)\n",
    "    def inverse_transform(self,X):\n",
    "        return X*self.scale_+self.mean_\n",
    "\n",
    "# Direct import to avoid the \"Dataset is not defined\" error\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ‚ñ∂Ô∏è Dataset & DataLoader builder\n",
    "class PrecipDataset(Dataset):\n",
    "    def __init__(self, ds, idx_list, input_window, horizon,\n",
    "                 sc_p, sc_x, features, y_dim_name='latitude', x_dim_name='longitude'):\n",
    "        self.ds = ds\n",
    "        self.idx = idx_list\n",
    "        self.w = input_window\n",
    "        self.h = horizon\n",
    "        self.scp = sc_p\n",
    "        self.scx = sc_x\n",
    "        self.features = features\n",
    "        self.y_dim_name = y_dim_name if y_dim_name in ds.dims else 'y'\n",
    "        self.x_dim_name = x_dim_name if x_dim_name in ds.dims else 'x'\n",
    "        \n",
    "        # Cache for static variables to avoid repeated access\n",
    "        self._static_cache = {}\n",
    "        \n",
    "        # Pre-validate all available features\n",
    "        self.valid_features = []\n",
    "        for feat in features:\n",
    "            if feat in ds.data_vars:\n",
    "                self.valid_features.append(feat)\n",
    "            else:\n",
    "                print(f\"Warning: Feature '{feat}' not found in the dataset\")\n",
    "                \n",
    "        if len(self.valid_features) == 0:\n",
    "            raise ValueError(\"No valid features found in the dataset\")\n",
    "            \n",
    "        print(f\"Valid features: {self.valid_features}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Unpack indices with correct dimension names\n",
    "        t, y_idx, x_idx = self.idx[i]\n",
    "        \n",
    "        try:\n",
    "            # Get window data\n",
    "            window_data = self.ds.isel(time=slice(t - self.w, t), \n",
    "                                    **{self.y_dim_name: y_idx, self.x_dim_name: x_idx})\n",
    "\n",
    "            # Get target data\n",
    "            target_data = self.ds.isel(time=slice(t, t + self.h), \n",
    "                                     **{self.y_dim_name: y_idx, self.x_dim_name: x_idx})\n",
    "            \n",
    "            # Extract and prepare the 'total_precipitation' target\n",
    "            if 'total_precipitation' in target_data:\n",
    "                tgt_values = target_data['total_precipitation'].values\n",
    "                if not np.issubdtype(tgt_values.dtype, np.number):\n",
    "                    tgt_values = tgt_values.astype(np.float32)\n",
    "                # Replace NaNs with zeros\n",
    "                tgt = np.nan_to_num(tgt_values, nan=0.0).astype(np.float32)\n",
    "            else:\n",
    "                # If no target precipitation, use zeros\n",
    "                tgt = np.zeros(self.h, dtype=np.float32)\n",
    "\n",
    "            # List to store features\n",
    "            input_features_list = []\n",
    "            \n",
    "            for var in self.valid_features:\n",
    "                try:\n",
    "                    if var == 'total_precipitation':\n",
    "                        # Historical precipitation\n",
    "                        if var in window_data:\n",
    "                            values = window_data[var].values\n",
    "                            if not np.issubdtype(values.dtype, np.number):\n",
    "                                values = values.astype(np.float32)\n",
    "                            \n",
    "                            # Replace NaNs with zeros\n",
    "                            values = np.nan_to_num(values, nan=0.0)\n",
    "                            feature_ts = self.scp.transform(values.reshape(-1, 1))\n",
    "                            input_features_list.append(feature_ts)\n",
    "                        else:\n",
    "                            # If variable doesn't exist, add zeros\n",
    "                            zeros = np.zeros((self.w, 1), dtype=np.float32)\n",
    "                            input_features_list.append(zeros)\n",
    "                            \n",
    "                    elif var.startswith('total_precipitation_lag'):\n",
    "                        # Lag variables\n",
    "                        if var in window_data:\n",
    "                            values = window_data[var].values\n",
    "                            if not np.issubdtype(values.dtype, np.number):\n",
    "                                values = values.astype(np.float32)\n",
    "                            \n",
    "                            # Replace NaNs with zeros\n",
    "                            values = np.nan_to_num(values, nan=0.0)\n",
    "                            feature_ts = self.scp.transform(values.reshape(-1, 1))\n",
    "                            input_features_list.append(feature_ts)\n",
    "                        else:\n",
    "                            # If variable doesn't exist, add zeros\n",
    "                            zeros = np.zeros((self.w, 1), dtype=np.float32)\n",
    "                            input_features_list.append(zeros)\n",
    "                            \n",
    "                    elif var in window_data:\n",
    "                        # Other variables (temporal or static)\n",
    "                        cache_key = f\"{var}_{y_idx}_{x_idx}\"\n",
    "                        \n",
    "                        if 'time' in window_data[var].dims:\n",
    "                            # Temporal variable\n",
    "                            values = window_data[var].values\n",
    "                            if not np.issubdtype(values.dtype, np.number):\n",
    "                                values = values.astype(np.float32)\n",
    "                            \n",
    "                            # Replace NaNs with zeros\n",
    "                            values = np.nan_to_num(values, nan=0.0)\n",
    "                            feature_ts = self.scx.transform(values.reshape(-1, 1))\n",
    "                            input_features_list.append(feature_ts)\n",
    "                        else:\n",
    "                            # Static variable - use cache if available\n",
    "                            if cache_key in self._static_cache:\n",
    "                                feature_ts = self._static_cache[cache_key]\n",
    "                            else:\n",
    "                                static_val = window_data[var].values\n",
    "                                \n",
    "                                # Ensure it's a numeric array\n",
    "                                if not isinstance(static_val, (np.ndarray, np.number)):\n",
    "                                    static_val = np.array([0.0], dtype=np.float32)\n",
    "                                elif not np.issubdtype(static_val.dtype, np.number):\n",
    "                                    static_val = np.array([0.0], dtype=np.float32)\n",
    "                                else:\n",
    "                                    static_val = np.asarray(static_val, dtype=np.float32)\n",
    "                                \n",
    "                                # Handle NaNs\n",
    "                                static_val = np.nan_to_num(static_val, nan=0.0)\n",
    "                                \n",
    "                                # Ensure correct shape for transformation\n",
    "                                static_val = static_val.reshape(-1, 1)\n",
    "                                \n",
    "                                # Transform and repeat for all timesteps\n",
    "                                try:\n",
    "                                    transformed = self.scx.transform(static_val)\n",
    "                                    feature_ts = np.repeat(transformed, self.w).reshape(self.w, -1)\n",
    "                                except Exception:\n",
    "                                    # In case of error, use zero values\n",
    "                                    feature_ts = np.zeros((self.w, 1), dtype=np.float32)\n",
    "                                \n",
    "                                # Save in cache\n",
    "                                self._static_cache[cache_key] = feature_ts\n",
    "                                \n",
    "                            input_features_list.append(feature_ts)\n",
    "                except Exception as e:\n",
    "                    # If there's an error with a specific feature, use zeros as fallback\n",
    "                    print(f\"Error with feature {var}: {str(e)}\")\n",
    "                    feature_ts = np.zeros((self.w, 1), dtype=np.float32)\n",
    "                    input_features_list.append(feature_ts)\n",
    "            \n",
    "            # If no features, use zero vector\n",
    "            if not input_features_list:\n",
    "                X_fallback = np.zeros((self.w, len(self.features)), dtype=np.float32)\n",
    "                return torch.tensor(X_fallback, dtype=torch.float32), torch.tensor(tgt, dtype=torch.float32)\n",
    "            \n",
    "            # Concatenate features\n",
    "            X = np.hstack(input_features_list).astype(np.float32)\n",
    "            \n",
    "            return torch.tensor(X, dtype=torch.float32), torch.tensor(tgt, dtype=torch.float32)\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Global error handling - report but return fallback tensors\n",
    "            print(f\"Error processing sample {i}, indices: {t},{y_idx},{x_idx}: {str(e)}\")\n",
    "            X_fallback = np.zeros((self.w, len(self.features)), dtype=np.float32)\n",
    "            y_fallback = np.zeros(self.h, dtype=np.float32)\n",
    "            return torch.tensor(X_fallback, dtype=torch.float32), torch.tensor(y_fallback, dtype=torch.float32)\n",
    "\n",
    "def build_dataloaders(val_year, use_lags, batch_size=BATCH_SIZE):\n",
    "    # Open and verify the dataset before using it\n",
    "    try:\n",
    "        ds = xr.open_dataset(DATASET_PATH)\n",
    "        \n",
    "        # Verify basic dimensions\n",
    "        required_dims = ['time']\n",
    "        for dim in required_dims:\n",
    "            if dim not in ds.dims:\n",
    "                raise ValueError(f\"Dataset does not contain the required dimension: {dim}\")\n",
    "        \n",
    "        # Ensure there is valid data\n",
    "        if ds.dims['time'] < INPUT_WINDOW + HORIZON:\n",
    "            raise ValueError(f\"Dataset does not contain enough time steps. At least {INPUT_WINDOW + HORIZON} required\")\n",
    "            \n",
    "        # Add time encodings\n",
    "        ds = add_time_encodings(ds)\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading or processing the dataset: {str(e)}\")\n",
    "    \n",
    "    # Diagnostic information\n",
    "    print(f\"Dataset loaded: variables {list(ds.data_vars.keys())[:10]}...\")\n",
    "    print(f\"Dimensions: {ds.dims}\")\n",
    "\n",
    "    train_start = np.datetime64(f'{val_year-4}-01-01')\n",
    "    train_end   = np.datetime64(f'{val_year-1}-12-31')\n",
    "    val_start   = np.datetime64(f'{val_year}-01-01')\n",
    "    val_end     = np.datetime64(f'{val_year}-12-31')\n",
    "\n",
    "    train_mask = (ds['time']>=train_start)&(ds['time']<=train_end)\n",
    "    val_mask   = (ds['time']>=val_start)&(ds['time']<=val_end)\n",
    "\n",
    "    # Ensure total_precipitation exists\n",
    "    if 'total_precipitation' not in ds.data_vars:\n",
    "        raise ValueError(f\"'total_precipitation' is not in the dataset. Available variables: {list(ds.data_vars.keys())}\")\n",
    "    \n",
    "    # Extract precipitation values and preprocess with robust handling\n",
    "    precip_values = ds['total_precipitation'].where(train_mask).values\n",
    "    \n",
    "    # Robust data verification and cleaning\n",
    "    try:\n",
    "        # Convert to float32 and handle NaNs\n",
    "        precip_values = precip_values.astype(np.float32)\n",
    "        precip_values = np.nan_to_num(precip_values, nan=0.0)\n",
    "        \n",
    "        # Verify that there are valid values\n",
    "        if np.all(precip_values == 0) or np.all(np.isnan(precip_values)):\n",
    "            print(\"WARNING: All precipitation values are zero or NaN\")\n",
    "            # Add a small noise to avoid divisions by zero\n",
    "            precip_values = precip_values + np.random.normal(0, 0.001, precip_values.shape)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing precipitation values: {str(e)}\")\n",
    "        print(\"Using fallback values...\")\n",
    "        # Create fallback values\n",
    "        precip_values = np.random.normal(0, 1.0, (100, 100)).astype(np.float32)\n",
    "    \n",
    "    # Fit RobustScaler with error handling\n",
    "    try:\n",
    "        sc_p = RobustScaler().fit(precip_values.reshape(-1, 1))\n",
    "    except Exception:\n",
    "        print(\"Error fitting RobustScaler for precipitation. Using StandardScaler as fallback.\")\n",
    "        sc_p = StandardScaler().fit(precip_values.reshape(-1, 1))\n",
    "    \n",
    "    # Numeric variables for StandardScaler with robust handling\n",
    "    numeric_vars = []\n",
    "    preds = []\n",
    "    \n",
    "    # List of variables we know are numeric\n",
    "    potential_numeric_vars = ['month_sin', 'month_cos', 'doy_sin', 'doy_cos', \n",
    "                             'elevation', 'slope', 'aspect']\n",
    "    \n",
    "    # Verify availability and type of each variable\n",
    "    for var in potential_numeric_vars:\n",
    "        if var in ds.data_vars:\n",
    "            try:\n",
    "                # Extract values and verify type\n",
    "                var_values = ds[var].where(train_mask).values\n",
    "                \n",
    "                # Clean and verify\n",
    "                var_values = np.nan_to_num(var_values, nan=0.0).astype(np.float32)\n",
    "                \n",
    "                if np.issubdtype(var_values.dtype, np.number):\n",
    "                    numeric_vars.append(var)\n",
    "                    preds.append(var_values.flatten())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing variable {var}: {str(e)}\")\n",
    "    \n",
    "    print(f\"Numeric variables used for StandardScaler: {numeric_vars}\")\n",
    "    \n",
    "    if not preds:\n",
    "        print(\"WARNING: No numeric variables found for StandardScaler\")\n",
    "        # Create fallback data for StandardScaler\n",
    "        dummy_data = np.random.normal(0, 1.0, 1000).reshape(-1, 1)\n",
    "        sc_x = StandardScaler().fit(dummy_data)\n",
    "    else:\n",
    "        try:\n",
    "            all_data = np.concatenate(preds).reshape(-1, 1)\n",
    "            # Clean data\n",
    "            all_data = np.nan_to_num(all_data, nan=0.0)\n",
    "            sc_x = StandardScaler().fit(all_data)\n",
    "        except Exception:\n",
    "            print(\"Error fitting StandardScaler. Using simple fallback.\")\n",
    "            sc_x = StandardScaler().fit(np.random.normal(0, 1.0, 1000).reshape(-1, 1))\n",
    "    \n",
    "    def make_idx(mask):\n",
    "        idx = []\n",
    "        y_dim_name = 'latitude' if 'latitude' in ds.sizes else 'y'\n",
    "        x_dim_name = 'longitude' if 'longitude' in ds.sizes else 'x'\n",
    "        \n",
    "        if y_dim_name not in ds.sizes or x_dim_name not in ds.sizes:\n",
    "            raise ValueError(f\"Dataset must contain dimensions '{y_dim_name}' and '{x_dim_name}'. Found: {list(ds.sizes.keys())}\")\n",
    "        \n",
    "        for t in range(INPUT_WINDOW, len(ds['time']) - HORIZON):\n",
    "            if t + HORIZON - 1 < len(mask) and mask[t + HORIZON - 1]:\n",
    "                for y in range(ds.sizes[y_dim_name]):\n",
    "                    for x in range(ds.sizes[x_dim_name]):\n",
    "                        idx.append((t, y, x))\n",
    "        return idx\n",
    "    \n",
    "    train_idx = make_idx(train_mask)\n",
    "    val_idx = make_idx(val_mask)\n",
    "    \n",
    "    print(f\"Training examples: {len(train_idx)}\")\n",
    "    print(f\"Validation examples: {len(val_idx)}\")\n",
    "    \n",
    "    # Build feature list with robust verification\n",
    "    feats = []\n",
    "    \n",
    "    # Always include 'total_precipitation'\n",
    "    feats.append('total_precipitation')\n",
    "    \n",
    "    # Include lags if requested\n",
    "    if use_lags:\n",
    "        for lag in ['total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag12']:\n",
    "            if lag in ds.data_vars:\n",
    "                feats.append(lag)\n",
    "            else:\n",
    "                print(f\"Warning: Lag '{lag}' not available in the dataset\")\n",
    "    \n",
    "    # Include temporal encoding variables\n",
    "    for var in ['month_sin', 'month_cos', 'doy_sin', 'doy_cos']:\n",
    "        if var in ds.data_vars:\n",
    "            feats.append(var)\n",
    "    \n",
    "    # Include topographic variables with safe verification\n",
    "    for var in ['elevation', 'slope', 'aspect', 'cluster_elevation']:\n",
    "        if var in ds.data_vars:\n",
    "            try:\n",
    "                # Verify it's accessible\n",
    "                test_val = ds[var].isel(\n",
    "                    **{dim: 0 for dim in ds[var].dims if dim != 'time'}\n",
    "                ).values\n",
    "                \n",
    "                # Handle data types\n",
    "                test_val = np.nan_to_num(test_val, nan=0.0).astype(np.float32)\n",
    "                \n",
    "                # If we get here, the variable is usable\n",
    "                feats.append(var)\n",
    "            except Exception as e:\n",
    "                print(f\"Error verifying variable '{var}': {str(e)}\")\n",
    "    \n",
    "    print(f\"Final features for the model: {feats}\")\n",
    "    \n",
    "    # Configure logging level to reduce warnings\n",
    "    warnings.filterwarnings('ignore', message=\"Feature.*not found in dataset slice\")\n",
    "    warnings.filterwarnings('ignore', message=\"invalid value encountered in divide\")\n",
    "    warnings.filterwarnings('ignore', message=\"overflow encountered in reduce\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_ds = PrecipDataset(ds, train_idx, INPUT_WINDOW, HORIZON, sc_p, sc_x, feats)\n",
    "    val_ds = PrecipDataset(ds, val_idx, INPUT_WINDOW, HORIZON, sc_p, sc_x, feats)\n",
    "    \n",
    "    # Verify real dimension by testing a batch with error handling\n",
    "    try:\n",
    "        sample_loader = DataLoader(train_ds, batch_size=1)\n",
    "        X_sample, _ = next(iter(sample_loader))\n",
    "        real_feature_dim = X_sample.shape[2]\n",
    "        print(f\"Real input dimension: {real_feature_dim}\")\n",
    "    except Exception:\n",
    "        print(\"Error determining real dimension. Using number of features as estimate.\")\n",
    "        real_feature_dim = len(feats)\n",
    "    \n",
    "    # Adjust workers based on system and stability\n",
    "    safe_workers = min(2, NUM_WORKERS)  # Limit to maximum 2 workers to avoid issues\n",
    "    print(f\"Using {safe_workers} workers for DataLoaders\")\n",
    "    \n",
    "    # Final DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=safe_workers, \n",
    "        pin_memory=True,\n",
    "        persistent_workers=safe_workers > 0,\n",
    "        prefetch_factor=2 if safe_workers > 0 else None\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=safe_workers, \n",
    "        pin_memory=True,\n",
    "        persistent_workers=safe_workers > 0,\n",
    "        prefetch_factor=2 if safe_workers > 0 else None\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, real_feature_dim\n",
    "\n",
    "\n",
    "# ‚ñ∂Ô∏è Model definitions\n",
    "# Ensure nn is imported directly before defining the models\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUEncoderDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size=128, num_layers=2, dropout=0.2, horizon=HORIZON):\n",
    "        super().__init__()\n",
    "        self.enc = nn.GRU(input_dim, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.dec = nn.GRU(1, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "        self.fc  = nn.Linear(hidden_size,1)\n",
    "        self.hor = horizon\n",
    "\n",
    "    def forward(self, x, teacher_forcing_ratio=0.5, y=None):\n",
    "        _, h = self.enc(x)\n",
    "        dec_in = x[:, -1:, 0:1]\n",
    "        outs=[]\n",
    "        for t in range(self.hor):\n",
    "            o, h = self.dec(dec_in, h)\n",
    "            pred = self.fc(o.squeeze(1))\n",
    "            outs.append(pred)\n",
    "            if self.training and y is not None and torch.rand(1)<teacher_forcing_ratio:\n",
    "                dec_in = y[:, t:t+1].unsqueeze(-1)\n",
    "            else:\n",
    "                dec_in = pred.unsqueeze(1)\n",
    "        \n",
    "        # Apilar las salidas y eliminar la √∫ltima dimensi√≥n para pasar de [batch, horizon, 1] a [batch, horizon]\n",
    "        return torch.stack(outs, dim=1).squeeze(-1)\n",
    "\n",
    "# Implementation aligned with documentation\n",
    "class Conv3DAutoEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, bottleneck_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "            nn.Conv3d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "            nn.Conv3d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*6*6*6, bottleneck_dim)  # Adjust dimensions based on your input\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class AEFusionGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size=128, num_layers=2, dropout=0.2, horizon=HORIZON):\n",
    "        super().__init__()\n",
    "        self.ae = Conv3DAutoEncoder(in_channels=3, bottleneck_dim=64)\n",
    "        \n",
    "        # Combined dim: original features + bottleneck\n",
    "        combined_dim = input_dim + 64;\n",
    "        \n",
    "        self.backbone = GRUEncoderDecoder(combined_dim, hidden_size, num_layers, dropout, horizon)\n",
    "    \n",
    "    def forward(self, x, teacher_forcing_ratio=0.5, y=None):\n",
    "        # Assuming x_imfs is processed elsewhere and passed with x\n",
    "        # This is a placeholder for the actual implementation\n",
    "        ae_features = torch.zeros((x.size(0), 64), device=x.device)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat([x, ae_features.unsqueeze(1).expand(-1, x.size(1), -1)], dim=2)\n",
    "        \n",
    "        return self.backbone(combined, teacher_forcing_ratio, y)\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, n_heads, dropout=dropout)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)  # Corregido de LayerNormalization a LayerNorm\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n",
    "        return self.norm(x + attn_out)\n",
    "\n",
    "class AEFusionGRUT(AEFusionGRU):\n",
    "    def __init__(self, input_dim, hidden_size=128, num_layers=2, dropout=0.2, horizon=HORIZON):\n",
    "        super().__init__(input_dim, hidden_size, num_layers, dropout, horizon)\n",
    "        self.attention = MultiHeadAttentionLayer(hidden_size, n_heads=4, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, teacher_forcing_ratio=0.5, y=None):\n",
    "        # Similar implementation as AEFusionGRU but with attention\n",
    "        # This is placeholder for the actual implementation with attention\n",
    "        return super().forward(x, teacher_forcing_ratio, y)\n",
    "\n",
    "class AEFusionGRUTMask(AEFusionGRUT):\n",
    "    def __init__(self, input_dim, hidden_size=128, num_layers=2, dropout=0.2, horizon=HORIZON):\n",
    "        super().__init__(input_dim, hidden_size, num_layers, dropout, horizon)\n",
    "        \n",
    "    def forward(self, x, teacher_forcing_ratio=0.5, y=None):\n",
    "        # Similar implementation but with causal masking for attention\n",
    "        # This is placeholder for the actual implementation with causal masking\n",
    "        return super().forward(x, teacher_forcing_ratio, y)\n",
    "\n",
    "# Update MODEL_FACTORY with proper implementations\n",
    "MODEL_FACTORY = {\n",
    "    'gru_ed': GRUEncoderDecoder,\n",
    "    'ae_fusion_gru': AEFusionGRU,\n",
    "    'ae_fusion_gru_t': AEFusionGRUT,\n",
    "    'ae_fusion_gru_t_mask': AEFusionGRUTMask,\n",
    "}\n",
    "\n",
    "\n",
    "# ‚ñ∂Ô∏è Training utilities\n",
    "from torchmetrics.functional import mean_squared_error\n",
    "def huber_weighted(preds, target):\n",
    "    # Asegurar que preds y target tienen la misma forma\n",
    "    if preds.dim() == 3 and preds.size(2) == 1:\n",
    "        preds = preds.squeeze(-1)  # Convertir de [batch, seq, 1] a [batch, seq]\n",
    "        \n",
    "    # Crear pesos para cada horizonte (1 + h/12)\n",
    "    h = torch.arange(1, target.size(1)+1, device=preds.device).float()\n",
    "    weights = 1 + h/12.0\n",
    "    \n",
    "    # Calcular p√©rdida Huber\n",
    "    loss = torch.nn.functional.huber_loss(preds, target, reduction='none')\n",
    "    \n",
    "    # Aplicar pesos al horizonte y promediar\n",
    "    weighted_loss = loss * weights.view(1, -1)\n",
    "    return weighted_loss.mean()\n",
    "\n",
    "def train_one_epoch(model, loader, opt, tf_ratio, scheduler=None):\n",
    "    model.train()\n",
    "    losses=[]\n",
    "    for X,y in loader:\n",
    "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "        preds = model(X, teacher_forcing_ratio=tf_ratio, y=y)\n",
    "        # Asegurar que preds tiene la forma correcta\n",
    "        if preds.dim() == 3 and preds.size(2) == 1:\n",
    "            preds = preds.squeeze(-1)\n",
    "        loss = huber_weighted(preds, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    rmses=[]\n",
    "    for X,y in loader:\n",
    "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
    "        preds = model(X, teacher_forcing_ratio=0.0)\n",
    "        # Asegurar que preds tiene la misma forma que y\n",
    "        if preds.dim() == 3 and preds.size(2) == 1:\n",
    "            preds = preds.squeeze(-1)  # Convertir de [batch, seq, 1] a [batch, seq]\n",
    "        \n",
    "        rmse = mean_squared_error(preds, y, squared=False)\n",
    "        rmses.append(rmse.item())\n",
    "    return np.mean(rmses)\n",
    "\n",
    "# Update the training function to handle shapes correctly\n",
    "def train_with_history(model, train_loader, val_loader, epochs=60, patience=20, \n",
    "                      lr=1e-3, weight_decay=1e-4, fold='', exp_name=''):\n",
    "    print_progress(f\"Starting training of {exp_name} on fold {fold}\", is_start=True)\n",
    "    \n",
    "    # Force model to be on GPU\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=lr, total_steps=epochs*len(train_loader),\n",
    "                         pct_start=0.3, anneal_strategy='cos')\n",
    "    \n",
    "    # Create GradScaler with updated syntax\n",
    "    if torch.cuda.is_available():\n",
    "        scaler = torch.amp.GradScaler('cuda')\n",
    "    else:\n",
    "        scaler = None\n",
    "    use_amp = scaler is not None\n",
    "    \n",
    "    # Initialize history for learning curves\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_rmse': [],\n",
    "        'learning_rate': [],\n",
    "        'teacher_forcing': []\n",
    "    }\n",
    "    \n",
    "    best_rmse = float('inf')\n",
    "    best_model_state = None\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Run between celdas to monitor\n",
    "        aggressive_memory_cleanup()\n",
    "        gpu_monitor()\n",
    "        # Calculate teacher forcing ratio with cosine decay (0.7‚Üí0.3)\n",
    "        tf_ratio = 0.7 - (epoch-1)*(0.4)/(epochs-1)\n",
    "        history['teacher_forcing'].append(tf_ratio)\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "            \n",
    "            # Use mixed precision for forward pass if available\n",
    "            if use_amp:\n",
    "                with torch.autocast(device_type='cuda'):\n",
    "                    preds = model(X, teacher_forcing_ratio=tf_ratio, y=y)\n",
    "                    if preds.dim() == 3 and preds.size(2) == 1:\n",
    "                        preds = preds.squeeze(-1)\n",
    "                    loss = huber_weighted(preds, y)\n",
    "                \n",
    "                # Scale gradients and optimize with mixed precision\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Normal flow if no AMP\n",
    "                preds = model(X, teacher_forcing_ratio=tf_ratio, y=y)\n",
    "                if preds.dim() == 3 and preds.size(2) == 1:\n",
    "                    preds = preds.squeeze(-1)\n",
    "                loss = huber_weighted(preds, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_rmses = []\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.autocast(device_type='cuda'):\n",
    "                        preds = model(X, teacher_forcing_ratio=0)\n",
    "                        if preds.dim() == 3 and preds.size(2) == 1:\n",
    "                            preds = preds.squeeze(-1)\n",
    "                        val_loss = huber_weighted(preds, y).item()\n",
    "                        val_rmse = mean_squared_error(preds, y, squared=False).item()\n",
    "                else:\n",
    "                    preds = model(X, teacher_forcing_ratio=0)\n",
    "                    if preds.dim() == 3 and preds.size(2) == 1:\n",
    "                        preds = preds.squeeze(-1)\n",
    "                    val_loss = huber_weighted(preds, y).item()\n",
    "                    val_rmse = mean_squared_error(preds, y, squared=False).item()\n",
    "                \n",
    "                val_losses.append(val_loss)\n",
    "                val_rmses.append(val_rmse)\n",
    "        \n",
    "        # Update history\n",
    "        epoch_train_loss = np.mean(train_losses)\n",
    "        epoch_val_loss = np.mean(val_losses)\n",
    "        epoch_val_rmse = np.mean(val_rmses)\n",
    "        \n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_rmse'].append(epoch_val_rmse)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch}/{epochs} - Train loss: {epoch_train_loss:.4f} - Val RMSE: {epoch_val_rmse:.4f} - LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Check early stopping (‚àÜRMSE < 1%)\n",
    "        if epoch_val_rmse < best_rmse * 0.99:  # Improvement of at least 1%\n",
    "            best_rmse = epoch_val_rmse\n",
    "            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            print_progress(f\"Epoch {epoch}: New best model with RMSE {best_rmse:.4f}\", level=1)\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if counter >= patience:\n",
    "            print_progress(f\"Early stopping at epoch {epoch}\", level=1)\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Visualize learning curves\n",
    "    plot_learning_curves(history, exp_name, fold)\n",
    "    \n",
    "    print_progress(f\"Training of {exp_name} on fold {fold} completed. Best RMSE: {best_rmse:.4f}\", is_end=True)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), MODEL_DIR / f\"{exp_name}_{fold}_model.pt\")\n",
    "    \n",
    "    return model, history, best_rmse\n",
    "\n",
    "def plot_learning_curves(history, exp_name, fold):\n",
    "    \"\"\"\n",
    "    Generates learning curve visualizations during training\n",
    "    \n",
    "    Args:\n",
    "        history: Dictionary with training history\n",
    "        exp_name: Experiment name\n",
    "        fold: Fold ID\n",
    "    \"\"\"\n",
    "    curves_dir = IMAGE_DIR / \"learning_curves\"\n",
    "    curves_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = GridSpec(2, 2, figure=fig)\n",
    "    \n",
    "    # 1. Training and validation loss\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(history['train_loss'], label='Training', color='#3498db', linewidth=2)\n",
    "    if 'val_loss' in history and len(history['val_loss']) > 0:\n",
    "        ax1.plot(history['val_loss'], label='Validation', color='#e74c3c', linewidth=2)\n",
    "    ax1.set_title('Loss during training', fontsize=14)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.grid(alpha=0.3)\n",
    "    ax1.legend(fontsize=12)\n",
    "    \n",
    "    # 2. Validation RMSE\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    if 'val_rmse' in history and len(history['val_rmse']) > 0:\n",
    "        ax2.plot(history['val_rmse'], color='#9b59b6', linewidth=2)\n",
    "        min_rmse = min(history['val_rmse'])\n",
    "        min_epoch = history['val_rmse'].index(min_rmse)\n",
    "        ax2.scatter(min_epoch, min_rmse, c='red', s=100, zorder=10, label=f'Best: {min_rmse:.4f}')\n",
    "    ax2.set_title('Validation RMSE', fontsize=14)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('RMSE', fontsize=12)\n",
    "    ax2.grid(alpha=0.3)\n",
    "    ax2.legend(fontsize=12)\n",
    "    \n",
    "    # 3. Learning rate and Teacher Forcing\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    if 'learning_rate' in history and len(history['learning_rate']) > 0:\n",
    "        ax3.plot(history['learning_rate'], color='#2ecc71', linewidth=2)\n",
    "        ax3.set_title('Learning Rate (OneCycleLR)', fontsize=14)\n",
    "        ax3.set_xlabel('Epoch', fontsize=12)\n",
    "        ax3.set_ylabel('Learning Rate', fontsize=12)\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.grid(alpha=0.3)\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    if 'teacher_forcing' in history and len(history['teacher_forcing']) > 0:\n",
    "        ax4.plot(history['teacher_forcing'], color='#f39c12', linewidth=2)\n",
    "        ax4.set_title('Teacher Forcing Ratio (0.7 ‚Üí 0.3)', fontsize=14)\n",
    "        ax4.set_xlabel('Epoch', fontsize=12)\n",
    "        ax4.set_ylabel('Teacher Forcing', fontsize=12)\n",
    "        ax4.set_ylim(0, 1)\n",
    "        ax4.grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{exp_name} - Fold {fold}', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.savefig(curves_dir / f'{exp_name}_{fold}_learning_curves.png', dpi=100, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print_progress(f\"Learning curves saved at: {curves_dir / f'{exp_name}_{fold}_learning_curves.png'}\", level=1)\n",
    "\n",
    "# ‚ñ∂Ô∏è Main experiment loop with learning curves and visualization\n",
    "RESULTS = []\n",
    "ALL_HISTORIES = {}\n",
    "ALL_MODELS = {}\n",
    "\n",
    "# Create folder for aggregated metrics\n",
    "metrics_dir = MODEL_DIR / \"metrics\"\n",
    "metrics_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for exp_name, cfg in EXPERIMENTS.items():\n",
    "    print_progress(f\"Running experiment: {exp_name}\", is_start=True)\n",
    "    exp_histories = {}\n",
    "    exp_models = {}\n",
    "    exp_metrics = []\n",
    "    \n",
    "    for fold, val_year in FOLDS.items():\n",
    "        print_progress(f\"Processing fold {fold} (validation: {val_year})\", level=1)\n",
    "        \n",
    "        # Build dataloaders\n",
    "        train_loader, val_loader, in_dim = build_dataloaders(val_year, cfg['use_lags'])\n",
    "        \n",
    "        # Adjust dropout according to documentation (0.25 for F4-F5, 0.20 for others)\n",
    "        dropout = 0.25 if fold in ['F4', 'F5'] else 0.20\n",
    "        print_progress(f\"Using dropout={dropout} for fold {fold}\", level=2)\n",
    "        print_progress(f\"Input dimension: {in_dim}\", level=2)\n",
    "        \n",
    "        # Create and train model with history tracking\n",
    "        model = MODEL_FACTORY[cfg['model']](in_dim, dropout=dropout).to(DEVICE)\n",
    "        model, history, best_rmse = train_with_history(\n",
    "            model, train_loader, val_loader, \n",
    "            epochs=60, patience=20, \n",
    "            lr=1e-3, weight_decay=1e-4,\n",
    "            fold=fold, exp_name=exp_name\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        RESULTS.append({\n",
    "            'exp': exp_name,\n",
    "            'fold': fold,\n",
    "            'rmse': best_rmse\n",
    "        })\n",
    "        \n",
    "        # Store model and history\n",
    "        exp_histories[fold] = history\n",
    "        exp_models[fold] = model\n",
    "        \n",
    "        # Generate prediction visualization if prepare_grid_data is implemented\n",
    "        try:\n",
    "            # Uncomment the following lines when prepare_grid_data is implemented\n",
    "            # visualize_predictions(model, xr.open_dataset(FULL_NC), val_year, exp_name, fold)\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print_progress(f\"Error in visualization: {str(e)}\", level=1)\n",
    "    \n",
    "    # Store histories and models\n",
    "    ALL_HISTORIES[exp_name] = exp_histories\n",
    "    ALL_MODELS[exp_name] = exp_models\n",
    "    \n",
    "    print_progress(f\"Experiment {exp_name} completed\", is_end=True)\n",
    "\n",
    "# ‚ñ∂Ô∏è Display results table\n",
    "df = pd.DataFrame(RESULTS)\n",
    "pivot_table = df.pivot(index='exp', columns='fold', values='rmse')\n",
    "print_progress(\"RMSE results summary:\", is_start=True)\n",
    "display(pivot_table)\n",
    "\n",
    "plt.title('RMSE comparison by experiment and fold', fontsize=14)\n",
    "plt.xlabel('Experiment')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(IMAGE_DIR / \"experiment_comparison.png\", dpi=100)\n",
    "plt.show()\n",
    "\n",
    "def visualize_predictions(model, dataset, val_year, exp_name, fold, scalers=None):\n",
    "    \"\"\"\n",
    "    Generates maps of predictions and MAPE errors for the 12 months of validation\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        dataset: Complete xarray dataset\n",
    "        val_year: Validation year\n",
    "        exp_name: Experiment name\n",
    "        fold: Fold ID\n",
    "        scalers: Tuple (sc_p, sc_x) of scalers to transform data\n",
    "    \"\"\"\n",
    "    print_progress(f\"Generating visualizations for {exp_name}, fold {fold}\", is_start=True)\n",
    "    \n",
    "    # Prepare directory to save visualizations\n",
    "    vis_dir = IMAGE_DIR / f\"{exp_name}_{fold}_maps\"\n",
    "    vis_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Get months from validation period\n",
    "    months = pd.date_range(f\"{val_year}-01-01\", f\"{val_year}-12-31\", freq='MS')\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    \n",
    "    # Extract coordinates\n",
    "    lats = dataset.latitude.values\n",
    "    lons = dataset.longitude.values\n",
    "    \n",
    "    # Create matrices to store results\n",
    "    predictions = np.zeros((len(months), len(lats), len(lons)))\n",
    "    true_values = np.zeros((len(months), len(lats), len(lons)))\n",
    "    mape_values = np.zeros((len(months), len(lats), len(lons)))\n",
    "    \n",
    "    # Get time indices for validation\n",
    "    val_times = dataset['time'].sel(time=slice(f\"{val_year}-01-01\", f\"{val_year}-12-31\")).values\n",
    "    \n",
    "    # Configure plots size\n",
    "    plt.rcParams['figure.figsize'] = (20, 10)\n",
    "    \n",
    "    # Generate predictions for each grid point\n",
    "    print_progress(f\"Generating predictions\", level=1)\n",
    "    \n",
    "    # This section depends on how your data is organized\n",
    "    # Simplified example using a helper function\n",
    "    input_tensor, target_tensor = prepare_grid_data(dataset, val_year, INPUT_WINDOW, HORIZON)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        preds = model(input_tensor.to(DEVICE)).cpu().numpy()\n",
    "    \n",
    "    # De-scale predictions if we have the scalers\n",
    "    if scalers:\n",
    "        sc_p, _ = scalers\n",
    "        preds = sc_p.inverse_transform(preds.reshape(-1, HORIZON)).reshape(-1, len(lats), len(lons), HORIZON)\n",
    "        # And rearrange axes to format (month, lat, lon)\n",
    "        preds = np.moveaxis(preds, 3, 0)\n",
    "    \n",
    "    # We also need to extract real values and rearrange\n",
    "    true_vals = target_tensor.numpy().reshape(-1, len(lats), len(lons), HORIZON)\n",
    "    true_vals = np.moveaxis(true_vals, 3, 0)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    for m in range(HORIZON):\n",
    "        valid_mask = true_vals[m] > 0.1  # Avoid divisions by ~0\n",
    "        mape_values[m, valid_mask] = np.abs((preds[m, valid_mask] - true_vals[m, valid_mask]) / true_vals[m, valid_mask]) * 100\n",
    "    \n",
    "    # Visualize maps for each month\n",
    "    print_progress(f\"Generating monthly maps\", level=1)\n",
    "    \n",
    "    for m in range(HORIZON):\n",
    "        fig = plt.figure(figsize=(18, 10))\n",
    "        plt.suptitle(f\"{exp_name} - {fold} - {month_names[m]} {val_year}\", fontsize=16)\n",
    "        \n",
    "        # Prepare limits for colorbar\n",
    "        vmin_pred = np.nanpercentile(true_vals, 1)\n",
    "        vmax_pred = np.nanpercentile(true_vals, 99)\n",
    "        vmin_mape = 0\n",
    "        vmax_mape = min(100, np.nanpercentile(mape_values, 95))\n",
    "        \n",
    "        # Create grid for lat/lon\n",
    "        lon2d, lat2d = np.meshgrid(lons, lats)\n",
    "        \n",
    "        # Prediction plot\n",
    "        ax1 = plt.subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "        ax1.set_title(f\"Predicted Precipitation (mm)\")\n",
    "        pcm = ax1.pcolormesh(lon2d, lat2d, preds[m], cmap='Blues', \n",
    "                           vmin=vmin_pred, vmax=vmax_pred, \n",
    "                           transform=ccrs.PlateCarree())\n",
    "        ax1.coastlines(resolution='10m')\n",
    "        ax1.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        gl = ax1.gridlines(draw_labels=True, linewidth=0.5)\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        plt.colorbar(pcm, ax=ax1, shrink=0.7, label='mm')\n",
    "        \n",
    "        # MAPE plot\n",
    "        ax2 = plt.subplot(1, 2, 2, projection=ccrs.PlateCarree())\n",
    "        ax2.set_title(f\"MAPE Error (%)\")\n",
    "        pcm2 = ax2.pcolormesh(lon2d, lat2d, mape_values[m], cmap='Reds', \n",
    "                             vmin=vmin_mape, vmax=vmax_mape, \n",
    "                             transform=ccrs.PlateCarree())\n",
    "        ax2.coastlines(resolution='10m')\n",
    "        ax2.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        gl = ax2.gridlines(draw_labels=True, linewidth=0.5)\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "        plt.colorbar(pcm2, ax=ax2, shrink=0.7, label='%')\n",
    "        \n",
    "        # Save figure\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        fig.savefig(vis_dir / f\"map_{month_names[m]}.png\", dpi=120, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Generate summary visualization (average)\n",
    "    print_progress(f\"Generating summary map\", level=1)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_pred = np.nanmean(preds, axis=0)\n",
    "    avg_true = np.nanmean(true_vals, axis=0)\n",
    "    avg_mape = np.nanmean(mape_values, axis=0)\n",
    "    \n",
    "    # Summary plot\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    plt.suptitle(f\"{exp_name} - {fold} - Annual Average {val_year}\", fontsize=16)\n",
    "    \n",
    "    # Average prediction plot\n",
    "    ax1 = plt.subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "    ax1.set_title(f\"Annual Mean Precipitation (mm)\")\n",
    "    pcm = ax1.pcolormesh(lon2d, lat2d, avg_pred, cmap='Blues', transform=ccrs.PlateCarree())\n",
    "    ax1.coastlines(resolution='10m')\n",
    "    ax1.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    gl = ax1.gridlines(draw_labels=True, linewidth=0.5)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    plt.colorbar(pcm, ax=ax1, shrink=0.7, label='mm')\n",
    "    \n",
    "    # Average MAPE plot\n",
    "    ax2 = plt.subplot(1, 2, 2, projection=ccrs.PlateCarree())\n",
    "    ax2.set_title(f\"Average MAPE (%)\")\n",
    "    pcm2 = ax2.pcolormesh(lon2d, lat2d, avg_mape, cmap='Reds', \n",
    "                         vmin=0, vmax=min(100, np.nanpercentile(avg_mape, 95)), \n",
    "                         transform=ccrs.PlateCarree())\n",
    "    ax2.coastlines(resolution='10m')\n",
    "    ax2.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    gl = ax2.gridlines(draw_labels=True, linewidth=0.5)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    plt.colorbar(pcm2, ax=ax2, shrink=0.7, label='%')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    fig.savefig(vis_dir / f\"map_annual_summary.png\", dpi=120, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # RMSE by horizon plot (1-12)\n",
    "    rmse_by_horizon = [np.sqrt(np.nanmean((preds[h] - true_vals[h])**2)) for h in range(HORIZON)]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, HORIZON+1), rmse_by_horizon, marker='o', linewidth=2)\n",
    "    plt.title(f\"{exp_name} - {fold} - RMSE by Horizon\", fontsize=14)\n",
    "    plt.xlabel('Prediction Horizon (months)', fontsize=12)\n",
    "    plt.ylabel('RMSE', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xticks(range(1, HORIZON+1))\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(vis_dir / f\"rmse_by_horizon.png\", dpi=120)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print_progress(f\"Visualizations saved in {vis_dir}\", is_end=True)\n",
    "    return preds, true_vals, mape_values\n",
    "\n",
    "# Helper function to prepare data in grid format\n",
    "def prepare_grid_data(dataset, val_year, input_window, horizon):\n",
    "    \"\"\"\n",
    "    Prepares input and target data for grid predictions\n",
    "    \n",
    "    This function is a placeholder - you'll need to implement it according to\n",
    "    your specific data structure\n",
    "    \"\"\"\n",
    "    print_progress(\"This function needs specific implementation for the dataset!\", level=2)\n",
    "    # Placeholder - returns empty tensors\n",
    "    return torch.zeros((1, input_window, 10)), torch.zeros((1, horizon))\n",
    "\n",
    "# ‚ñ∂Ô∏è Performance and GPU optimizations for training\n",
    "import torch.cuda.amp as amp  # For mixed precision\n",
    "\n",
    "# PyTorch memory and performance optimizations\n",
    "torch.backends.cudnn.benchmark = True  # Optimize repetitive operations\n",
    "torch.backends.cudnn.enabled = True    # Ensure cuDNN is enabled\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Allow TF32 on Ampere GPUs\n",
    "torch.backends.cudnn.allow_tf32 = True        # Allow TF32 in convolution operations\n",
    "\n",
    "# Optimized parameters for better GPU utilization\n",
    "BATCH_SIZE_FAST = 128          # Larger batches for better GPU utilization\n",
    "EPOCHS_FAST =  10               # Quick training for exploration\n",
    "TRANSFER_MODE = 'non_blocking'  # Asynchronous CPU-GPU transfer\n",
    "\n",
    "# Function to monitor and free GPU memory\n",
    "\n",
    "def gpu_monitor(reset=False):\n",
    "    \"\"\"Monitors GPU memory usage and optionally frees it\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        mem_alloc = torch.cuda.memory_allocated()/1e9  # GB\n",
    "        mem_reserved = torch.cuda.memory_reserved()/1e9  # GB\n",
    "        print(f\"üß† GPU: {mem_alloc:.2f} GB alloc | {mem_reserved:.2f} GB reserved\")\n",
    "        if reset:\n",
    "            print(\"üßπ Freeing GPU memory...\")\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"   After: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    else:\n",
    "        print(\"‚ùå GPU not available\")\n",
    "\n",
    "# ‚ñ∂Ô∏è Memory management functions\n",
    "def aggressive_memory_cleanup():\n",
    "    \"\"\"Aggressively frees memory resources, especially GPU memory\"\"\"\n",
    "    # Empty CUDA cache if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Force garbage collection\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    # Close any matplotlib figures that may be open\n",
    "    try:\n",
    "        plt.close('all')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Clean PyTorch memory\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) and obj.is_cuda and not obj.is_grad:\n",
    "                del obj\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# ‚ñ∂Ô∏è Additional optimizations to speed up training\n",
    "\n",
    "def optimize_training_pipeline():\n",
    "    \"\"\"Implements additional optimizations to speed up training\"\"\"\n",
    "    enhanced_logger(\"Applying additional optimizations to accelerate training...\", is_start=True)\n",
    "    \n",
    "    # 1. Data loading optimizations\n",
    "    # Pre-load and cache in memory for small datasets\n",
    "    if ENV_INFO['memory']['available_gb'] > 20:  # If enough RAM\n",
    "        enhanced_logger(\"Enabling data caching in memory\", level=1)\n",
    "        torch.utils.data.dataloader.default_collate = lambda x: x  # Avoid unnecessary reconstruction\n",
    "    \n",
    "    # 2. JIT compilation optimizations for critical functions\n",
    "    if hasattr(torch, 'compile'):  # PyTorch 2.0+\n",
    "        enhanced_logger(\"Enabling JIT compilation for critical functions (PyTorch 2.0+)\", level=1)\n",
    "        # Functions will be compiled when models are defined\n",
    "    else:\n",
    "        enhanced_logger(\"PyTorch compile not available (requires PyTorch 2.0+)\", level=1)\n",
    "    \n",
    "    # 3. Optimize GPU-specific parameters\n",
    "    if DEVICE.type == 'cuda':\n",
    "        # Reserve cache memory to avoid fragmentation\n",
    "        if ENV_INFO['torch']['cuda_available'] and torch.cuda.is_available():\n",
    "            enhanced_logger(\"Optimizing GPU memory...\", level=1)\n",
    "            # Set memory limits based on detected GPU\n",
    "            if any('T4' in gpu['name'] for gpu in ENV_INFO['gpu']):\n",
    "                # Configuration for Google Colab T4 (16GB)\n",
    "                torch.cuda.set_per_process_memory_fraction(0.85)  # Use 85% of memory\n",
    "                enhanced_logger(\"Optimized configuration for T4 GPU\", level=2)\n",
    "                torch.cuda.set_per_process_memory_fraction(0.85)  # Use 85% of memory\n",
    "                enhanced_logger(\"Optimized configuration for T4 GPU\", level=2)\n",
    "            elif any('K80' in gpu['name'] for gpu in ENV_INFO['gpu']):\n",
    "                # Configuration for Google Colab K80 (12GB)\n",
    "                torch.cuda.set_per_process_memory_fraction(0.8)  # Use 80% of memory\n",
    "                enhanced_logger(\"Optimized configuration for K80 GPU\", level=2)\n",
    "            elif sum(gpu['memory_gb'] for gpu in ENV_INFO['gpu']) > 24:\n",
    "                # High-memory GPU (>24GB)\n",
    "                batch_mult = 2.0\n",
    "                enhanced_logger(f\"High memory GPU detected - batch multiplier x{batch_mult}\", level=2)\n",
    "            else:\n",
    "                # Generic configuration\n",
    "                torch.cuda.set_per_process_memory_fraction(0.75)  # Use 75% for buffer\n",
    "    \n",
    "    # 4. Adjust data transfer strategy based on hardware\n",
    "    if ENV_INFO['torch']['cuda_available']:\n",
    "        if ENV_INFO['cpu']['cores_physical'] > 12:\n",
    "            # For powerful CPUs, use more workers for transfer\n",
    "            global NUM_WORKERS\n",
    "            NUM_WORKERS = min(8, ENV_INFO['cpu']['cores_logical'] // 2)\n",
    "            enhanced_logger(f\"Powerful CPU detected - increasing workers to {NUM_WORKERS}\", level=1)\n",
    "    \n",
    "    enhanced_logger(\"Optimizations successfully applied\", is_end=True)\n",
    "    return True\n",
    "\n",
    "# Call this function before starting training\n",
    "optimize_training_pipeline()\n",
    "\n",
    "# Add JIT compilation for GRU (only if PyTorch >= 2.0)\n",
    "if hasattr(torch, 'compile'):\n",
    "    class CompiledGRUEncoderDecoder(GRUEncoderDecoder):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            # Compile critical parts to optimize performance\n",
    "            self.forward_compiled = torch.compile(super().forward)\n",
    "        \n",
    "        def forward(self, x, teacher_forcing_ratio=0.5, y=None):\n",
    "            return self.forward_compiled(x, teacher_forcing_ratio, y)\n",
    "    \n",
    "    # Replace in MODEL_FACTORY\n",
    "    MODEL_FACTORY['gru_ed'] = CompiledGRUEncoderDecoder\n",
    "    enhanced_logger(\"GRU models compiled with torch.compile for faster speed\", level=1)\n",
    "\n",
    "# Modify build_dataloaders to implement data caching\n",
    "def fast_build_dataloaders(val_year, use_lags, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Optimized version of the dataloader builder with caching and efficient sampling\"\"\"\n",
    "    # Use global cache to avoid reloading datasets\n",
    "    global _dataset_cache\n",
    "    if not '_dataset_cache' in globals():\n",
    "        _dataset_cache = {}\n",
    "    \n",
    "    cache_key = f\"{val_year}_{use_lags}\"\n",
    "    if cache_key in _dataset_cache:\n",
    "        enhanced_logger(f\"Using cached dataset for {val_year}\", level=1)\n",
    "        train_loader, val_loader, real_feature_dim = _dataset_cache[cache_key]\n",
    "        \n",
    "        # Update only batch size if different\n",
    "        if train_loader.batch_size != batch_size:\n",
    "            train_loader = DataLoader(\n",
    "                train_loader.dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=NUM_WORKERS,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=NUM_WORKERS > 0,\n",
    "                prefetch_factor=2 if NUM_WORKERS > 0 else None\n",
    "            )\n",
    "            \n",
    "            val_loader = DataLoader(\n",
    "                val_loader.dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=NUM_WORKERS,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=NUM_WORKERS > 0,\n",
    "                prefetch_factor=2 if NUM_WORKERS > 0 else None\n",
    "            )\n",
    "        \n",
    "        return train_loader, val_loader, real_feature_dim\n",
    "    \n",
    "    # If not in cache, build normally\n",
    "    train_loader, val_loader, real_feature_dim = build_dataloaders(val_year, use_lags, batch_size)\n",
    "    \n",
    "    # Save in cache\n",
    "    _dataset_cache[cache_key] = (train_loader, val_loader, real_feature_dim)\n",
    "    \n",
    "    return train_loader, val_loader, real_feature_dim\n",
    "\n",
    "# Modify run_experiments to use these optimizations\n",
    "def run_experiments_fast(fast_mode=False, transfer_learning=False):\n",
    "    \"\"\"Optimized version for fast execution of experiments\"\"\"\n",
    "    # Apply optimizations\n",
    "    optimize_training_pipeline()\n",
    "    \n",
    "    # Use partial function to avoid modifying all the code\n",
    "    import functools\n",
    "    original_build = build_dataloaders\n",
    "    build_dataloaders = fast_build_dataloaders\n",
    "    \n",
    "    try:\n",
    "        # Run with optimized pipeline\n",
    "        return run_experiments(fast_mode, transfer_learning)\n",
    "    finally:\n",
    "        # Restore original function\n",
    "        build_dataloaders = original_build\n",
    "\n",
    "# Define the missing run_experiments function that was referenced but not implemented\n",
    "def run_experiments(fast_mode=False, transfer_learning=False):\n",
    "    \"\"\"\n",
    "    Runs the main pipeline experiments\n",
    "    \n",
    "    Args:\n",
    "        fast_mode: If True, uses fewer epochs and larger batch size for quick tests\n",
    "        transfer_learning: If True, initializes each model with the best from the previous fold\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    all_histories = {}\n",
    "    all_models = {}\n",
    "    \n",
    "    # Create folder for aggregated metrics if it doesn't exist\n",
    "    metrics_dir = MODEL_DIR / \"metrics\"\n",
    "    metrics_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Configure epochs based on mode\n",
    "    epochs = 20 if fast_mode else 60\n",
    "    batch_size = BATCH_SIZE_FAST if fast_mode else BATCH_SIZE\n",
    "    \n",
    "    for exp_name, cfg in EXPERIMENTS.items():\n",
    "        print_progress(f\"Running experiment: {exp_name}\", is_start=True)\n",
    "        exp_histories = {}\n",
    "        exp_models = {}\n",
    "        \n",
    "        prev_model = None  # For transfer learning\n",
    "        \n",
    "        for fold, val_year in FOLDS.items():\n",
    "            print_progress(f\"Processing fold {fold} (validation: {val_year})\", level=1)\n",
    "            \n",
    "            # Build dataloaders\n",
    "            train_loader, val_loader, in_dim = build_dataloaders(val_year, cfg['use_lags'], batch_size)\n",
    "            \n",
    "            # Adjust dropout according to documentation\n",
    "            dropout = 0.25 if fold in ['F4', 'F5'] else 0.20\n",
    "            print_progress(f\"Using dropout={dropout} for fold {fold}\", level=2)\n",
    "            print_progress(f\"Input dimension: {in_dim}\", level=2)\n",
    "            \n",
    "            # Create model\n",
    "            model = MODEL_FACTORY[cfg['model']](in_dim, dropout=dropout).to(DEVICE)\n",
    "            \n",
    "            # Apply transfer learning if enabled and there's a previous model\n",
    "            if transfer_learning and prev_model is not None:\n",
    "                print_progress(f\"Applying transfer learning from previous fold\", level=2)\n",
    "                # Copy weights from previous model that match in size\n",
    "                with torch.no_grad():\n",
    "                    for name, param in model.named_parameters():\n",
    "                        if name in prev_model.state_dict() and param.size() == prev_model.state_dict()[name].size():\n",
    "                            param.copy_(prev_model.state_dict()[name])\n",
    "            \n",
    "            # Train model\n",
    "            model, history, best_rmse = train_with_history(\n",
    "                model, train_loader, val_loader,\n",
    "                epochs=epochs, patience=20 if not fast_mode else 10,\n",
    "                lr=1e-3, weight_decay=1e-4,\n",
    "                fold=fold, exp_name=exp_name\n",
    "            )\n",
    "            \n",
    "            # Save results\n",
    "            results.append({\n",
    "                'exp': exp_name,\n",
    "                'fold': fold,\n",
    "                'rmse': best_rmse\n",
    "            })\n",
    "            \n",
    "            # Store model and history\n",
    "            exp_histories[fold] = history\n",
    "            exp_models[fold] = model\n",
    "            \n",
    "            # Save as previous model for transfer learning\n",
    "            prev_model = model\n",
    "            \n",
    "            # Generate visualization if prepare_grid_data is implemented\n",
    "            try:\n",
    "                # Visualize predictions if function is implemented\n",
    "                # visualize_predictions(model, xr.open_dataset(FULL_NC), val_year, exp_name, fold)\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                print_progress(f\"Error in visualization: {str(e)}\", level=1)\n",
    "        \n",
    "        # Store histories and models\n",
    "        all_histories[exp_name] = exp_histories\n",
    "        all_models[exp_name] = exp_models\n",
    "        \n",
    "        print_progress(f\"Experiment {exp_name} completed\", is_end=True)\n",
    "    \n",
    "    return results, all_histories, all_models"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
