{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ecede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del entorno (compatible con Colab y local)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Detectar si estamos en Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')   \n",
    "    # Si estamos en Colab, clonar el repositorio\n",
    "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
    "    %cd ml_precipitation_prediction\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn itertools tqdm\n",
    "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
    "else:\n",
    "    # Si estamos en local, usar la ruta actual\n",
    "    if '/notebooks' in os.getcwd():\n",
    "        BASE_PATH = Path('..')\n",
    "    else:\n",
    "        BASE_PATH = Path('.')\n",
    "\n",
    "print(f\"Entorno configurado. Usando ruta base: {BASE_PATH}\")\n",
    "\n",
    "# Si BASE_PATH viene como string, lo convertimos\n",
    "BASE_PATH = Path(BASE_PATH)\n",
    "\n",
    "# Ahora puedes concatenar correctamente\n",
    "model_output_dir = BASE_PATH / 'models' / 'output'\n",
    "model_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features.nc'\n",
    "print(f\"Buscando archivo en: {data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f434a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU‐aware + RAM‐optimized pipeline con saneamiento de datos\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    ConvLSTM2D,\n",
    "    MaxPooling2D,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Dense\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1) Configurar GPU y mixed precision (opcionales)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for g in gpus:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    device = '/GPU:0'\n",
    "    print(\"✅ GPU habilitada, memory_growth ON\")\n",
    "else:\n",
    "    device = '/CPU:0'\n",
    "    print(\"ℹ️ No se detectó GPU, usando CPU\")\n",
    "\n",
    "# Forzamos float32 para evitar problemas de dtype\n",
    "mixed_precision.set_global_policy('float32')\n",
    "\n",
    "# 2) Rutas y carga de datos\n",
    "DATA_PATH  = data_file\n",
    "MODEL_PATH = model_output_dir\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "# Carga de la serie\n",
    "ds     = xr.open_dataset(DATA_PATH)\n",
    "precip = ds['total_precipitation'].sum(dim=['latitude','longitude']).to_series()\n",
    "\n",
    "# 3) Visualización rápida\n",
    "plt.figure(figsize=(12,4))\n",
    "precip.plot(title='Monthly Global Precipitation')\n",
    "plt.ylabel('mm'); plt.grid(); plt.show()\n",
    "\n",
    "# 4) STL + ACF/PACF\n",
    "res = STL(precip, seasonal=13).fit()\n",
    "fig = res.plot(); fig.set_size_inches(12,6)\n",
    "plt.suptitle('STL Decomposition'); plt.tight_layout(); plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(14,4))\n",
    "plot_acf(precip, lags=48, ax=axes[0]); axes[0].set_title('ACF')\n",
    "plot_pacf(precip, lags=48, ax=axes[1]); axes[1].set_title('PACF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df174c",
   "metadata": {},
   "source": [
    "## Conclusiones y recomendaciones de ventana de entrenamiento\n",
    "\n",
    "A partir del análisis de la serie temporal y las funciones ACF/PACF, se extraen las siguientes conclusiones y pautas para diseñar la ventana de entrada de un modelo que prediga 12 meses por delante:\n",
    "\n",
    "### 1. Comportamiento de la serie\n",
    "\n",
    "- **Tendencia**  \n",
    "  Ligera caída hasta mediados de los 90 ’s, posterior estabilización y repunte en los últimos años.\n",
    "\n",
    "- **Estacionalidad**  \n",
    "  Ciclo anual muy marcado, con amplitud aproximada de ± 400 000 mm.\n",
    "\n",
    "- **Residuo**  \n",
    "  Ruido estacionario con variaciones dentro de ± 100 000 mm.\n",
    "\n",
    "### 2. Dependencias temporales (ACF / PACF)\n",
    "\n",
    "| Lag  | ACF     | PACF    |\n",
    "|------|---------|---------|\n",
    "| 1    | ~ 0.80  | corte   |\n",
    "| 2–4  | moderado descenso ↘︎  | valores residuales insignificantes |\n",
    "| 12   | ~ 0.75  | refuerzo ~ 0.35 |\n",
    "| 24, 36 | ≈ 0.50–0.70 | débiles  |\n",
    "\n",
    "### 3. Recomendaciones de ventana de entrada\n",
    "\n",
    "Para capturar tanto la **dependencia inmediata** como la **estacionalidad anual**, se recomienda una ventana deslizante que incluya:\n",
    "\n",
    "1. **Lags cortos** (dependencia inmediata):  \n",
    "   – Meses 1–4 anteriores  \n",
    "2. **Lags estacionales** (dependencia anual):  \n",
    "   – Meses 12, 24 y 36 anteriores  \n",
    "\n",
    "Además, para predecir eficientemente **12 meses por delante**, es aconsejable usar al menos **36–48 meses** de historia como entrada:\n",
    "\n",
    "```python\n",
    "window_size = 36  # ó 48 para mayor contexto\n",
    "features = [\n",
    "    series.shift(1), series.shift(2), series.shift(3), series.shift(4),\n",
    "    series.shift(12), series.shift(24), series.shift(36)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    r2_score\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, MaxPooling1D, Dropout, Flatten, Dense,\n",
    "    ConvLSTM2D, BatchNormalization, Conv2D\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ─── CONFIGURACIÓN GENERAL ─────────────────────────────────────────────────────\n",
    "warnings.filterwarnings('ignore')\n",
    "mixed_precision.set_global_policy('float32')\n",
    "\n",
    "# GPU / CPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for g in gpus:\n",
    "        tf.config.experimental.set_memory_growth(g, True)\n",
    "    DEVICE = '/GPU:0'\n",
    "else:\n",
    "    DEVICE = '/CPU:0'\n",
    "\n",
    "# Silenciar logs de LightGBM\n",
    "logging.getLogger(\"lightgbm\").setLevel(logging.ERROR)\n",
    "\n",
    "# Rutas base\n",
    "DATA_DIR = Path(BASE_PATH) / 'data' / 'output'\n",
    "\n",
    "# Archivos a procesar\n",
    "FILES = [\n",
    "    \"complete_dataset_with_features.nc\",\n",
    "    \"complete_dataset_with_features_with_clusters_elevation.nc\"\n",
    "]\n",
    "\n",
    "# Parámetros comunes\n",
    "WINDOW    = 48        # tamaño de ventana\n",
    "TEST_SIZE = 0.2       # 20% validación\n",
    "EPOCHS    = 50        # épocas default\n",
    "BATCH     = 16        # batch size default\n",
    "\n",
    "# ─── FUNCIONES AUXILIARES ───────────────────────────────────────────────────────\n",
    "def create_supervised_1d(series, window):\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(series)):\n",
    "        X.append(series.iloc[i-window:i].values)\n",
    "        y.append(series.iloc[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def scale_split(X, y, test_size=TEST_SIZE):\n",
    "    n = len(X)\n",
    "    split = int(n * (1 - test_size))\n",
    "    sx = StandardScaler().fit(X.reshape(-1, 1))\n",
    "    sy = StandardScaler().fit(y.reshape(-1, 1))\n",
    "    Xs = sx.transform(X.reshape(-1, 1)).reshape(X.shape)\n",
    "    ys = sy.transform(y.reshape(-1, 1)).reshape(-1)\n",
    "    return Xs[:split], Xs[split:], ys[:split], ys[split:], sx, sy\n",
    "\n",
    "def plot_keras_curves(history, name):\n",
    "    for metric in ('loss','mae','mape'):\n",
    "        if metric in history.history:\n",
    "            plt.figure(figsize=(6,2.5))\n",
    "            plt.plot(history.history[metric], label='train')\n",
    "            plt.plot(history.history[f'val_{metric}'], label='val')\n",
    "            plt.title(f'{name} {metric.upper()}')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# ─── BUCLE PRINCIPAL SOBRE AMBOS DATASETS ──────────────────────────────────────\n",
    "for fname in FILES:\n",
    "    print(f\"\\n\\n===== Procesando {fname} =====\")\n",
    "    path = DATA_DIR / fname\n",
    "    ds   = xr.open_dataset(path)\n",
    "\n",
    "    # Serie global agregada\n",
    "    ts_global = ds['total_precipitation'].sum(dim=['latitude','longitude']).to_series()\n",
    "\n",
    "    # ─── BLOQUE 1: Univariado por celda (RF, XGB, LGBM) ─────────────────────────\n",
    "    print(\">> Bloque 1: modelos univariados por celda…\")\n",
    "    models_1d = {\n",
    "        'RF':  RandomForestRegressor(random_state=0),\n",
    "        'XGB': XGBRegressor(use_label_encoder=False, eval_metric='rmse', verbosity=0),\n",
    "        'LGB': LGBMRegressor(min_child_samples=1, min_data_in_leaf=1, verbose=-1)\n",
    "    }\n",
    "    results_cells = []\n",
    "\n",
    "    coords = list(product(ds.latitude.values, ds.longitude.values))\n",
    "    for lat, lon in tqdm(coords, desc=f\"  celdas en {fname}\", unit=\"celda\"):\n",
    "        ts_cell = ds['total_precipitation'].sel(latitude=lat, longitude=lon).to_series()\n",
    "        if ts_cell.isnull().any(): \n",
    "            continue\n",
    "\n",
    "        X1d, y1d = create_supervised_1d(ts_cell, WINDOW)\n",
    "        Xtr1d, Xv1d, ytr1d, yv1d, _, _ = scale_split(X1d, y1d)\n",
    "\n",
    "        for name, mdl in models_1d.items():\n",
    "            mdl.fit(Xtr1d, ytr1d)\n",
    "            ypred = mdl.predict(Xv1d)\n",
    "            rmse = np.sqrt(mean_squared_error(yv1d, ypred))\n",
    "            r2   = r2_score(yv1d, ypred)\n",
    "            results_cells.append({\n",
    "                'dataset': fname,\n",
    "                'model':   name,\n",
    "                'lat':     float(lat),\n",
    "                'lon':     float(lon),\n",
    "                'rmse':    rmse,\n",
    "                'r2':      r2\n",
    "            })\n",
    "\n",
    "    df_cells = pd.DataFrame(results_cells)\n",
    "    summary_cells = df_cells.groupby('model')[['rmse','r2']].mean()\n",
    "    print(\"Univariado por celda — RMSE y R² medios:\")\n",
    "    print(summary_cells)\n",
    "\n",
    "    # ─── BLOQUE 2: Espaciotemporal many‐to‐one con ConvLSTM2D ───────────────────\n",
    "    print(\">> Bloque 2: ConvLSTM2D espaciotemporal…\")\n",
    "    H, W = len(ds.latitude), len(ds.longitude)\n",
    "    XX, YY = [], []\n",
    "\n",
    "    times = ds.time.values\n",
    "    for i in range(WINDOW, len(times)):\n",
    "        past = ds['total_precipitation'].isel(time=slice(i-WINDOW, i))\n",
    "        XX.append(past.values[..., None])     # (WINDOW,H,W,1)\n",
    "        YY.append(ds['total_precipitation'].isel(time=i).values[..., None])  # (H,W,1)\n",
    "\n",
    "    XX = np.stack(XX)\n",
    "    YY = np.stack(YY)\n",
    "    N  = len(XX)\n",
    "    split = int((1-TEST_SIZE)*N)\n",
    "    Xtr_st, Xv_st = XX[:split], XX[split:]\n",
    "    ytr_st, yv_st = YY[:split], YY[split:]\n",
    "\n",
    "    def build_convlstm_2d(window, h, w, lr=1e-3, drop=0.2):\n",
    "        m = Sequential([\n",
    "            Input((window, h, w, 1)),\n",
    "            ConvLSTM2D(32, (3,3), padding='same', return_sequences=False),\n",
    "            BatchNormalization(), Dropout(drop),\n",
    "            Conv2D(1, (1,1), activation='linear', padding='same')\n",
    "        ])\n",
    "        m.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "        return m\n",
    "\n",
    "    with tf.device(DEVICE):\n",
    "        model_st = build_convlstm_2d(WINDOW, H, W)\n",
    "        history = model_st.fit(\n",
    "            Xtr_st, ytr_st,\n",
    "            validation_data=(Xv_st, yv_st),\n",
    "            epochs=EPOCHS, batch_size=BATCH,\n",
    "            callbacks=[EarlyStopping('val_loss', patience=5, restore_best_weights=True)],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    ypred_st = model_st.predict(Xv_st)\n",
    "    rmse_st  = np.sqrt(mean_squared_error(yv_st.ravel(), ypred_st.ravel()))\n",
    "    r2_st    = r2_score(yv_st.ravel(), ypred_st.ravel())\n",
    "    print(f\"ConvLSTM2D → RMSE: {rmse_st:.2f} mm, R²: {r2_st:.3f}\")\n",
    "    plot_keras_curves(history, f\"ConvLSTM2D ({fname})\")\n",
    "\n",
    "print(\"\\n✓ Procesamiento completado para todos los datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c72ce",
   "metadata": {},
   "source": [
    "### 4. Decisión final\n",
    "\n",
    "Se recomienda implementar un modelo que utilice una ventana de entrada de **48 meses** con las **7 features** seleccionadas (lags 1–4, 12, 24, 36). Esto permitirá capturar tanto las dependencias inmediatas como las estacionales, optimizando la predicción de **12 meses por delante**. Se hacen las modificaciones en el script _data/transformation/update/dataset_update.py_ de este repositorio.\n",
    "\n",
    "Teniendo en cuenta los modelos específicos las ventanas de entrada pueden variar de 60 hasta 80 meses, por lo que se recomienda ajustar la ventana de entrada según el modelo a utilizar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
