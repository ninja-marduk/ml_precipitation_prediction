{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V5: GNN-ConvLSTM Stacking Ensemble\n",
    "\n",
    "**Version:** 5.0  \n",
    "**Date:** January 2026  \n",
    "**Author:** Manuel Ricardo Perez Reyes  \n",
    "**Status:** PROTOTYPE\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "V5 implements a novel dual-branch stacking architecture that combines:\n",
    "\n",
    "1. **Branch 1 (Grid):** ConvLSTM-Residual with BASIC features\n",
    "2. **Branch 2 (Graph):** GNN-TAT-GAT with KCE features\n",
    "3. **Grid-Graph Fusion:** Cross-attention alignment module\n",
    "4. **Meta-Learner:** Context-aware weighted fusion\n",
    "\n",
    "### Key Innovation\n",
    "\n",
    "- **Architecture-specific feature routing:** BASIC -> ConvLSTM, KCE -> GNN\n",
    "- **Grid-Graph fusion via cross-attention**\n",
    "- **Interpretable elevation-dependent branch weighting**\n",
    "\n",
    "### Target Performance\n",
    "\n",
    "| Metric | ConvLSTM | GNN-TAT | V5 Target | V5 Excellent |\n",
    "|--------|----------|---------|-----------|--------------|\n",
    "| R2 (mean) | 0.653 | 0.628 | > 0.65 | > 0.70 |\n",
    "| RMSE (mm) | 112.02 | 92.12 | < 85 | < 80 |\n",
    "| Parameters | 2M | 98K | ~200K | <180K |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Environment Detection and Package Installation\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Version info from local machine where dataset was created\n",
    "LOCAL_VERSIONS = {\n",
    "    'netCDF4': '1.7.2',\n",
    "    'h5py': '3.14.0',\n",
    "    'HDF5_lib': '1.14.4',\n",
    "    'netCDF_lib': '4.9.2'\n",
    "}\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'torch-geometric', '-q'], check=True)\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'netcdf4', 'h5py', 'xarray', 'h5netcdf', '-q'], check=True)\n",
    "\n",
    "    import netCDF4\n",
    "    import h5py\n",
    "\n",
    "    print(f\"Version check:\")\n",
    "    print(f\"  netCDF4: {netCDF4.__version__}\")\n",
    "    print(f\"  h5py: {h5py.__version__}\")\n",
    "\n",
    "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
    "    DRIVE_DATA_FILE = f\"{BASE_PATH}/data/output/complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc\"\n",
    "    LOCAL_DATA_FILE = '/content/dataset_local.nc'\n",
    "\n",
    "    if os.path.exists(DRIVE_DATA_FILE):\n",
    "        if os.path.exists(LOCAL_DATA_FILE):\n",
    "            os.remove(LOCAL_DATA_FILE)\n",
    "        print(\"Copying dataset to local disk...\")\n",
    "        shutil.copy(DRIVE_DATA_FILE, LOCAL_DATA_FILE)\n",
    "        print(\"Copy completed!\")\n",
    "        USE_LOCAL_DATA = True\n",
    "    else:\n",
    "        print(f\"WARNING: Dataset not found: {DRIVE_DATA_FILE}\")\n",
    "        USE_LOCAL_DATA = False\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    BASE_PATH = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "    if not BASE_PATH or BASE_PATH == '':\n",
    "        BASE_PATH = r'd:\\github.com\\ninja-marduk\\ml_precipitation_prediction'\n",
    "    USE_LOCAL_DATA = False\n",
    "    LOCAL_DATA_FILE = None\n",
    "\n",
    "print(f\"Base path: {BASE_PATH}\")\n",
    "\n",
    "if BASE_PATH not in sys.path:\n",
    "    sys.path.insert(0, BASE_PATH)\n",
    "\n",
    "print(\"Environment setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Core Imports\n",
    "# =============================================================================\n",
    "\n",
    "# Disable TF oneDNN warnings\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Force CPU if CUDA has issues (cuDNN not found)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Comment this line if you have working CUDA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "# PyTorch - force CPU mode for local without CUDA\n",
    "import torch\n",
    "torch.set_default_device('cpu')  # Force CPU\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Device setup (CPU for local, GPU for Colab)\n",
    "if IN_COLAB and torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "# PyTorch Geometric\n",
    "try:\n",
    "    import torch_geometric\n",
    "    from torch_geometric.nn import GCNConv, GATConv, SAGEConv, global_mean_pool\n",
    "    from torch_geometric.data import Data, Batch\n",
    "    from torch_geometric.utils import dense_to_sparse, to_dense_adj\n",
    "    TORCH_GEOMETRIC_AVAILABLE = True\n",
    "    print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "except ImportError:\n",
    "    TORCH_GEOMETRIC_AVAILABLE = False\n",
    "    print(\"WARNING: PyTorch Geometric not available.\")\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Utilities\n",
    "import gc\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# Visualization (700 DPI for publication quality)\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi'] = 150\n",
    "matplotlib.rcParams['savefig.dpi'] = 700\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    GEOPANDAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GEOPANDAS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.feature as cfeature\n",
    "    CARTOPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CARTOPY_AVAILABLE = False\n",
    "\n",
    "# Print versions\n",
    "print(f\"Environment Summary:\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f\"  CUDA device: {torch.cuda.get_device_name(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: V5 Configuration (V4 Compatible Format)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class V5Config:\n",
    "    \"\"\"V5 GNN-ConvLSTM Stacking Configuration (V4 Compatible).\"\"\"\n",
    "    \n",
    "    # Grid dimensions\n",
    "    n_lat: int = 61\n",
    "    n_lon: int = 65\n",
    "    n_nodes: int = 61 * 65  # 3965 nodes\n",
    "    \n",
    "    # Feature sets (V4 compatible)\n",
    "    n_basic_features: int = 12  # For ConvLSTM branch\n",
    "    n_kce_features: int = 15    # For GNN branch (BASIC + 3 clusters)\n",
    "    \n",
    "    # Sequence parameters\n",
    "    seq_len: int = 12  # 12 months input (V4: input_window = 60)\n",
    "    input_window: int = 60  # V4 compatible\n",
    "    horizons: List[int] = field(default_factory=lambda: [1, 3, 6, 12])\n",
    "    \n",
    "    # Feature sets dictionary (V4 compatible)\n",
    "    feature_sets: Dict = field(default_factory=lambda: {\n",
    "        'BASIC': [\n",
    "            'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "            'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "            'elevation', 'slope', 'aspect'\n",
    "        ],\n",
    "        'KCE': [\n",
    "            'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "            'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "            'elevation', 'slope', 'aspect', 'elev_high', 'elev_med', 'elev_low'\n",
    "        ],\n",
    "        'PAFC': [\n",
    "            'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "            'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "            'elevation', 'slope', 'aspect', 'elev_high', 'elev_med', 'elev_low',\n",
    "            'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag12'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Branch 1: ConvLSTM Configuration\n",
    "    convlstm_config: Dict = field(default_factory=lambda: {\n",
    "        'hidden_channels': 32,\n",
    "        'kernel_size': 3,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.2,\n",
    "        'output_dim': 64,\n",
    "        'use_residual': True,\n",
    "        'use_attention': True\n",
    "    })\n",
    "    \n",
    "    # Branch 2: GNN-TAT Configuration (V4 compatible)\n",
    "    gnn_config: Dict = field(default_factory=lambda: {\n",
    "        'gnn_type': 'GAT',\n",
    "        'hidden_dim': 64,\n",
    "        'num_gnn_layers': 2,\n",
    "        'num_heads': 4,\n",
    "        'num_temporal_heads': 4,\n",
    "        'num_lstm_layers': 1,\n",
    "        'dropout': 0.2,\n",
    "        'temporal_dropout': 0.1,\n",
    "        'output_dim': 64,\n",
    "        # V4 graph construction parameters\n",
    "        'k_neighbors': 8,\n",
    "        'distance_threshold_km': 50.0,\n",
    "        'use_elevation_edges': True,\n",
    "        'use_correlation_edges': True,\n",
    "        'correlation_threshold': 0.3,\n",
    "        'elevation_scale': 0.2,\n",
    "        'max_edges': 500000\n",
    "    })\n",
    "    \n",
    "    # Grid-Graph Fusion Configuration\n",
    "    fusion_config: Dict = field(default_factory=lambda: {\n",
    "        'type': 'cross_attention',\n",
    "        'num_heads': 4,\n",
    "        'hidden_dim': 64,\n",
    "        'dropout': 0.1\n",
    "    })\n",
    "    \n",
    "    # Meta-Learner Configuration\n",
    "    meta_config: Dict = field(default_factory=lambda: {\n",
    "        'context_features': ['elevation', 'season', 'horizon'],\n",
    "        'hidden_dim': 64,\n",
    "        'dropout': 0.1\n",
    "    })\n",
    "    \n",
    "    # Training Configuration (V4 compatible)\n",
    "    training_config: Dict = field(default_factory=lambda: {\n",
    "        'batch_size': 2,          # V4: batch_size = 2 for large grids\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-5,\n",
    "        'epochs': 150,            # V4: epochs = 150\n",
    "        'patience': 50,           # V4: patience = 50\n",
    "        'min_delta': 0.001,\n",
    "        'warmup_epochs': 5,\n",
    "        'gradient_accumulation_steps': 1,\n",
    "        'prediction_batch_size': 2\n",
    "    })\n",
    "    \n",
    "    # Data handling (V4 compatible)\n",
    "    train_val_split: float = 0.8  # V4: train_val_split = 0.8\n",
    "    light_mode: bool = False\n",
    "    light_grid_size: int = 5\n",
    "\n",
    "# Set paths based on environment\n",
    "if IN_COLAB:\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    if USE_LOCAL_DATA:\n",
    "        DATA_FILE = Path(LOCAL_DATA_FILE)\n",
    "    else:\n",
    "        DATA_FILE = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "else:\n",
    "    BASE_PATH = Path(BASE_PATH) if 'BASE_PATH' in dir() else Path('.')\n",
    "    DATA_FILE = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "\n",
    "# Output directory (V4 compatible structure)\n",
    "OUTPUT_ROOT = BASE_PATH / 'models' / 'output' / 'V5_STACKING'\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize config\n",
    "config = V5Config()\n",
    "\n",
    "print(f\"V5 Configuration initialized (V4 Compatible)\")\n",
    "print(f\"  Grid: {config.n_lat}x{config.n_lon} = {config.n_nodes} nodes\")\n",
    "print(f\"  BASIC features: {config.n_basic_features}\")\n",
    "print(f\"  KCE features: {config.n_kce_features}\")\n",
    "print(f\"  Horizons: {config.horizons}\")\n",
    "print(f\"  Output: {OUTPUT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture Components\n",
    "\n",
    "### 2.1 Branch 1: ConvLSTM with Residual and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.1 ConvLSTM Cell\n",
    "# =============================================================================\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"Convolutional LSTM Cell for spatiotemporal modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, hidden_channels: int, kernel_size: int = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_channels = hidden_channels\n",
    "        padding = kernel_size // 2\n",
    "        \n",
    "        # Gates: input, forget, cell, output\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels + hidden_channels,\n",
    "            out_channels=4 * hidden_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            bias=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n",
    "               ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (B, C, H, W)\n",
    "            state: Tuple of (h, c) each (B, hidden_channels, H, W)\n",
    "        Returns:\n",
    "            h_new: New hidden state\n",
    "            (h_new, c_new): New state tuple\n",
    "        \"\"\"\n",
    "        h, c = state\n",
    "        \n",
    "        # Concatenate input and hidden state\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        gates = self.conv(combined)\n",
    "        \n",
    "        # Split into 4 gates\n",
    "        i, f, g, o = torch.chunk(gates, 4, dim=1)\n",
    "        \n",
    "        # Apply activations\n",
    "        i = torch.sigmoid(i)  # Input gate\n",
    "        f = torch.sigmoid(f)  # Forget gate\n",
    "        g = torch.tanh(g)     # Cell gate\n",
    "        o = torch.sigmoid(o)  # Output gate\n",
    "        \n",
    "        # Update cell and hidden state\n",
    "        c_new = f * c + i * g\n",
    "        h_new = o * torch.tanh(c_new)\n",
    "        \n",
    "        return h_new, (h_new, c_new)\n",
    "    \n",
    "    def init_hidden(self, batch_size: int, height: int, width: int, \n",
    "                    device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Initialize hidden state.\"\"\"\n",
    "        h = torch.zeros(batch_size, self.hidden_channels, height, width, device=device)\n",
    "        c = torch.zeros(batch_size, self.hidden_channels, height, width, device=device)\n",
    "        return (h, c)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial attention module for ConvLSTM output.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 4, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // 4, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply spatial attention.\"\"\"\n",
    "        attn = self.conv(x)\n",
    "        return x * attn\n",
    "\n",
    "\n",
    "class ConvLSTMBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Branch 1: ConvLSTM with Residual connections and Spatial Attention.\n",
    "    \n",
    "    Processes grid-based BASIC features using convolutional LSTM layers\n",
    "    with residual connections for better gradient flow.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: V5Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        cfg = config.convlstm_config\n",
    "        in_channels = config.n_basic_features\n",
    "        hidden_channels = cfg['hidden_channels']\n",
    "        num_layers = cfg['num_layers']\n",
    "        output_dim = cfg['output_dim']\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.use_residual = cfg['use_residual']\n",
    "        self.use_attention = cfg['use_attention']\n",
    "        \n",
    "        # ConvLSTM layers\n",
    "        self.cells = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            in_ch = in_channels if i == 0 else hidden_channels\n",
    "            self.cells.append(ConvLSTMCell(in_ch, hidden_channels, cfg['kernel_size']))\n",
    "            self.norms.append(nn.BatchNorm2d(hidden_channels))\n",
    "        \n",
    "        # Residual projection (if input channels differ)\n",
    "        if self.use_residual:\n",
    "            self.residual_proj = nn.Conv2d(in_channels, hidden_channels, kernel_size=1)\n",
    "        \n",
    "        # Spatial attention\n",
    "        if self.use_attention:\n",
    "            self.spatial_attn = SpatialAttention(hidden_channels)\n",
    "        \n",
    "        # Output projection: flatten grid and project to output_dim\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # Global average pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden_channels, output_dim),\n",
    "            nn.LayerNorm(output_dim),\n",
    "            nn.Dropout(cfg['dropout'])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (B, T, C, H, W) - batch, time, channels, height, width\n",
    "        Returns:\n",
    "            output: (B, output_dim) - branch embedding\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, channels, height, width = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Initialize hidden states for all layers\n",
    "        states = [cell.init_hidden(batch_size, height, width, device) \n",
    "                  for cell in self.cells]\n",
    "        \n",
    "        # Process sequence through ConvLSTM layers\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t]  # (B, C, H, W)\n",
    "            \n",
    "            # Save first layer input for residual\n",
    "            if self.use_residual and t == seq_len - 1:\n",
    "                residual = self.residual_proj(x_t)\n",
    "            \n",
    "            # Pass through all layers\n",
    "            for layer_idx, (cell, norm) in enumerate(zip(self.cells, self.norms)):\n",
    "                h, states[layer_idx] = cell(x_t, states[layer_idx])\n",
    "                h = norm(h)\n",
    "                x_t = h  # Input to next layer\n",
    "        \n",
    "        # Get final hidden state\n",
    "        output = h\n",
    "        \n",
    "        # Add residual connection\n",
    "        if self.use_residual:\n",
    "            output = output + residual\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        if self.use_attention:\n",
    "            output = self.spatial_attn(output)\n",
    "        \n",
    "        # Project to output dimension\n",
    "        output = self.output_proj(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Test ConvLSTM Branch\n",
    "print(\"Testing ConvLSTM Branch...\")\n",
    "convlstm_branch = ConvLSTMBranch(config).to(DEVICE)\n",
    "test_input = torch.randn(2, 12, 12, 61, 65).to(DEVICE)  # (B, T, C, H, W)\n",
    "test_output = convlstm_branch(test_input)\n",
    "print(f\"  Input shape: {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in convlstm_branch.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Branch 2: GNN-TAT (from V4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.2 GNN-TAT Components (adapted from V4)\n",
    "# =============================================================================\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Temporal Attention module with residual connection.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.q_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Residual projection\n",
    "        self.residual_proj = nn.Linear(input_dim, hidden_dim) if input_dim != hidden_dim else nn.Identity()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"(B, T, D) -> (B, T, hidden_dim)\"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        residual = self.residual_proj(x)\n",
    "        \n",
    "        Q = self.q_proj(x)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        # Residual + LayerNorm\n",
    "        out = self.layer_norm(out + residual)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class SpatialGNNEncoder(nn.Module):\n",
    "    \"\"\"GNN Encoder for spatial dependencies (GAT, GCN, or SAGE).\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int,\n",
    "                 gnn_type: str = 'GAT', num_heads: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.gnn_type = gnn_type\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            if gnn_type == 'GAT':\n",
    "                layer = GATConv(hidden_dim, hidden_dim // num_heads, \n",
    "                               heads=num_heads, dropout=dropout, concat=True)\n",
    "            elif gnn_type == 'SAGE':\n",
    "                layer = SAGEConv(hidden_dim, hidden_dim, aggr='mean')\n",
    "            else:  # GCN\n",
    "                layer = GCNConv(hidden_dim, hidden_dim, add_self_loops=True, normalize=True)\n",
    "            \n",
    "            self.gnn_layers.append(layer)\n",
    "            self.norms.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                edge_weight: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"(N, D_in) -> (N, hidden_dim)\"\"\"\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        for gnn, norm in zip(self.gnn_layers, self.norms):\n",
    "            residual = x\n",
    "            \n",
    "            if self.gnn_type == 'GAT':\n",
    "                x = gnn(x, edge_index)\n",
    "            else:\n",
    "                x = gnn(x, edge_index, edge_weight=edge_weight)\n",
    "            \n",
    "            x = F.relu(x)\n",
    "            x = norm(x)\n",
    "            x = self.dropout(x)\n",
    "            x = x + residual  # Residual connection\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class GNNTATBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Branch 2: GNN-TAT (Graph Neural Network with Temporal Attention).\n",
    "    \n",
    "    Processes graph-based KCE features using:\n",
    "    1. Spatial GNN encoder (GAT/GCN/SAGE)\n",
    "    2. Temporal attention across timesteps\n",
    "    3. LSTM for sequence modeling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: V5Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        cfg = config.gnn_config\n",
    "        n_features = config.n_kce_features\n",
    "        hidden_dim = cfg['hidden_dim']\n",
    "        output_dim = cfg['output_dim']\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_nodes = config.n_nodes\n",
    "        \n",
    "        # Spatial GNN Encoder\n",
    "        self.gnn_encoder = SpatialGNNEncoder(\n",
    "            input_dim=n_features,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=cfg['num_gnn_layers'],\n",
    "            gnn_type=cfg['gnn_type'],\n",
    "            num_heads=cfg['num_heads'],\n",
    "            dropout=cfg['dropout']\n",
    "        )\n",
    "        \n",
    "        # Temporal Attention\n",
    "        self.temporal_attn = TemporalAttention(\n",
    "            input_dim=hidden_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_heads=cfg['num_temporal_heads'],\n",
    "            dropout=cfg['temporal_dropout']\n",
    "        )\n",
    "        \n",
    "        # LSTM for sequence modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=cfg['num_lstm_layers'],\n",
    "            batch_first=True,\n",
    "            dropout=cfg['dropout'] if cfg['num_lstm_layers'] > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.LayerNorm(output_dim),\n",
    "            nn.Dropout(cfg['dropout'])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor,\n",
    "                edge_weight: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (B, T, N, F) - batch, time, nodes, features\n",
    "            edge_index: Graph connectivity (2, E)\n",
    "            edge_weight: Optional edge weights (E,)\n",
    "        Returns:\n",
    "            output: Global embedding (B, output_dim)\n",
    "            node_embeddings: Per-node embeddings (B, N, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, n_nodes, n_features = x.shape\n",
    "        \n",
    "        # Process each timestep through GNN\n",
    "        gnn_outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t]  # (B, N, F)\n",
    "            \n",
    "            # Flatten batch for GNN processing\n",
    "            x_flat = x_t.reshape(-1, n_features)  # (B*N, F)\n",
    "            \n",
    "            # Adjust edge_index for batched graphs\n",
    "            batch_edge_index = self._expand_edge_index(edge_index, batch_size, n_nodes)\n",
    "            \n",
    "            # GNN forward\n",
    "            h = self.gnn_encoder(x_flat, batch_edge_index, edge_weight)\n",
    "            h = h.view(batch_size, n_nodes, -1)  # (B, N, hidden_dim)\n",
    "            \n",
    "            gnn_outputs.append(h)\n",
    "        \n",
    "        # Stack temporal outputs: (B, T, N, hidden_dim)\n",
    "        gnn_outputs = torch.stack(gnn_outputs, dim=1)\n",
    "        \n",
    "        # Apply temporal attention per node\n",
    "        # Reshape to (B*N, T, hidden_dim) for temporal processing\n",
    "        temporal_input = gnn_outputs.permute(0, 2, 1, 3).reshape(-1, seq_len, self.hidden_dim)\n",
    "        temporal_out = self.temporal_attn(temporal_input)  # (B*N, T, hidden_dim)\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(temporal_out)  # (B*N, T, hidden_dim)\n",
    "        lstm_out = lstm_out[:, -1]  # Take last timestep: (B*N, hidden_dim)\n",
    "        \n",
    "        # Reshape to (B, N, hidden_dim)\n",
    "        node_embeddings = lstm_out.view(batch_size, n_nodes, -1)\n",
    "        \n",
    "        # Project node embeddings\n",
    "        node_embeddings = self.output_proj(node_embeddings)  # (B, N, output_dim)\n",
    "        \n",
    "        # Global embedding via mean pooling\n",
    "        global_embedding = node_embeddings.mean(dim=1)  # (B, output_dim)\n",
    "        \n",
    "        return global_embedding, node_embeddings\n",
    "    \n",
    "    def _expand_edge_index(self, edge_index: torch.Tensor, batch_size: int, \n",
    "                           n_nodes: int) -> torch.Tensor:\n",
    "        \"\"\"Expand edge_index for batched graph processing.\"\"\"\n",
    "        expanded = []\n",
    "        for b in range(batch_size):\n",
    "            offset = b * n_nodes\n",
    "            expanded.append(edge_index + offset)\n",
    "        return torch.cat(expanded, dim=1)\n",
    "\n",
    "\n",
    "print(\"GNN-TAT Branch defined.\")\n",
    "print(\"Note: Full testing requires graph construction (see Section 3).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Grid-Graph Fusion Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.3 Grid-Graph Fusion via Cross-Attention\n",
    "# =============================================================================\n",
    "\n",
    "class GridGraphFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Grid-Graph Fusion Module using Cross-Attention.\n",
    "    \n",
    "    Aligns grid-based ConvLSTM embeddings with graph-based GNN embeddings\n",
    "    through cross-attention mechanism.\n",
    "    \n",
    "    - Query: GNN node embeddings\n",
    "    - Key/Value: ConvLSTM grid embeddings (flattened to node positions)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: V5Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        cfg = config.fusion_config\n",
    "        hidden_dim = cfg['hidden_dim']\n",
    "        num_heads = cfg['num_heads']\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Cross-attention projections\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)  # From GNN\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)  # From ConvLSTM\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)  # From ConvLSTM\n",
    "        \n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(cfg['dropout'])\n",
    "        \n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Feed-forward network for fusion\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, gnn_embeddings: torch.Tensor, \n",
    "                convlstm_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gnn_embeddings: Node embeddings from GNN branch (B, N, D)\n",
    "            convlstm_embedding: Global embedding from ConvLSTM branch (B, D)\n",
    "        Returns:\n",
    "            fused: Fused embedding (B, D)\n",
    "        \"\"\"\n",
    "        batch_size, n_nodes, dim = gnn_embeddings.shape\n",
    "        \n",
    "        # Expand ConvLSTM embedding for cross-attention\n",
    "        # Use it as key/value for all nodes\n",
    "        convlstm_expanded = convlstm_embedding.unsqueeze(1).expand(-1, n_nodes, -1)\n",
    "        \n",
    "        # Cross-attention: GNN queries attend to ConvLSTM\n",
    "        Q = self.q_proj(gnn_embeddings)  # (B, N, D)\n",
    "        K = self.k_proj(convlstm_expanded)  # (B, N, D)\n",
    "        V = self.v_proj(convlstm_expanded)  # (B, N, D)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        Q = Q.view(batch_size, n_nodes, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, n_nodes, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, n_nodes, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        attn = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, n_nodes, self.hidden_dim)\n",
    "        out = self.out_proj(out)\n",
    "        \n",
    "        # Residual connection with GNN embeddings\n",
    "        out = self.layer_norm(out + gnn_embeddings)\n",
    "        \n",
    "        # Global pooling\n",
    "        fused_node = out.mean(dim=1)  # (B, D)\n",
    "        \n",
    "        # Combine with original ConvLSTM embedding through FFN\n",
    "        combined = torch.cat([fused_node, convlstm_embedding], dim=-1)  # (B, 2D)\n",
    "        fused = self.ffn(combined)  # (B, D)\n",
    "        \n",
    "        return fused\n",
    "\n",
    "\n",
    "print(\"Grid-Graph Fusion Module defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Interpretable Meta-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.4 Context-Aware Meta-Learner\n",
    "# =============================================================================\n",
    "\n",
    "class ContextAwareMetaLearner(nn.Module):\n",
    "    \"\"\"\n",
    "    Interpretable Meta-Learner for context-dependent branch weighting.\n",
    "    \n",
    "    Uses context features (elevation, season, horizon) to learn\n",
    "    dynamic weights for combining branch outputs.\n",
    "    \n",
    "    Provides interpretability through:\n",
    "    - Branch contribution weights (w1, w2)\n",
    "    - Context-specific weight patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: V5Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        cfg = config.meta_config\n",
    "        hidden_dim = cfg['hidden_dim']\n",
    "        branch_dim = config.convlstm_config['output_dim']  # Same for both branches\n",
    "        \n",
    "        # Context encoder\n",
    "        # Input: [mean_elevation, season_sin, season_cos, horizon]\n",
    "        context_dim = 4\n",
    "        self.context_encoder = nn.Sequential(\n",
    "            nn.Linear(context_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Weight generator: context -> branch weights\n",
    "        self.weight_generator = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + branch_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(cfg['dropout']),\n",
    "            nn.Linear(hidden_dim, 2),  # 2 branch weights\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Final output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(branch_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Precipitation prediction\n",
    "        )\n",
    "        \n",
    "    def forward(self, convlstm_emb: torch.Tensor, gnn_emb: torch.Tensor,\n",
    "                fused_emb: torch.Tensor, context: torch.Tensor\n",
    "               ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            convlstm_emb: ConvLSTM branch embedding (B, D)\n",
    "            gnn_emb: GNN branch embedding (B, D)\n",
    "            fused_emb: Grid-Graph fused embedding (B, D)\n",
    "            context: Context features (B, 4) - [elevation, sin, cos, horizon]\n",
    "        Returns:\n",
    "            prediction: Precipitation prediction (B, 1)\n",
    "            weights: Branch weights (B, 2) for interpretability\n",
    "        \"\"\"\n",
    "        # Encode context\n",
    "        context_encoded = self.context_encoder(context)  # (B, hidden_dim)\n",
    "        \n",
    "        # Generate weights based on context and branch embeddings\n",
    "        weight_input = torch.cat([context_encoded, convlstm_emb, gnn_emb], dim=-1)\n",
    "        weights = self.weight_generator(weight_input)  # (B, 2)\n",
    "        \n",
    "        # Weighted combination of branch outputs\n",
    "        w1, w2 = weights[:, 0:1], weights[:, 1:2]  # (B, 1) each\n",
    "        combined = w1 * convlstm_emb + w2 * gnn_emb  # (B, D)\n",
    "        \n",
    "        # Add fused embedding as residual\n",
    "        combined = combined + fused_emb\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction = self.output_proj(combined)  # (B, 1)\n",
    "        \n",
    "        return prediction, weights\n",
    "\n",
    "\n",
    "print(\"Context-Aware Meta-Learner defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Complete V5 Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.5 V5 GNN-ConvLSTM Stacking Model\n",
    "# =============================================================================\n",
    "\n",
    "class V5StackingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    V5: GNN-ConvLSTM Stacking Ensemble.\n",
    "    \n",
    "    Architecture:\n",
    "    ┌──────────────────────────────────────────────────────────────────┐\n",
    "    │                    V5 GNN-ConvLSTM STACKING                      │\n",
    "    ├──────────────────────────────────────────────────────────────────┤\n",
    "    │  BRANCH 1 (Grid)              BRANCH 2 (Graph)                   │\n",
    "    │  ─────────────────           ──────────────────                  │\n",
    "    │  ConvLSTM-Residual           GNN-TAT-GAT                         │\n",
    "    │  BASIC features              KCE features                        │\n",
    "    │         │                           │                            │\n",
    "    │         └─────────┬─────────────────┘                            │\n",
    "    │                   ▼                                              │\n",
    "    │         GRID-GRAPH FUSION MODULE                                 │\n",
    "    │         (Cross-Attention Alignment)                              │\n",
    "    │                   │                                              │\n",
    "    │                   ▼                                              │\n",
    "    │         INTERPRETABLE META-LEARNER                               │\n",
    "    │         (Weighted fusion + error attribution)                    │\n",
    "    └──────────────────────────────────────────────────────────────────┘\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: V5Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        # Branch 1: ConvLSTM with BASIC features\n",
    "        self.convlstm_branch = ConvLSTMBranch(config)\n",
    "        \n",
    "        # Branch 2: GNN-TAT with KCE features\n",
    "        self.gnn_branch = GNNTATBranch(config)\n",
    "        \n",
    "        # Grid-Graph Fusion\n",
    "        self.fusion = GridGraphFusion(config)\n",
    "        \n",
    "        # Meta-Learner\n",
    "        self.meta_learner = ContextAwareMetaLearner(config)\n",
    "        \n",
    "    def forward(self, x_basic: torch.Tensor, x_kce: torch.Tensor,\n",
    "                edge_index: torch.Tensor, context: torch.Tensor,\n",
    "                edge_weight: Optional[torch.Tensor] = None\n",
    "               ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_basic: BASIC features for ConvLSTM (B, T, C_basic, H, W)\n",
    "            x_kce: KCE features for GNN (B, T, N, C_kce)\n",
    "            edge_index: Graph connectivity (2, E)\n",
    "            context: Context features (B, 4)\n",
    "            edge_weight: Optional edge weights (E,)\n",
    "        Returns:\n",
    "            prediction: Precipitation prediction (B, 1)\n",
    "            aux_outputs: Dictionary with branch weights and embeddings\n",
    "        \"\"\"\n",
    "        # Branch 1: ConvLSTM\n",
    "        convlstm_emb = self.convlstm_branch(x_basic)  # (B, D)\n",
    "        \n",
    "        # Branch 2: GNN-TAT\n",
    "        gnn_emb, node_embeddings = self.gnn_branch(x_kce, edge_index, edge_weight)\n",
    "        \n",
    "        # Grid-Graph Fusion\n",
    "        fused_emb = self.fusion(node_embeddings, convlstm_emb)  # (B, D)\n",
    "        \n",
    "        # Meta-Learner\n",
    "        prediction, weights = self.meta_learner(\n",
    "            convlstm_emb, gnn_emb, fused_emb, context\n",
    "        )\n",
    "        \n",
    "        aux_outputs = {\n",
    "            'branch_weights': weights,  # (B, 2)\n",
    "            'convlstm_embedding': convlstm_emb,\n",
    "            'gnn_embedding': gnn_emb,\n",
    "            'fused_embedding': fused_emb,\n",
    "            'node_embeddings': node_embeddings\n",
    "        }\n",
    "        \n",
    "        return prediction, aux_outputs\n",
    "    \n",
    "    def count_parameters(self) -> Dict[str, int]:\n",
    "        \"\"\"Count parameters by component.\"\"\"\n",
    "        return {\n",
    "            'convlstm_branch': sum(p.numel() for p in self.convlstm_branch.parameters()),\n",
    "            'gnn_branch': sum(p.numel() for p in self.gnn_branch.parameters()),\n",
    "            'fusion': sum(p.numel() for p in self.fusion.parameters()),\n",
    "            'meta_learner': sum(p.numel() for p in self.meta_learner.parameters()),\n",
    "            'total': sum(p.numel() for p in self.parameters())\n",
    "        }\n",
    "\n",
    "\n",
    "# Create model and count parameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"V5 STACKING MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = V5StackingModel(config).to(DEVICE)\n",
    "param_counts = model.count_parameters()\n",
    "\n",
    "print(f\"\\nParameter counts:\")\n",
    "for name, count in param_counts.items():\n",
    "    print(f\"  {name}: {count:,}\")\n",
    "\n",
    "print(f\"\\nTarget: ~200K parameters\")\n",
    "print(f\"Actual: {param_counts['total']:,} parameters\")\n",
    "print(f\"Status: {'OK' if param_counts['total'] < 250000 else 'EXCEEDS TARGET'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Construction (from V4)\n",
    "\n",
    "TODO: Copy graph construction code from V4 notebook:\n",
    "- Elevation-weighted edges\n",
    "- Distance-based connectivity\n",
    "- Edge weight calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3.1 SpatialGraphBuilder (from V4)\n",
    "# =============================================================================\n",
    "\n",
    "class SpatialGraphBuilder:\n",
    "    \"\"\"\n",
    "    Builds a spatial graph from precipitation grid data.\n",
    "    \n",
    "    Nodes: Each grid cell (lat, lon) is a node\n",
    "    Edges: Based on:\n",
    "        1. Geographic distance (k-NN or threshold)\n",
    "        2. Elevation similarity\n",
    "        3. Historical precipitation correlation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lat_coords: np.ndarray, lon_coords: np.ndarray,\n",
    "                 elevation: np.ndarray, config: Dict):\n",
    "        \"\"\"\n",
    "        Initialize graph builder.\n",
    "        \n",
    "        Args:\n",
    "            lat_coords: Latitude coordinates (1D array)\n",
    "            lon_coords: Longitude coordinates (1D array)\n",
    "            elevation: Elevation values (2D array: lat x lon)\n",
    "            config: GNN configuration dictionary\n",
    "        \"\"\"\n",
    "        self.lat_coords = lat_coords\n",
    "        self.lon_coords = lon_coords\n",
    "        self.elevation = elevation\n",
    "        self.config = config\n",
    "        \n",
    "        self.n_lat = len(lat_coords)\n",
    "        self.n_lon = len(lon_coords)\n",
    "        self.n_nodes = self.n_lat * self.n_lon\n",
    "        \n",
    "        # Create node positions (lat, lon for each grid cell)\n",
    "        self.node_positions = self._create_node_positions()\n",
    "        \n",
    "        # Flatten elevation for node features\n",
    "        self.node_elevations = elevation.flatten()\n",
    "        \n",
    "        print(f\"SpatialGraphBuilder initialized:\")\n",
    "        print(f\"  Nodes: {self.n_nodes} ({self.n_lat} x {self.n_lon})\")\n",
    "    \n",
    "    def _create_node_positions(self) -> np.ndarray:\n",
    "        \"\"\"Create (lat, lon) positions for each node.\"\"\"\n",
    "        positions = []\n",
    "        for i, lat in enumerate(self.lat_coords):\n",
    "            for j, lon in enumerate(self.lon_coords):\n",
    "                positions.append([lat, lon])\n",
    "        return np.array(positions)\n",
    "    \n",
    "    def _node_index(self, i: int, j: int) -> int:\n",
    "        \"\"\"Convert (lat_idx, lon_idx) to flat node index.\"\"\"\n",
    "        return i * self.n_lon + j\n",
    "    \n",
    "    def _flat_to_grid(self, idx: int) -> Tuple[int, int]:\n",
    "        \"\"\"Convert flat node index to (lat_idx, lon_idx).\"\"\"\n",
    "        return idx // self.n_lon, idx % self.n_lon\n",
    "    \n",
    "    def compute_distance_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute pairwise distance matrix between all nodes.\n",
    "        Uses Haversine formula for geographic coordinates.\n",
    "        \"\"\"\n",
    "        # Convert to radians\n",
    "        pos_rad = np.radians(self.node_positions)\n",
    "        \n",
    "        # Haversine distance\n",
    "        lat1 = pos_rad[:, 0:1]\n",
    "        lon1 = pos_rad[:, 1:2]\n",
    "        lat2 = pos_rad[:, 0:1].T\n",
    "        lon2 = pos_rad[:, 1:2].T\n",
    "        \n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        \n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(a))\n",
    "        \n",
    "        # Earth radius in km\n",
    "        R = 6371.0\n",
    "        distance_km = R * c\n",
    "        \n",
    "        return distance_km\n",
    "    \n",
    "    def compute_elevation_similarity(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute elevation similarity matrix.\n",
    "        Similarity = exp(-|elev_i - elev_j| / scale)\n",
    "        \"\"\"\n",
    "        elev = self.node_elevations.reshape(-1, 1)\n",
    "        elev_diff = np.abs(elev - elev.T)\n",
    "        \n",
    "        # Normalize by elevation range with configurable scale\n",
    "        elev_range = elev.max() - elev.min() + 1e-6\n",
    "        elevation_scale = self.config.get('elevation_scale', 0.2)\n",
    "        similarity = np.exp(-elev_diff / (elev_range * elevation_scale))\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    def compute_correlation_matrix(self, precip_series: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute precipitation correlation matrix.\n",
    "        \n",
    "        Args:\n",
    "            precip_series: Precipitation time series (T, lat, lon)\n",
    "        Returns:\n",
    "            correlation: (n_nodes, n_nodes) correlation matrix\n",
    "        \"\"\"\n",
    "        # Reshape to (T, n_nodes)\n",
    "        T = precip_series.shape[0]\n",
    "        precip_flat = precip_series.reshape(T, -1)\n",
    "        \n",
    "        # Compute correlation matrix\n",
    "        correlation = np.corrcoef(precip_flat.T)\n",
    "        \n",
    "        # Handle NaN values\n",
    "        correlation = np.nan_to_num(correlation, nan=0.0)\n",
    "        \n",
    "        return correlation\n",
    "    \n",
    "    def build_adjacency_matrix(self, precip_series: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Build the weighted adjacency matrix combining all edge types.\n",
    "        \n",
    "        Args:\n",
    "            precip_series: Optional precipitation series for correlation edges\n",
    "        \n",
    "        Returns:\n",
    "            edge_index: (2, num_edges) array of edge indices\n",
    "            edge_weight: (num_edges,) array of edge weights\n",
    "        \"\"\"\n",
    "        max_neighbors = self.config.get('max_neighbors', 8)\n",
    "        edge_threshold = self.config.get('edge_threshold', 0.5)\n",
    "        \n",
    "        # Get configurable parameters (with defaults)\n",
    "        distance_scale = self.config.get('distance_scale_km', 10.0)\n",
    "        elevation_weight = self.config.get('elevation_weight', 0.3)\n",
    "        correlation_weight = self.config.get('correlation_weight', 0.5)\n",
    "        min_edge_weight = self.config.get('min_edge_weight', 0.01)\n",
    "        \n",
    "        # Initialize adjacency with zeros\n",
    "        adj = np.zeros((self.n_nodes, self.n_nodes))\n",
    "        \n",
    "        # 1. Distance-based edges (k-NN)\n",
    "        if self.config.get('use_distance_edges', True):\n",
    "            dist_matrix = self.compute_distance_matrix()\n",
    "            \n",
    "            # Convert distance to similarity (inverse distance)\n",
    "            dist_matrix[dist_matrix == 0] = 1e-6  # Avoid division by zero\n",
    "            dist_similarity = 1.0 / (1.0 + dist_matrix / distance_scale)\n",
    "            \n",
    "            # Keep only k nearest neighbors\n",
    "            for i in range(self.n_nodes):\n",
    "                # Find k nearest neighbors (excluding self)\n",
    "                neighbors = np.argsort(dist_matrix[i])[:max_neighbors + 1]\n",
    "                neighbors = neighbors[neighbors != i][:max_neighbors]\n",
    "                adj[i, neighbors] += dist_similarity[i, neighbors]\n",
    "            \n",
    "            print(f\"  Distance edges added (k={max_neighbors}, scale={distance_scale}km)\")\n",
    "        \n",
    "        # 2. Elevation-based edges\n",
    "        if self.config.get('use_elevation_edges', True):\n",
    "            elev_sim = self.compute_elevation_similarity()\n",
    "            \n",
    "            # Add elevation similarity to existing edges\n",
    "            adj += elev_sim * elevation_weight\n",
    "            print(f\"  Elevation edges added (weight={elevation_weight})\")\n",
    "        \n",
    "        # 3. Correlation-based edges\n",
    "        if self.config.get('use_correlation_edges', False) and precip_series is not None:\n",
    "            corr_matrix = self.compute_correlation_matrix(precip_series)\n",
    "            \n",
    "            # Only keep strong positive correlations\n",
    "            corr_edges = np.maximum(corr_matrix - edge_threshold, 0)\n",
    "            adj += corr_edges * correlation_weight\n",
    "            print(f\"  Correlation edges added (threshold={edge_threshold}, weight={correlation_weight})\")\n",
    "        \n",
    "        # Remove self-loops (GCN will add them back if needed)\n",
    "        np.fill_diagonal(adj, 0)\n",
    "        \n",
    "        # Normalize\n",
    "        adj_max = adj.max()\n",
    "        if adj_max > 0:\n",
    "            adj = adj / adj_max\n",
    "        \n",
    "        # Symmetrize\n",
    "        adj = (adj + adj.T) / 2\n",
    "        \n",
    "        # Convert to sparse format (edge_index, edge_weight)\n",
    "        edge_index = []\n",
    "        edge_weight = []\n",
    "        \n",
    "        for i in range(self.n_nodes):\n",
    "            for j in range(self.n_nodes):\n",
    "                if adj[i, j] > min_edge_weight:\n",
    "                    edge_index.append([i, j])\n",
    "                    edge_weight.append(adj[i, j])\n",
    "        \n",
    "        edge_index = np.array(edge_index).T  # Shape: (2, num_edges)\n",
    "        edge_weight = np.array(edge_weight)  # Shape: (num_edges,)\n",
    "        \n",
    "        print(f\"  Total edges: {len(edge_weight)}\")\n",
    "        print(f\"  Average degree: {len(edge_weight) / self.n_nodes:.2f}\")\n",
    "        \n",
    "        return edge_index, edge_weight\n",
    "    \n",
    "    def get_node_features(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get static node features (position, elevation).\n",
    "        \n",
    "        Returns:\n",
    "            features: (n_nodes, 3) array with [lat, lon, elevation]\n",
    "        \"\"\"\n",
    "        features = np.column_stack([\n",
    "            self.node_positions,\n",
    "            self.node_elevations\n",
    "        ])\n",
    "        return features\n",
    "\n",
    "\n",
    "def create_v5_graph(lat_coords: np.ndarray, lon_coords: np.ndarray,\n",
    "                    elevation: np.ndarray, precip_series: np.ndarray = None,\n",
    "                    max_edges: int = 500000) -> Tuple[torch.Tensor, torch.Tensor, SpatialGraphBuilder]:\n",
    "    \"\"\"\n",
    "    Create spatial graph for V5 with memory-efficient edge pruning.\n",
    "    \n",
    "    Args:\n",
    "        lat_coords: Latitude coordinates\n",
    "        lon_coords: Longitude coordinates\n",
    "        elevation: Elevation grid\n",
    "        precip_series: Optional precipitation time series\n",
    "        max_edges: Maximum number of edges (for memory efficiency)\n",
    "    \n",
    "    Returns:\n",
    "        edge_index: (2, E) tensor\n",
    "        edge_weight: (E,) tensor\n",
    "        graph_builder: SpatialGraphBuilder instance\n",
    "    \"\"\"\n",
    "    # Graph configuration\n",
    "    graph_config = {\n",
    "        'max_neighbors': 8,\n",
    "        'edge_threshold': 0.5,\n",
    "        'distance_scale_km': 10.0,\n",
    "        'elevation_weight': 0.3,\n",
    "        'elevation_scale': 0.2,\n",
    "        'correlation_weight': 0.5,\n",
    "        'min_edge_weight': 0.01,\n",
    "        'use_distance_edges': True,\n",
    "        'use_elevation_edges': True,\n",
    "        'use_correlation_edges': precip_series is not None\n",
    "    }\n",
    "    \n",
    "    # Build graph\n",
    "    graph_builder = SpatialGraphBuilder(lat_coords, lon_coords, elevation, graph_config)\n",
    "    edge_index, edge_weight = graph_builder.build_adjacency_matrix(precip_series)\n",
    "    \n",
    "    # Prune edges if too many (for memory efficiency)\n",
    "    if len(edge_weight) > max_edges:\n",
    "        print(f\"\\nPruning edges from {len(edge_weight):,} to {max_edges:,}...\")\n",
    "        top_indices = np.argsort(edge_weight)[-max_edges:]\n",
    "        edge_index = edge_index[:, top_indices]\n",
    "        edge_weight = edge_weight[top_indices]\n",
    "        print(f\"  Reduced to {len(edge_weight):,} edges\")\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    edge_index_tensor = torch.tensor(edge_index, dtype=torch.long)\n",
    "    edge_weight_tensor = torch.tensor(edge_weight, dtype=torch.float32)\n",
    "    \n",
    "    return edge_index_tensor, edge_weight_tensor, graph_builder\n",
    "\n",
    "\n",
    "# Test graph construction with dummy data\n",
    "print(\"Testing SpatialGraphBuilder...\")\n",
    "dummy_lat = np.linspace(5.0, 8.0, 61)\n",
    "dummy_lon = np.linspace(-74.0, -71.0, 65)\n",
    "dummy_elevation = np.random.uniform(500, 4000, (61, 65))\n",
    "\n",
    "edge_index, edge_weight, graph_builder = create_v5_graph(\n",
    "    dummy_lat, dummy_lon, dummy_elevation, max_edges=100000\n",
    ")\n",
    "\n",
    "print(f\"\\nGraph created:\")\n",
    "print(f\"  Nodes: {graph_builder.n_nodes}\")\n",
    "print(f\"  Edges: {edge_index.shape[1]}\")\n",
    "print(f\"  Edge index shape: {edge_index.shape}\")\n",
    "print(f\"  Edge weight shape: {edge_weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading\n",
    "\n",
    "Real data loading pipeline for V5:\n",
    "- **BASIC features** → ConvLSTM branch (grid format: B, T, C, H, W)\n",
    "- **KCE features** → GNN branch (graph format: B, T, N, C)\n",
    "\n",
    "### Feature Sets\n",
    "\n",
    "| Set | Features | Count | Branch |\n",
    "|-----|----------|-------|--------|\n",
    "| BASIC | temporal + precip stats + base topo | 12 | ConvLSTM |\n",
    "| KCE | BASIC + elevation clusters | 15 | GNN |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.1 Data Loading Configuration\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for data loading.\"\"\"\n",
    "    \n",
    "    # Feature sets for each branch\n",
    "    basic_features: List[str] = field(default_factory=lambda: [\n",
    "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "        'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "        'elevation', 'slope', 'aspect'\n",
    "    ])\n",
    "    \n",
    "    kce_features: List[str] = field(default_factory=lambda: [\n",
    "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
    "        'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
    "        'elevation', 'slope', 'aspect', 'elev_high', 'elev_med', 'elev_low'\n",
    "    ])\n",
    "    \n",
    "    # Data parameters\n",
    "    input_window: int = 60  # 60 months input (5 years)\n",
    "    train_val_split: float = 0.9\n",
    "    target_var: str = 'total_precipitation'\n",
    "    \n",
    "    # Light mode settings\n",
    "    light_mode: bool = False\n",
    "    light_grid_size: int = 5\n",
    "\n",
    "\n",
    "# Initialize data config\n",
    "data_config = DataConfig()\n",
    "print(f\"Data Configuration:\")\n",
    "print(f\"  BASIC features: {len(data_config.basic_features)}\")\n",
    "print(f\"  KCE features: {len(data_config.kce_features)}\")\n",
    "print(f\"  Input window: {data_config.input_window} months\")\n",
    "print(f\"  Train/Val split: {data_config.train_val_split}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.2 Dataset Loading Functions\n",
    "# =============================================================================\n",
    "\n",
    "import xarray as xr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def load_dataset(data_path: Path, light_mode: bool = False, \n",
    "                 light_grid_size: int = 5) -> Tuple[xr.Dataset, np.ndarray, np.ndarray, int, int]:\n",
    "    \"\"\"\n",
    "    Load NetCDF dataset with optional light mode for testing.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to NetCDF file\n",
    "        light_mode: If True, use subset of grid for faster testing\n",
    "        light_grid_size: Size of subset grid (e.g., 5 for 5x5)\n",
    "    \n",
    "    Returns:\n",
    "        ds: xarray Dataset\n",
    "        lat_coords: Latitude coordinates\n",
    "        lon_coords: Longitude coordinates  \n",
    "        n_lat: Number of latitude points\n",
    "        n_lon: Number of longitude points\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from: {data_path}\")\n",
    "    \n",
    "    # Try different engines\n",
    "    engines = ['h5netcdf', 'netcdf4', 'scipy']\n",
    "    ds = None\n",
    "    \n",
    "    for engine in engines:\n",
    "        try:\n",
    "            print(f\"  Trying engine: {engine}...\")\n",
    "            ds = xr.open_dataset(data_path, engine=engine)\n",
    "            print(f\"  Success with engine: {engine}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"  Engine {engine} failed: {type(e).__name__}\")\n",
    "            continue\n",
    "    \n",
    "    if ds is None:\n",
    "        raise RuntimeError(f\"Could not open dataset with any engine.\")\n",
    "    \n",
    "    print(f\"  Original shape: time={len(ds.time)}, lat={len(ds.latitude)}, lon={len(ds.longitude)}\")\n",
    "    \n",
    "    # Optional light mode: subset to center region\n",
    "    if light_mode:\n",
    "        lat_center = len(ds.latitude) // 2\n",
    "        lon_center = len(ds.longitude) // 2\n",
    "        lat_start = lat_center - light_grid_size // 2\n",
    "        lat_end = lat_start + light_grid_size\n",
    "        lon_start = lon_center - light_grid_size // 2\n",
    "        lon_end = lon_start + light_grid_size\n",
    "        \n",
    "        ds = ds.isel(\n",
    "            latitude=slice(lat_start, lat_end),\n",
    "            longitude=slice(lon_start, lon_end)\n",
    "        )\n",
    "        print(f\"  Light mode applied: lat={len(ds.latitude)}, lon={len(ds.longitude)}\")\n",
    "    \n",
    "    n_lat = len(ds.latitude)\n",
    "    n_lon = len(ds.longitude)\n",
    "    lat_coords = ds.latitude.values\n",
    "    lon_coords = ds.longitude.values\n",
    "    \n",
    "    print(f\"  Final grid: {n_lat} x {n_lon} = {n_lat * n_lon} nodes\")\n",
    "    \n",
    "    return ds, lat_coords, lon_coords, n_lat, n_lon\n",
    "\n",
    "\n",
    "def extract_features(ds: xr.Dataset, feature_list: List[str], \n",
    "                     n_lat: int, n_lon: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract and stack features from dataset.\n",
    "    \n",
    "    Args:\n",
    "        ds: xarray Dataset\n",
    "        feature_list: List of feature names\n",
    "        n_lat: Number of latitude points\n",
    "        n_lon: Number of longitude points\n",
    "    \n",
    "    Returns:\n",
    "        X: Feature array (T, lat, lon, n_features)\n",
    "    \"\"\"\n",
    "    feature_arrays = []\n",
    "    \n",
    "    for feat in feature_list:\n",
    "        if feat in ds.data_vars:\n",
    "            arr = ds[feat].values\n",
    "        elif feat in ds.coords:\n",
    "            if feat == 'time':\n",
    "                continue\n",
    "            # Broadcast coordinate to full grid\n",
    "            arr = np.broadcast_to(\n",
    "                ds.coords[feat].values.reshape(-1, 1, 1),\n",
    "                (len(ds.time), n_lat, n_lon)\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  WARNING: Feature '{feat}' not found, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Ensure 3D shape (T, lat, lon)\n",
    "        if len(arr.shape) == 2:\n",
    "            # Static feature: broadcast to time\n",
    "            arr = np.broadcast_to(arr, (len(ds.time), n_lat, n_lon))\n",
    "        \n",
    "        feature_arrays.append(arr)\n",
    "    \n",
    "    # Stack features: (T, lat, lon, n_features)\n",
    "    X = np.stack(feature_arrays, axis=-1).astype(np.float32)\n",
    "    \n",
    "    # Handle NaN\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def windowed_arrays(X: np.ndarray, y: np.ndarray, input_window: int,\n",
    "                   horizon: int, start_indices: List[int] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create windowed arrays for sequence-to-sequence learning.\n",
    "    \n",
    "    Args:\n",
    "        X: Features (T, lat, lon, n_features)\n",
    "        y: Target (T, lat, lon)\n",
    "        input_window: Input sequence length\n",
    "        horizon: Prediction horizon\n",
    "        start_indices: Optional list of starting indices for windows\n",
    "    \n",
    "    Returns:\n",
    "        X_windows: (N, input_window, lat, lon, n_features)\n",
    "        y_windows: (N, horizon, lat, lon)\n",
    "    \"\"\"\n",
    "    T = X.shape[0]\n",
    "    \n",
    "    if start_indices is None:\n",
    "        max_start = T - input_window - horizon + 1\n",
    "        start_indices = list(range(max_start))\n",
    "    \n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "    \n",
    "    for start in start_indices:\n",
    "        x_end = start + input_window\n",
    "        y_start = x_end\n",
    "        y_end = y_start + horizon\n",
    "        \n",
    "        if y_end <= T:\n",
    "            X_windows.append(X[start:x_end])\n",
    "            y_windows.append(y[y_start:y_end])\n",
    "    \n",
    "    return np.array(X_windows), np.array(y_windows)\n",
    "\n",
    "\n",
    "def compute_split_indices(T: int, input_window: int, horizon: int, \n",
    "                          split_ratio: float) -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Compute train/validation split indices for time series.\n",
    "    \n",
    "    Args:\n",
    "        T: Total timesteps\n",
    "        input_window: Input sequence length\n",
    "        horizon: Prediction horizon\n",
    "        split_ratio: Fraction for training (e.g., 0.9)\n",
    "    \n",
    "    Returns:\n",
    "        train_indices: Starting indices for training windows\n",
    "        val_indices: Starting indices for validation windows\n",
    "    \"\"\"\n",
    "    max_start = T - input_window - horizon + 1\n",
    "    n_train = int(max_start * split_ratio)\n",
    "    \n",
    "    train_indices = list(range(n_train))\n",
    "    val_start = n_train\n",
    "    val_indices = list(range(val_start, max_start))\n",
    "    \n",
    "    return train_indices, val_indices\n",
    "\n",
    "\n",
    "print(\"Data loading functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.3 V5 Dual Dataset (Real Data)\n",
    "# =============================================================================\n",
    "\n",
    "class V5DualDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset providing both BASIC (grid) and KCE (graph) features for V5.\n",
    "    \n",
    "    Handles:\n",
    "    - BASIC features for ConvLSTM branch (B, T, C, H, W)\n",
    "    - KCE features for GNN branch (B, T, N, C)\n",
    "    - Context features for meta-learner\n",
    "    - Target precipitation values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X_basic: np.ndarray, X_kce: np.ndarray, \n",
    "                 y: np.ndarray, elevation: np.ndarray,\n",
    "                 horizon: int, scaler_y: StandardScaler = None):\n",
    "        \"\"\"\n",
    "        Initialize V5 dataset.\n",
    "        \n",
    "        Args:\n",
    "            X_basic: BASIC features (N, T, lat, lon, C_basic)\n",
    "            X_kce: KCE features (N, T, lat, lon, C_kce)\n",
    "            y: Target precipitation (N, H, lat, lon)\n",
    "            elevation: Mean elevation for context (N,) or scalar\n",
    "            horizon: Prediction horizon\n",
    "            scaler_y: Scaler for target (for inverse transform)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_samples = X_basic.shape[0]\n",
    "        self.seq_len = X_basic.shape[1]\n",
    "        self.n_lat = X_basic.shape[2]\n",
    "        self.n_lon = X_basic.shape[3]\n",
    "        self.n_nodes = self.n_lat * self.n_lon\n",
    "        self.horizon = horizon\n",
    "        self.scaler_y = scaler_y\n",
    "        \n",
    "        # Store elevation for context\n",
    "        if isinstance(elevation, np.ndarray) and len(elevation.shape) > 0:\n",
    "            self.mean_elevation = elevation.mean() if len(elevation.shape) > 1 else elevation.mean()\n",
    "        else:\n",
    "            self.mean_elevation = float(elevation)\n",
    "        \n",
    "        # BASIC features: (N, T, lat, lon, C) -> (N, T, C, lat, lon) for ConvLSTM\n",
    "        self.X_basic = torch.tensor(\n",
    "            np.transpose(X_basic, (0, 1, 4, 2, 3)), \n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # KCE features: (N, T, lat, lon, C) -> (N, T, N_nodes, C) for GNN\n",
    "        X_kce_reshaped = X_kce.reshape(\n",
    "            X_kce.shape[0], X_kce.shape[1], -1, X_kce.shape[-1]\n",
    "        )\n",
    "        self.X_kce = torch.tensor(X_kce_reshaped, dtype=torch.float32)\n",
    "        \n",
    "        # Target: aggregate over horizon for single prediction\n",
    "        # Shape: (N, H, lat, lon) -> (N, 1) mean precipitation\n",
    "        self.y = torch.tensor(\n",
    "            y[:, 0, :, :].mean(axis=(1, 2), keepdims=True),  # First horizon step, spatial mean\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        \n",
    "        # Full spatial target for evaluation\n",
    "        self.y_spatial = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        print(f\"V5DualDataset initialized:\")\n",
    "        print(f\"  Samples: {self.n_samples}\")\n",
    "        print(f\"  X_basic shape: {self.X_basic.shape} (N, T, C, H, W)\")\n",
    "        print(f\"  X_kce shape: {self.X_kce.shape} (N, T, N, C)\")\n",
    "        print(f\"  y shape: {self.y.shape}\")\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        # Create context features\n",
    "        # [mean_elevation (normalized), season_sin, season_cos, horizon (normalized)]\n",
    "        # Using placeholder values - actual values come from metadata\n",
    "        month = (idx % 12)  # Approximate month from index\n",
    "        season_sin = np.sin(2 * np.pi * month / 12)\n",
    "        season_cos = np.cos(2 * np.pi * month / 12)\n",
    "        \n",
    "        context = torch.tensor([\n",
    "            self.mean_elevation / 4000.0,  # Normalize elevation (max ~4000m in Boyaca)\n",
    "            season_sin,\n",
    "            season_cos,\n",
    "            self.horizon / 12.0  # Normalize horizon\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            'x_basic': self.X_basic[idx],      # (T, C, H, W)\n",
    "            'x_kce': self.X_kce[idx],          # (T, N, C)\n",
    "            'context': context,                 # (4,)\n",
    "            'y': self.y[idx],                  # (1,)\n",
    "            'y_spatial': self.y_spatial[idx]   # (H, lat, lon)\n",
    "        }\n",
    "\n",
    "\n",
    "def prepare_v5_data(ds: xr.Dataset, data_config: DataConfig, \n",
    "                    n_lat: int, n_lon: int, horizon: int\n",
    "                   ) -> Tuple[V5DualDataset, V5DualDataset, np.ndarray, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Prepare V5 dual datasets for training and validation.\n",
    "    \n",
    "    Args:\n",
    "        ds: xarray Dataset\n",
    "        data_config: DataConfig with feature lists\n",
    "        n_lat: Number of latitude points\n",
    "        n_lon: Number of longitude points\n",
    "        horizon: Prediction horizon\n",
    "    \n",
    "    Returns:\n",
    "        train_dataset: V5DualDataset for training\n",
    "        val_dataset: V5DualDataset for validation\n",
    "        elevation: Elevation array for graph construction\n",
    "        scaler_y: Target scaler\n",
    "    \"\"\"\n",
    "    print(f\"\\nPreparing V5 data for horizon H={horizon}...\")\n",
    "    \n",
    "    # Extract features\n",
    "    print(f\"  Extracting BASIC features ({len(data_config.basic_features)} features)...\")\n",
    "    X_basic = extract_features(ds, data_config.basic_features, n_lat, n_lon)\n",
    "    print(f\"    X_basic shape: {X_basic.shape}\")\n",
    "    \n",
    "    print(f\"  Extracting KCE features ({len(data_config.kce_features)} features)...\")\n",
    "    X_kce = extract_features(ds, data_config.kce_features, n_lat, n_lon)\n",
    "    print(f\"    X_kce shape: {X_kce.shape}\")\n",
    "    \n",
    "    # Target\n",
    "    y = ds[data_config.target_var].values.astype(np.float32)\n",
    "    y = np.nan_to_num(y, nan=0.0)\n",
    "    print(f\"  Target shape: {y.shape}\")\n",
    "    \n",
    "    # Elevation for context and graph\n",
    "    if 'elevation' in ds.data_vars:\n",
    "        elevation = ds['elevation'].values\n",
    "        if len(elevation.shape) == 3:\n",
    "            elevation = elevation[0]  # Take first timestep (static)\n",
    "    elif 'elevation' in ds.coords:\n",
    "        elevation = ds.coords['elevation'].values\n",
    "    else:\n",
    "        print(\"  WARNING: Elevation not found, using zeros\")\n",
    "        elevation = np.zeros((n_lat, n_lon))\n",
    "    \n",
    "    # Compute split indices\n",
    "    T = len(ds.time)\n",
    "    train_idx, val_idx = compute_split_indices(\n",
    "        T, data_config.input_window, horizon, data_config.train_val_split\n",
    "    )\n",
    "    print(f\"  Train windows: {len(train_idx)}, Val windows: {len(val_idx)}\")\n",
    "    \n",
    "    # Create windowed arrays\n",
    "    X_basic_tr, y_tr = windowed_arrays(X_basic, y, data_config.input_window, horizon, train_idx)\n",
    "    X_basic_va, y_va = windowed_arrays(X_basic, y, data_config.input_window, horizon, val_idx)\n",
    "    X_kce_tr, _ = windowed_arrays(X_kce, y, data_config.input_window, horizon, train_idx)\n",
    "    X_kce_va, _ = windowed_arrays(X_kce, y, data_config.input_window, horizon, val_idx)\n",
    "    \n",
    "    print(f\"  X_basic_tr: {X_basic_tr.shape}, X_kce_tr: {X_kce_tr.shape}\")\n",
    "    print(f\"  X_basic_va: {X_basic_va.shape}, X_kce_va: {X_kce_va.shape}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler_basic = StandardScaler()\n",
    "    scaler_kce = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    # Fit on training data (flatten for scaling)\n",
    "    X_basic_tr_flat = X_basic_tr.reshape(-1, X_basic_tr.shape[-1])\n",
    "    X_kce_tr_flat = X_kce_tr.reshape(-1, X_kce_tr.shape[-1])\n",
    "    y_tr_flat = y_tr.reshape(-1, 1)\n",
    "    \n",
    "    scaler_basic.fit(X_basic_tr_flat)\n",
    "    scaler_kce.fit(X_kce_tr_flat)\n",
    "    scaler_y.fit(y_tr_flat)\n",
    "    \n",
    "    # Transform\n",
    "    X_basic_tr_sc = scaler_basic.transform(X_basic_tr.reshape(-1, X_basic_tr.shape[-1])).reshape(X_basic_tr.shape)\n",
    "    X_basic_va_sc = scaler_basic.transform(X_basic_va.reshape(-1, X_basic_va.shape[-1])).reshape(X_basic_va.shape)\n",
    "    X_kce_tr_sc = scaler_kce.transform(X_kce_tr.reshape(-1, X_kce_tr.shape[-1])).reshape(X_kce_tr.shape)\n",
    "    X_kce_va_sc = scaler_kce.transform(X_kce_va.reshape(-1, X_kce_va.shape[-1])).reshape(X_kce_va.shape)\n",
    "    y_tr_sc = scaler_y.transform(y_tr.reshape(-1, 1)).reshape(y_tr.shape)\n",
    "    y_va_sc = scaler_y.transform(y_va.reshape(-1, 1)).reshape(y_va.shape)\n",
    "    \n",
    "    print(f\"  Scaling complete.\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = V5DualDataset(\n",
    "        X_basic_tr_sc, X_kce_tr_sc, y_tr_sc, elevation, horizon, scaler_y\n",
    "    )\n",
    "    val_dataset = V5DualDataset(\n",
    "        X_basic_va_sc, X_kce_va_sc, y_va_sc, elevation, horizon, scaler_y\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, elevation, scaler_y\n",
    "\n",
    "\n",
    "print(\"V5DualDataset and prepare_v5_data defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.4 Load Real Data (or Synthetic Fallback)\n",
    "# =============================================================================\n",
    "\n",
    "# Data paths\n",
    "if IN_COLAB:\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    DATA_FILE = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "else:\n",
    "    BASE_PATH = Path('.')\n",
    "    DATA_FILE = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
    "\n",
    "# Configuration\n",
    "USE_LIGHT_MODE = True  # Set to False for full grid\n",
    "LIGHT_GRID_SIZE = 5     # 5x5 for testing\n",
    "HORIZON = 12            # Prediction horizon\n",
    "\n",
    "print(f\"Data file: {DATA_FILE}\")\n",
    "print(f\"Light mode: {USE_LIGHT_MODE} (grid: {LIGHT_GRID_SIZE}x{LIGHT_GRID_SIZE})\")\n",
    "print(f\"Horizon: {HORIZON} months\")\n",
    "\n",
    "# Try to load real data, fall back to synthetic\n",
    "USE_REAL_DATA = False\n",
    "ds = None\n",
    "train_dataset = None\n",
    "val_dataset = None\n",
    "elevation = None\n",
    "\n",
    "try:\n",
    "    if DATA_FILE.exists():\n",
    "        print(\"\\nLoading real data...\")\n",
    "        ds, lat_coords, lon_coords, n_lat, n_lon = load_dataset(\n",
    "            DATA_FILE, \n",
    "            light_mode=USE_LIGHT_MODE, \n",
    "            light_grid_size=LIGHT_GRID_SIZE\n",
    "        )\n",
    "        \n",
    "        # Update config with actual dimensions\n",
    "        config.n_lat = n_lat\n",
    "        config.n_lon = n_lon\n",
    "        config.n_nodes = n_lat * n_lon\n",
    "        \n",
    "        # Prepare datasets\n",
    "        train_dataset, val_dataset, elevation, scaler_y = prepare_v5_data(\n",
    "            ds, data_config, n_lat, n_lon, HORIZON\n",
    "        )\n",
    "        \n",
    "        USE_REAL_DATA = True\n",
    "        print(f\"\\nReal data loaded successfully!\")\n",
    "    else:\n",
    "        print(f\"\\nData file not found: {DATA_FILE}\")\n",
    "        print(\"Using synthetic data for testing...\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading real data: {e}\")\n",
    "    print(\"Using synthetic data for testing...\")\n",
    "\n",
    "# Fallback to synthetic data\n",
    "if not USE_REAL_DATA:\n",
    "    print(\"\\nGenerating synthetic data for prototype testing...\")\n",
    "    \n",
    "    if USE_LIGHT_MODE:\n",
    "        config.n_lat = LIGHT_GRID_SIZE\n",
    "        config.n_lon = LIGHT_GRID_SIZE\n",
    "        config.n_nodes = LIGHT_GRID_SIZE * LIGHT_GRID_SIZE\n",
    "    \n",
    "    # Create synthetic datasets\n",
    "    n_train = 100\n",
    "    n_val = 20\n",
    "    seq_len = 60\n",
    "    \n",
    "    # BASIC: (N, T, C, H, W) for ConvLSTM\n",
    "    X_basic_tr = np.random.randn(n_train, seq_len, config.n_lat, config.n_lon, config.n_basic_features).astype(np.float32)\n",
    "    X_basic_va = np.random.randn(n_val, seq_len, config.n_lat, config.n_lon, config.n_basic_features).astype(np.float32)\n",
    "    \n",
    "    # KCE: (N, T, lat, lon, C) for GNN\n",
    "    X_kce_tr = np.random.randn(n_train, seq_len, config.n_lat, config.n_lon, config.n_kce_features).astype(np.float32)\n",
    "    X_kce_va = np.random.randn(n_val, seq_len, config.n_lat, config.n_lon, config.n_kce_features).astype(np.float32)\n",
    "    \n",
    "    # Target: (N, H, lat, lon)\n",
    "    y_tr = np.random.randn(n_train, HORIZON, config.n_lat, config.n_lon).astype(np.float32)\n",
    "    y_va = np.random.randn(n_val, HORIZON, config.n_lat, config.n_lon).astype(np.float32)\n",
    "    \n",
    "    # Elevation\n",
    "    elevation = np.random.uniform(500, 4000, (config.n_lat, config.n_lon)).astype(np.float32)\n",
    "    \n",
    "    # Coordinates\n",
    "    lat_coords = np.linspace(5.0, 8.0, config.n_lat)\n",
    "    lon_coords = np.linspace(-74.0, -71.0, config.n_lon)\n",
    "    \n",
    "    train_dataset = V5DualDataset(X_basic_tr, X_kce_tr, y_tr, elevation, HORIZON)\n",
    "    val_dataset = V5DualDataset(X_basic_va, X_kce_va, y_va, elevation, HORIZON)\n",
    "    \n",
    "    print(f\"Synthetic data generated.\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"DATA LOADING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Data source: {'Real (CHIRPS/SRTM)' if USE_REAL_DATA else 'Synthetic'}\")\n",
    "print(f\"Grid: {config.n_lat} x {config.n_lon} = {config.n_nodes} nodes\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(f\"Horizon: {HORIZON} months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4.5 Build Graph and Create Data Loaders\n",
    "# =============================================================================\n",
    "\n",
    "# Build spatial graph\n",
    "print(\"Building spatial graph...\")\n",
    "\n",
    "# Get precipitation series for correlation edges (if real data)\n",
    "if USE_REAL_DATA and ds is not None:\n",
    "    precip_var = 'total_precipitation' if 'total_precipitation' in ds.data_vars else list(ds.data_vars)[0]\n",
    "    precip_series = ds[precip_var].values\n",
    "else:\n",
    "    # Synthetic: no correlation edges\n",
    "    precip_series = None\n",
    "\n",
    "# Create graph\n",
    "edge_index, edge_weight, graph_builder = create_v5_graph(\n",
    "    lat_coords, lon_coords, elevation, \n",
    "    precip_series=precip_series,\n",
    "    max_edges=100000 if USE_LIGHT_MODE else 500000\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "edge_index = edge_index.to(DEVICE)\n",
    "edge_weight = edge_weight.to(DEVICE)\n",
    "\n",
    "print(f\"\\nGraph Summary:\")\n",
    "print(f\"  Nodes: {config.n_nodes}\")\n",
    "print(f\"  Edges: {edge_index.shape[1]}\")\n",
    "print(f\"  Avg degree: {edge_index.shape[1] / config.n_nodes:.2f}\")\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = config.training_config['batch_size']\n",
    "if USE_LIGHT_MODE:\n",
    "    BATCH_SIZE = min(BATCH_SIZE, 8)  # Smaller batch for light mode\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nData Loaders:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Test batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample Batch Shapes:\")\n",
    "for key, value in sample_batch.items():\n",
    "    print(f\"  {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Complete training pipeline with:\n",
    "- Separate learning rates for branches and meta-learner\n",
    "- Early stopping with patience\n",
    "- Branch contribution logging for interpretability\n",
    "- Checkpoint saving for best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5.1 V5 Trainer with Full Functionality\n",
    "# =============================================================================\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "class V5Trainer:\n",
    "    \"\"\"\n",
    "    Trainer for V5 GNN-ConvLSTM Stacking Model.\n",
    "    \n",
    "    Features:\n",
    "    - Separate learning rates for branches and fusion/meta-learner\n",
    "    - Early stopping with patience\n",
    "    - Branch weight tracking for interpretability\n",
    "    - Checkpoint saving\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: V5StackingModel, config: V5Config, \n",
    "                 edge_index: torch.Tensor, edge_weight: torch.Tensor,\n",
    "                 output_dir: Path = None):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.edge_index = edge_index.to(DEVICE)\n",
    "        self.edge_weight = edge_weight.to(DEVICE)\n",
    "        \n",
    "        train_cfg = config.training_config\n",
    "        \n",
    "        # Output directory for checkpoints\n",
    "        self.output_dir = output_dir or Path('models/output/V5_STACKING')\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Optimizer with parameter groups (different LR for branches vs fusion)\n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': model.convlstm_branch.parameters(), \n",
    "             'lr': train_cfg['learning_rate'], 'name': 'convlstm'},\n",
    "            {'params': model.gnn_branch.parameters(), \n",
    "             'lr': train_cfg['learning_rate'], 'name': 'gnn'},\n",
    "            {'params': model.fusion.parameters(), \n",
    "             'lr': train_cfg['learning_rate'] * 0.5, 'name': 'fusion'},\n",
    "            {'params': model.meta_learner.parameters(), \n",
    "             'lr': train_cfg['learning_rate'] * 0.5, 'name': 'meta_learner'}\n",
    "        ], weight_decay=train_cfg['weight_decay'])\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=5\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Training state\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.epoch = 0\n",
    "        \n",
    "        # History tracking\n",
    "        self.history = {\n",
    "            'epoch': [],\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'branch_weights_convlstm': [],\n",
    "            'branch_weights_gnn': [],\n",
    "            'learning_rate': [],\n",
    "            'epoch_time': []\n",
    "        }\n",
    "        \n",
    "    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, np.ndarray]:\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        weight_sum = torch.zeros(2, device=DEVICE)\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x_basic = batch['x_basic'].to(DEVICE)\n",
    "            x_kce = batch['x_kce'].to(DEVICE)\n",
    "            context = batch['context'].to(DEVICE)\n",
    "            y = batch['y'].to(DEVICE)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            pred, aux = self.model(\n",
    "                x_basic, x_kce, self.edge_index, context, self.edge_weight\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.criterion(pred, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Optimizer step\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item()\n",
    "            weight_sum += aux['branch_weights'].mean(dim=0).detach()\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / n_batches\n",
    "        avg_weights = (weight_sum / n_batches).cpu().numpy()\n",
    "        \n",
    "        return avg_loss, avg_weights\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_loader: DataLoader) -> Tuple[float, np.ndarray]:\n",
    "        \"\"\"Validate model.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        weight_sum = torch.zeros(2, device=DEVICE)\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in val_loader:\n",
    "            x_basic = batch['x_basic'].to(DEVICE)\n",
    "            x_kce = batch['x_kce'].to(DEVICE)\n",
    "            context = batch['context'].to(DEVICE)\n",
    "            y = batch['y'].to(DEVICE)\n",
    "            \n",
    "            pred, aux = self.model(\n",
    "                x_basic, x_kce, self.edge_index, context, self.edge_weight\n",
    "            )\n",
    "            \n",
    "            loss = self.criterion(pred, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            weight_sum += aux['branch_weights'].mean(dim=0)\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / n_batches\n",
    "        avg_weights = (weight_sum / n_batches).cpu().numpy()\n",
    "        \n",
    "        return avg_loss, avg_weights\n",
    "    \n",
    "    def save_checkpoint(self, filepath: Path, is_best: bool = False):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': self.epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'config': self.config,\n",
    "            'history': self.history\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        \n",
    "        if is_best:\n",
    "            best_path = self.output_dir / 'v5_best_model.pt'\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"    Best model saved: {best_path}\")\n",
    "    \n",
    "    def load_checkpoint(self, filepath: Path):\n",
    "        \"\"\"Load model checkpoint.\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=DEVICE)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.epoch = checkpoint['epoch']\n",
    "        self.best_val_loss = checkpoint['best_val_loss']\n",
    "        self.history = checkpoint['history']\n",
    "        print(f\"Checkpoint loaded from epoch {self.epoch}\")\n",
    "    \n",
    "    def train(self, train_loader: DataLoader, val_loader: DataLoader,\n",
    "              epochs: int = None, patience: int = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Full training loop with early stopping.\n",
    "        \n",
    "        Args:\n",
    "            train_loader: Training data loader\n",
    "            val_loader: Validation data loader\n",
    "            epochs: Number of epochs (uses config if None)\n",
    "            patience: Early stopping patience (uses config if None)\n",
    "        \n",
    "        Returns:\n",
    "            Training history dictionary\n",
    "        \"\"\"\n",
    "        train_cfg = self.config.training_config\n",
    "        epochs = epochs or train_cfg['epochs']\n",
    "        patience = patience or train_cfg['patience']\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"V5 TRAINING START\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Epochs: {epochs}, Patience: {patience}\")\n",
    "        print(f\"Device: {DEVICE}\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.epoch, epochs):\n",
    "            self.epoch = epoch\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_weights = self.train_epoch(train_loader)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_weights = self.validate(val_loader)\n",
    "            \n",
    "            # Update scheduler\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # Record history\n",
    "            epoch_time = time.time() - epoch_start\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            self.history['epoch'].append(epoch)\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['branch_weights_convlstm'].append(train_weights[0])\n",
    "            self.history['branch_weights_gnn'].append(train_weights[1])\n",
    "            self.history['learning_rate'].append(current_lr)\n",
    "            self.history['epoch_time'].append(epoch_time)\n",
    "            \n",
    "            # Check for improvement\n",
    "            is_best = val_loss < self.best_val_loss\n",
    "            if is_best:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "                  f\"Train: {train_loss:.6f} | Val: {val_loss:.6f} | \"\n",
    "                  f\"W=[{train_weights[0]:.2f}, {train_weights[1]:.2f}] | \"\n",
    "                  f\"LR: {current_lr:.2e} | Time: {epoch_time:.1f}s\"\n",
    "                  f\"{' *' if is_best else ''}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 10 == 0 or is_best:\n",
    "                checkpoint_path = self.output_dir / f'v5_checkpoint_e{epoch+1}.pt'\n",
    "                self.save_checkpoint(checkpoint_path, is_best)\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1} (patience={patience})\")\n",
    "                break\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TRAINING COMPLETE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "        print(f\"Best val loss: {self.best_val_loss:.6f}\")\n",
    "        print(f\"Final branch weights: ConvLSTM={train_weights[0]:.3f}, GNN={train_weights[1]:.3f}\")\n",
    "        \n",
    "        # Save final history\n",
    "        history_path = self.output_dir / 'v5_training_history.json'\n",
    "        with open(history_path, 'w') as f:\n",
    "            # Convert numpy to list for JSON\n",
    "            history_json = {k: [float(v) for v in vals] for k, vals in self.history.items()}\n",
    "            json.dump(history_json, f, indent=2)\n",
    "        print(f\"History saved: {history_path}\")\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "\n",
    "print(\"V5Trainer with full functionality defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Saving and Visualization\n",
    "\n",
    "V4-compatible output format:\n",
    "- metrics_spatial_v5_stacking_h{H}.csv - Main metrics\n",
    "- experiment_state_v5.json - Configuration state\n",
    "- h{H}/{EXPERIMENT}/training_metrics/ - Per-experiment outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.1 Save Results Function (V4 Compatible Format)\n",
    "# =============================================================================\n",
    "\n",
    "def save_v5_results(trainer, config, val_loader, horizon, experiment='BASIC_KCE'):\n",
    "    '''Save results in V4-compatible format for benchmark integration.'''\n",
    "    import json\n",
    "    import gc\n",
    "\n",
    "    # Create output directories (V4 structure)\n",
    "    metrics_dir = OUTPUT_ROOT / f'h{horizon}' / experiment / 'training_metrics'\n",
    "    metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1. Save model checkpoint (V4 format)\n",
    "    model_path = metrics_dir / f'v5_stacking_best_h{horizon}.pt'\n",
    "    checkpoint = {\n",
    "        'epoch': trainer.epoch,\n",
    "        'model_state_dict': trainer.model.state_dict(),\n",
    "        'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "        'best_val_loss': trainer.best_val_loss,\n",
    "        'config': {\n",
    "            'n_lat': config.n_lat,\n",
    "            'n_lon': config.n_lon,\n",
    "            'n_nodes': config.n_nodes,\n",
    "            'n_basic_features': config.n_basic_features,\n",
    "            'n_kce_features': config.n_kce_features,\n",
    "            'horizons': config.horizons\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "\n",
    "    # 2. Save training history JSON\n",
    "    history_path = metrics_dir / f'v5_stacking_history.json'\n",
    "    with open(history_path, 'w') as f:\n",
    "        history_data = {\n",
    "            'model_name': 'V5_Stacking',\n",
    "            'experiment': experiment,\n",
    "            'horizon': horizon,\n",
    "            'best_epoch': int(np.argmin(trainer.history['val_loss'])) + 1,\n",
    "            'total_epochs': len(trainer.history['epoch']),\n",
    "            'best_val_loss': float(trainer.best_val_loss),\n",
    "            'final_train_loss': float(trainer.history['train_loss'][-1]),\n",
    "            'final_val_loss': float(trainer.history['val_loss'][-1]),\n",
    "            'parameters': sum(p.numel() for p in trainer.model.parameters())\n",
    "        }\n",
    "        json.dump(history_data, f, indent=2)\n",
    "    print(f\"History saved: {history_path}\")\n",
    "\n",
    "    # 3. Save training log CSV\n",
    "    log_path = metrics_dir / f'v5_stacking_training_log_h{horizon}.csv'\n",
    "    log_df = pd.DataFrame({\n",
    "        'epoch': trainer.history['epoch'],\n",
    "        'train_loss': trainer.history['train_loss'],\n",
    "        'val_loss': trainer.history['val_loss'],\n",
    "        'lr': trainer.history['learning_rate'],\n",
    "        'w_convlstm': trainer.history['branch_weights_convlstm'],\n",
    "        'w_gnn': trainer.history['branch_weights_gnn']\n",
    "    })\n",
    "    log_df.to_csv(log_path, index=False)\n",
    "    print(f\"Training log saved: {log_path}\")\n",
    "\n",
    "    # 4. Compute validation metrics\n",
    "    trainer.model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_basic = batch['x_basic'].to(DEVICE)\n",
    "            x_kce = batch['x_kce'].to(DEVICE)\n",
    "            context = batch['context'].to(DEVICE)\n",
    "            y = batch['y'].to(DEVICE)\n",
    "\n",
    "            pred, _ = trainer.model(\n",
    "                x_basic, x_kce, trainer.edge_index, context, trainer.edge_weight\n",
    "            )\n",
    "\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_targets.append(y.cpu().numpy())\n",
    "\n",
    "    y_pred = np.concatenate(all_preds, axis=0)\n",
    "    y_true = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Compute metrics\n",
    "    rmse = np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
    "    mae = np.mean(np.abs(y_pred - y_true))\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
    "\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    bias_mm = mean_pred - mean_true\n",
    "    bias_pct = 100 * bias_mm / mean_true if mean_true != 0 else 0\n",
    "\n",
    "    # 5. Save metrics CSV (V4 format)\n",
    "    results = pd.DataFrame({\n",
    "        'TotalHorizon': [horizon],\n",
    "        'Experiment': [experiment],\n",
    "        'Model': ['V5_Stacking'],\n",
    "        'H': [horizon],\n",
    "        'RMSE': [rmse],\n",
    "        'MAE': [mae],\n",
    "        'R^2': [r2],\n",
    "        'Mean_True_mm': [mean_true],\n",
    "        'Mean_Pred_mm': [mean_pred],\n",
    "        'mean_bias_mm': [bias_mm],\n",
    "        'mean_bias_pct': [bias_pct]\n",
    "    })\n",
    "\n",
    "    metrics_csv = OUTPUT_ROOT / f'metrics_spatial_v5_stacking_h{horizon}.csv'\n",
    "    results.to_csv(metrics_csv, index=False)\n",
    "    print(f\"Metrics saved: {metrics_csv}\")\n",
    "\n",
    "    # 6. Save experiment state\n",
    "    state_path = OUTPUT_ROOT / 'experiment_state_v5.json'\n",
    "    state = {\n",
    "        'version': 'V5',\n",
    "        'architecture': 'GNN-ConvLSTM Stacking',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'best_results': {'rmse': float(rmse), 'mae': float(mae), 'r2': float(r2)}\n",
    "    }\n",
    "    with open(state_path, 'w') as f:\n",
    "        json.dump(state, f, indent=2, default=str)\n",
    "    print(f\"State saved: {state_path}\")\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"\\nV5 Results: RMSE={rmse:.2f}mm, MAE={mae:.2f}mm, R2={r2:.4f}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"save_v5_results function defined (V4 compatible).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.2 Visualization Functions (700 DPI for Publication)\n",
    "# =============================================================================\n",
    "\n",
    "def plot_v5_training_results(trainer, config, output_dir, horizon):\n",
    "    '''Generate V4-quality training visualizations (700 DPI).'''\n",
    "    history = trainer.history\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(f'V5 GNN-ConvLSTM Stacking - H{horizon}', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 1. Loss curves\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(history['epoch'], history['train_loss'], label='Train', color='blue', lw=2)\n",
    "    ax1.plot(history['epoch'], history['val_loss'], label='Val', color='orange', lw=2)\n",
    "    ax1.axhline(y=trainer.best_val_loss, color='green', ls='--', alpha=0.7,\n",
    "                label=f'Best: {trainer.best_val_loss:.6f}')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss (MSE)')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "\n",
    "    # 2. Branch weights evolution\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(history['epoch'], history['branch_weights_convlstm'],\n",
    "             label='ConvLSTM', color='green', lw=2)\n",
    "    ax2.plot(history['epoch'], history['branch_weights_gnn'],\n",
    "             label='GNN-TAT', color='purple', lw=2)\n",
    "    ax2.fill_between(history['epoch'], history['branch_weights_convlstm'], alpha=0.3, color='green')\n",
    "    ax2.fill_between(history['epoch'], history['branch_weights_gnn'], alpha=0.3, color='purple')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Branch Weight')\n",
    "    ax2.set_title('Meta-Learner Branch Weight Evolution')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    # 3. Learning rate schedule\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(history['epoch'], history['learning_rate'], color='red', lw=2)\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Summary\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    param_count = sum(p.numel() for p in trainer.model.parameters())\n",
    "    summary = f'''\n",
    "V5 Training Summary\n",
    "========================================\n",
    "Grid: {config.n_lat} x {config.n_lon}\n",
    "Horizon: {horizon} months\n",
    "Total Epochs: {len(history['epoch'])}\n",
    "Best Val Loss: {trainer.best_val_loss:.6f}\n",
    "Final Weights:\n",
    "  ConvLSTM={history['branch_weights_convlstm'][-1]:.3f}\n",
    "  GNN={history['branch_weights_gnn'][-1]:.3f}\n",
    "Parameters: {param_count:,}\n",
    "'''\n",
    "    ax4.text(0.05, 0.95, summary, transform=ax4.transAxes, fontsize=10,\n",
    "             va='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save at 700 DPI for publication\n",
    "    fig_path = output_dir / f'v5_training_results_h{horizon}.png'\n",
    "    plt.savefig(fig_path, dpi=700, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Figure saved: {fig_path} (700 DPI)\")\n",
    "\n",
    "    # Preview at 150 DPI\n",
    "    preview_path = output_dir / f'v5_training_preview_h{horizon}.png'\n",
    "    plt.savefig(preview_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"plot_v5_training_results function defined (700 DPI).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Execution\n",
    "\n",
    "Run training on light mode (5x5 grid) for initial validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.1 Initialize Model and Trainer\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"V5 MODEL INITIALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create fresh model with updated config\n",
    "model = V5StackingModel(config).to(DEVICE)\n",
    "param_counts = model.count_parameters()\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Grid: {config.n_lat} x {config.n_lon} = {config.n_nodes} nodes\")\n",
    "print(f\"  BASIC features: {config.n_basic_features}\")\n",
    "print(f\"  KCE features: {config.n_kce_features}\")\n",
    "\n",
    "print(f\"\\nParameter counts:\")\n",
    "for name, count in param_counts.items():\n",
    "    print(f\"  {name}: {count:,}\")\n",
    "\n",
    "print(f\"\\nTarget: ~200K parameters\")\n",
    "print(f\"Actual: {param_counts['total']:,} parameters\")\n",
    "\n",
    "# Create trainer\n",
    "OUTPUT_DIR = Path('models/output/V5_STACKING')\n",
    "trainer = V5Trainer(model, config, edge_index, edge_weight, output_dir=OUTPUT_DIR)\n",
    "\n",
    "print(f\"\\nTrainer initialized.\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "### Completed (Phase 1)\n",
    "- [x] V5 Architecture design (ConvLSTM + GNN-TAT + Fusion + Meta-Learner)\n",
    "- [x] Graph construction from V4 (SpatialGraphBuilder)\n",
    "- [x] Real data loading pipeline (CHIRPS + SRTM)\n",
    "- [x] Dual Dataset for BASIC and KCE features\n",
    "- [x] Full training pipeline with checkpointing\n",
    "- [x] Branch weight tracking for interpretability\n",
    "\n",
    "### In Progress (Phase 2)\n",
    "- [ ] Initial experiments on light mode (5x5 grid)\n",
    "- [ ] Validation with real CHIRPS/SRTM data\n",
    "- [ ] Memory optimization for full grid\n",
    "\n",
    "### Upcoming (Phase 3)\n",
    "- [ ] Full grid experiments (61x65)\n",
    "- [ ] Ablation studies (A1-A6 from paper_5_spec.md)\n",
    "- [ ] Branch weight analysis by elevation\n",
    "\n",
    "### Paper Preparation (Phase 4)\n",
    "- [ ] Statistical significance tests\n",
    "- [ ] Generate figures (700 DPI)\n",
    "- [ ] Paper-5 draft writing\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created: January 2026*  \n",
    "*Status: TRAINING PIPELINE COMPLETE - Ready for experiments*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6.3 Visualize Training Results\n",
    "# =============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = trainer.history\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Loss curves\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['epoch'], history['train_loss'], label='Train', color='blue')\n",
    "ax1.plot(history['epoch'], history['val_loss'], label='Val', color='orange')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss (MSE)')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Branch weights evolution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['epoch'], history['branch_weights_convlstm'], \n",
    "         label='ConvLSTM', color='green')\n",
    "ax2.plot(history['epoch'], history['branch_weights_gnn'], \n",
    "         label='GNN-TAT', color='purple')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Branch Weight')\n",
    "ax2.set_title('Branch Weight Evolution')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# 3. Learning rate\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history['epoch'], history['learning_rate'], color='red')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Learning Rate')\n",
    "ax3.set_title('Learning Rate Schedule')\n",
    "ax3.set_yscale('log')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Final summary\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "summary_text = f\"\"\"\n",
    "V5 Training Summary\n",
    "{'='*40}\n",
    "\n",
    "Data Source: {'Real (CHIRPS/SRTM)' if USE_REAL_DATA else 'Synthetic'}\n",
    "Grid Size: {config.n_lat} x {config.n_lon}\n",
    "Horizon: {HORIZON} months\n",
    "\n",
    "Best Val Loss: {trainer.best_val_loss:.6f}\n",
    "Total Epochs: {len(history['epoch'])}\n",
    "\n",
    "Final Branch Weights:\n",
    "  ConvLSTM: {history['branch_weights_convlstm'][-1]:.3f}\n",
    "  GNN-TAT:  {history['branch_weights_gnn'][-1]:.3f}\n",
    "\n",
    "Parameters: {param_counts['total']:,}\n",
    "\"\"\"\n",
    "ax4.text(0.1, 0.5, summary_text, transform=ax4.transAxes, fontsize=12,\n",
    "         verticalalignment='center', fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'v5_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {OUTPUT_DIR / 'v5_training_curves.png'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "### Immediate (Phase 1 - Weeks 1-2)\n",
    "1. [ ] Copy full graph construction from V4\n",
    "2. [ ] Implement real data loading pipeline\n",
    "3. [ ] Add feature preprocessing (BASIC, KCE separation)\n",
    "\n",
    "### Short-term (Phase 2 - Weeks 3-4)\n",
    "4. [ ] Full training loop with checkpointing\n",
    "5. [ ] Memory optimization (gradient checkpointing)\n",
    "6. [ ] Initial experiments on light mode (5x5 grid)\n",
    "\n",
    "### Medium-term (Phase 3 - Weeks 5-7)\n",
    "7. [ ] Full grid experiments (61x65)\n",
    "8. [ ] Ablation studies (A1-A6)\n",
    "9. [ ] Branch weight analysis by elevation\n",
    "\n",
    "### Paper Preparation (Phase 4 - Weeks 8-11)\n",
    "10. [ ] Statistical significance tests\n",
    "11. [ ] Generate figures (700 DPI)\n",
    "12. [ ] Paper-5 draft writing\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created: January 2026*  \n",
    "*Status: PROTOTYPE - Architecture validated with synthetic data*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
