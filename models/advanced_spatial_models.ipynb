{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/advanced_spatial_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CozjYqgoXqrJ"
      },
      "source": [
        "# 🚀 Mejoras Propuestas para Modelos Espaciales de Precipitación\n",
        "\n",
        "## 📊 Análisis de Resultados Actuales\n",
        "\n",
        "### Problemas Identificados:\n",
        "\n",
        "1. **R² Negativos**: Varios modelos muestran R² < 0, especialmente en H=2\n",
        "   - ConvLSTM en KCE/PAFC: R² = -0.042 y -0.161\n",
        "   - ConvRNN en KCE: R² = -0.340\n",
        "   - ConvGRU en PAFC: R² = -0.052\n",
        "\n",
        "2. **Degradación en H=3**: RMSE > 100 en muchos casos\n",
        "   - ConvLSTM: RMSE hasta 111.3\n",
        "   - ConvGRU: RMSE hasta 106.9\n",
        "   - ConvRNN: RMSE hasta 87.3\n",
        "\n",
        "3. **Inestabilidad**: Batch size = 4 es muy pequeño\n",
        "\n",
        "### Mejores Resultados Actuales:\n",
        "- **H=1**: ConvRNN BASIC (RMSE=43.49, R²=0.767)\n",
        "- **H=2**: ConvGRU BASIC (RMSE=32.40, R²=0.401)\n",
        "- **H=3**: ConvRNN KCE (RMSE=71.47, R²=0.611)\n",
        "\n",
        "## 🔧 Mejoras Implementadas\n",
        "\n",
        "### 1. Optimización de Hiperparámetros\n",
        "\n",
        "| Parámetro | Valor Original | Valor Mejorado | Justificación |\n",
        "|-----------|----------------|----------------|---------------|\n",
        "| Batch Size | 4 | **16** | Mayor estabilidad en gradientes |\n",
        "| Learning Rate | 1e-3 | **5e-4** | Convergencia más suave |\n",
        "| Epochs | 50 | **100** | Más tiempo con early stopping |\n",
        "| Patience | 6 | **10** | Evitar detención prematura |\n",
        "| Dropout | 0 | **0.2** | Regularización |\n",
        "| L2 Reg | 0 | **1e-5** | Prevenir overfitting |\n",
        "\n",
        "### 2. Arquitecturas Mejoradas\n",
        "\n",
        "#### ConvLSTM con Atención (ConvLSTM_Att)\n",
        "```python\n",
        "- 3 capas ConvLSTM (64→32→16 filtros)\n",
        "- CBAM (Channel + Spatial Attention)\n",
        "- BatchNorm + Dropout en cada capa\n",
        "- Cabeza multi-escala (1×1, 3×3, 5×5)\n",
        "```\n",
        "\n",
        "#### ConvGRU Residual (ConvGRU_Res)\n",
        "```python\n",
        "- Skip connections desde input\n",
        "- BatchNorm mejorado\n",
        "- 2 bloques ConvGRU (64→32 filtros)\n",
        "- Conexión residual final\n",
        "```\n",
        "\n",
        "#### Transformer Híbrido (Hybrid_Trans)\n",
        "```python\n",
        "- Encoder CNN temporal\n",
        "- Multi-head attention (4 heads)\n",
        "- LSTM para agregación temporal\n",
        "- Decoder espacial\n",
        "```\n",
        "\n",
        "### 3. Técnicas Avanzadas\n",
        "\n",
        "#### Learning Rate Scheduling\n",
        "- **Warmup**: 5 épocas iniciales\n",
        "- **Cosine Decay**: Reducción suave después del warmup\n",
        "- **ReduceLROnPlateau**: Reducción adicional si se estanca\n",
        "\n",
        "#### Data Augmentation\n",
        "- Ruido gaussiano (σ=0.005)\n",
        "- Preserva coherencia espacial y temporal\n",
        "\n",
        "#### Regularización\n",
        "- Dropout espacial (0.2)\n",
        "- L2 en todos los pesos\n",
        "- Batch Normalization\n",
        "\n",
        "## 📈 Mejoras Esperadas\n",
        "\n",
        "### Por Horizonte:\n",
        "- **H=1**: RMSE < 40 (mejora ~8%)\n",
        "- **H=2**: RMSE < 30, R² > 0.5 (mejora significativa)\n",
        "- **H=3**: RMSE < 65, R² > 0.65 (mejora ~10%)\n",
        "\n",
        "### Por Modelo:\n",
        "1. **ConvLSTM_Att**: Mejor captura de patrones espaciales relevantes\n",
        "2. **ConvGRU_Res**: Mayor estabilidad y menos degradación temporal\n",
        "3. **Hybrid_Trans**: Mejor modelado de dependencias largas\n",
        "\n",
        "## 🚀 Próximos Pasos\n",
        "\n",
        "### Corto Plazo:\n",
        "1. Entrenar modelos con configuración mejorada\n",
        "2. Validar mejoras en métricas\n",
        "3. Análisis de errores por región\n",
        "\n",
        "### Medio Plazo:\n",
        "1. **Ensemble Methods**: Combinar mejores modelos\n",
        "2. **Multi-Task Learning**: Predecir múltiples variables\n",
        "3. **Physics-Informed Loss**: Incorporar restricciones físicas\n",
        "\n",
        "### Largo Plazo:\n",
        "1. **Modelos 3D**: ConvLSTM3D para capturar altura\n",
        "2. **Graph Neural Networks**: Para relaciones espaciales irregulares\n",
        "3. **Uncertainty Quantification**: Intervalos de confianza\n",
        "\n",
        "## 💻 Uso del Script\n",
        "\n",
        "```bash\n",
        "# Entrenar modelos avanzados\n",
        "python models/train_advanced_models.py\n",
        "\n",
        "# Con GPU específica\n",
        "CUDA_VISIBLE_DEVICES=0 python models/train_advanced_models.py\n",
        "```\n",
        "\n",
        "## 📊 Monitoreo\n",
        "\n",
        "Los resultados se guardan en:\n",
        "- `models/output/Advanced_Spatial/advanced_results.csv`\n",
        "- Historiales de entrenamiento por experimento\n",
        "- Modelos guardados en formato .keras\n",
        "\n",
        "## 🔍 Comparación con Baseline\n",
        "\n",
        "El script genera automáticamente comparaciones con los modelos originales, mostrando:\n",
        "- % de mejora en RMSE\n",
        "- Evolución de R² por horizonte\n",
        "- Tabla resumen de mejores modelos\n",
        "\n",
        "## 📊 Análisis de Resultados y Mejoras Propuestas\n",
        "\n",
        "### Problemas Identificados en los Modelos Originales:\n",
        "1. **R² negativos** en varios casos (especialmente H=2)\n",
        "2. **Degradación severa** en H=3 (RMSE >100)\n",
        "3. **Batch size muy pequeño** (4) causando inestabilidad\n",
        "4. **Arquitecturas muy simples** (solo 2 capas)\n",
        "\n",
        "### Mejoras Implementadas:\n",
        "\n",
        "#### 1. **Hiperparámetros Optimizados**\n",
        "- Batch size: 4 → 16 (mejor estabilidad)\n",
        "- Learning rate: 1e-3 → 5e-4 (más conservador)\n",
        "- Epochs: 50 → 100 (con early stopping)\n",
        "- Regularización: Dropout (0.2) + L2 (1e-5)\n",
        "\n",
        "#### 2. **Arquitecturas Mejoradas**\n",
        "- **ConvLSTM con Atención**: CBAM (Channel + Spatial Attention)\n",
        "- **ConvGRU con Skip Connections**: Conexiones residuales\n",
        "- **PredRNN++**: Estado del arte para predicción espacio-temporal\n",
        "- **ConvTransformer**: Híbrido CNN + Transformer\n",
        "\n",
        "#### 3. **Técnicas Avanzadas**\n",
        "- Learning rate scheduling (cosine decay con warmup)\n",
        "- Data augmentation (ruido gaussiano)\n",
        "- Multi-scale processing en la cabeza de salida\n",
        "- Batch normalization en todas las capas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiFmcqSEXqrM",
        "outputId": "6c6ed6ab-ecdc-4bde-9447-eef89d6e8a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (2025.3.1)\n",
            "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: ace_tools_open in /usr/local/lib/python3.11/dist-packages (0.1.0)\n",
            "Requirement already satisfied: cartopy in /usr/local/lib/python3.11/dist-packages (0.24.1)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from xarray) (24.2)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.2.2)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.11/dist-packages (from netCDF4) (1.6.4.post1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2025.4.26)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: itables in /usr/local/lib/python3.11/dist-packages (from ace_tools_open) (2.4.2)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (from ace_tools_open) (7.34.0)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.1.1)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.7.1)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas) (0.11.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython->ace_tools_open) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython->ace_tools_open) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->ace_tools_open) (0.2.13)\n",
            "BASE_PATH = /content/drive/MyDrive/ml_precipitation_prediction\n",
            "📁 BASE_PATH: /content/drive/MyDrive/ml_precipitation_prediction\n",
            "📊 Dataset: complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc\n",
            "Mounted at /content/drive\n",
            "✅ Imports completados\n"
          ]
        }
      ],
      "source": [
        "# ───────────────────────── IMPORTS Y CONFIGURACIÓN ─────────────────────────\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import sys, os, gc, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, ConvLSTM2D, LSTM,SimpleRNN, LSTM, GRU, Flatten, Dense, Reshape, RepeatVector,\n",
        "    Lambda, Permute, Layer, TimeDistributed, BatchNormalization, Dropout, Add,\n",
        "    Add, Multiply, Concatenate, GlobalAveragePooling2D, Activation,\n",
        "    LayerNormalization, MultiHeadAttention, MaxPooling2D, Embedding, Conv3D\n",
        ")\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,\n",
        "    CSVLogger, Callback, LearningRateScheduler\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam, AdamW\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import geopandas as gpd\n",
        "import imageio.v2 as imageio\n",
        "from IPython.display import clear_output, display, Image\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "\n",
        "\n",
        "## ╭─────────────────────────── Rutas ──────────────────────────╮\n",
        "# ▶️ Path configuration\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "print('BASE_PATH =', BASE_PATH)\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = BASE_PATH / 'data' / 'output'\n",
        "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
        "OUT_ROOT = BASE_PATH / 'models' / 'output' / 'advanced_spatial'\n",
        "OUT_ROOT.mkdir(exist_ok=True)\n",
        "BASE_MODEL_DIR = BASE_PATH / 'models' / 'output' / 'advanced_spatial' / 'base_models'\n",
        "BASE_MODEL_DIR.mkdir(exist_ok=True)\n",
        "MODEL_DIR = OUT_ROOT\n",
        "SHAPE_DIR = BASE_PATH / 'data' / 'input' / 'shapes'\n",
        "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
        "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IMAGE_DIR = MODEL_DIR/'images'\n",
        "IMAGE_DIR.mkdir(exist_ok=True)\n",
        "GIF_DIR = MODEL_DIR / \"gifs\"\n",
        "GIF_DIR.mkdir(exist_ok=True)\n",
        "# Dataset paths\n",
        "FULL_NC = DATA_DIR / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation.nc'\n",
        "FULL_NC_CLEAN = DATA_DIR / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "DEPT_GDF = gpd.read_file(SHAPE_DIR/'MGN_Departamento.shp')\n",
        "\n",
        "print(f\"📁 BASE_PATH: {BASE_PATH}\")\n",
        "print(f\"📊 Dataset: {FULL_NC_CLEAN.name if FULL_NC_CLEAN.exists() else FULL_NC.name}\")\n",
        "\n",
        "# ───────────────────────── CONFIGURACIÓN MEJORADA ─────────────────────────\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context('notebook')\n",
        "\n",
        "# GPU config\n",
        "for g in tf.config.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(g, True)\n",
        "\n",
        "print(\"✅ Imports completados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMyBUMZulFhR",
        "outputId": "e41a4c34-a249-41a8-fe42-647536c8f8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🟢 Dataset limpio localizado → complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc\n"
          ]
        }
      ],
      "source": [
        "# ╭──────────────────────── Datasets ────────────────────────────╮\n",
        "LAG_VARS = ['total_precipitation_lag1',\n",
        "            'total_precipitation_lag2',\n",
        "            'total_precipitation_lag12']\n",
        "\n",
        "if FULL_NC_CLEAN.exists():\n",
        "    print(f\"🟢 Dataset limpio localizado → {FULL_NC_CLEAN.name}\")\n",
        "    ds = xr.open_dataset(FULL_NC_CLEAN)\n",
        "\n",
        "else:\n",
        "    # ============================================================\n",
        "    print(f\"🟠 Aviso: no se encontró el dataset limpio.\\n\")\n",
        "    ds = xr.open_dataset(FULL_NC)\n",
        "    print(\"\\n📊  Resumen global de NaNs\")\n",
        "    print(\"─\"*55)\n",
        "    for var in LAG_VARS:\n",
        "        arr    = ds[var].values\n",
        "        total  = arr.size\n",
        "        n_nans = int(np.isnan(arr).sum())\n",
        "        print(f\"{var:<28}: {n_nans:>8,} / {total:,}  ({n_nans/total:6.2%})\")\n",
        "\n",
        "    # ============================================================\n",
        "    print(\"\\n🕒  Fechas con NaNs por variable\")\n",
        "    print(\"─\"*55)\n",
        "    for var in LAG_VARS:\n",
        "        arr         = ds[var].values\n",
        "        nan_per_ts  = np.isnan(arr).reshape(len(ds.time), -1).sum(axis=1)\n",
        "        if nan_per_ts.sum() == 0:\n",
        "            print(f\"{var}: sin NaNs ✔️\")\n",
        "            continue\n",
        "\n",
        "        df_nan = (pd\n",
        "                  .DataFrame({\"time\": pd.to_datetime(ds.time.values),\n",
        "                              \"na_cells\": nan_per_ts})\n",
        "                  .query(\"na_cells > 0\"))\n",
        "\n",
        "        # primeras 3 y últimas 3 fechas con NaNs\n",
        "        head = df_nan.head(3).to_string(index=False)\n",
        "        tail = df_nan.tail(3).to_string(index=False)\n",
        "        last = df_nan[\"time\"].iloc[-1].strftime(\"%Y-%m\")\n",
        "\n",
        "        print(f\"\\n{var}\")\n",
        "        print(head)\n",
        "        if len(df_nan) > 6:\n",
        "            print(\"   …\")\n",
        "        print(tail)\n",
        "        print(f\"   ⇢  última fecha con NaNs: {last}\")\n",
        "\n",
        "    # ============================================================\n",
        "    # Primera fecha en la que las TRES variables están 100 % limpias\n",
        "    # ------------------------------------------------------------\n",
        "    def last_nan_index(var: str) -> int:\n",
        "        \"\"\"Índice del último timestamp que contiene al menos un NaN en `var`.\"\"\"\n",
        "        nan_per_ts = np.isnan(ds[var].values).reshape(len(ds.time), -1).sum(axis=1)\n",
        "        idxs       = np.where(nan_per_ts > 0)[0]\n",
        "        return idxs[-1] if len(idxs) else -1\n",
        "\n",
        "    last_nan_any = max(last_nan_index(v) for v in LAG_VARS)\n",
        "    first_clean  = pd.to_datetime(ds.time.values[last_nan_any + 1])\n",
        "\n",
        "    print(\"\\nPrimera fecha 100 % libre de NaNs en TODOS los lags:\",\n",
        "          first_clean.strftime(\"%Y-%m\"))\n",
        "\n",
        "    ds_clean = ds.sel(time=~(ds['time.year'] == 1981))   # descarta TODO 1981\n",
        "\n",
        "    print(\"🔎  Timestamps antes :\", len(ds.time))\n",
        "    print(\"🔎  Timestamps después:\", len(ds_clean.time))\n",
        "\n",
        "    # 3) Guarda nuevo archivo NetCDF\n",
        "    ds_clean.to_netcdf(FULL_NC_CLEAN, mode='w')\n",
        "    print(f\"💾  Dataset sin 1981 guardado en {FULL_NC_CLEAN}\")\n",
        "\n",
        "    # 4) (-- opcional --)  verifica que ya no queden NaNs en los lags\n",
        "    LAG_VARS = ['total_precipitation_lag1',\n",
        "                'total_precipitation_lag2',\n",
        "                'total_precipitation_lag12']\n",
        "\n",
        "    print(\"\\n📊  NaNs restantes tras quitar 1981\")\n",
        "    print(\"─\"*50)\n",
        "    for var in LAG_VARS:\n",
        "        n_nan = int(np.isnan(ds_clean[var].values).sum())\n",
        "        print(f\"{var:<28}: {n_nan:,} NaNs\")\n",
        "\n",
        "    ds = ds_clean\n",
        "# ╰────────────────────────────────────────────────────────────╯"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWbAk1lBXqrO",
        "outputId": "f8d8b2d2-5dc4-4486-a721-24537a3d22f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "▶️  [1/3] ConvLSTM-ED – F1 (val=2018)\n",
            "Ventanas train: 382 · val: 10\n",
            "⏩ ConvLSTM-ED_F1 ya existe → skip\n",
            "\n",
            "▶️  [2/3] ConvLSTM-ED-KCE – F1 (val=2018)\n",
            "Ventanas train: 382 · val: 10\n"
          ]
        }
      ],
      "source": [
        "# ╭──────────────────────── Shapes ────────────────────────────╮\n",
        "lat, lon    = len(ds.latitude), len(ds.longitude)\n",
        "cells       = lat * lon\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭──────────────────── Hyper‑parámetros globales ─────────────╮\n",
        "INPUT_WINDOW   = 60\n",
        "HORIZON        = 3\n",
        "TARGET_VAR     = 'total_precipitation'\n",
        "EPOCHS         = 1\n",
        "BATCH_SIZE     = 16           # tamaño pequeño → menor RAM GPU\n",
        "PATIENCE       = 1\n",
        "LR             = 1e-3\n",
        "L2_REG         = 1e-5 # Regularización L2\n",
        "DROPOUT        = 0.2 # Regularización\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭────────────────────── Modelo base ConvLSTM ────────────────╮\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def tile_step_emb(batch_ref, step_emb_tab):\n",
        "    # Forma estática (TensorShape / TensorSpec)\n",
        "    if isinstance(batch_ref, (tf.TensorShape, tf.TensorSpec)):\n",
        "        return tf.TensorShape([batch_ref[0],\n",
        "                               step_emb_tab.shape[0],\n",
        "                               step_emb_tab.shape[1]])\n",
        "    # Ejecución\n",
        "    b   = tf.shape(batch_ref)[0]\n",
        "    emb = tf.expand_dims(step_emb_tab, 0)\n",
        "    return tf.tile(emb, [b, 1, 1])\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "custom = {'tile_step_emb': tile_step_emb}\n",
        "\n",
        "def _build_convlstm_ed(\n",
        "        *,\n",
        "        input_window: int,\n",
        "        output_horizon: int,\n",
        "        spatial_height: int,\n",
        "        spatial_width: int,\n",
        "        n_features: int,\n",
        "        n_filters: int = 64,\n",
        "        n_heads: int = 4,\n",
        "        use_attention: bool = True,\n",
        "        use_positional_emb: bool = True,\n",
        "        lr: float = 1e-3\n",
        "    ) -> Model:\n",
        "    \"\"\"\n",
        "    Encoder-Decoder ConvLSTM + GRU.\n",
        "    Si `use_positional_emb` = True añade un embedding del paso de salida\n",
        "    que evita que el modelo genere la misma predicción en todos los horizontes.\n",
        "    \"\"\"\n",
        "\n",
        "    # ──────────────── Encoder ────────────────\n",
        "    enc_inputs = Input(\n",
        "        shape=(input_window, spatial_height, spatial_width, n_features),\n",
        "        name=\"enc_input\"\n",
        "    )\n",
        "\n",
        "    x = ConvLSTM2D(n_filters, (3, 3), padding='same',\n",
        "                   return_sequences=True,  name=\"enc_lstm_1\")(enc_inputs)\n",
        "    x = ConvLSTM2D(n_filters // 2, (3, 3), padding='same',\n",
        "                   return_sequences=False, name=\"enc_lstm_2\")(x)\n",
        "\n",
        "    # ── Aplana grilla y repite contexto T_out veces ──\n",
        "    flat = Flatten(name=\"flatten_spatial\")(x)                 # (B, H·W·C)\n",
        "    ctx  = RepeatVector(output_horizon, name=\"context\")(flat) # (B, T_out, H·W·C)\n",
        "\n",
        "    # ── Positional embedding CORREGIDO ──\n",
        "    if use_positional_emb:\n",
        "        # Crear IDs de pasos como input constante\n",
        "        step_ids_input = Input(shape=(output_horizon,), dtype=tf.int32, name=\"step_ids\")\n",
        "\n",
        "        # Embedding layer\n",
        "        step_emb_layer = Embedding(output_horizon, n_filters, name=\"step_embedding\")\n",
        "        step_emb = step_emb_layer(step_ids_input)  # (B, T_out, D)\n",
        "\n",
        "        # Concatenar con contexto\n",
        "        dec_in = Concatenate(name=\"dec_concat\")([ctx, step_emb])\n",
        "\n",
        "        # Actualizar inputs del modelo\n",
        "        model_inputs = [enc_inputs, step_ids_input]\n",
        "    else:\n",
        "        dec_in = ctx\n",
        "        model_inputs = enc_inputs\n",
        "\n",
        "    # ─────────────── Decoder temporal ───────────────\n",
        "    dec = GRU(2 * n_filters, return_sequences=True, name=\"dec_gru\")(dec_in) # (B, T_out, 2·F)\n",
        "\n",
        "    # ─────── Attention (opcional) ───────\n",
        "    if use_attention:\n",
        "        attn = MultiHeadAttention(num_heads=n_heads,\n",
        "                                  key_dim=n_filters,\n",
        "                                  dropout=0.1,\n",
        "                                  name=\"mha\")(dec, dec)\n",
        "        dec  = Add(name=\"mha_residual\")([dec, attn])\n",
        "        dec  = LayerNormalization(name=\"mha_norm\")(dec)\n",
        "\n",
        "    # ───────────── Proyección a grilla ─────────────\n",
        "    proj = TimeDistributed(\n",
        "        Dense(spatial_height * spatial_width, activation='linear'),\n",
        "        name=\"dense_proj\"\n",
        "    )(dec)                                                    # (B, T_out, H·W)\n",
        "\n",
        "    out = Reshape(\n",
        "        (output_horizon, spatial_height, spatial_width, 1),\n",
        "        name=\"reshape_out\"\n",
        "    )(proj)\n",
        "\n",
        "    name = (\"ConvLSTM_ED_Attn_PE\" if use_attention else \"ConvLSTM_ED_PE\") \\\n",
        "           if use_positional_emb else \\\n",
        "           (\"ConvLSTM_ED_Attn\"     if use_attention else \"ConvLSTM_ED\")\n",
        "\n",
        "    model = Model(model_inputs, out, name=name)\n",
        "    model.compile(optimizer=Adam(lr), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Factories ---------------------------------------------------\n",
        "\n",
        "def factory_no_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=False, **kw)\n",
        "\n",
        "def factory_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=True, **kw)\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭────────────────────────── Métricas ────────────────────────╮\n",
        "\n",
        "def evaluate(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae  = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-5))) * 100\n",
        "    r2   = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, mape, r2\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭──────────────────────── Quick‑plot ────────────────────────╮\n",
        "\n",
        "def quick_plot(ax, data, cmap, title, date_label, vmin=None, vmax=None):\n",
        "    mesh = ax.pcolormesh(ds.longitude, ds.latitude, data, cmap=cmap, shading='nearest', vmin=vmin, vmax=vmax, transform=ccrs.PlateCarree())\n",
        "    ax.coastlines(); ax.add_geometries(DEPT_GDF.geometry, ccrs.PlateCarree(), edgecolor='black', facecolor='none', linewidth=1)\n",
        "    gl = ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
        "    ax.set_title(f\"{title}\\n{date_label}\", pad=12)\n",
        "    return mesh\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭────────────────────── Experiments & Folds ─────────────────╮\n",
        "# ▸ Solo mostramos los tres primeros niveles; añade los demás igual\n",
        "BASE_FEATURES = [\n",
        "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
        "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
        "    'elevation','slope','aspect'\n",
        "]\n",
        "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
        "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
        "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
        "\n",
        "FOLDS = {'F1': {'year': 2018,'active': True}}\n",
        "\n",
        "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
        "    'ConvLSTM-ED': {\n",
        "        'active': True,\n",
        "        'feature_list': BASE_FEATURES,\n",
        "        'builder': factory_attn, #factory_no_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE': {\n",
        "        'active': True,\n",
        "        'feature_list': KCE_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4,\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE-PAFC': {\n",
        "        'active': True,\n",
        "        'feature_list': PAFC_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 96,\n",
        "        'n_heads'  : 6,\n",
        "    },\n",
        "}\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭──────────────────── Ventanas deslizadas ───────────────────╮\n",
        "\n",
        "def make_windows(mask:np.ndarray, allow_past_context:bool)->tuple[np.ndarray,np.ndarray]:\n",
        "    \"\"\"Genera ventanas **descartando** las que contienen NaNs.  # 🔸 NEW\"\"\"\n",
        "    seq_X, seq_y = [], []\n",
        "    lim = len(mask) - INPUT_WINDOW - HORIZON + 1\n",
        "    for start in range(lim):\n",
        "        end_w = start + INPUT_WINDOW; end_y = end_w + HORIZON\n",
        "        if allow_past_context:\n",
        "            if not mask[end_w:end_y].all():\n",
        "                continue\n",
        "        else:\n",
        "            if not mask[start:end_y].all():\n",
        "                continue\n",
        "        Xw = Xarr[start:end_w]; yw = yarr[end_w:end_y]\n",
        "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
        "            continue  # 🔸 NEW — descarta ventana con NaNs\n",
        "        seq_X.append(Xw); seq_y.append(yw)\n",
        "    return np.array(seq_X), np.array(seq_y)\n",
        "\n",
        "\n",
        "# ╭────────────────── Bucle principal de entrenamiento ────────╮\n",
        "RESULTS: List[Dict[str, Any]] = []\n",
        "\n",
        "# 🔸 NEW helper ------------------------------------------------\n",
        "\n",
        "def _impute_nans(a:np.ndarray, per_feature_mean:np.ndarray|None=None, is_target:bool=False)->np.ndarray:\n",
        "    \"\"\"Imputa NaNs restantes (seguridad extra).\"\"\"\n",
        "    if not np.isnan(a).any():\n",
        "        return a\n",
        "    if is_target:\n",
        "        a[np.isnan(a)] = 0.0  # 🔸 NEW – 0 para y\n",
        "        return a\n",
        "    if per_feature_mean is None:\n",
        "        raise ValueError('per_feature_mean required for imputing X')\n",
        "    flat = a.reshape(-1, a.shape[-1])\n",
        "    nan_idx = np.isnan(flat)\n",
        "    for f in range(a.shape[-1]):\n",
        "        flat[nan_idx[:,f], f] = per_feature_mean[f]  # 🔸 NEW\n",
        "    return flat.reshape(a.shape)\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "def run_all_experiments():\n",
        "    times = pd.to_datetime(ds.time.values)\n",
        "    total = sum(e['active'] for e in EXPERIMENTS.values()) * sum(f['active'] for f in FOLDS.values())\n",
        "    cnt   = 0\n",
        "\n",
        "    for exp_name, exp_cfg in EXPERIMENTS.items():\n",
        "        if not exp_cfg['active']:\n",
        "            continue\n",
        "        vars_     = exp_cfg['feature_list']\n",
        "        builder   = exp_cfg['builder']      # fábrica específica\n",
        "        n_filters = exp_cfg.get('n_filters',64)\n",
        "        n_heads   = exp_cfg.get('n_heads',4)\n",
        "\n",
        "        # ─ Pre‑load features por experimento ─────────────────────\n",
        "        global Xarr, yarr\n",
        "        Xarr = ds[vars_].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "        yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
        "        feats = Xarr.shape[-1]\n",
        "\n",
        "        for fold_name, fold_cfg in FOLDS.items():\n",
        "            if not fold_cfg['active']:\n",
        "                continue\n",
        "            cnt += 1\n",
        "            year_val = fold_cfg['year']\n",
        "            print(f\"\\n▶️  [{cnt}/{total}] {exp_name} – {fold_name} (val={year_val})\")\n",
        "\n",
        "            mask_val = times.year == year_val\n",
        "            mask_tr  = ~mask_val\n",
        "            if mask_val.sum() < HORIZON:\n",
        "                print(\"⚠️ Año sin pasos suficientes → skip\"); continue\n",
        "\n",
        "            X_tr, y_tr = make_windows(mask_tr,  allow_past_context=False)\n",
        "            X_va, y_va = make_windows(mask_val, allow_past_context=True)\n",
        "            print(f\"Ventanas train: {len(X_tr)} · val: {len(X_va)}\")\n",
        "            if len(X_tr)==0 or len(X_va)==0:\n",
        "                print(\"⚠️ Sin ventanas válidas → skip\"); continue\n",
        "\n",
        "            # 🔸 NEW — Imputación de seguridad\n",
        "            feat_mean = np.nanmean(X_tr.reshape(-1,feats),axis=0)\n",
        "            X_tr = _impute_nans(X_tr,feat_mean); X_va=_impute_nans(X_va,feat_mean)\n",
        "            y_tr = _impute_nans(y_tr,is_target=True); y_va=_impute_nans(y_va,is_target=True)\n",
        "\n",
        "            # ─ Scaling (fit solo en train) ─────────────────────\n",
        "            sx = StandardScaler().fit(X_tr.reshape(-1, feats))\n",
        "            sy = StandardScaler().fit(y_tr.reshape(-1, 1))\n",
        "            X_tr_sc = sx.transform(X_tr.reshape(-1, feats)).reshape(X_tr.shape)\n",
        "            X_va_sc = sx.transform(X_va.reshape(-1, feats)).reshape(X_va.shape)\n",
        "            y_tr_sc = sy.transform(y_tr.reshape(-1, 1)).reshape(y_tr.shape)[..., None]\n",
        "            y_va_sc = sy.transform(y_va.reshape(-1, 1)).reshape(y_va.shape)[..., None]\n",
        "\n",
        "            # ─ Build & train model (factory) ───────────────────\n",
        "            tag        = f\"{exp_name.replace('+','_')}_{fold_name}\"\n",
        "            model_path = BASE_MODEL_DIR / f\"{tag}.keras\"\n",
        "            if model_path.exists():\n",
        "                print(f\"⏩ {tag} ya existe → skip\"); continue\n",
        "\n",
        "            model = builder(\n",
        "                input_window=INPUT_WINDOW,\n",
        "                output_horizon=HORIZON,\n",
        "                spatial_height=lat,\n",
        "                spatial_width=lon,\n",
        "                n_features=feats,\n",
        "                n_filters=n_filters,\n",
        "                n_heads=n_heads,\n",
        "                lr=LR\n",
        "            )\n",
        "\n",
        "            # Preparar step_ids para entrenamiento\n",
        "            step_ids_train = np.tile(np.arange(HORIZON), (len(X_tr_sc), 1))\n",
        "            step_ids_val = np.tile(np.arange(HORIZON), (len(X_va_sc), 1))\n",
        "\n",
        "            # Verificar si el modelo usa positional embedding\n",
        "            uses_pe = len(model.inputs) > 1\n",
        "\n",
        "            if uses_pe:\n",
        "                X_train_input = [X_tr_sc, step_ids_train]\n",
        "                X_val_input = [X_va_sc, step_ids_val]\n",
        "            else:\n",
        "                X_train_input = X_tr_sc\n",
        "                X_val_input = X_va_sc\n",
        "\n",
        "            es   = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
        "            hist = model.fit(X_train_input, y_tr_sc, validation_data=(X_val_input, y_va_sc), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=1)\n",
        "\n",
        "            # ─ Evaluación ─────────────────────────────────────\n",
        "            if uses_pe:\n",
        "                y_hat_sc = model.predict([X_va_sc, step_ids_val], verbose=0)\n",
        "            else:\n",
        "                y_hat_sc = model.predict(X_va_sc, verbose=0)\n",
        "            y_hat    = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(y_hat_sc.shape)\n",
        "            y_true   = sy.inverse_transform(y_va_sc.reshape(-1,1)).reshape(y_va_sc.shape)\n",
        "\n",
        "            rmse, mae, mape, r2 = evaluate(y_true.ravel(), y_hat.ravel())\n",
        "            RESULTS.append(dict(experiment=exp_name, fold=fold_name, RMSE=rmse, MAE=mae, MAPE=mape, R2=r2, epochs=len(hist.history['loss'])))\n",
        "\n",
        "            # ─ Guardado artefactos ────────────────────────────\n",
        "            model.save(model_path)\n",
        "            plt.figure(); plt.plot(hist.history['loss'], label='train'); plt.plot(hist.history['val_loss'], label='val'); plt.legend(); plt.title(tag); plt.savefig(IMAGE_DIR/f\"{tag}.png\"); plt.close()\n",
        "\n",
        "            # Verificar que las predicciones varían entre horizontes\n",
        "            print(f\"Verificación de predicciones para {tag}:\")\n",
        "            for h in range(HORIZON):\n",
        "                pred_h = y_hat[0, h, ..., 0]  # Primera muestra, horizonte h\n",
        "                print(f\"  H{h+1}: min={pred_h.min():.3f}, max={pred_h.max():.3f}, mean={pred_h.mean():.3f}, std={pred_h.std():.3f}\")\n",
        "\n",
        "            # Usar la última ventana de validación para mejor visualización\n",
        "            last_idx = min(len(y_hat)-1, 10)  # Usar una de las últimas ventanas\n",
        "            _generate_gif(y_true[last_idx], y_hat[last_idx], tag)\n",
        "            print(f\"✅ Guardado {model_path.name}\")\n",
        "\n",
        "    # ─ Métricas globales ────────────────────────────────────\n",
        "    df = pd.DataFrame(RESULTS)\n",
        "    out_csv = BASE_MODEL_DIR / \"metrics_experiments_folds.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"\\n📑 Tabla de métricas en {out_csv}\")\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭──────────────────── Generador de GIF ──────────────────────╮\n",
        "\n",
        "def _generate_gif(y_true_sample, y_pred_sample, tag):\n",
        "    pcm_min, pcm_max = 0, np.max(y_pred_sample)\n",
        "    frames = []\n",
        "    for h in range(HORIZON):\n",
        "        pmap = y_pred_sample[h, ..., 0]\n",
        "        fig, ax = plt.subplots(1,1, figsize=(6,5), subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "        mesh = ax.pcolormesh(ds.longitude, ds.latitude, pmap, cmap='Blues', shading='nearest', vmin=pcm_min, vmax=pcm_max, transform=ccrs.PlateCarree())\n",
        "        ax.coastlines(); ax.gridlines(draw_labels=True)\n",
        "        ax.set_title(f\"{tag} – H{h+1}\")\n",
        "        fig.colorbar(mesh, ax=ax, fraction=0.046, pad=0.04)\n",
        "        tmp = GIF_DIR/f\"tmp_{tag}_h{h}.png\"\n",
        "        fig.savefig(tmp, bbox_inches='tight'); plt.close(fig)\n",
        "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
        "    gif_path = GIF_DIR/f\"{tag}.gif\"\n",
        "    imageio.mimsave(gif_path, frames, fps=0.5)\n",
        "    print(f\"💾 GIF {gif_path.name} listo\")\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭────────────────────── Bucle principal ─────────────────────╮\n",
        "run_all_experiments()\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "#╭────────────────────── Experiments & Folds ─────────────────╮\n",
        "\n",
        "# Factories ---------------------------------------------------\n",
        "\n",
        "def factory_no_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=False, **kw)\n",
        "\n",
        "def factory_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=True, **kw)\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def tile_step_emb(batch_ref, step_emb_tab):\n",
        "    \"\"\"\n",
        "    Replica la tabla de embedding (T_out, D) → (B, T_out, D).\n",
        "\n",
        "    · Durante la inferencia de forma, `batch_ref` es TensorShape\n",
        "      → devolvemos TensorShape (None, T_out, D).\n",
        "    · En ejecución, `batch_ref` es tensor\n",
        "      → devolvemos tensor (B, T_out, D).\n",
        "\n",
        "    ▸ `step_emb_tab` SIEMPRE llega desde el cierre de la Lambda original,\n",
        "      así que NO lo pongas opcional.\n",
        "    \"\"\"\n",
        "    # ——— 1) Forma estática ———\n",
        "    if isinstance(batch_ref, (tf.TensorShape, tf.TensorSpec)):\n",
        "        return tf.TensorShape([batch_ref[0],\n",
        "                               step_emb_tab.shape[0],\n",
        "                               step_emb_tab.shape[1]])\n",
        "\n",
        "    # ——— 2) Ejecución ———\n",
        "    b   = tf.shape(batch_ref)[0]\n",
        "    emb = tf.expand_dims(step_emb_tab, 0)    # (1, T_out, D)\n",
        "    return tf.tile(emb, [b, 1, 1])           # (B, T_out, D)\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "custom = {'tile_step_emb': tile_step_emb}\n",
        "\n",
        "# ▸ Solo mostramos los tres primeros niveles; añade los demás igual\n",
        "BASE_FEATURES = [\n",
        "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
        "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
        "    'elevation','slope','aspect'\n",
        "]\n",
        "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
        "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
        "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
        "\n",
        "FOLDS = {'F1': {'year': 2018,'active': True}}\n",
        "\n",
        "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
        "    'ConvLSTM-ED': {\n",
        "        'active': True,\n",
        "        'feature_list': BASE_FEATURES,\n",
        "        'builder': factory_attn, #factory_no_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE': {\n",
        "        'active': True,\n",
        "        'feature_list': KCE_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4,\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE-PAFC': {\n",
        "        'active': True,\n",
        "        'feature_list': PAFC_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 96,\n",
        "        'n_heads'  : 6,\n",
        "    },\n",
        "}\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "def quick_plot(ax,data,cmap,title,date_label,vmin=None,vmax=None):\n",
        "    mesh=ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
        "    ax.coastlines(); ax.add_geometries(DEPT_GDF.geometry,ccrs.PlateCarree(),edgecolor='black',facecolor='none',linewidth=1)\n",
        "    gl=ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
        "    ax.set_title(f\"{title}\\n{date_label}\",pad=10); return mesh\n",
        "\n",
        "# ───────── Recuperamos diccionario EXPERIMENTS (del bloque de entrenamiento) ─────────\n",
        "EXPERIMENTS:Dict[str,Dict[str,Any]] = {\n",
        "    'ConvLSTM-ED':              {'feature_list': \"+\".join(BASE_FEATURES).split(\"+\")},\n",
        "    'ConvLSTM-ED-KCE':          {'feature_list': \"+\".join(KCE_FEATURES).split(\"+\")},\n",
        "    'ConvLSTM-ED-KCE-PAFC':     {'feature_list': \"+\".join(PAFC_FEATURES).split(\"+\")},\n",
        "    # otros experimentos\n",
        "}\n",
        "\n",
        "# ———————————————————— Evaluación ————————————————————\n",
        "all_metrics=[]; times=pd.to_datetime(ds.time.values)\n",
        "for mpath in sorted(BASE_MODEL_DIR.glob(\"*.keras\")):\n",
        "    tag   = mpath.stem                        # p.ej. ConvLSTM-ED_F1\n",
        "    parts = tag.split(\"_\")\n",
        "    fold  = parts[-1]                         # F1\n",
        "    exp_token = \"_\".join(parts[:-1])\n",
        "    exp_name  = exp_token.replace(\"_\",\"+\")  # vuelve al nombre original con +\n",
        "    if exp_name not in EXPERIMENTS:\n",
        "        print(\"⚠️ Exp no encontrado para\",tag); continue\n",
        "    feats = EXPERIMENTS[exp_name]['feature_list']\n",
        "    print(f\"\\n🔍 Evaluando {tag} …\")\n",
        "\n",
        "    # — Extracción de arrays —\n",
        "    Xarr = ds[feats].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "    yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
        "    T,_,_,F = Xarr.shape\n",
        "    Xfull = Xarr; yfull=yarr  # mantenemos (T,H,W,F)\n",
        "\n",
        "    # ventana final (idéntica lógica del cuaderno original)\n",
        "    start=T-INPUT_WINDOW-HORIZON; end_w=start+INPUT_WINDOW; end_y=end_w+HORIZON\n",
        "    X_eval = Xfull[start:end_w]                 # (60,H,W,F)\n",
        "    y_eval = yfull[end_w:end_y]                 # (3,H,W)\n",
        "\n",
        "    # — Scalers (fit vectorizado) —\n",
        "    flat_X = Xfull.reshape(-1, F)      # (T·H·W, F)\n",
        "    flat_y = yfull.reshape(-1, 1)      # (T·H·W, 1)\n",
        "\n",
        "    sx = StandardScaler().fit(flat_X)\n",
        "    sy = StandardScaler().fit(flat_y)\n",
        "\n",
        "    Xe_sc = sx.transform(X_eval.reshape(-1, F)).reshape(1, INPUT_WINDOW, lat, lon, F)\n",
        "    ye_sc = sy.transform(y_eval.reshape(-1, 1)).reshape(1, HORIZON, lat, lon, 1)\n",
        "\n",
        "\n",
        "    model = tf.keras.models.load_model(\n",
        "    mpath,\n",
        "    compile=False,\n",
        "    custom_objects={'tile_step_emb': tile_step_emb}\n",
        "    )\n",
        "\n",
        "    # Verificar si el modelo usa positional embedding\n",
        "    uses_pe = len(model.inputs) > 1\n",
        "\n",
        "    if uses_pe:\n",
        "        step_ids_eval = np.tile(np.arange(HORIZON), (1, 1))\n",
        "        yhat_sc = model.predict([Xe_sc, step_ids_eval], verbose=0)  # (1,3,H,W,1)\n",
        "    else:\n",
        "        yhat_sc = model.predict(Xe_sc, verbose=0)  # (1,3,H,W,1)\n",
        "    print(f\"Verificación de predicciones para {tag}:\")\n",
        "    for h in range(HORIZON):\n",
        "        pred_h = yhat_sc[0, h, ..., 0]  # Primera muestra, horizonte h\n",
        "        print(f\"  H{h+1}: min={pred_h.min():.3f}, max={pred_h.max():.3f}, mean={pred_h.mean():.3f}, std={pred_h.std():.3f}\")\n",
        "    # Verificar si las predicciones son idénticas\n",
        "    if HORIZON > 1:\n",
        "        diff_h1_h2 = np.abs(yhat_sc[0, 0] - yhat_sc[0, 1]).mean()\n",
        "        print(f\"  Diferencia promedio H1 vs H2: {diff_h1_h2:.6f}\")\n",
        "    yhat   = sy.inverse_transform(yhat_sc.reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
        "    ytrue  = y_eval\n",
        "\n",
        "    # — Métricas por horizonte —\n",
        "    for h in range(HORIZON):\n",
        "        yt = ytrue[h].ravel()\n",
        "        yp = yhat[h].ravel()\n",
        "\n",
        "        # ---------- filtro NaN / ±∞ ----------\n",
        "        mask = np.isfinite(yt) & np.isfinite(yp)\n",
        "        if mask.sum() == 0:          # ventana vacía → se ignora\n",
        "            print(f\"   · h={h+1}: todos los valores son NaN/Inf → skip\")\n",
        "            continue\n",
        "        yt, yp = yt[mask], yp[mask]\n",
        "        # -------------------------------------\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(yt, yp))\n",
        "        mae  = mean_absolute_error(yt, yp)\n",
        "        mape = np.mean(np.abs((yt - yp) / (yt + 1e-5))) * 100\n",
        "        r2   = r2_score(yt, yp)\n",
        "\n",
        "        all_metrics.append(dict(\n",
        "            model      = tag,\n",
        "            experiment = exp_name,\n",
        "            fold       = fold,\n",
        "            horizon    = h + 1,\n",
        "            RMSE       = rmse,\n",
        "            MAE        = mae,\n",
        "            MAPE       = mape,\n",
        "            R2         = r2\n",
        "        ))\n",
        "\n",
        "    # — Figura Real vs Pred vs MAPE —\n",
        "    fig,axes=plt.subplots(HORIZON,3,figsize=(14,4*HORIZON),subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "    dates=pd.date_range(times[end_w],periods=HORIZON,freq='MS')\n",
        "    vmin=0; vmax=max(yhat.max(),ytrue.max())\n",
        "    for h in range(HORIZON):\n",
        "        quick_plot(axes[h,0],ytrue[h],'Blues',f\"Real h={h+1}\",dates[h].strftime('%Y-%m'),vmin,vmax)\n",
        "        quick_plot(axes[h,1],yhat [h],'Blues',f\"Pred h={h+1}\",dates[h].strftime('%Y-%m'),vmin,vmax)\n",
        "        err=np.clip(np.abs((ytrue[h]-yhat[h])/(ytrue[h]+1e-5))*100,0,100)\n",
        "        quick_plot(axes[h,2],err,'Reds',f\"MAPE% h={h+1}\",dates[h].strftime('%Y-%m'),0,100)\n",
        "    fig.suptitle(f\"{tag}  — Eval final ventana\",fontsize=16); fig.tight_layout();\n",
        "    fig.savefig(BASE_MODEL_DIR/f\"fig_{tag}.png\"); plt.close(fig)\n",
        "\n",
        "    # — GIF —\n",
        "    frames=[]; pcm_min,pcm_max=0,yhat.max()\n",
        "    for h in range(HORIZON):\n",
        "        figg,ax=plt.subplots(1,1,figsize=(6,5),subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "        m=ax.pcolormesh(ds.longitude,ds.latitude,yhat[h],cmap='Blues',shading='nearest',vmin=pcm_min,vmax=pcm_max,transform=ccrs.PlateCarree())\n",
        "        ax.coastlines(); ax.set_title(f\"{tag} – H{h+1}\"); figg.colorbar(m,ax=ax,fraction=0.046,pad=0.04)\n",
        "        tmp=GIF_DIR/f\"tmp_{tag}_{h}.png\"; figg.savefig(tmp,bbox_inches='tight'); plt.close(figg)\n",
        "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
        "    imageio.mimsave(GIF_DIR/f\"{tag}.gif\",frames,fps=0.5)\n",
        "    print(\"💾 GIF\",f\"{tag}.gif\",\"creado\")\n",
        "\n",
        "# ——— Guardar tabla ———\n",
        "pd.DataFrame(all_metrics).to_csv(BASE_MODEL_DIR/'metrics_eval.csv',index=False)\n",
        "print(\"📑 Métricas guardadas en\",BASE_MODEL_DIR/'metrics_eval.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qcvm08VIguq1"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import imageio.v2 as imageio\n",
        "\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, ConvLSTM2D, GRU, Flatten, RepeatVector, Reshape, TimeDistributed,\n",
        "    Dense, MultiHeadAttention, Add, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "## ╭─────────────────────────── Rutas ──────────────────────────╮\n",
        "# ▶️ Path configuration\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "print('BASE_PATH =', BASE_PATH)\n",
        "\n",
        "# Dataset paths\n",
        "DATA_DIR = BASE_PATH/'data'/'output'\n",
        "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
        "MODEL_DIR = BASE_PATH/'models'/'output'/'HybridLSTMModels'\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
        "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IMAGE_DIR = MODEL_DIR/'images'\n",
        "IMAGE_DIR.mkdir(exist_ok=True)\n",
        "FULL_NC = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation.nc'\n",
        "FULL_NC_CLEAN = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "dept_gdf = gpd.read_file(MODEL_INPUT_DIR/'MGN_Departamento.shp')\n",
        "\n",
        "BASE_MODEL_DIR = MODEL_DIR\n",
        "GIF_DIR        = MODEL_DIR / \"gifs\"\n",
        "GIF_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# ╭──────────────────────── Dataset & Shapes ──────────────────╮\n",
        "ds          = xr.open_dataset(FULL_NC)\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "LAG_VARS = ['total_precipitation_lag1',\n",
        "            'total_precipitation_lag2',\n",
        "            'total_precipitation_lag12']\n",
        "\n",
        "# ============================================================\n",
        "print(\"\\n📊  Resumen global de NaNs\")\n",
        "print(\"─\"*55)\n",
        "for var in LAG_VARS:\n",
        "    arr    = ds[var].values\n",
        "    total  = arr.size\n",
        "    n_nans = int(np.isnan(arr).sum())\n",
        "    print(f\"{var:<28}: {n_nans:>8,} / {total:,}  ({n_nans/total:6.2%})\")\n",
        "\n",
        "# ============================================================\n",
        "print(\"\\n🕒  Fechas con NaNs por variable\")\n",
        "print(\"─\"*55)\n",
        "for var in LAG_VARS:\n",
        "    arr         = ds[var].values\n",
        "    nan_per_ts  = np.isnan(arr).reshape(len(ds.time), -1).sum(axis=1)\n",
        "    if nan_per_ts.sum() == 0:\n",
        "        print(f\"{var}: sin NaNs ✔️\")\n",
        "        continue\n",
        "\n",
        "    df_nan = (pd\n",
        "              .DataFrame({\"time\": pd.to_datetime(ds.time.values),\n",
        "                          \"na_cells\": nan_per_ts})\n",
        "              .query(\"na_cells > 0\"))\n",
        "\n",
        "    # primeras 3 y últimas 3 fechas con NaNs\n",
        "    head = df_nan.head(3).to_string(index=False)\n",
        "    tail = df_nan.tail(3).to_string(index=False)\n",
        "    last = df_nan[\"time\"].iloc[-1].strftime(\"%Y-%m\")\n",
        "\n",
        "    print(f\"\\n{var}\")\n",
        "    print(head)\n",
        "    if len(df_nan) > 6:\n",
        "        print(\"   …\")\n",
        "    print(tail)\n",
        "    print(f\"   ⇢  última fecha con NaNs: {last}\")\n",
        "\n",
        "# ============================================================\n",
        "# Primera fecha en la que las TRES variables están 100 % limpias\n",
        "# ------------------------------------------------------------\n",
        "def last_nan_index(var: str) -> int:\n",
        "    \"\"\"Índice del último timestamp que contiene al menos un NaN en `var`.\"\"\"\n",
        "    nan_per_ts = np.isnan(ds[var].values).reshape(len(ds.time), -1).sum(axis=1)\n",
        "    idxs       = np.where(nan_per_ts > 0)[0]\n",
        "    return idxs[-1] if len(idxs) else -1\n",
        "\n",
        "last_nan_any = max(last_nan_index(v) for v in LAG_VARS)\n",
        "first_clean  = pd.to_datetime(ds.time.values[last_nan_any + 1])\n",
        "\n",
        "print(\"\\nPrimera fecha 100 % libre de NaNs en TODOS los lags:\",\n",
        "      first_clean.strftime(\"%Y-%m\"))\n",
        "\n",
        "ds_clean = ds.sel(time=~(ds['time.year'] == 1981))   # descarta TODO 1981\n",
        "\n",
        "print(\"🔎  Timestamps antes :\", len(ds.time))\n",
        "print(\"🔎  Timestamps después:\", len(ds_clean.time))\n",
        "\n",
        "# 3) Guarda nuevo archivo NetCDF\n",
        "ds_clean.to_netcdf(FULL_NC_CLEAN, mode='w')\n",
        "print(f\"💾  Dataset sin 1981 guardado en {FULL_NC_CLEAN}\")\n",
        "\n",
        "# 4) (-- opcional --)  verifica que ya no queden NaNs en los lags\n",
        "LAG_VARS = ['total_precipitation_lag1',\n",
        "            'total_precipitation_lag2',\n",
        "            'total_precipitation_lag12']\n",
        "\n",
        "print(\"\\n📊  NaNs restantes tras quitar 1981\")\n",
        "print(\"─\"*50)\n",
        "for var in LAG_VARS:\n",
        "    n_nan = int(np.isnan(ds_clean[var].values).sum())\n",
        "    print(f\"{var:<28}: {n_nan:,} NaNs\")\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import geopandas as gpd\n",
        "import imageio.v2 as imageio\n",
        "\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, ConvLSTM2D, GRU, Flatten, RepeatVector, Reshape,\n",
        "    TimeDistributed, Dense, MultiHeadAttention, Add,\n",
        "    LayerNormalization, Embedding, Concatenate, Lambda\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "## ╭─────────────────────────── Rutas ──────────────────────────╮\n",
        "# ▶️ Path configuration\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "from functools import partial\n",
        "print('BASE_PATH =', BASE_PATH)\n",
        "\n",
        "# Dataset paths\n",
        "DATA_DIR = BASE_PATH/'data'/'output'\n",
        "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
        "MODEL_DIR = BASE_PATH/'models'/'output'/'HybridLSTMModels'\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
        "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IMAGE_DIR = MODEL_DIR/'images'\n",
        "IMAGE_DIR.mkdir(exist_ok=True)\n",
        "FULL_NC = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "dept_gdf = gpd.read_file(MODEL_INPUT_DIR/'MGN_Departamento.shp')\n",
        "\n",
        "BASE_MODEL_DIR = MODEL_DIR\n",
        "GIF_DIR        = MODEL_DIR / \"gifs\"\n",
        "GIF_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭──────────────────────── Dataset & Shapes ──────────────────╮\n",
        "ds          = xr.open_dataset(FULL_NC)\n",
        "lat, lon    = len(ds.latitude), len(ds.longitude)\n",
        "cells       = lat * lon\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭──────────────────── Hyper‑parámetros globales ─────────────╮\n",
        "INPUT_WINDOW   = 60\n",
        "HORIZON        = 3\n",
        "TARGET_VAR     = 'total_precipitation'\n",
        "EPOCHS         = 50\n",
        "BATCH_SIZE     = 16           # tamaño pequeño → menor RAM GPU\n",
        "PATIENCE       = 40\n",
        "LR             = 1e-3\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ╭────────────────────── Modelo base ConvLSTM ────────────────╮\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def tile_step_emb(batch_ref, step_emb_tab):\n",
        "    # Forma estática (TensorShape / TensorSpec)\n",
        "    if isinstance(batch_ref, (tf.TensorShape, tf.TensorSpec)):\n",
        "        return tf.TensorShape([batch_ref[0],\n",
        "                               step_emb_tab.shape[0],\n",
        "                               step_emb_tab.shape[1]])\n",
        "    # Ejecución\n",
        "    b   = tf.shape(batch_ref)[0]\n",
        "    emb = tf.expand_dims(step_emb_tab, 0)\n",
        "    return tf.tile(emb, [b, 1, 1])\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "custom = {'tile_step_emb': tile_step_emb}\n",
        "\n",
        "def _build_convlstm_ed(\n",
        "        *,\n",
        "        input_window: int,\n",
        "        output_horizon: int,\n",
        "        spatial_height: int,\n",
        "        spatial_width: int,\n",
        "        n_features: int,\n",
        "        n_filters: int = 64,\n",
        "        n_heads: int = 4,\n",
        "        use_attention: bool = True,\n",
        "        use_positional_emb: bool = True,\n",
        "        lr: float = 1e-3\n",
        "    ) -> Model:\n",
        "    \"\"\"\n",
        "    Encoder-Decoder ConvLSTM + GRU.\n",
        "    Si `use_positional_emb` = True añade un embedding del paso de salida\n",
        "    que evita que el modelo genere la misma predicción en todos los horizontes.\n",
        "    \"\"\"\n",
        "\n",
        "    # ──────────────── Encoder ────────────────\n",
        "    enc_inputs = Input(\n",
        "        shape=(input_window, spatial_height, spatial_width, n_features),\n",
        "        name=\"enc_input\"\n",
        "    )\n",
        "\n",
        "    x = ConvLSTM2D(n_filters, (3, 3), padding='same',\n",
        "                   return_sequences=True,  name=\"enc_lstm_1\")(enc_inputs)\n",
        "    x = ConvLSTM2D(n_filters // 2, (3, 3), padding='same',\n",
        "                   return_sequences=False, name=\"enc_lstm_2\")(x)\n",
        "\n",
        "    # ── Aplana grilla y repite contexto T_out veces ──\n",
        "    flat = Flatten(name=\"flatten_spatial\")(x)                 # (B, H·W·C)\n",
        "    ctx  = RepeatVector(output_horizon, name=\"context\")(flat) # (B, T_out, H·W·C)\n",
        "\n",
        "    # ── Positional embedding CORREGIDO ──\n",
        "    if use_positional_emb:\n",
        "        # Crear IDs de pasos como input constante\n",
        "        step_ids_input = Input(shape=(output_horizon,), dtype=tf.int32, name=\"step_ids\")\n",
        "\n",
        "        # Embedding layer\n",
        "        step_emb_layer = Embedding(output_horizon, n_filters, name=\"step_embedding\")\n",
        "        step_emb = step_emb_layer(step_ids_input)  # (B, T_out, D)\n",
        "\n",
        "        # Concatenar con contexto\n",
        "        dec_in = Concatenate(name=\"dec_concat\")([ctx, step_emb])\n",
        "\n",
        "        # Actualizar inputs del modelo\n",
        "        model_inputs = [enc_inputs, step_ids_input]\n",
        "    else:\n",
        "        dec_in = ctx\n",
        "        model_inputs = enc_inputs\n",
        "\n",
        "    # ─────────────── Decoder temporal ───────────────\n",
        "    dec = GRU(2 * n_filters, return_sequences=True, name=\"dec_gru\")(dec_in) # (B, T_out, 2·F)\n",
        "\n",
        "    # ─────── Attention (opcional) ───────\n",
        "    if use_attention:\n",
        "        attn = MultiHeadAttention(num_heads=n_heads,\n",
        "                                  key_dim=n_filters,\n",
        "                                  dropout=0.1,\n",
        "                                  name=\"mha\")(dec, dec)\n",
        "        dec  = Add(name=\"mha_residual\")([dec, attn])\n",
        "        dec  = LayerNormalization(name=\"mha_norm\")(dec)\n",
        "\n",
        "    # ───────────── Proyección a grilla ─────────────\n",
        "    proj = TimeDistributed(\n",
        "        Dense(spatial_height * spatial_width, activation='linear'),\n",
        "        name=\"dense_proj\"\n",
        "    )(dec)                                                    # (B, T_out, H·W)\n",
        "\n",
        "    out = Reshape(\n",
        "        (output_horizon, spatial_height, spatial_width, 1),\n",
        "        name=\"reshape_out\"\n",
        "    )(proj)\n",
        "\n",
        "    name = (\"ConvLSTM_ED_Attn_PE\" if use_attention else \"ConvLSTM_ED_PE\") \\\n",
        "           if use_positional_emb else \\\n",
        "           (\"ConvLSTM_ED_Attn\"     if use_attention else \"ConvLSTM_ED\")\n",
        "\n",
        "    model = Model(model_inputs, out, name=name)\n",
        "    model.compile(optimizer=Adam(lr), loss='mse')\n",
        "    return model\n",
        "\n",
        "# Factories ---------------------------------------------------\n",
        "\n",
        "def factory_no_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=False, **kw)\n",
        "\n",
        "def factory_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=True, **kw)\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭────────────────────────── Métricas ────────────────────────╮\n",
        "\n",
        "def evaluate(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae  = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-5))) * 100\n",
        "    r2   = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, mape, r2\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭──────────────────────── Quick‑plot ────────────────────────╮\n",
        "\n",
        "def quick_plot(ax, data, cmap, title, date_label, vmin=None, vmax=None):\n",
        "    mesh = ax.pcolormesh(ds.longitude, ds.latitude, data, cmap=cmap, shading='nearest', vmin=vmin, vmax=vmax, transform=ccrs.PlateCarree())\n",
        "    ax.coastlines(); ax.add_geometries(dept_gdf.geometry, ccrs.PlateCarree(), edgecolor='black', facecolor='none', linewidth=1)\n",
        "    gl = ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
        "    ax.set_title(f\"{title}\\n{date_label}\", pad=12)\n",
        "    return mesh\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭────────────────────── Experiments & Folds ─────────────────╮\n",
        "# ▸ Solo mostramos los tres primeros niveles; añade los demás igual\n",
        "BASE_FEATURES = [\n",
        "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
        "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
        "    'elevation','slope','aspect'\n",
        "]\n",
        "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
        "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
        "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
        "\n",
        "FOLDS = {'F1': {'year': 2018,'active': True}}\n",
        "\n",
        "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
        "    'ConvLSTM-ED': {\n",
        "        'active': True,\n",
        "        'feature_list': BASE_FEATURES,\n",
        "        'builder': factory_attn, #factory_no_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE': {\n",
        "        'active': True,\n",
        "        'feature_list': KCE_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4,\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE-PAFC': {\n",
        "        'active': True,\n",
        "        'feature_list': PAFC_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 96,\n",
        "        'n_heads'  : 6,\n",
        "    },\n",
        "}\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭──────────────────── Ventanas deslizadas ───────────────────╮\n",
        "\n",
        "def make_windows(mask:np.ndarray, allow_past_context:bool)->tuple[np.ndarray,np.ndarray]:\n",
        "    \"\"\"Genera ventanas **descartando** las que contienen NaNs.  # 🔸 NEW\"\"\"\n",
        "    seq_X, seq_y = [], []\n",
        "    lim = len(mask) - INPUT_WINDOW - HORIZON + 1\n",
        "    for start in range(lim):\n",
        "        end_w = start + INPUT_WINDOW; end_y = end_w + HORIZON\n",
        "        if allow_past_context:\n",
        "            if not mask[end_w:end_y].all():\n",
        "                continue\n",
        "        else:\n",
        "            if not mask[start:end_y].all():\n",
        "                continue\n",
        "        Xw = Xarr[start:end_w]; yw = yarr[end_w:end_y]\n",
        "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
        "            continue  # 🔸 NEW — descarta ventana con NaNs\n",
        "        seq_X.append(Xw); seq_y.append(yw)\n",
        "    return np.array(seq_X), np.array(seq_y)\n",
        "\n",
        "\n",
        "# ╭────────────────── Bucle principal de entrenamiento ────────╮\n",
        "RESULTS: List[Dict[str, Any]] = []\n",
        "\n",
        "# 🔸 NEW helper ------------------------------------------------\n",
        "\n",
        "def _impute_nans(a:np.ndarray, per_feature_mean:np.ndarray|None=None, is_target:bool=False)->np.ndarray:\n",
        "    \"\"\"Imputa NaNs restantes (seguridad extra).\"\"\"\n",
        "    if not np.isnan(a).any():\n",
        "        return a\n",
        "    if is_target:\n",
        "        a[np.isnan(a)] = 0.0  # 🔸 NEW – 0 para y\n",
        "        return a\n",
        "    if per_feature_mean is None:\n",
        "        raise ValueError('per_feature_mean required for imputing X')\n",
        "    flat = a.reshape(-1, a.shape[-1])\n",
        "    nan_idx = np.isnan(flat)\n",
        "    for f in range(a.shape[-1]):\n",
        "        flat[nan_idx[:,f], f] = per_feature_mean[f]  # 🔸 NEW\n",
        "    return flat.reshape(a.shape)\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "def run_all_experiments():\n",
        "    times = pd.to_datetime(ds.time.values)\n",
        "    total = sum(e['active'] for e in EXPERIMENTS.values()) * sum(f['active'] for f in FOLDS.values())\n",
        "    cnt   = 0\n",
        "\n",
        "    for exp_name, exp_cfg in EXPERIMENTS.items():\n",
        "        if not exp_cfg['active']:\n",
        "            continue\n",
        "        vars_     = exp_cfg['feature_list']\n",
        "        builder   = exp_cfg['builder']      # fábrica específica\n",
        "        n_filters = exp_cfg.get('n_filters',64)\n",
        "        n_heads   = exp_cfg.get('n_heads',4)\n",
        "\n",
        "        # ─ Pre‑load features por experimento ─────────────────────\n",
        "        global Xarr, yarr\n",
        "        Xarr = ds[vars_].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "        yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
        "        feats = Xarr.shape[-1]\n",
        "\n",
        "        for fold_name, fold_cfg in FOLDS.items():\n",
        "            if not fold_cfg['active']:\n",
        "                continue\n",
        "            cnt += 1\n",
        "            year_val = fold_cfg['year']\n",
        "            print(f\"\\n▶️  [{cnt}/{total}] {exp_name} – {fold_name} (val={year_val})\")\n",
        "\n",
        "            mask_val = times.year == year_val\n",
        "            mask_tr  = ~mask_val\n",
        "            if mask_val.sum() < HORIZON:\n",
        "                print(\"⚠️ Año sin pasos suficientes → skip\"); continue\n",
        "\n",
        "            X_tr, y_tr = make_windows(mask_tr,  allow_past_context=False)\n",
        "            X_va, y_va = make_windows(mask_val, allow_past_context=True)\n",
        "            print(f\"Ventanas train: {len(X_tr)} · val: {len(X_va)}\")\n",
        "            if len(X_tr)==0 or len(X_va)==0:\n",
        "                print(\"⚠️ Sin ventanas válidas → skip\"); continue\n",
        "\n",
        "            # 🔸 NEW — Imputación de seguridad\n",
        "            feat_mean = np.nanmean(X_tr.reshape(-1,feats),axis=0)\n",
        "            X_tr = _impute_nans(X_tr,feat_mean); X_va=_impute_nans(X_va,feat_mean)\n",
        "            y_tr = _impute_nans(y_tr,is_target=True); y_va=_impute_nans(y_va,is_target=True)\n",
        "\n",
        "            # ─ Scaling (fit solo en train) ─────────────────────\n",
        "            sx = StandardScaler().fit(X_tr.reshape(-1, feats))\n",
        "            sy = StandardScaler().fit(y_tr.reshape(-1, 1))\n",
        "            X_tr_sc = sx.transform(X_tr.reshape(-1, feats)).reshape(X_tr.shape)\n",
        "            X_va_sc = sx.transform(X_va.reshape(-1, feats)).reshape(X_va.shape)\n",
        "            y_tr_sc = sy.transform(y_tr.reshape(-1, 1)).reshape(y_tr.shape)[..., None]\n",
        "            y_va_sc = sy.transform(y_va.reshape(-1, 1)).reshape(y_va.shape)[..., None]\n",
        "\n",
        "            # ─ Build & train model (factory) ───────────────────\n",
        "            tag        = f\"{exp_name.replace('+','_')}_{fold_name}\"\n",
        "            model_path = BASE_MODEL_DIR / f\"{tag}.keras\"\n",
        "            if model_path.exists():\n",
        "                print(f\"⏩ {tag} ya existe → skip\"); continue\n",
        "\n",
        "            model = builder(\n",
        "                input_window=INPUT_WINDOW,\n",
        "                output_horizon=HORIZON,\n",
        "                spatial_height=lat,\n",
        "                spatial_width=lon,\n",
        "                n_features=feats,\n",
        "                n_filters=n_filters,\n",
        "                n_heads=n_heads,\n",
        "                lr=LR\n",
        "            )\n",
        "\n",
        "            # Preparar step_ids para entrenamiento\n",
        "            step_ids_train = np.tile(np.arange(HORIZON), (len(X_tr_sc), 1))\n",
        "            step_ids_val = np.tile(np.arange(HORIZON), (len(X_va_sc), 1))\n",
        "\n",
        "            # Verificar si el modelo usa positional embedding\n",
        "            uses_pe = len(model.inputs) > 1\n",
        "\n",
        "            if uses_pe:\n",
        "                X_train_input = [X_tr_sc, step_ids_train]\n",
        "                X_val_input = [X_va_sc, step_ids_val]\n",
        "            else:\n",
        "                X_train_input = X_tr_sc\n",
        "                X_val_input = X_va_sc\n",
        "\n",
        "            es   = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
        "            hist = model.fit(X_train_input, y_tr_sc, validation_data=(X_val_input, y_va_sc), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es], verbose=1)\n",
        "\n",
        "            # ─ Evaluación ─────────────────────────────────────\n",
        "            if uses_pe:\n",
        "                y_hat_sc = model.predict([X_va_sc, step_ids_val], verbose=0)\n",
        "            else:\n",
        "                y_hat_sc = model.predict(X_va_sc, verbose=0)\n",
        "            y_hat    = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(y_hat_sc.shape)\n",
        "            y_true   = sy.inverse_transform(y_va_sc.reshape(-1,1)).reshape(y_va_sc.shape)\n",
        "\n",
        "            rmse, mae, mape, r2 = evaluate(y_true.ravel(), y_hat.ravel())\n",
        "            RESULTS.append(dict(experiment=exp_name, fold=fold_name, RMSE=rmse, MAE=mae, MAPE=mape, R2=r2, epochs=len(hist.history['loss'])))\n",
        "\n",
        "            # ─ Guardado artefactos ────────────────────────────\n",
        "            model.save(model_path)\n",
        "            plt.figure(); plt.plot(hist.history['loss'], label='train'); plt.plot(hist.history['val_loss'], label='val'); plt.legend(); plt.title(tag); plt.savefig(IMAGE_DIR/f\"{tag}.png\"); plt.close()\n",
        "\n",
        "            # Verificar que las predicciones varían entre horizontes\n",
        "            print(f\"Verificación de predicciones para {tag}:\")\n",
        "            for h in range(HORIZON):\n",
        "                pred_h = y_hat[0, h, ..., 0]  # Primera muestra, horizonte h\n",
        "                print(f\"  H{h+1}: min={pred_h.min():.3f}, max={pred_h.max():.3f}, mean={pred_h.mean():.3f}, std={pred_h.std():.3f}\")\n",
        "\n",
        "            # Usar la última ventana de validación para mejor visualización\n",
        "            last_idx = min(len(y_hat)-1, 10)  # Usar una de las últimas ventanas\n",
        "            _generate_gif(y_true[last_idx], y_hat[last_idx], tag)\n",
        "            print(f\"✅ Guardado {model_path.name}\")\n",
        "\n",
        "    # ─ Métricas globales ────────────────────────────────────\n",
        "    df = pd.DataFrame(RESULTS)\n",
        "    out_csv = BASE_MODEL_DIR / \"metrics_experiments_folds.csv\"\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"\\n📑 Tabla de métricas en {out_csv}\")\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭──────────────────── Generador de GIF ──────────────────────╮\n",
        "\n",
        "def _generate_gif(y_true_sample, y_pred_sample, tag):\n",
        "    pcm_min, pcm_max = 0, np.max(y_pred_sample)\n",
        "    frames = []\n",
        "    for h in range(HORIZON):\n",
        "        pmap = y_pred_sample[h, ..., 0]\n",
        "        fig, ax = plt.subplots(1,1, figsize=(6,5), subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "        mesh = ax.pcolormesh(ds.longitude, ds.latitude, pmap, cmap='Blues', shading='nearest', vmin=pcm_min, vmax=pcm_max, transform=ccrs.PlateCarree())\n",
        "        ax.coastlines(); ax.gridlines(draw_labels=True)\n",
        "        ax.set_title(f\"{tag} – H{h+1}\")\n",
        "        fig.colorbar(mesh, ax=ax, fraction=0.046, pad=0.04)\n",
        "        tmp = GIF_DIR/f\"tmp_{tag}_h{h}.png\"\n",
        "        fig.savefig(tmp, bbox_inches='tight'); plt.close(fig)\n",
        "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
        "    gif_path = GIF_DIR/f\"{tag}.gif\"\n",
        "    imageio.mimsave(gif_path, frames, fps=0.5)\n",
        "    print(f\"💾 GIF {gif_path.name} listo\")\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "# ╭────────────────────── Bucle principal ─────────────────────╮\n",
        "run_all_experiments()\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# 📈 **Evaluador para salidas espaciales ConvLSTM**\n",
        "\n",
        "# ───────── Imports ──────────\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, xarray as xr, tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt, geopandas as gpd, imageio.v2 as imageio\n",
        "import sys\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# ───────── Paths & Constantes ─────────\n",
        "# ▶️ Path configuration\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "print('BASE_PATH =', BASE_PATH)\n",
        "\n",
        "# Dataset paths\n",
        "DATA_DIR = BASE_PATH/'data'/'output'\n",
        "MODEL_OUTPUT_DIR = BASE_PATH/'models'/'output'\n",
        "MODEL_DIR = BASE_PATH/'models'/'output'/'HybridLSTMModels'\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODEL_INPUT_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
        "MODEL_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IMAGE_DIR = MODEL_DIR/'images'\n",
        "IMAGE_DIR.mkdir(exist_ok=True)\n",
        "FULL_NC = DATA_DIR/'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "departamentos = gpd.read_file(MODEL_INPUT_DIR/'MGN_Departamento.shp')\n",
        "\n",
        "BASE_MODEL_DIR = MODEL_DIR\n",
        "GIF_DIR        = MODEL_DIR / \"gifs\"\n",
        "GIF_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# ╭──────────────────── Hyper‑parámetros globales ─────────────╮\n",
        "INPUT_WINDOW   = 60\n",
        "HORIZON        = 3\n",
        "TARGET_VAR     = 'total_precipitation'\n",
        "EPOCHS         = 12\n",
        "BATCH_SIZE     = 4           # tamaño pequeño → menor RAM GPU\n",
        "PATIENCE       = 10\n",
        "LR             = 1e-3\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "# ───────── Dataset & shapes ─────────\n",
        "ds = xr.open_dataset(FULL_NC); lat,lon=len(ds.latitude),len(ds.longitude)\n",
        "\n",
        "#╭────────────────────── Experiments & Folds ─────────────────╮\n",
        "\n",
        "# Factories ---------------------------------------------------\n",
        "\n",
        "def factory_no_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=False, **kw)\n",
        "\n",
        "def factory_attn(**kw):\n",
        "    return _build_convlstm_ed(use_attention=True, **kw)\n",
        "\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def tile_step_emb(batch_ref, step_emb_tab):\n",
        "    \"\"\"\n",
        "    Replica la tabla de embedding (T_out, D) → (B, T_out, D).\n",
        "\n",
        "    · Durante la inferencia de forma, `batch_ref` es TensorShape\n",
        "      → devolvemos TensorShape (None, T_out, D).\n",
        "    · En ejecución, `batch_ref` es tensor\n",
        "      → devolvemos tensor (B, T_out, D).\n",
        "\n",
        "    ▸ `step_emb_tab` SIEMPRE llega desde el cierre de la Lambda original,\n",
        "      así que NO lo pongas opcional.\n",
        "    \"\"\"\n",
        "    # ——— 1) Forma estática ———\n",
        "    if isinstance(batch_ref, (tf.TensorShape, tf.TensorSpec)):\n",
        "        return tf.TensorShape([batch_ref[0],\n",
        "                               step_emb_tab.shape[0],\n",
        "                               step_emb_tab.shape[1]])\n",
        "\n",
        "    # ——— 2) Ejecución ———\n",
        "    b   = tf.shape(batch_ref)[0]\n",
        "    emb = tf.expand_dims(step_emb_tab, 0)    # (1, T_out, D)\n",
        "    return tf.tile(emb, [b, 1, 1])           # (B, T_out, D)\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "custom = {'tile_step_emb': tile_step_emb}\n",
        "\n",
        "# ▸ Solo mostramos los tres primeros niveles; añade los demás igual\n",
        "BASE_FEATURES = [\n",
        "    'year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
        "    'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
        "    'elevation','slope','aspect'\n",
        "]\n",
        "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
        "KCE_FEATURES = BASE_FEATURES + ELEV_CLUSTER\n",
        "PAFC_FEATURES= KCE_FEATURES + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
        "\n",
        "FOLDS = {'F1': {'year': 2018,'active': True}}\n",
        "\n",
        "EXPERIMENTS: Dict[str, Dict[str, Any]] = {\n",
        "    'ConvLSTM-ED': {\n",
        "        'active': True,\n",
        "        'feature_list': BASE_FEATURES,\n",
        "        'builder': factory_attn, #factory_no_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE': {\n",
        "        'active': True,\n",
        "        'feature_list': KCE_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 64,\n",
        "        'n_heads'  : 4,\n",
        "    },\n",
        "    'ConvLSTM-ED-KCE-PAFC': {\n",
        "        'active': True,\n",
        "        'feature_list': PAFC_FEATURES,\n",
        "        'builder': factory_attn,\n",
        "        'n_filters': 96,\n",
        "        'n_heads'  : 6,\n",
        "    },\n",
        "}\n",
        "# ╰────────────────────────────────────────────────────────────╯\n",
        "\n",
        "\n",
        "def quick_plot(ax,data,cmap,title,date_label,vmin=None,vmax=None):\n",
        "    mesh=ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
        "    ax.coastlines(); ax.add_geometries(departamentos.geometry,ccrs.PlateCarree(),edgecolor='black',facecolor='none',linewidth=1)\n",
        "    gl=ax.gridlines(draw_labels=True); gl.top_labels=False; gl.right_labels=False\n",
        "    ax.set_title(f\"{title}\\n{date_label}\",pad=10); return mesh\n",
        "\n",
        "# ───────── Recuperamos diccionario EXPERIMENTS (del bloque de entrenamiento) ─────────\n",
        "from typing import Dict\n",
        "EXPERIMENTS:Dict[str,Dict[str,Any]] = {\n",
        "    'ConvLSTM-ED':              {'feature_list': \"+\".join(BASE_FEATURES).split(\"+\")},\n",
        "    'ConvLSTM-ED-KCE':          {'feature_list': \"+\".join(KCE_FEATURES).split(\"+\")},\n",
        "    'ConvLSTM-ED-KCE-PAFC':     {'feature_list': \"+\".join(PAFC_FEATURES).split(\"+\")},\n",
        "    # otros experimentos\n",
        "}\n",
        "\n",
        "# ———————————————————— Evaluación ————————————————————\n",
        "all_metrics=[]; times=pd.to_datetime(ds.time.values)\n",
        "for mpath in sorted(BASE_MODEL_DIR.glob(\"*.keras\")):\n",
        "    tag   = mpath.stem                        # p.ej. ConvLSTM-ED_F1\n",
        "    parts = tag.split(\"_\")\n",
        "    fold  = parts[-1]                         # F1\n",
        "    exp_token = \"_\".join(parts[:-1])\n",
        "    exp_name  = exp_token.replace(\"_\",\"+\")  # vuelve al nombre original con +\n",
        "    if exp_name not in EXPERIMENTS:\n",
        "        print(\"⚠️ Exp no encontrado para\",tag); continue\n",
        "    feats = EXPERIMENTS[exp_name]['feature_list']\n",
        "    print(f\"\\n🔍 Evaluando {tag} …\")\n",
        "\n",
        "    # — Extracción de arrays —\n",
        "    Xarr = ds[feats].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "    yarr = ds[TARGET_VAR].values.astype(np.float32)\n",
        "    T,_,_,F = Xarr.shape\n",
        "    Xfull = Xarr; yfull=yarr  # mantenemos (T,H,W,F)\n",
        "\n",
        "    # ventana final (idéntica lógica del cuaderno original)\n",
        "    start=T-INPUT_WINDOW-HORIZON; end_w=start+INPUT_WINDOW; end_y=end_w+HORIZON\n",
        "    X_eval = Xfull[start:end_w]                 # (60,H,W,F)\n",
        "    y_eval = yfull[end_w:end_y]                 # (3,H,W)\n",
        "\n",
        "    # — Scalers (fit vectorizado) —\n",
        "    flat_X = Xfull.reshape(-1, F)      # (T·H·W, F)\n",
        "    flat_y = yfull.reshape(-1, 1)      # (T·H·W, 1)\n",
        "\n",
        "    sx = StandardScaler().fit(flat_X)\n",
        "    sy = StandardScaler().fit(flat_y)\n",
        "\n",
        "    Xe_sc = sx.transform(X_eval.reshape(-1, F)).reshape(1, INPUT_WINDOW, lat, lon, F)\n",
        "    ye_sc = sy.transform(y_eval.reshape(-1, 1)).reshape(1, HORIZON, lat, lon, 1)\n",
        "\n",
        "\n",
        "    model = tf.keras.models.load_model(\n",
        "    mpath,\n",
        "    compile=False,\n",
        "    custom_objects={'tile_step_emb': tile_step_emb}\n",
        "    )\n",
        "\n",
        "    # Verificar si el modelo usa positional embedding\n",
        "    uses_pe = len(model.inputs) > 1\n",
        "\n",
        "    if uses_pe:\n",
        "        step_ids_eval = np.tile(np.arange(HORIZON), (1, 1))\n",
        "        yhat_sc = model.predict([Xe_sc, step_ids_eval], verbose=0)  # (1,3,H,W,1)\n",
        "    else:\n",
        "        yhat_sc = model.predict(Xe_sc, verbose=0)  # (1,3,H,W,1)\n",
        "    print(f\"Verificación de predicciones para {tag}:\")\n",
        "    for h in range(HORIZON):\n",
        "        pred_h = yhat_sc[0, h, ..., 0]  # Primera muestra, horizonte h\n",
        "        print(f\"  H{h+1}: min={pred_h.min():.3f}, max={pred_h.max():.3f}, mean={pred_h.mean():.3f}, std={pred_h.std():.3f}\")\n",
        "    # Verificar si las predicciones son idénticas\n",
        "    if HORIZON > 1:\n",
        "        diff_h1_h2 = np.abs(yhat_sc[0, 0] - yhat_sc[0, 1]).mean()\n",
        "        print(f\"  Diferencia promedio H1 vs H2: {diff_h1_h2:.6f}\")\n",
        "    yhat   = sy.inverse_transform(yhat_sc.reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
        "    ytrue  = y_eval\n",
        "\n",
        "    # — Métricas por horizonte —\n",
        "    for h in range(HORIZON):\n",
        "        yt = ytrue[h].ravel()\n",
        "        yp = yhat[h].ravel()\n",
        "\n",
        "        # ---------- filtro NaN / ±∞ ----------\n",
        "        mask = np.isfinite(yt) & np.isfinite(yp)\n",
        "        if mask.sum() == 0:          # ventana vacía → se ignora\n",
        "            print(f\"   · h={h+1}: todos los valores son NaN/Inf → skip\")\n",
        "            continue\n",
        "        yt, yp = yt[mask], yp[mask]\n",
        "        # -------------------------------------\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(yt, yp))\n",
        "        mae  = mean_absolute_error(yt, yp)\n",
        "        mape = np.mean(np.abs((yt - yp) / (yt + 1e-5))) * 100\n",
        "        r2   = r2_score(yt, yp)\n",
        "\n",
        "        all_metrics.append(dict(\n",
        "            model      = tag,\n",
        "            experiment = exp_name,\n",
        "            fold       = fold,\n",
        "            horizon    = h + 1,\n",
        "            RMSE       = rmse,\n",
        "            MAE        = mae,\n",
        "            MAPE       = mape,\n",
        "            R2         = r2\n",
        "        ))\n",
        "\n",
        "    # — Figura Real vs Pred vs MAPE —\n",
        "    fig,axes=plt.subplots(HORIZON,3,figsize=(14,4*HORIZON),subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "    dates=pd.date_range(times[end_w],periods=HORIZON,freq='MS')\n",
        "    vmin=0; vmax=max(yhat.max(),ytrue.max())\n",
        "    for h in range(HORIZON):\n",
        "        quick_plot(axes[h,0],ytrue[h],'Blues',f\"Real h={h+1}\",dates[h].strftime('%Y-%m'),vmin,vmax)\n",
        "        quick_plot(axes[h,1],yhat [h],'Blues',f\"Pred h={h+1}\",dates[h].strftime('%Y-%m'),vmin,vmax)\n",
        "        err=np.clip(np.abs((ytrue[h]-yhat[h])/(ytrue[h]+1e-5))*100,0,100)\n",
        "        quick_plot(axes[h,2],err,'Reds',f\"MAPE% h={h+1}\",dates[h].strftime('%Y-%m'),0,100)\n",
        "    fig.suptitle(f\"{tag}  — Eval final ventana\",fontsize=16); fig.tight_layout();\n",
        "    fig.savefig(BASE_MODEL_DIR/f\"fig_{tag}.png\"); plt.close(fig)\n",
        "\n",
        "    # — GIF —\n",
        "    frames=[]; pcm_min,pcm_max=0,yhat.max()\n",
        "    for h in range(HORIZON):\n",
        "        figg,ax=plt.subplots(1,1,figsize=(6,5),subplot_kw={'projection':ccrs.PlateCarree()})\n",
        "        m=ax.pcolormesh(ds.longitude,ds.latitude,yhat[h],cmap='Blues',shading='nearest',vmin=pcm_min,vmax=pcm_max,transform=ccrs.PlateCarree())\n",
        "        ax.coastlines(); ax.set_title(f\"{tag} – H{h+1}\"); figg.colorbar(m,ax=ax,fraction=0.046,pad=0.04)\n",
        "        tmp=GIF_DIR/f\"tmp_{tag}_{h}.png\"; figg.savefig(tmp,bbox_inches='tight'); plt.close(figg)\n",
        "        frames.append(imageio.imread(tmp)); tmp.unlink(missing_ok=True)\n",
        "    imageio.mimsave(GIF_DIR/f\"{tag}.gif\",frames,fps=0.5)\n",
        "    print(\"💾 GIF\",f\"{tag}.gif\",\"creado\")\n",
        "\n",
        "# ——— Guardar tabla ———\n",
        "pd.DataFrame(all_metrics).to_csv(BASE_MODEL_DIR/'metrics_eval.csv',index=False)\n",
        "print(\"📑 Métricas guardadas en\",BASE_MODEL_DIR/'metrics_eval.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbyj_vPBXqrQ"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── CAPAS DE ATENCIÓN ─────────────────────────\n",
        "\n",
        "class SpatialAttention(Layer):\n",
        "    \"\"\"Atención espacial para resaltar regiones importantes\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.conv = Conv2D(1, (7, 7), padding='same', activation='sigmoid')\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Calcular estadísticas del canal\n",
        "        avg_pool = K.mean(inputs, axis=-1, keepdims=True)\n",
        "        max_pool = K.max(inputs, axis=-1, keepdims=True)\n",
        "        concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
        "\n",
        "        # Generar mapa de atención\n",
        "        attention = self.conv(concat)\n",
        "\n",
        "        return Multiply()([inputs, attention])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class ChannelAttention(Layer):\n",
        "    \"\"\"Atención de canal para ponderar features importantes\"\"\"\n",
        "\n",
        "    def __init__(self, reduction_ratio=8, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "        self.fc1 = Dense(channels // self.reduction_ratio, activation='relu')\n",
        "        self.fc2 = Dense(channels, activation='sigmoid')\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Global pooling\n",
        "        avg_pool = GlobalAveragePooling2D()(inputs)\n",
        "        max_pool = K.max(inputs, axis=[1, 2])\n",
        "\n",
        "        # Shared MLP\n",
        "        avg_out = self.fc2(self.fc1(avg_pool))\n",
        "        max_out = self.fc2(self.fc1(max_pool))\n",
        "\n",
        "        # Combinar\n",
        "        attention = avg_out + max_out\n",
        "        attention = K.expand_dims(K.expand_dims(attention, 1), 1)\n",
        "\n",
        "        return Multiply()([inputs, attention])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'reduction_ratio': self.reduction_ratio\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class CBAM(Layer):\n",
        "    \"\"\"Convolutional Block Attention Module\"\"\"\n",
        "\n",
        "    def __init__(self, reduction_ratio=8, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.channel_attention = ChannelAttention(self.reduction_ratio)\n",
        "        self.spatial_attention = SpatialAttention()\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.channel_attention(inputs)\n",
        "        x = self.spatial_attention(x)\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'reduction_ratio': self.reduction_ratio\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"✅ Capas de atención implementadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yqAzSOTXqrQ"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── CAPAS AVANZADAS ─────────────────────────\n",
        "\n",
        "class ConvGRU2DCell(Layer):\n",
        "    \"\"\"Celda ConvGRU2D mejorada con BatchNorm\"\"\"\n",
        "\n",
        "    def __init__(self, filters, kernel_size, padding='same', activation='tanh',\n",
        "                 recurrent_activation='sigmoid', use_batch_norm=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.padding = padding\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.recurrent_activation = tf.keras.activations.get(recurrent_activation)\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.state_size = (filters,)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        # Kernels\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(*self.kernel_size, input_dim, self.filters * 3),\n",
        "            initializer='glorot_uniform',\n",
        "            regularizer=l1_l2(l1=0, l2=L2_REG),\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(*self.kernel_size, self.filters, self.filters * 3),\n",
        "            initializer='orthogonal',\n",
        "            regularizer=l1_l2(l1=0, l2=L2_REG),\n",
        "            name='recurrent_kernel'\n",
        "        )\n",
        "\n",
        "        self.bias = self.add_weight(\n",
        "            shape=(self.filters * 3,),\n",
        "            initializer='zeros',\n",
        "            name='bias'\n",
        "        )\n",
        "\n",
        "        if self.use_batch_norm:\n",
        "            self.bn_x = BatchNormalization()\n",
        "            self.bn_h = BatchNormalization()\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, states, training=None):\n",
        "        h_tm1 = states[0]\n",
        "\n",
        "        # Convoluciones\n",
        "        x_conv = K.conv2d(inputs, self.kernel, padding=self.padding)\n",
        "        h_conv = K.conv2d(h_tm1, self.recurrent_kernel, padding=self.padding)\n",
        "\n",
        "        if self.use_batch_norm:\n",
        "            x_conv = self.bn_x(x_conv, training=training)\n",
        "            h_conv = self.bn_h(h_conv, training=training)\n",
        "\n",
        "        x_z, x_r, x_h = tf.split(x_conv, 3, axis=-1)\n",
        "        h_z, h_r, h_h = tf.split(h_conv, 3, axis=-1)\n",
        "        b_z, b_r, b_h = tf.split(self.bias, 3)\n",
        "\n",
        "        # Gates\n",
        "        z = self.recurrent_activation(x_z + h_z + b_z)\n",
        "        r = self.recurrent_activation(x_r + h_r + b_r)\n",
        "\n",
        "        # Hidden state\n",
        "        h_candidate = self.activation(x_h + r * h_h + b_h)\n",
        "        h = (1 - z) * h_tm1 + z * h_candidate\n",
        "\n",
        "        return h, [h]\n",
        "\n",
        "\n",
        "class ConvGRU2D(Layer):\n",
        "    \"\"\"ConvGRU2D mejorado con soporte para BatchNorm y Dropout\"\"\"\n",
        "\n",
        "    def __init__(self, filters, kernel_size, padding='same', activation='tanh',\n",
        "                 recurrent_activation='sigmoid', return_sequences=False,\n",
        "                 use_batch_norm=True, dropout=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.activation = activation\n",
        "        self.recurrent_activation = recurrent_activation\n",
        "        self.return_sequences = return_sequences\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.cell = ConvGRU2DCell(\n",
        "            filters, kernel_size, padding, activation,\n",
        "            recurrent_activation, use_batch_norm\n",
        "        )\n",
        "\n",
        "        if dropout > 0:\n",
        "            self.dropout_layer = Dropout(dropout)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.cell.build(input_shape[2:])\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        time_steps = tf.shape(inputs)[1]\n",
        "        height = tf.shape(inputs)[2]\n",
        "        width = tf.shape(inputs)[3]\n",
        "\n",
        "        # Estado inicial\n",
        "        initial_state = tf.zeros((batch_size, height, width, self.filters))\n",
        "\n",
        "        # Procesar secuencia\n",
        "        outputs = []\n",
        "        state = initial_state\n",
        "\n",
        "        for t in range(inputs.shape[1]):\n",
        "            output, [state] = self.cell(inputs[:, t], [state], training=training)\n",
        "\n",
        "            if self.dropout > 0:\n",
        "                output = self.dropout_layer(output, training=training)\n",
        "\n",
        "            outputs.append(output)\n",
        "\n",
        "        outputs = tf.stack(outputs, axis=1)\n",
        "\n",
        "        if self.return_sequences:\n",
        "            return outputs\n",
        "        else:\n",
        "            return outputs[:, -1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'filters': self.filters,\n",
        "            'kernel_size': self.kernel_size,\n",
        "            'padding': self.padding,\n",
        "            'activation': self.activation,\n",
        "            'recurrent_activation': self.recurrent_activation,\n",
        "            'return_sequences': self.return_sequences,\n",
        "            'use_batch_norm': self.use_batch_norm,\n",
        "            'dropout': self.dropout\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"✅ Capas ConvGRU mejoradas implementadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0t1ixIxXqrR"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── MODEL BUILDERS AVANZADOS ─────────────────────────\n",
        "\n",
        "def _advanced_spatial_head(x, use_attention=True):\n",
        "    \"\"\"Cabeza de proyección mejorada con atención opcional\"\"\"\n",
        "\n",
        "    if use_attention:\n",
        "        x = CBAM()(x)\n",
        "\n",
        "    # Multi-scale processing\n",
        "    conv1 = Conv2D(HORIZON, (1, 1), padding='same')(x)\n",
        "    conv3 = Conv2D(HORIZON, (3, 3), padding='same')(x)\n",
        "    conv5 = Conv2D(HORIZON, (5, 5), padding='same')(x)\n",
        "\n",
        "    # Combine multi-scale features\n",
        "    x = Add()([conv1, conv3, conv5])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('linear')(x)\n",
        "\n",
        "    # Reshape to output format usando Permute y Reshape\n",
        "    x = Permute((3, 1, 2))(x)  # De (batch, H, W, HORIZON) a (batch, HORIZON, H, W)\n",
        "    x = Reshape((HORIZON, lat, lon, 1))(x)  # Añadir dimensión de canal\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_convlstm_attention(n_feats: int):\n",
        "    \"\"\"ConvLSTM con mecanismo de atención\"\"\"\n",
        "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
        "\n",
        "    # Primera capa con más filtros\n",
        "    x = ConvLSTM2D(64, (3, 3), padding='same', return_sequences=True,\n",
        "                   kernel_regularizer=l1_l2(l1=0, l2=L2_REG))(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(DROPOUT)(x)\n",
        "\n",
        "    # Segunda capa con atención\n",
        "    x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True,\n",
        "                   kernel_regularizer=l1_l2(l1=0, l2=L2_REG))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Aplicar atención temporal\n",
        "    x = TimeDistributed(CBAM())(x)\n",
        "\n",
        "    # Capa final\n",
        "    x = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=False,\n",
        "                   kernel_regularizer=l1_l2(l1=0, l2=L2_REG))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    out = _advanced_spatial_head(x)\n",
        "    return Model(inp, out, name='ConvLSTM_Attention')\n",
        "\n",
        "\n",
        "def build_convgru_residual(n_feats: int):\n",
        "    \"\"\"ConvGRU con skip connections\"\"\"\n",
        "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
        "\n",
        "    # Encoder path\n",
        "    enc1 = ConvGRU2D(64, (3, 3), return_sequences=True,\n",
        "                     use_batch_norm=True, dropout=DROPOUT)(inp)\n",
        "\n",
        "    enc2 = ConvGRU2D(32, (3, 3), return_sequences=True,\n",
        "                     use_batch_norm=True, dropout=DROPOUT)(enc1)\n",
        "\n",
        "    # Bottleneck\n",
        "    bottleneck = ConvGRU2D(16, (3, 3), return_sequences=False,\n",
        "                           use_batch_norm=True)(enc2)\n",
        "\n",
        "    # Skip connection from input - usar solo la última timestep\n",
        "    skip = TimeDistributed(Conv2D(16, (1, 1), padding='same'))(inp)\n",
        "    # Usar Lambda para el slicing\n",
        "    skip = Lambda(lambda x: x[:, -1, :, :, :])(skip)  # Take last timestep\n",
        "\n",
        "    # Combine\n",
        "    x = Add()([bottleneck, skip])\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    out = _advanced_spatial_head(x)\n",
        "    return Model(inp, out, name='ConvGRU_Residual')\n",
        "\n",
        "\n",
        "def build_hybrid_transformer(n_feats: int):\n",
        "    \"\"\"Modelo híbrido CNN + Transformer\"\"\"\n",
        "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
        "\n",
        "    # Encoder convolucional\n",
        "    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(inp)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "\n",
        "    # Reducir dimensionalidad espacial\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2), padding='same'))(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # Self-attention temporal\n",
        "    x = MultiHeadAttention(num_heads=4, key_dim=32, dropout=DROPOUT)(x, x)\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    # Agregación temporal con LSTM\n",
        "    x = LSTM(128, return_sequences=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(DROPOUT)(x)\n",
        "\n",
        "    # Decodificador espacial\n",
        "    x = Dense(lat * lon * 16)(x)\n",
        "    x = Reshape((lat, lon, 16))(x)\n",
        "\n",
        "    out = _advanced_spatial_head(x)\n",
        "    return Model(inp, out, name='Hybrid_Transformer')\n",
        "\n",
        "\n",
        "# Diccionario de modelos\n",
        "ADVANCED_MODELS = {\n",
        "    'ConvLSTM_Att': build_convlstm_attention,\n",
        "    'ConvGRU_Res': build_convgru_residual,\n",
        "    'Hybrid_Trans': build_hybrid_transformer\n",
        "}\n",
        "\n",
        "print(\"✅ Model builders avanzados creados\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66lB5g6BXqrR"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── CALLBACKS AVANZADOS ─────────────────────────\n",
        "\n",
        "class AdvancedTrainingMonitor(Callback):\n",
        "    \"\"\"Monitor de entrenamiento con visualización en tiempo real\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, experiment_name, patience=10):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.experiment_name = experiment_name\n",
        "        self.patience = patience\n",
        "        self.history = {'loss': [], 'val_loss': [], 'lr': [], 'epoch': []}\n",
        "        self.wait = 0\n",
        "        self.best_val_loss = np.inf\n",
        "        self.converged = False\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Actualizar historial\n",
        "        self.history['loss'].append(logs.get('loss', 0))\n",
        "        self.history['val_loss'].append(logs.get('val_loss', 0))\n",
        "        self.history['lr'].append(K.get_value(self.model.optimizer.learning_rate))\n",
        "        self.history['epoch'].append(epoch + 1)\n",
        "\n",
        "        # Verificar mejora\n",
        "        current_val_loss = logs.get('val_loss', 0)\n",
        "        if current_val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = current_val_loss\n",
        "            self.wait = 0\n",
        "        else:\n",
        "            self.wait += 1\n",
        "\n",
        "        # Verificar convergencia\n",
        "        if len(self.history['val_loss']) > 5:\n",
        "            recent_losses = self.history['val_loss'][-5:]\n",
        "            loss_std = np.std(recent_losses)\n",
        "            loss_mean = np.mean(recent_losses)\n",
        "            if loss_std / loss_mean < 0.01:  # Menos del 1% de variación\n",
        "                self.converged = True\n",
        "\n",
        "        # Visualización cada 5 épocas o en la última\n",
        "        if (epoch + 1) % 5 == 0 or (epoch + 1) == self.params['epochs']:\n",
        "            self._plot_progress()\n",
        "\n",
        "    def _plot_progress(self):\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "        # Loss curves\n",
        "        ax1 = plt.subplot(141)\n",
        "        ax1.plot(self.history['epoch'], self.history['loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "        ax1.plot(self.history['epoch'], self.history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title(f'{self.model_name} - {self.experiment_name}')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss ratio\n",
        "        ax2 = plt.subplot(142)\n",
        "        if len(self.history['loss']) > 0:\n",
        "            ratio = [v/t if t > 0 else 1 for v, t in zip(self.history['val_loss'], self.history['loss'])]\n",
        "            ax2.plot(self.history['epoch'], ratio, 'g-', linewidth=2)\n",
        "            ax2.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
        "            ax2.fill_between(self.history['epoch'], 1, ratio,\n",
        "                           where=[r > 1 for r in ratio],\n",
        "                           color='red', alpha=0.2, label='Overfitting')\n",
        "            ax2.set_xlabel('Epoch')\n",
        "            ax2.set_ylabel('Val Loss / Train Loss')\n",
        "            ax2.set_title('Overfitting Monitor')\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Tasa de mejora y convergencia\n",
        "        ax3 = plt.subplot(143)\n",
        "        if len(self.history['val_loss']) > 1:\n",
        "            # Calcular tasa de mejora epoch a epoch\n",
        "            improvements = []\n",
        "            for i in range(1, len(self.history['val_loss'])):\n",
        "                prev_loss = self.history['val_loss'][i-1]\n",
        "                curr_loss = self.history['val_loss'][i]\n",
        "                improvement = ((prev_loss - curr_loss) / prev_loss) * 100\n",
        "                improvements.append(improvement)\n",
        "\n",
        "            # Plot de tasa de mejora\n",
        "            ax3.plot(self.history['epoch'][1:], improvements, 'purple', linewidth=2, alpha=0.7)\n",
        "            ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "            ax3.fill_between(self.history['epoch'][1:], 0, improvements,\n",
        "                           where=[imp > 0 for imp in improvements],\n",
        "                           color='green', alpha=0.3, label='Mejora')\n",
        "            ax3.fill_between(self.history['epoch'][1:], 0, improvements,\n",
        "                           where=[imp < 0 for imp in improvements],\n",
        "                           color='red', alpha=0.3, label='Empeoramiento')\n",
        "\n",
        "            # Línea de tendencia\n",
        "            if len(improvements) > 3:\n",
        "                z = np.polyfit(range(len(improvements)), improvements, 2)\n",
        "                p = np.poly1d(z)\n",
        "                ax3.plot(self.history['epoch'][1:], p(range(len(improvements))),\n",
        "                       'orange', linewidth=2, linestyle='--', label='Tendencia')\n",
        "\n",
        "            ax3.set_xlabel('Epoch')\n",
        "            ax3.set_ylabel('Mejora (%)')\n",
        "            ax3.set_title('Tasa de Mejora y Convergencia')\n",
        "            ax3.legend()\n",
        "            ax3.grid(True, alpha=0.3)\n",
        "\n",
        "            # Indicador de convergencia\n",
        "            if self.converged:\n",
        "                ax3.text(0.02, 0.98, '✓ Convergido', transform=ax3.transAxes,\n",
        "                       va='top', bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n",
        "\n",
        "        # Training stats\n",
        "        ax4 = plt.subplot(144)\n",
        "        ax4.axis('off')\n",
        "        stats_text = f\"\"\"\n",
        "        {self.model_name} - {self.experiment_name}\n",
        "\n",
        "        Época: {self.history['epoch'][-1]}/{self.params['epochs']}\n",
        "\n",
        "        Loss actual:\n",
        "        • Train: {self.history['loss'][-1]:.6f}\n",
        "        • Val: {self.history['val_loss'][-1]:.6f}\n",
        "\n",
        "        Mejor val loss: {self.best_val_loss:.6f}\n",
        "        Épocas sin mejora: {self.wait}/{self.patience}\n",
        "\n",
        "        Learning rate: {self.history['lr'][-1]:.2e}\n",
        "\n",
        "        Estado: {'Convergido ✓' if self.converged else 'Entrenando...'}\n",
        "        \"\"\"\n",
        "        ax4.text(0.1, 0.9, stats_text, transform=ax4.transAxes,\n",
        "                fontsize=12, verticalalignment='top', fontfamily='monospace')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def create_callbacks(model_name, experiment_name, model_path):\n",
        "    \"\"\"Crea callbacks optimizados para el entrenamiento\"\"\"\n",
        "\n",
        "    # Learning rate scheduler con warmup\n",
        "    def lr_schedule(epoch, lr):\n",
        "        warmup_epochs = 5\n",
        "        if epoch < warmup_epochs:\n",
        "            return LR * (epoch + 1) / warmup_epochs\n",
        "        else:\n",
        "            # Cosine decay después del warmup\n",
        "            progress = (epoch - warmup_epochs) / (EPOCHS - warmup_epochs)\n",
        "            return LR * 0.5 * (1 + np.cos(np.pi * progress))\n",
        "\n",
        "    callbacks = [\n",
        "        # Monitor de entrenamiento avanzado\n",
        "        AdvancedTrainingMonitor(model_name, experiment_name, patience=PATIENCE),\n",
        "\n",
        "        # Early stopping mejorado\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=PATIENCE,\n",
        "            restore_best_weights=True,\n",
        "            verbose=0,\n",
        "            mode='min',\n",
        "            min_delta=1e-4\n",
        "        ),\n",
        "\n",
        "        # Model checkpoint\n",
        "        ModelCheckpoint(\n",
        "            str(model_path),\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode='min',\n",
        "            verbose=0\n",
        "        ),\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        LearningRateScheduler(lr_schedule, verbose=0),\n",
        "\n",
        "        # Reduce LR on plateau como backup\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6,\n",
        "            verbose=0,\n",
        "            mode='min',\n",
        "            min_delta=1e-4\n",
        "        ),\n",
        "\n",
        "        # CSV logger para análisis posterior\n",
        "        CSVLogger(\n",
        "            str(model_path.parent / f\"{model_name}_training.csv\"),\n",
        "            separator=',',\n",
        "            append=False\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    return callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evb3tTGPXqrS"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── HELPERS ─────────────────────────\n",
        "\n",
        "def windowed_arrays(X:np.ndarray, y:np.ndarray):\n",
        "    \"\"\"Crear ventanas deslizantes para series temporales\"\"\"\n",
        "    seq_X, seq_y = [], []\n",
        "    T = len(X)\n",
        "    for start in range(T-INPUT_WINDOW-HORIZON+1):\n",
        "        end_w = start+INPUT_WINDOW\n",
        "        end_y = end_w+HORIZON\n",
        "        Xw, yw = X[start:end_w], y[end_w:end_y]\n",
        "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
        "            continue\n",
        "        seq_X.append(Xw)\n",
        "        seq_y.append(yw)\n",
        "    return np.asarray(seq_X,dtype=np.float32), np.asarray(seq_y,dtype=np.float32)\n",
        "\n",
        "\n",
        "def quick_plot(ax,data,cmap,title,vmin=None,vmax=None):\n",
        "    \"\"\"Plotear mapa geográfico\"\"\"\n",
        "    try:\n",
        "        import cartopy.crs as ccrs\n",
        "        mesh = ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',\n",
        "                             vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
        "        ax.coastlines()\n",
        "        ax.add_geometries(DEPT_GDF.geometry,ccrs.PlateCarree(),\n",
        "                          edgecolor='black',facecolor='none',linewidth=1)\n",
        "        ax.gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
        "\n",
        "        # Agregar colorbar para cada mapa\n",
        "        fig = ax.figure\n",
        "        cbar = fig.colorbar(mesh, ax=ax, fraction=0.046, pad=0.04)\n",
        "        if cmap == 'Blues':\n",
        "            cbar.set_label('Precipitación (mm)', rotation=270, labelpad=15)\n",
        "        elif cmap == 'Reds':\n",
        "            cbar.set_label('MAPE (%)', rotation=270, labelpad=15)\n",
        "    except ImportError:\n",
        "        # Si no hay cartopy, hacer un plot simple\n",
        "        mesh = ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',\n",
        "                             vmin=vmin,vmax=vmax)\n",
        "        ax.set_xlabel('Longitude')\n",
        "        ax.set_ylabel('Latitude')\n",
        "\n",
        "        # Agregar colorbar para cada mapa\n",
        "        fig = ax.figure\n",
        "        cbar = fig.colorbar(mesh, ax=ax, fraction=0.046, pad=0.04)\n",
        "        if cmap == 'Blues':\n",
        "            cbar.set_label('Precipitación (mm)', rotation=270, labelpad=15)\n",
        "        elif cmap == 'Reds':\n",
        "            cbar.set_label('MAPE (%)', rotation=270, labelpad=15)\n",
        "\n",
        "    ax.set_title(title,fontsize=9)\n",
        "    return mesh\n",
        "\n",
        "\n",
        "def save_hyperparameters(exp_path, model_name, hyperparams):\n",
        "    \"\"\"Guarda los hiperparámetros en un archivo JSON\"\"\"\n",
        "    hp_file = exp_path / f\"{model_name}_hyperparameters.json\"\n",
        "    with open(hp_file, 'w') as f:\n",
        "        json.dump(hyperparams, f, indent=4)\n",
        "    print(f\"   💾 Hiperparámetros guardados en: {hp_file.name}\")\n",
        "\n",
        "\n",
        "def plot_learning_curves(history, exp_path, model_name, show=True):\n",
        "    \"\"\"Genera y guarda las curvas de aprendizaje\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "    axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss (MSE)')\n",
        "    axes[0].set_title(f'{model_name} - Loss Evolution')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Análisis de Convergencia y Estabilidad\n",
        "    val_losses = history.history['val_loss']\n",
        "    train_losses = history.history['loss']\n",
        "\n",
        "    if len(val_losses) > 1:\n",
        "        # Calcular métricas de convergencia\n",
        "        epochs = range(1, len(val_losses) + 1)\n",
        "\n",
        "        # 1. Ratio de overfitting\n",
        "        overfit_ratio = [val_losses[i] / train_losses[i] for i in range(len(val_losses))]\n",
        "\n",
        "        # 2. Estabilidad (desviación estándar móvil)\n",
        "        window = min(5, len(val_losses)//3)\n",
        "        val_std = pd.Series(val_losses).rolling(window=window).std()\n",
        "\n",
        "        # Crear subplot con dos ejes Y\n",
        "        ax2_left = axes[1]\n",
        "        ax2_right = ax2_left.twinx()\n",
        "\n",
        "        # Plot ratio de overfitting\n",
        "        line1 = ax2_left.plot(epochs, overfit_ratio, 'r-', linewidth=2,\n",
        "                             label='Ratio Val/Train', alpha=0.8)\n",
        "        ax2_left.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
        "        ax2_left.fill_between(epochs, 1.0, overfit_ratio,\n",
        "                            where=[x > 1.0 for x in overfit_ratio],\n",
        "                            color='red', alpha=0.2)\n",
        "        ax2_left.set_xlabel('Epoch')\n",
        "        ax2_left.set_ylabel('Ratio Val Loss / Train Loss', color='red')\n",
        "        ax2_left.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "        # Plot estabilidad\n",
        "        line2 = ax2_right.plot(epochs[window-1:], val_std[window-1:], 'b-',\n",
        "                             linewidth=2, label='Estabilidad', alpha=0.8)\n",
        "        ax2_right.set_ylabel('Desviación Estándar (ventana móvil)', color='blue')\n",
        "        ax2_right.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "        # Título y leyenda combinada\n",
        "        ax2_left.set_title(f'{model_name} - Análisis de Convergencia')\n",
        "\n",
        "        # Combinar leyendas\n",
        "        lines = line1 + line2\n",
        "        labels = [l.get_label() for l in lines]\n",
        "        ax2_left.legend(lines, labels, loc='upper left')\n",
        "\n",
        "        ax2_left.grid(True, alpha=0.3)\n",
        "\n",
        "        # Añadir zonas de interpretación\n",
        "        if max(overfit_ratio) > 1.5:\n",
        "            ax2_left.text(0.02, 0.98, '⚠️ Alto overfitting detectado',\n",
        "                        transform=ax2_left.transAxes, va='top',\n",
        "                        bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
        "        elif min(val_std[window-1:]) < 0.001:\n",
        "            ax2_left.text(0.02, 0.98, '✓ Entrenamiento estable',\n",
        "                        transform=ax2_left.transAxes, va='top',\n",
        "                        bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n",
        "    else:\n",
        "        axes[1].text(0.5, 0.5, 'Insufficient data for convergence analysis',\n",
        "                    transform=axes[1].transAxes, ha='center', va='center',\n",
        "                    fontsize=12, color='gray')\n",
        "        axes[1].set_title(f'{model_name} - Convergence Analysis')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Guardar\n",
        "    curves_path = exp_path / f\"{model_name}_learning_curves.png\"\n",
        "    plt.savefig(curves_path, dpi=150, bbox_inches='tight')\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "    return curves_path\n",
        "\n",
        "\n",
        "def print_training_summary(history, model_name, exp_name):\n",
        "    \"\"\"Imprime un resumen del entrenamiento\"\"\"\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    final_val_loss = history.history['val_loss'][-1]\n",
        "    best_val_loss = min(history.history['val_loss'])\n",
        "    best_epoch = history.history['val_loss'].index(best_val_loss) + 1\n",
        "\n",
        "    print(f\"\\n   📊 Resumen de entrenamiento {model_name} - {exp_name}:\")\n",
        "    print(f\"      • Épocas totales: {len(history.history['loss'])}\")\n",
        "    print(f\"      • Loss final (train): {final_loss:.6f}\")\n",
        "    print(f\"      • Loss final (val): {final_val_loss:.6f}\")\n",
        "    print(f\"      • Mejor loss (val): {best_val_loss:.6f} en época {best_epoch}\")\n",
        "    if 'lr' in history.history and len(history.history['lr']) > 0:\n",
        "        final_lr = history.history['lr'][-1]\n",
        "        print(f\"      • Learning rate final: {final_lr:.2e}\")\n",
        "    else:\n",
        "        print(f\"      • Learning rate final: No disponible\")\n",
        "\n",
        "print(\"✅ Funciones helper creadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dU6DGcHXqrS"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── TRAIN + EVAL LOOP ─────────────────────────\n",
        "\n",
        "# Diccionario para almacenar historiales de entrenamiento\n",
        "all_histories = {}\n",
        "results = []\n",
        "\n",
        "for exp, feat_list in EXPERIMENTS.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"🔬 EXPERIMENTO: {exp} ({len(feat_list)} features)\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Preparar datos\n",
        "    Xarr = ds[feat_list].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "    yarr = ds['total_precipitation'].values.astype(np.float32)[...,None]\n",
        "    X, y = windowed_arrays(Xarr, yarr)\n",
        "    split = int(0.8*len(X))\n",
        "    val_split = int(0.9*len(X))\n",
        "\n",
        "    # Normalización\n",
        "    sx = StandardScaler().fit(X[:split].reshape(-1,len(feat_list)))\n",
        "    sy = StandardScaler().fit(y[:split].reshape(-1,1))\n",
        "    X_sc = sx.transform(X.reshape(-1,len(feat_list))).reshape(X.shape)\n",
        "    y_sc = sy.transform(y.reshape(-1,1)).reshape(y.shape)\n",
        "\n",
        "    # Splits\n",
        "    X_tr, X_va, X_te = X_sc[:split], X_sc[split:val_split], X_sc[val_split:]\n",
        "    y_tr, y_va, y_te = y_sc[:split], y_sc[split:val_split], y_sc[val_split:]\n",
        "\n",
        "    print(f\"   Datos: Train={len(X_tr)}, Val={len(X_va)}, Test={len(X_te)}\")\n",
        "\n",
        "    OUT_EXP = OUT_ROOT/exp\n",
        "    OUT_EXP.mkdir(exist_ok=True)\n",
        "\n",
        "    # Crear subdirectorio para métricas de entrenamiento\n",
        "    METRICS_DIR = OUT_EXP / 'training_metrics'\n",
        "    METRICS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "    for mdl_name, builder in ADVANCED_MODELS.items():\n",
        "        print(f\"\\n{'─'*50}\")\n",
        "        print(f\"🤖 Modelo: {mdl_name}\")\n",
        "        print(f\"{'─'*50}\")\n",
        "\n",
        "        model_path = OUT_EXP/f\"{mdl_name.lower()}_best.keras\"\n",
        "        if model_path.exists():\n",
        "            model_path.unlink()\n",
        "\n",
        "        try:\n",
        "            # Construir modelo\n",
        "            model = builder(n_feats=len(feat_list))\n",
        "\n",
        "            # Definir optimizador con configuración explícita\n",
        "            optimizer = AdamW(learning_rate=LR, weight_decay=L2_REG)\n",
        "            model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "            # Hiperparámetros\n",
        "            hyperparams = {\n",
        "                'experiment': exp,\n",
        "                'model': mdl_name,\n",
        "                'features': [str(f) for f in feat_list],  # Convertir a strings\n",
        "                'n_features': int(len(feat_list)),\n",
        "                'input_window': int(INPUT_WINDOW),\n",
        "                'horizon': int(HORIZON),\n",
        "                'batch_size': int(BATCH_SIZE),\n",
        "                'initial_lr': float(LR),\n",
        "                'epochs': int(EPOCHS),\n",
        "                'patience': int(PATIENCE),\n",
        "                'dropout': float(DROPOUT),\n",
        "                'l2_reg': float(L2_REG),\n",
        "                'train_samples': int(len(X_tr)),\n",
        "                'val_samples': int(len(X_va)),\n",
        "                'test_samples': int(len(X_te)),\n",
        "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'model_params': int(model.count_params())\n",
        "            }\n",
        "\n",
        "            # Guardar hiperparámetros\n",
        "            save_hyperparameters(METRICS_DIR, mdl_name, hyperparams)\n",
        "\n",
        "            # Callbacks\n",
        "            callbacks = create_callbacks(mdl_name, exp, model_path)\n",
        "\n",
        "            # Entrenar con verbose=0 para usar nuestro monitor personalizado\n",
        "            print(f\"\\n🏃 Iniciando entrenamiento...\")\n",
        "            print(f\"   📊 Visualización en tiempo real activada\")\n",
        "            print(f\"   📈 Parámetros del modelo: {model.count_params():,}\")\n",
        "\n",
        "            history = model.fit(\n",
        "                X_tr, y_tr,\n",
        "                validation_data=(X_va, y_va),\n",
        "                epochs=EPOCHS,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                callbacks=callbacks,\n",
        "                verbose=0  # Usar 0 para que solo se muestre nuestro monitor\n",
        "            )\n",
        "\n",
        "            # Guardar historial\n",
        "            all_histories[f\"{exp}_{mdl_name}\"] = history\n",
        "\n",
        "            # Mostrar resumen de entrenamiento\n",
        "            print_training_summary(history, mdl_name, exp)\n",
        "\n",
        "            # Plotear y guardar curvas de aprendizaje\n",
        "            plot_learning_curves(history, METRICS_DIR, mdl_name, show=True)\n",
        "\n",
        "            # Guardar historial como JSON\n",
        "            # Obtener learning rates del monitor de entrenamiento si no están en history\n",
        "            training_monitor = [cb for cb in callbacks if isinstance(cb, AdvancedTrainingMonitor)][0]\n",
        "            lr_values = history.history.get('lr', [])\n",
        "            if not lr_values and hasattr(training_monitor, 'history'):\n",
        "                lr_values = training_monitor.history['lr']\n",
        "\n",
        "            history_dict = {\n",
        "                'loss': [float(x) for x in history.history['loss']],\n",
        "                'val_loss': [float(x) for x in history.history['val_loss']],\n",
        "                'mae': [float(x) for x in history.history.get('mae', [])],\n",
        "                'val_mae': [float(x) for x in history.history.get('val_mae', [])],\n",
        "                'lr': [float(x) for x in lr_values] if lr_values else []\n",
        "            }\n",
        "\n",
        "            with open(METRICS_DIR / f\"{mdl_name}_history.json\", 'w') as f:\n",
        "                json.dump(history_dict, f, indent=4)\n",
        "\n",
        "            # ─ Evaluación en Test Set ─\n",
        "            print(f\"\\n📊 Evaluando en test set...\")\n",
        "            test_loss, test_mae = model.evaluate(X_te, y_te, verbose=0)\n",
        "            print(f\"   Test Loss: {test_loss:.6f}, Test MAE: {test_mae:.6f}\")\n",
        "\n",
        "            # ─ Predicciones y visualización ─\n",
        "            print(f\"\\n🎯 Generando predicciones...\")\n",
        "            # Usar las primeras 5 muestras del test set\n",
        "            sample_indices = min(5, len(X_te))\n",
        "            y_hat_sc = model.predict(X_te[:sample_indices], verbose=0)\n",
        "            y_hat = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(-1,HORIZON,lat,lon)\n",
        "            y_true = sy.inverse_transform(y_te[:sample_indices].reshape(-1,1)).reshape(-1,HORIZON,lat,lon)\n",
        "\n",
        "            # ─ Métricas de evaluación por horizonte ─\n",
        "            for h in range(HORIZON):\n",
        "                rmse = np.sqrt(mean_squared_error(y_true[:,h].ravel(), y_hat[:,h].ravel()))\n",
        "                mae = mean_absolute_error(y_true[:,h].ravel(), y_hat[:,h].ravel())\n",
        "                r2 = r2_score(y_true[:,h].ravel(), y_hat[:,h].ravel())\n",
        "\n",
        "                results.append({\n",
        "                    'Experiment': exp,\n",
        "                    'Model': mdl_name,\n",
        "                    'H': h+1,\n",
        "                    'RMSE': rmse,\n",
        "                    'MAE': mae,\n",
        "                    'R2': r2,\n",
        "                    'Test_Loss': test_loss,\n",
        "                    'Parameters': model.count_params()\n",
        "                })\n",
        "\n",
        "                print(f\"   📈 H={h+1}: RMSE={rmse:.4f}, MAE={mae:.4f}, R²={r2:.4f}\")\n",
        "\n",
        "            # ─ Mapas & GIF ─\n",
        "            print(f\"\\n🎨 Generando visualizaciones...\")\n",
        "            # Usar la primera muestra para visualización\n",
        "            sample_idx = 0\n",
        "            vmin, vmax = 0, max(y_true[sample_idx].max(), y_hat[sample_idx].max())\n",
        "            frames = []\n",
        "            dates = pd.date_range(ds.time.values[-HORIZON], periods=HORIZON, freq='MS')\n",
        "\n",
        "            for h in range(HORIZON):\n",
        "                err = np.clip(np.abs((y_true[sample_idx,h]-y_hat[sample_idx,h])/(y_true[sample_idx,h]+1e-5))*100, 0, 100)\n",
        "                try:\n",
        "                    import cartopy.crs as ccrs\n",
        "                    fig, axs = plt.subplots(1, 3, figsize=(16, 4.5), subplot_kw={'projection': ccrs.PlateCarree()})\n",
        "                except ImportError:\n",
        "                    fig, axs = plt.subplots(1, 3, figsize=(16, 4.5))\n",
        "\n",
        "                # Real\n",
        "                real_mesh = axs[0].pcolormesh(ds.longitude, ds.latitude, y_true[sample_idx,h],\n",
        "                                            cmap='Blues', shading='nearest', vmin=vmin, vmax=vmax,\n",
        "                                            transform=ccrs.PlateCarree())\n",
        "                axs[0].coastlines()\n",
        "                axs[0].add_geometries(DEPT_GDF.geometry, ccrs.PlateCarree(),\n",
        "                                    edgecolor='black', facecolor='none', linewidth=1)\n",
        "                axs[0].gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
        "                axs[0].set_title(f\"Real h={h+1}\", fontsize=11)\n",
        "                real_cbar = fig.colorbar(real_mesh, ax=axs[0], fraction=0.046, pad=0.04)\n",
        "                real_cbar.set_label('Precipitación (mm)', rotation=270, labelpad=15)\n",
        "\n",
        "                # Predicción\n",
        "                pred_mesh = axs[1].pcolormesh(ds.longitude, ds.latitude, y_hat[sample_idx,h],\n",
        "                                            cmap='Blues', shading='nearest', vmin=vmin, vmax=vmax,\n",
        "                                            transform=ccrs.PlateCarree())\n",
        "                axs[1].coastlines()\n",
        "                axs[1].add_geometries(DEPT_GDF.geometry, ccrs.PlateCarree(),\n",
        "                                     edgecolor='black', facecolor='none', linewidth=1)\n",
        "                axs[1].gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
        "                axs[1].set_title(f\"{mdl_name} h={h+1}\", fontsize=11)\n",
        "                pred_cbar = fig.colorbar(pred_mesh, ax=axs[1], fraction=0.046, pad=0.04)\n",
        "                pred_cbar.set_label('Precipitación (mm)', rotation=270, labelpad=15)\n",
        "\n",
        "                # Error\n",
        "                err_mesh = axs[2].pcolormesh(ds.longitude, ds.latitude, err,\n",
        "                                           cmap='Reds', shading='nearest', vmin=0, vmax=100,\n",
        "                                           transform=ccrs.PlateCarree())\n",
        "                axs[2].coastlines()\n",
        "                axs[2].add_geometries(DEPT_GDF.geometry, ccrs.PlateCarree(),\n",
        "                                    edgecolor='black', facecolor='none', linewidth=1)\n",
        "                axs[2].gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
        "                axs[2].set_title(f\"MAPE% h={h+1}\", fontsize=11)\n",
        "                err_cbar = fig.colorbar(err_mesh, ax=axs[2], fraction=0.046, pad=0.04)\n",
        "                err_cbar.set_label('MAPE (%)', rotation=270, labelpad=15)\n",
        "\n",
        "                fig.suptitle(f\"{mdl_name} – {exp} – {dates[h].strftime('%Y-%m')}\", fontsize=13)\n",
        "                png = OUT_EXP/f\"{mdl_name}_{h+1}.png\"\n",
        "                fig.tight_layout()\n",
        "                fig.savefig(png, bbox_inches='tight')\n",
        "                plt.close(fig)\n",
        "                frames.append(imageio.imread(png))\n",
        "\n",
        "            imageio.mimsave(OUT_EXP/f\"{mdl_name}.gif\", frames, fps=0.5)\n",
        "            print(f\"   ✅ GIF guardado: {OUT_EXP/f'{mdl_name}.gif'}\")\n",
        "\n",
        "            tf.keras.backend.clear_session()\n",
        "            gc.collect()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️ Error en {mdl_name}: {str(e)}\")\n",
        "            print(f\"  → Saltando {mdl_name} para {exp}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "# ───────────────────────── CSV FINAL ─────────────────────────\n",
        "res_df = pd.DataFrame(results)\n",
        "res_df.to_csv(OUT_ROOT/'metrics_advanced.csv', index=False)\n",
        "print(\"\\n📑 Metrics saved →\", OUT_ROOT/'metrics_advanced.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXK-acO1XqrT"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── VISUALIZACIÓN COMPARATIVA ─────────────────────────\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📊 GENERANDO VISUALIZACIONES COMPARATIVAS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Crear directorio para comparaciones\n",
        "COMP_DIR = OUT_ROOT / 'comparisons'\n",
        "COMP_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# 1. Comparación de métricas entre modelos\n",
        "if 'res_df' in locals() and res_df is not None and len(res_df) > 0:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "    # RMSE por modelo y experimento\n",
        "    pivot_rmse = res_df.pivot_table(values='RMSE', index='Model', columns='Experiment', aggfunc='mean')\n",
        "    pivot_rmse.plot(kind='bar', ax=axes[0,0])\n",
        "    axes[0,0].set_title('RMSE Promedio por Modelo y Experimento', fontsize=14, pad=10)\n",
        "    axes[0,0].set_ylabel('RMSE')\n",
        "    axes[0,0].set_xlabel('Modelo')\n",
        "    axes[0,0].legend(title='Experimento', loc='upper left')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    axes[0,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # MAE por modelo y experimento\n",
        "    pivot_mae = res_df.pivot_table(values='MAE', index='Model', columns='Experiment', aggfunc='mean')\n",
        "    pivot_mae.plot(kind='bar', ax=axes[0,1])\n",
        "    axes[0,1].set_title('MAE Promedio por Modelo y Experimento', fontsize=14, pad=10)\n",
        "    axes[0,1].set_ylabel('MAE')\n",
        "    axes[0,1].set_xlabel('Modelo')\n",
        "    axes[0,1].legend(title='Experimento', loc='upper left')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # R² por modelo y experimento\n",
        "    pivot_r2 = res_df.pivot_table(values='R2', index='Model', columns='Experiment', aggfunc='mean')\n",
        "    pivot_r2.plot(kind='bar', ax=axes[1,0])\n",
        "    axes[1,0].set_title('R² Promedio por Modelo y Experimento', fontsize=14, pad=10)\n",
        "    axes[1,0].set_ylabel('R²')\n",
        "    axes[1,0].set_xlabel('Modelo')\n",
        "    axes[1,0].legend(title='Experimento', loc='lower right')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    axes[1,0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Evolución de métricas por horizonte\n",
        "    ax_horizon = axes[1,1]\n",
        "\n",
        "    for model in res_df['Model'].unique():\n",
        "        model_data = res_df[res_df['Model'] == model]\n",
        "        horizon_means = model_data.groupby('H')['RMSE'].mean()\n",
        "        ax_horizon.plot(horizon_means.index, horizon_means.values,\n",
        "                       marker='o', label=model, linewidth=2.5, markersize=8)\n",
        "\n",
        "    ax_horizon.set_xlabel('Horizonte (meses)')\n",
        "    ax_horizon.set_ylabel('RMSE')\n",
        "    ax_horizon.set_title('Evolución de RMSE por Horizonte', fontsize=14, pad=10)\n",
        "    ax_horizon.legend(title='Modelo', loc='best')\n",
        "    ax_horizon.grid(True, alpha=0.3)\n",
        "    ax_horizon.set_xticks([1, 2, 3])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(COMP_DIR / 'metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# 2. Tabla resumen de mejores modelos\n",
        "if 'res_df' in locals() and res_df is not None and len(res_df) > 0:\n",
        "    print(\"\\n📋 TABLA RESUMEN - MEJORES MODELOS POR EXPERIMENTO:\")\n",
        "    print(\"─\" * 60)\n",
        "\n",
        "    best_models = res_df.groupby('Experiment').apply(\n",
        "        lambda x: x.loc[x['RMSE'].idxmin()]\n",
        "    )[['Model', 'RMSE', 'MAE', 'R2']]\n",
        "\n",
        "    print(best_models.to_string())\n",
        "else:\n",
        "    print(\"\\n⚠️ No hay resultados disponibles para mostrar el resumen\")\n",
        "\n",
        "# 3. Comparación con modelos originales si existen\n",
        "old_metrics_path = BASE_PATH / 'models' / 'output' / 'Spatial_CONVRNN' / 'metrics_spatial.csv'\n",
        "if old_metrics_path.exists():\n",
        "    print(\"\\n📊 COMPARACIÓN CON MODELOS ORIGINALES:\")\n",
        "    print(\"─\" * 60)\n",
        "\n",
        "    old_df = pd.read_csv(old_metrics_path)\n",
        "\n",
        "    # Calcular mejoras promedio\n",
        "    if 'res_df' in locals() and res_df is not None and len(res_df) > 0:\n",
        "        for exp in EXPERIMENTS.keys():\n",
        "            print(f\"\\n{exp}:\")\n",
        "\n",
        "            # Mejores modelos nuevos\n",
        "            exp_data = res_df[res_df['Experiment'] == exp]\n",
        "            if len(exp_data) > 0:\n",
        "                new_best = exp_data.groupby('Model')['RMSE'].mean().idxmin()\n",
        "                new_rmse = exp_data[exp_data['Model'] == new_best]['RMSE'].mean()\n",
        "\n",
        "                # Mejor modelo original\n",
        "                old_exp_data = old_df[old_df['Experiment'] == exp]\n",
        "                if len(old_exp_data) > 0:\n",
        "                    old_best_rmse = old_exp_data['RMSE'].min()\n",
        "\n",
        "                    improvement = (old_best_rmse - new_rmse) / old_best_rmse * 100\n",
        "\n",
        "                    print(f\"  • Mejor modelo nuevo: {new_best} (RMSE: {new_rmse:.4f})\")\n",
        "                    print(f\"  • Mejor RMSE original: {old_best_rmse:.4f}\")\n",
        "                    print(f\"  • Mejora: {improvement:.2f}%\")\n",
        "                else:\n",
        "                    print(f\"  • No hay datos originales para comparar\")\n",
        "\n",
        "print(\"\\n✅ Visualizaciones comparativas completadas!\")\n",
        "print(f\"📂 Resultados guardados en: {COMP_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJcQwNvLXqrT"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── ANÁLISIS DETALLADO DE RESULTADOS ─────────────────────────\n",
        "\n",
        "if 'res_df' in locals() and res_df is not None and len(res_df) > 0:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"📊 ANÁLISIS DETALLADO DE RESULTADOS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. Métricas por horizonte de predicción\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "    metrics = ['RMSE', 'MAE', 'R2']\n",
        "    titles = ['RMSE por Horizonte', 'MAE por Horizonte', 'R² por Horizonte']\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(res_df['Model'].unique())))\n",
        "\n",
        "    for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
        "        ax = axes[idx]\n",
        "\n",
        "        # Obtener datos pivoteados\n",
        "        data = res_df.groupby(['H', 'Model'])[metric].mean().unstack()\n",
        "\n",
        "        # Plotear cada modelo\n",
        "        for i, model in enumerate(data.columns):\n",
        "            ax.plot(data.index, data[model],\n",
        "                   marker='o',\n",
        "                   label=model,\n",
        "                   color=colors[i],\n",
        "                   linewidth=2.5,\n",
        "                   markersize=8,\n",
        "                   markeredgewidth=2,\n",
        "                   markeredgecolor='white')\n",
        "\n",
        "        ax.set_xlabel('Horizonte (meses)', fontsize=12)\n",
        "        ax.set_ylabel(metric, fontsize=12)\n",
        "        ax.set_title(title, fontsize=14, fontweight='bold', pad=10)\n",
        "        ax.grid(True, alpha=0.3, linestyle='--')\n",
        "        ax.set_xticks(data.index)\n",
        "\n",
        "        # Leyenda solo en el primer gráfico\n",
        "        if idx == 0:\n",
        "            ax.legend(title='Modelo', loc='best', frameon=True, fancybox=True, shadow=True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(COMP_DIR / 'metrics_evolution_by_horizon.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Tabla visual de métricas\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Preparar datos para la tabla\n",
        "    summary_data = []\n",
        "    experiments = res_df['Experiment'].unique()\n",
        "    models = res_df['Model'].unique()\n",
        "\n",
        "    # Headers\n",
        "    headers = ['Experimento', 'Modelo', 'RMSE↓', 'MAE↓', 'R²↑', 'Mejor H', 'Parámetros']\n",
        "\n",
        "    for exp in experiments:\n",
        "        for model in models:\n",
        "            exp_model_data = res_df[(res_df['Experiment'] == exp) & (res_df['Model'] == model)]\n",
        "            if not exp_model_data.empty:\n",
        "                avg_rmse = exp_model_data['RMSE'].mean()\n",
        "                avg_mae = exp_model_data['MAE'].mean()\n",
        "                avg_r2 = exp_model_data['R2'].mean()\n",
        "                best_h = exp_model_data.loc[exp_model_data['RMSE'].idxmin(), 'H']\n",
        "                params = exp_model_data['Parameters'].iloc[0]\n",
        "\n",
        "                summary_data.append([\n",
        "                    exp, model,\n",
        "                    f'{avg_rmse:.4f}',\n",
        "                    f'{avg_mae:.4f}',\n",
        "                    f'{avg_r2:.4f}',\n",
        "                    f'H={best_h}',\n",
        "                    f'{params:,}'\n",
        "                ])\n",
        "\n",
        "    # Crear tabla\n",
        "    table = ax.table(cellText=summary_data, colLabels=headers,\n",
        "                    cellLoc='center', loc='center')\n",
        "\n",
        "    # Estilizar tabla\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1.2, 2)\n",
        "\n",
        "    # Colorear celdas según rendimiento\n",
        "    for i in range(len(summary_data)):\n",
        "        # Obtener valores para comparación\n",
        "        rmse_val = float(summary_data[i][2])\n",
        "        mae_val = float(summary_data[i][3])\n",
        "        r2_val = float(summary_data[i][4])\n",
        "\n",
        "        # Encontrar min/max para normalización\n",
        "        all_rmse = [float(row[2]) for row in summary_data]\n",
        "        all_mae = [float(row[3]) for row in summary_data]\n",
        "        all_r2 = [float(row[4]) for row in summary_data]\n",
        "\n",
        "        # Normalizar y colorear RMSE (menor es mejor)\n",
        "        rmse_norm = (rmse_val - min(all_rmse)) / (max(all_rmse) - min(all_rmse))\n",
        "        rmse_color = plt.cm.RdYlGn(1 - rmse_norm)\n",
        "        table[(i+1, 2)].set_facecolor(rmse_color)\n",
        "\n",
        "        # Normalizar y colorear MAE (menor es mejor)\n",
        "        mae_norm = (mae_val - min(all_mae)) / (max(all_mae) - min(all_mae))\n",
        "        mae_color = plt.cm.RdYlGn(1 - mae_norm)\n",
        "        table[(i+1, 3)].set_facecolor(mae_color)\n",
        "\n",
        "        # Normalizar y colorear R² (mayor es mejor)\n",
        "        r2_norm = (r2_val - min(all_r2)) / (max(all_r2) - min(all_r2))\n",
        "        r2_color = plt.cm.RdYlGn(r2_norm)\n",
        "        table[(i+1, 4)].set_facecolor(r2_color)\n",
        "\n",
        "        # Colorear experimento\n",
        "        exp_colors = {'BASIC': '#e8f4f8', 'KCE': '#f0e8f8', 'PAFC': '#f8e8f0'}\n",
        "        table[(i+1, 0)].set_facecolor(exp_colors.get(summary_data[i][0], 'white'))\n",
        "\n",
        "    # Colorear headers\n",
        "    for j in range(len(headers)):\n",
        "        table[(0, j)].set_facecolor('#4a86e8')\n",
        "        table[(0, j)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "    plt.title('Resumen de Métricas por Modelo y Experimento\\n(Verde=Mejor, Rojo=Peor)',\n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "\n",
        "    # Añadir leyenda\n",
        "    plt.text(0.5, -0.05, '↓ = Menor es mejor, ↑ = Mayor es mejor',\n",
        "            transform=ax.transAxes, ha='center', fontsize=10, style='italic')\n",
        "\n",
        "    plt.savefig(COMP_DIR / 'metrics_summary_table.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Identificar el mejor modelo global\n",
        "    print(\"\\n🏆 MEJOR MODELO GLOBAL:\")\n",
        "    print(\"─\" * 50)\n",
        "\n",
        "    # Calcular score compuesto (normalizado)\n",
        "    res_df['score'] = (\n",
        "        (1 - (res_df['RMSE'] - res_df['RMSE'].min()) / (res_df['RMSE'].max() - res_df['RMSE'].min())) +\n",
        "        (1 - (res_df['MAE'] - res_df['MAE'].min()) / (res_df['MAE'].max() - res_df['MAE'].min())) +\n",
        "        ((res_df['R2'] - res_df['R2'].min()) / (res_df['R2'].max() - res_df['R2'].min()))\n",
        "    ) / 3\n",
        "\n",
        "    best_overall = res_df.loc[res_df['score'].idxmax()]\n",
        "    print(f\"Modelo: {best_overall['Model']}\")\n",
        "    print(f\"Experimento: {best_overall['Experiment']}\")\n",
        "    print(f\"Horizonte: {best_overall['H']}\")\n",
        "    print(f\"RMSE: {best_overall['RMSE']:.4f}\")\n",
        "    print(f\"MAE: {best_overall['MAE']:.4f}\")\n",
        "    print(f\"R²: {best_overall['R2']:.4f}\")\n",
        "    print(f\"Score compuesto: {best_overall['score']:.4f}\")\n",
        "\n",
        "    # 4. Análisis de mejora por horizonte\n",
        "    print(\"\\n📈 ANÁLISIS DE MEJORA POR HORIZONTE:\")\n",
        "    print(\"─\" * 50)\n",
        "\n",
        "    for h in [1, 2, 3]:\n",
        "        h_data = res_df[res_df['H'] == h]\n",
        "        best_h = h_data.loc[h_data['RMSE'].idxmin()]\n",
        "\n",
        "        print(f\"\\nHorizonte {h}:\")\n",
        "        print(f\"  • Mejor modelo: {best_h['Model']} - {best_h['Experiment']}\")\n",
        "        print(f\"  • RMSE: {best_h['RMSE']:.4f}\")\n",
        "        print(f\"  • R²: {best_h['R2']:.4f}\")\n",
        "\n",
        "print(\"\\n✅ Análisis detallado completado!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdvrmOioXqrT"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────── MOSTRAR PREDICCIONES MÁS RECIENTES ─────────────────────────\n",
        "print(\"\\n🖼️ PREDICCIONES MÁS RECIENTES:\")\n",
        "for exp in EXPERIMENTS.keys():\n",
        "    exp_dir = OUT_ROOT / exp\n",
        "    if exp_dir.exists():\n",
        "        print(f\"\\n{exp}:\")\n",
        "        # Mostrar primera imagen de cada modelo\n",
        "        for model in ADVANCED_MODELS.keys():\n",
        "            img_path = exp_dir / f\"{model}_1.png\"\n",
        "            gif_path = exp_dir / f\"{model}.gif\"\n",
        "\n",
        "            if img_path.exists():\n",
        "                from IPython.display import Image, display\n",
        "                print(f\"  {model} - Primera predicción (H=1):\")\n",
        "                display(Image(str(img_path), width=800))\n",
        "\n",
        "            if gif_path.exists():\n",
        "                print(f\"  📹 GIF animado disponible: {gif_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🎉 NOTEBOOK COMPLETADO!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n📊 Resultados guardados en: {OUT_ROOT}\")\n",
        "if 'res_df' in locals() and res_df is not None and len(res_df) > 0:\n",
        "    print(f\"📈 Métricas en: {OUT_ROOT/'metrics_advanced.csv'}\")\n",
        "    print(f\"🖼️ Visualizaciones en: {COMP_DIR if 'COMP_DIR' in locals() else 'N/A'}\")\n",
        "else:\n",
        "    print(\"⚠️ No se generaron métricas en esta ejecución\")\n",
        "print(\"\\n💡 Próximos pasos:\")\n",
        "print(\"   1. Revisar las métricas y seleccionar el mejor modelo\")\n",
        "print(\"   2. Hacer fine-tuning de hiperparámetros si es necesario\")\n",
        "print(\"   3. Entrenar un ensemble con los mejores modelos\")\n",
        "print(\"   4. Evaluar en datos más recientes o diferentes regiones\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}