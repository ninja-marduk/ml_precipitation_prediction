{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f64a97",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/hybrid_models_TopoRain_NET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "itarOiHGzTAU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "itarOiHGzTAU",
    "outputId": "92741f25-a5ad-49cd-ed9e-e851ec87059e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-29 13:05:32,954 [INFO] Configuraci√≥n de threading de TensorFlow aplicada\n",
      "Entorno configurado. Usando ruta base: ..\n",
      "2025-05-29 13:05:32,957 [INFO] Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\n",
      "Entorno configurado. Usando ruta base: ..\n",
      "2025-05-29 13:05:32,957 [INFO] Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\n",
      "2025-05-29 13:05:33,127 [INFO] Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 13:05:33,128 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 13:05:33,129 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 13:05:33,129 [INFO] Definiendo m√°scaras para los niveles de elevaci√≥n...\n",
      "2025-05-29 13:05:33,129 [INFO] Distribuci√≥n de celdas por nivel de elevaci√≥n:\n",
      "2025-05-29 13:05:33,130 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 13:05:33,130 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 13:05:33,131 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 13:05:33,132 [INFO] Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\n",
      "2025-05-29 13:05:33,127 [INFO] Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 13:05:33,128 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 13:05:33,129 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 13:05:33,129 [INFO] Definiendo m√°scaras para los niveles de elevaci√≥n...\n",
      "2025-05-29 13:05:33,129 [INFO] Distribuci√≥n de celdas por nivel de elevaci√≥n:\n",
      "2025-05-29 13:05:33,130 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 13:05:33,130 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 13:05:33,131 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 13:05:33,132 [INFO] Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\n",
      "2025-05-29 13:05:33,277 [INFO] Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 13:05:33,277 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 13:05:33,278 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 13:05:33,278 [INFO] Definiendo m√°scaras para los niveles de elevaci√≥n...\n",
      "2025-05-29 13:05:33,281 [INFO] Distribuci√≥n de celdas por nivel de elevaci√≥n:\n",
      "2025-05-29 13:05:33,277 [INFO] Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 13:05:33,277 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 13:05:33,278 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 13:05:33,278 [INFO] Definiendo m√°scaras para los niveles de elevaci√≥n...\n",
      "2025-05-29 13:05:33,281 [INFO] Distribuci√≥n de celdas por nivel de elevaci√≥n:\n",
      "2025-05-29 13:05:33,287 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 13:05:33,292 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 13:05:33,293 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 13:05:33,299 [INFO] Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\n",
      "2025-05-29 13:05:33,287 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 13:05:33,292 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 13:05:33,293 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 13:05:33,299 [INFO] Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\n",
      "2025-05-29 13:05:33,445 [INFO] Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 13:05:33,446 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 13:05:33,446 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 13:05:33,447 [INFO] Definiendo m√°scaras para los niveles de elevaci√≥n...\n",
      "2025-05-29 13:05:33,447 [INFO] Distribuci√≥n de celdas por nivel de elevaci√≥n:\n",
      "2025-05-29 13:05:33,448 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 13:05:33,449 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 13:05:33,449 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "Directorio asegurado: logs\n",
      "Directorio asegurado: ../models/output/trained_models\n",
      "Directorio asegurado: ../models/output/predictions\n",
      "Directorio asegurado: ../models/output/histories\n",
      "2025-05-29 13:05:33,456 [INFO] üöÄ Iniciando proceso TopoRain-NET\n",
      "2025-05-29 13:05:33,445 [INFO] Clusters codificados de texto a n√∫meros: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 13:05:33,446 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 13:05:33,446 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 13:05:33,447 [INFO] Definiendo m√°scaras para los niveles de elevaci√≥n...\n",
      "2025-05-29 13:05:33,447 [INFO] Distribuci√≥n de celdas por nivel de elevaci√≥n:\n",
      "2025-05-29 13:05:33,448 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 13:05:33,449 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 13:05:33,449 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "Directorio asegurado: logs\n",
      "Directorio asegurado: ../models/output/trained_models\n",
      "Directorio asegurado: ../models/output/predictions\n",
      "Directorio asegurado: ../models/output/histories\n",
      "2025-05-29 13:05:33,456 [INFO] üöÄ Iniciando proceso TopoRain-NET\n",
      "2025-05-29 13:05:33,456 [INFO] ‚ñ∂Ô∏è INICIANDO: Proceso completo\n",
      "2025-05-29 13:05:33,456 [INFO] ‚ñ∂Ô∏è INICIANDO: Proceso completo\n",
      "2025-05-29 13:05:33,561 [INFO] ‚úì COMPLETADO: Proceso completo en 0.11 segundos\n",
      "2025-05-29 13:05:33,561 [INFO] ‚úì COMPLETADO: Proceso completo en 0.11 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--force-retrain]\n",
      "ipykernel_launcher.py: error: argument --force-retrain: ignored explicit argument '/Users/riperez/Library/Jupyter/runtime/kernel-v319d7d8c1b7a304bdfb8b46549569819f7d24cb0e.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-29 13:05:33,667 [INFO] ‚ñ∂Ô∏è INICIANDO: Preparaci√≥n de datos\n",
      "2025-05-29 13:05:33,769 [INFO] Optimizando fusi√≥n de CEEMDAN y TFV-EMD con XGBoost...\n",
      "2025-05-29 13:05:33,769 [INFO] ‚úì COMPLETADO: Preparaci√≥n de datos en 0.10 segundos\n",
      "2025-05-29 13:05:33,769 [INFO] Optimizando fusi√≥n de CEEMDAN y TFV-EMD con XGBoost...\n",
      "2025-05-29 13:05:33,769 [INFO] ‚úì COMPLETADO: Preparaci√≥n de datos en 0.10 segundos\n",
      "2025-05-29 13:05:33,875 [INFO] ‚ñ∂Ô∏è INICIANDO: Optimizaci√≥n de fusi√≥n\n",
      "2025-05-29 13:05:33,875 [INFO] ‚ñ∂Ô∏è INICIANDO: Optimizaci√≥n de fusi√≥n\n",
      "\n",
      "üñ•Ô∏è  Recursos detectados: 10 CPUs, 16.0GB RAM (2.9GB disponible)\n",
      "üîß Configuraci√≥n optimizada: 8 workers en paralelo FORZADOS, tree_method=hist\n",
      "üß† Memoria disponible: 2.88GB (82.0% usado)\n",
      "\n",
      "üìä Iniciando entrenamiento acelerado de 9 componentes (3 niveles √ó 3 componentes)\n",
      "\n",
      "‚ö° Activando procesamiento paralelo forzado con 8 workers para acelerar el entrenamiento\n",
      "‚ñ∂Ô∏è  Nivel nivel_1, componente 0: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_1, componente 1: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_2, componente 0: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_1, componente 2: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_2, componente 1: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_2, componente 2: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_3, componente 0: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_3, componente 1: Iniciando entrenamiento r√°pido...\n",
      "   Datos: 530 muestras, 921 celdas\n",
      "   Datos: 530 muestras, 996 celdas\n",
      "   Datos: 530 muestras, 996 celdas\n",
      "   Datos: 530 muestras, 2048 celdas\n",
      "   Datos: 530 muestras, 921 celdas\n",
      "   Datos: 530 muestras, 2048 celdas\n",
      "   Datos: 530 muestras, 921 celdas\n",
      "   Datos: 530 muestras, 2048 celdas\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "\n",
      "üñ•Ô∏è  Recursos detectados: 10 CPUs, 16.0GB RAM (2.9GB disponible)\n",
      "üîß Configuraci√≥n optimizada: 8 workers en paralelo FORZADOS, tree_method=hist\n",
      "üß† Memoria disponible: 2.88GB (82.0% usado)\n",
      "\n",
      "üìä Iniciando entrenamiento acelerado de 9 componentes (3 niveles √ó 3 componentes)\n",
      "\n",
      "‚ö° Activando procesamiento paralelo forzado con 8 workers para acelerar el entrenamiento\n",
      "‚ñ∂Ô∏è  Nivel nivel_1, componente 0: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_1, componente 1: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_2, componente 0: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_1, componente 2: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_2, componente 1: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_2, componente 2: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_3, componente 0: Iniciando entrenamiento r√°pido...\n",
      "‚ñ∂Ô∏è  Nivel nivel_3, componente 1: Iniciando entrenamiento r√°pido...\n",
      "   Datos: 530 muestras, 921 celdas\n",
      "   Datos: 530 muestras, 996 celdas\n",
      "   Datos: 530 muestras, 996 celdas\n",
      "   Datos: 530 muestras, 2048 celdas\n",
      "   Datos: 530 muestras, 921 celdas\n",
      "   Datos: 530 muestras, 2048 celdas\n",
      "   Datos: 530 muestras, 921 celdas\n",
      "   Datos: 530 muestras, 2048 celdas\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n",
      "   Split: 424 train, 106 test (simple (optimizado para velocidad))\n",
      "   Entrenamiento ultra-r√°pido: 60 estimators, depth=4, lr=0.200\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TopoRain-Net: entrenamiento y evaluaci√≥n de modelos espec√≠ficos por nivel de elevaci√≥n.\n",
    "Implementa modelos BiGRU autoencoder-decoder para cada nivel de elevaci√≥n,\n",
    "con fusi√≥n optimizada de caracter√≠sticas CEEMDAN y TFV-EMD usando XGBoost.\n",
    "Un meta-modelo integra las predicciones de los tres modelos de elevaci√≥n.\n",
    "Genera m√©tricas, scatter, mapas y tablas (global, por elevaci√≥n, por percentiles).\n",
    "\"\"\"\n",
    "\n",
    "import warnings, logging\n",
    "from pathlib import Path\n",
    "# Configuraci√≥n del entorno (compatible con Colab y local)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuraci√≥n de logging y trazabilidad mejorada\n",
    "# -----------------------------------------------------------------------------\n",
    "# Crear directorio para logs\n",
    "LOG_DIR = Path(\"logs\")\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configurar formato de timestamp\n",
    "timestamp_format = \"%Y-%m-%d_%H-%M-%S\"\n",
    "run_timestamp = datetime.datetime.now().strftime(timestamp_format)\n",
    "log_filename = f\"toporain_net_run_{run_timestamp}.log\"\n",
    "\n",
    "# Configurar logging con formato detallado y salida a archivo\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_DIR / log_filename),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Clase para trazabilidad del proceso\n",
    "class ProcessTracker:\n",
    "    def __init__(self, name=\"TopoRain-NET\"):\n",
    "        self.name = name\n",
    "        self.start_time = time.time()\n",
    "        self.section_times = {}\n",
    "        self.current_section = None\n",
    "        self.section_start = None\n",
    "        self.metrics = defaultdict(dict)\n",
    "        self.resources = []\n",
    "        self.checkpoints = []\n",
    "        \n",
    "    def start_section(self, section_name):\n",
    "        \"\"\"Inicia el cron√≥metro para una secci√≥n del proceso\"\"\"\n",
    "        if self.current_section:\n",
    "            self.end_section()\n",
    "            \n",
    "        self.current_section = section_name\n",
    "        self.section_start = time.time()\n",
    "        logger.info(f\"‚ñ∂Ô∏è INICIANDO: {section_name}\")\n",
    "        # Registrar recursos al inicio\n",
    "        self._log_resources()\n",
    "        \n",
    "    def end_section(self):\n",
    "        \"\"\"Finaliza la secci√≥n actual y registra el tiempo transcurrido\"\"\"\n",
    "        if not self.current_section:\n",
    "            return\n",
    "            \n",
    "        elapsed = time.time() - self.section_start\n",
    "        self.section_times[self.current_section] = elapsed\n",
    "        logger.info(f\"‚úì COMPLETADO: {self.current_section} en {elapsed:.2f} segundos\")\n",
    "        # Registrar recursos al final\n",
    "        self._log_resources()\n",
    "        self.current_section = None\n",
    "        \n",
    "    def log_metric(self, section, metric_name, value):\n",
    "        \"\"\"Registra una m√©trica\"\"\"\n",
    "        self.metrics[section][metric_name] = value\n",
    "        logger.info(f\"üìä M√âTRICA: {section} - {metric_name}: {value}\")\n",
    "        \n",
    "    def add_checkpoint(self, description, data=None):\n",
    "        \"\"\"A√±ade un punto de control con datos opcionales\"\"\"\n",
    "        checkpoint = {\n",
    "            'timestamp': time.time(),\n",
    "            'description': description,\n",
    "            'elapsed_total': time.time() - self.start_time,\n",
    "            'data': data\n",
    "        }\n",
    "        self.checkpoints.append(checkpoint)\n",
    "        logger.info(f\"üîñ CHECKPOINT: {description}\")\n",
    "        \n",
    "    def _log_resources(self):\n",
    "        \"\"\"Registra el uso de recursos actual\"\"\"\n",
    "        mem_info = get_memory_info()\n",
    "        cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "        \n",
    "        # Obtener informaci√≥n de GPU si est√° disponible\n",
    "        gpu_info = get_gpu_memory_info()\n",
    "        gpu_usage = None\n",
    "        if gpu_info and gpu_info[0]['memory_used_mb'] > 0:\n",
    "            gpu_usage = {\n",
    "                'used_mb': gpu_info[0]['memory_used_mb'],\n",
    "                'total_mb': gpu_info[0]['memory_total_mb'],\n",
    "                'percent': gpu_info[0]['memory_used_percent']\n",
    "            }\n",
    "        \n",
    "        resources = {\n",
    "            'timestamp': time.time(),\n",
    "            'memory_used_gb': mem_info['total_gb'] - mem_info['free_gb'],\n",
    "            'memory_total_gb': mem_info['total_gb'],\n",
    "            'memory_percent': mem_info['used_percent'],\n",
    "            'cpu_percent': cpu_percent,\n",
    "            'gpu': gpu_usage\n",
    "        }\n",
    "        self.resources.append(resources)\n",
    "        \n",
    "    def _convert_numpy_types(self, obj):\n",
    "        \"\"\"\n",
    "        Convierte recursivamente tipos de numpy a tipos nativos de Python\n",
    "        para hacer el objeto JSON serializable\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (np.bool_)):\n",
    "            return bool(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self._convert_numpy_types(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_numpy_types(item) for item in obj]\n",
    "        elif isinstance(obj, tuple):\n",
    "            return tuple(self._convert_numpy_types(item) for item in obj)\n",
    "        else:\n",
    "            return obj\n",
    "        \n",
    "    def summary(self):\n",
    "        \"\"\"Genera un resumen del proceso\"\"\"\n",
    "        total_time = time.time() - self.start_time\n",
    "        \n",
    "        # Calcular estad√≠sticas de recursos\n",
    "        if self.resources:\n",
    "            avg_mem = sum(r['memory_percent'] for r in self.resources) / len(self.resources)\n",
    "            max_mem = max(r['memory_percent'] for r in self.resources)\n",
    "            avg_cpu = sum(r['cpu_percent'] for r in self.resources) / len(self.resources)\n",
    "            max_cpu = max(r['cpu_percent'] for r in self.resources)\n",
    "        else:\n",
    "            avg_mem = max_mem = avg_cpu = max_cpu = 0\n",
    "        \n",
    "        summary_dict = {\n",
    "            'name': self.name,\n",
    "            'total_time': total_time,\n",
    "            'start_time': self.start_time,\n",
    "            'end_time': time.time(),\n",
    "            'section_times': self.section_times,\n",
    "            'metrics': dict(self.metrics),\n",
    "            'resources': {\n",
    "                'avg_memory_percent': avg_mem,\n",
    "                'max_memory_percent': max_mem,\n",
    "                'avg_cpu_percent': avg_cpu,\n",
    "                'max_cpu_percent': max_cpu\n",
    "            },\n",
    "            'num_checkpoints': len(self.checkpoints)\n",
    "        }\n",
    "        \n",
    "        # Convertir tipos numpy a tipos nativos de Python para JSON\n",
    "        summary_dict = self._convert_numpy_types(summary_dict)\n",
    "        \n",
    "        # Guardar resumen en formato JSON\n",
    "        summary_path = LOG_DIR / f\"summary_{run_timestamp}.json\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary_dict, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"üìë RESUMEN DEL PROCESO GUARDADO: {summary_path}\")\n",
    "        \n",
    "        # Imprimir resumen\n",
    "        logger.info(f\"üìã RESUMEN DE EJECUCI√ìN - {self.name}\")\n",
    "        logger.info(f\"  Tiempo total: {total_time:.2f} segundos\")\n",
    "        logger.info(f\"  Secciones completadas: {len(self.section_times)}\")\n",
    "        for section, time_taken in sorted(self.section_times.items(), key=lambda x: x[1], reverse=True):\n",
    "            logger.info(f\"    - {section}: {time_taken:.2f} segundos\")\n",
    "        logger.info(f\"  Checkpoints registrados: {len(self.checkpoints)}\")\n",
    "        logger.info(f\"  Uso de recursos:\")\n",
    "        logger.info(f\"    - Memoria promedio: {avg_mem:.1f}%\")\n",
    "        logger.info(f\"    - Memoria m√°xima: {max_mem:.1f}%\")\n",
    "        logger.info(f\"    - CPU promedio: {avg_cpu:.1f}%\")\n",
    "        logger.info(f\"    - CPU m√°xima: {max_cpu:.1f}%\")\n",
    "        \n",
    "        return summary_dict\n",
    "\n",
    "# Inicializar el rastreador de procesos\n",
    "tracker = ProcessTracker()\n",
    "\n",
    "# Funci√≥n para decorar funciones con trazabilidad\n",
    "def trace(section_name=None):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            func_name = section_name or func.__name__\n",
    "            tracker.start_section(func_name)\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                tracker.end_section()\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå ERROR en {func_name}: {str(e)}\")\n",
    "                tracker.end_section()\n",
    "                raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Intentar configurar el paralelismo antes de cualquier operaci√≥n que inicialice el contexto\n",
    "try:\n",
    "    # Configurar threading para TensorFlow\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "    logger.info(\"Configuraci√≥n de threading de TensorFlow aplicada\")\n",
    "except RuntimeError as e:\n",
    "    # Si ya se inicializ√≥ el contexto, informar pero seguir adelante\n",
    "    logger.warning(f\"No se pudo configurar threading de TensorFlow: {str(e)}. Continuando con valores por defecto.\")\n",
    "\n",
    "# Detectar si estamos en Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Si estamos en Colab, clonar el repositorio\n",
    "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
    "    %cd ml_precipitation_prediction\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy\n",
    "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
    "else:\n",
    "    # Si estamos en local, usar la ruta actual\n",
    "    if '/models' in os.getcwd():\n",
    "        BASE_PATH = Path('..')\n",
    "    else:\n",
    "        BASE_PATH = Path('.')\n",
    "\n",
    "BASE = Path(BASE_PATH)\n",
    "print(f\"Entorno configurado. Usando ruta base: {BASE}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FULL_NC      = BASE/\"data\"/\"output\"/\"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
    "FUSION_NC    = BASE/\"models\"/\"output\"/\"features_fusion_branches.nc\"\n",
    "TRAINED_DIR  = BASE/\"models\"/\"output\"/\"trained_models\"\n",
    "TRAINED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PRED_DIR = BASE/\"models\"/\"output\"/\"predictions\"\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HISTORY_DIR = BASE/\"models\"/\"output\"/\"histories\"\n",
    "HISTORY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INPUT_WINDOW   = 60\n",
    "OUTPUT_HORIZON = 3\n",
    "\n",
    "import numpy            as np\n",
    "import pandas           as pd\n",
    "import xarray           as xr\n",
    "import geopandas        as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs      as ccrs\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics        import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from xgboost                import XGBRegressor\n",
    "# A√±adir importaci√≥n de LightGBM y reducci√≥n de dimensionalidad\n",
    "from lightgbm               import LGBMRegressor\n",
    "from sklearn.decomposition  import PCA\n",
    "from sklearn.pipeline       import Pipeline\n",
    "\n",
    "from tensorflow.keras.models    import Sequential, Model\n",
    "from tensorflow.keras.layers    import Input, Dense, LSTM, GRU, Flatten, Reshape, Dropout, Concatenate, BatchNormalization, TimeDistributed, RepeatVector, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Importar TensorFlow aqu√≠ y configurarlo antes de cualquier operaci√≥n\n",
    "\n",
    "# Actualizar importaci√≥n de mixed_precision para compatibilidad con versiones recientes de TF\n",
    "try:\n",
    "    # Para TensorFlow 2.4+\n",
    "    from tensorflow.keras import mixed_precision\n",
    "except ImportError:\n",
    "    # Fallback para versiones m√°s antiguas de TF\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "import ace_tools_open as tools\n",
    "\n",
    "# Configurar crecimiento de memoria GPU din√°mico para evitar ResourceExhaustedError\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logger.info(f\"GPU configurada para crecimiento din√°mico de memoria: {len(gpus)} GPUs disponibles\")\n",
    "    except RuntimeError as e:\n",
    "        logger.error(f\"Error configurando GPU: {str(e)}\")\n",
    "\n",
    "# Tambi√©n limitar la memoria de TensorFlow para operaciones CPU\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "\n",
    "# Funciones auxiliares para gesti√≥n eficiente de memoria\n",
    "def get_memory_info():\n",
    "    \"\"\"Obtiene informaci√≥n de memoria del sistema\"\"\"\n",
    "    mem_info = psutil.virtual_memory()\n",
    "    return {\n",
    "        'total_gb': mem_info.total / (1024**3),\n",
    "        'available_gb': mem_info.available / (1024**3),\n",
    "        'used_percent': mem_info.percent,\n",
    "        'free_gb': mem_info.free / (1024**3)\n",
    "    }\n",
    "\n",
    "# Funciones auxiliares para persistencia de modelos\n",
    "def get_model_path(model_type, level_name, component_idx=None):\n",
    "    \"\"\"\n",
    "    Genera la ruta para guardar o cargar un modelo espec√≠fico\n",
    "    \n",
    "    Args:\n",
    "        model_type: Tipo de modelo ('fusion', 'bigru', 'meta')\n",
    "        level_name: Nombre del nivel de elevaci√≥n\n",
    "        component_idx: √çndice de componente (para modelos de fusi√≥n)\n",
    "        \n",
    "    Returns:\n",
    "        Path: Ruta completa del archivo del modelo\n",
    "    \"\"\"\n",
    "    if model_type == 'fusion':\n",
    "        return TRAINED_DIR / f\"fusion_xgb_{level_name}_comp{component_idx}.pkl\"\n",
    "    elif model_type == 'bigru':\n",
    "        return TRAINED_DIR / f\"BiGRU_{level_name}_model.keras\"\n",
    "    elif model_type == 'meta':\n",
    "        return TRAINED_DIR / \"meta_fusion_model.pkl\"\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo de modelo no reconocido: {model_type}\")\n",
    "\n",
    "def save_model(model, model_type, level_name, component_idx=None, extra_info=None):\n",
    "    \"\"\"\n",
    "    Guarda un modelo con su informaci√≥n asociada\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a guardar\n",
    "        model_type: Tipo de modelo ('fusion', 'bigru', 'meta')\n",
    "        level_name: Nombre del nivel de elevaci√≥n\n",
    "        component_idx: √çndice de componente (para modelos de fusi√≥n)\n",
    "        extra_info: Informaci√≥n adicional a guardar (pesos, m√©tricas, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True si se guard√≥ correctamente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_path = get_model_path(model_type, level_name, component_idx)\n",
    "        \n",
    "        # Para modelos XGBoost y otros que requieren pickle\n",
    "        if model_type in ['fusion', 'meta']:\n",
    "            with open(model_path, 'wb') as f:\n",
    "                import pickle\n",
    "                data_to_save = {'model': model}\n",
    "                if extra_info:\n",
    "                    data_to_save['info'] = extra_info\n",
    "                pickle.dump(data_to_save, f)\n",
    "        \n",
    "        # Para modelos Keras\n",
    "        elif model_type == 'bigru':\n",
    "            model.save(model_path)\n",
    "            \n",
    "            # Si hay info adicional, guardarla por separado\n",
    "            if extra_info:\n",
    "                info_path = model_path.parent / f\"{model_path.stem}_info.pkl\"\n",
    "                with open(info_path, 'wb') as f:\n",
    "                    import pickle\n",
    "                    pickle.dump(extra_info, f)\n",
    "        \n",
    "        logger.info(f\"Modelo {model_type} para {level_name} guardado en: {model_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al guardar modelo {model_type} para {level_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def load_model(model_type, level_name, component_idx=None):\n",
    "    \"\"\"\n",
    "    Carga un modelo previamente guardado\n",
    "    \n",
    "    Args:\n",
    "        model_type: Tipo de modelo ('fusion', 'bigru', 'meta')\n",
    "        level_name: Nombre del nivel de elevaci√≥n\n",
    "        component_idx: √çndice de componente (para modelos de fusi√≥n)\n",
    "        \n",
    "    Returns:\n",
    "        model: Modelo cargado o None si no existe\n",
    "        extra_info: Informaci√≥n adicional o None si no existe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_path = get_model_path(model_type, level_name, component_idx)\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            return None, None\n",
    "            \n",
    "        # Para modelos XGBoost y otros almacenados con pickle\n",
    "        if model_type in ['fusion', 'meta']:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                import pickle\n",
    "                data = pickle.load(f)\n",
    "                if isinstance(data, dict) and 'model' in data:\n",
    "                    model = data['model']\n",
    "                    extra_info = data.get('info')\n",
    "                else:\n",
    "                    # Compatibilidad con formato antiguo\n",
    "                    model = data\n",
    "                    extra_info = None\n",
    "        \n",
    "        # Para modelos Keras\n",
    "        elif model_type == 'bigru':\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "            \n",
    "            # Intentar cargar info adicional si existe\n",
    "            extra_info = None\n",
    "            info_path = model_path.parent / f\"{model_path.stem}_info.pkl\"\n",
    "            if info_path.exists():\n",
    "                with open(info_path, 'rb') as f:\n",
    "                    import pickle\n",
    "                    extra_info = pickle.load(f)\n",
    "        \n",
    "        logger.info(f\"Modelo {model_type} para {level_name} cargado desde: {model_path}\")\n",
    "        return model, extra_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al cargar modelo {model_type} para {level_name}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def model_exists(model_type, level_name, component_idx=None):\n",
    "    \"\"\"\n",
    "    Verifica si existe un modelo previamente guardado\n",
    "    \n",
    "    Args:\n",
    "        model_type: Tipo de modelo ('fusion', 'bigru', 'meta')\n",
    "        level_name: Nombre del nivel de elevaci√≥n\n",
    "        component_idx: √çndice de componente (para modelos de fusi√≥n)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True si el modelo existe\n",
    "    \"\"\"\n",
    "    model_path = get_model_path(model_type, level_name, component_idx)\n",
    "    return model_path.exists()\n",
    "\n",
    "# Funciones auxiliares para monitorear la memoria de la GPU\n",
    "def get_gpu_memory_info():\n",
    "    \"\"\"Obtiene la informaci√≥n de memoria de la GPU disponible\"\"\"\n",
    "    if not gpus:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Intentar usar NVIDIA-SMI a trav√©s de subprocess if est√° disponible\n",
    "        import subprocess\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used,memory.free,memory.total', '--format=csv,noheader,nounits'],\n",
    "            encoding='utf-8')\n",
    "        gpu_info = []\n",
    "        for line in result.strip().split('\\n'):\n",
    "            values = [float(x) for x in line.split(',')]\n",
    "            gpu_info.append({\n",
    "                'memory_used_mb': values[0],\n",
    "                'memory_free_mb': values[1],\n",
    "                'memory_total_mb': values[2],\n",
    "                'memory_used_percent': values[0] / values[2] * 100\n",
    "            })\n",
    "        return gpu_info\n",
    "    except (ImportError, subprocess.SubprocessError, FileNotFoundError):\n",
    "        # Si nvidia-smi no est√° disponible, usar tensorflow para obtener informaci√≥n limitada\n",
    "        try:\n",
    "            memory_info = []\n",
    "            for i, gpu in enumerate(gpus):\n",
    "                # En versiones nuevas de TF podemos obtener informaci√≥n de memoria usando experimental.VirtualDeviceConfiguration\n",
    "                try:\n",
    "                    mem_info = tf.config.experimental.get_memory_info(f'GPU:{i}')\n",
    "                    total_memory = mem_info['current'] + mem_info['peak']  # Aproximaci√≥n\n",
    "                    memory_info.append({\n",
    "                        'memory_used_mb': mem_info['current'] / (1024 * 1024),\n",
    "                        'memory_free_mb': (total_memory - mem_info['current']) / (1024 * 1024),\n",
    "                        'memory_total_mb': total_memory / (1024 * 1024),\n",
    "                        'memory_used_percent': mem_info['current'] / total_memory * 100 if total_memory else 0\n",
    "                    })\n",
    "                except (KeyError, AttributeError, ValueError):\n",
    "                    # Si no podemos obtener informaci√≥n espec√≠fica, proveer una estimaci√≥n\n",
    "                    memory_info.append({\n",
    "                        'memory_used_mb': -1,  # No conocido\n",
    "                        'memory_free_mb': -1,  # No conocido\n",
    "                        'memory_total_mb': -1,  # No conocido\n",
    "                        'memory_used_percent': -1  # No conocido\n",
    "                    })\n",
    "            return memory_info\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Funci√≥n mejorada para limpiar la memoria\n",
    "def clear_memory(force_garbage_collection=True):\n",
    "    \"\"\"\n",
    "    Limpia la memoria de manera m√°s agresiva, liberando recursos de TensorFlow y Python\n",
    "\n",
    "    Args:\n",
    "        force_garbage_collection: Si es True, fuerza la recolecci√≥n de basura\n",
    "    \"\"\"\n",
    "    # 1. Limpiar sesi√≥n de TensorFlow para liberar variables y tensores\n",
    "    try:\n",
    "        tf.keras.backend.clear_session()\n",
    "        logger.debug(\"Sesi√≥n de Keras limpiada\")\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Error al limpiar sesi√≥n de Keras: {str(e)}\")\n",
    "\n",
    "    # 2. Reiniciar gr√°fico de operaciones de TF si est√° disponible\n",
    "    try:\n",
    "        # Para versiones antiguas de TF que tienen reset_default_graph\n",
    "        if hasattr(tf, 'reset_default_graph'):\n",
    "            tf.reset_default_graph()\n",
    "            logger.debug(\"Gr√°fico de TF reiniciado\")\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Error al reiniciar gr√°fico de TF: {str(e)}\")\n",
    "\n",
    "    # 3. Forzar recolecci√≥n de basura de Python\n",
    "    if force_garbage_collection:\n",
    "        import gc\n",
    "        # Realizar m√∫ltiples pasadas para asegurar la limpieza completa\n",
    "        collected = gc.collect()\n",
    "        logger.debug(f\"GC recolect√≥ {collected} objetos\")\n",
    "\n",
    "        # Segunda pasada para objetos que posiblemente se liberaron en la primera\n",
    "        collected = gc.collect()\n",
    "        logger.debug(f\"GC recolect√≥ {collected} objetos adicionales\")\n",
    "\n",
    "    # 4. Intentar liberar memoria al sistema operativo\n",
    "    if 'psutil' in sys.modules:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        try:\n",
    "            # En Linux\n",
    "            if hasattr(process, 'memory_full_info'):\n",
    "                mi = process.memory_full_info()\n",
    "                logger.debug(f\"Memoria usada: RSS={mi.rss/1e6:.1f}MB, VMS={mi.vms/1e6:.1f}MB\")\n",
    "\n",
    "            # En sistemas POSIX, sincronizar filesystem para liberar buffers\n",
    "            if hasattr(os, 'sync'):\n",
    "                os.sync()\n",
    "                logger.debug(\"Sincronizado el sistema de archivos\")\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error en operaciones de memoria del proceso: {str(e)}\")\n",
    "\n",
    "    # 5. Intentar liberar la memoria GPU espec√≠ficamente\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Ejecutar operaciones vac√≠as para forzar sincronizaci√≥n de GPU\n",
    "            dummy = tf.random.normal([1, 1])\n",
    "            _ = dummy.numpy()  # Forzar ejecuci√≥n\n",
    "            logger.debug(\"Operaciones GPU sincronizadas\")\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error al sincronizar GPU: {str(e)}\")\n",
    "\n",
    "# Funci√≥n para crear un conjunto de datos de TensorFlow con mejor manejo de errores de memoria\n",
    "def create_tf_dataset(X, Y, batch_size=32, force_cpu=False, max_retries=3):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow Dataset from numpy arrays with batching.\n",
    "    Includes robust error handling with automatic CPU fallback and batch size adjustment.\n",
    "\n",
    "    Args:\n",
    "        X: Input features array\n",
    "        Y: Target labels array\n",
    "        batch_size: Size of batches for training\n",
    "        force_cpu: If True, forces operations to run on CPU\n",
    "        max_retries: Maximum number of retry attempts with smaller batch size\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset object configured for training\n",
    "    \"\"\"\n",
    "    # Verificar la memoria disponible y ajustar par√°metros autom√°ticamente\n",
    "    gpu_info = get_gpu_memory_info()\n",
    "    mem_info = get_memory_info()\n",
    "\n",
    "    # Estimar si la GPU est√° cerca del l√≠mite (>80% usada) para decidir si forzar CPU\n",
    "    auto_force_cpu = False\n",
    "    if gpu_info and not force_cpu:\n",
    "        for gpu in gpu_info:\n",
    "            if gpu['memory_used_percent'] > 80:\n",
    "                logger.warning(f\"GPU usage high ({gpu['memory_used_percent']:.1f}%), forcing CPU execution\")\n",
    "                auto_force_cpu = True\n",
    "                break\n",
    "\n",
    "    # Si hay m√°s de un intento, reducir el batch size\n",
    "    actual_force_cpu = force_cpu or auto_force_cpu\n",
    "    actual_batch_size = batch_size\n",
    "\n",
    "    # Bucle de reintento con tama√±os de batch m√°s peque√±os\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Verificar si hay NaNs antes de convertir a tensores\n",
    "            if np.isnan(X).any() or np.isnan(Y).any():\n",
    "                logger.warning(\"Se detectaron NaNs en los datos. Reemplazando con ceros.\")\n",
    "                X = np.nan_to_num(X, nan=0.0)\n",
    "                Y = np.nan_to_num(Y, nan=0.0)\n",
    "\n",
    "            # Estrategia espec√≠fica para CPU o GPU\n",
    "            if actual_force_cpu:\n",
    "                with tf.device('/CPU:0'):\n",
    "                    # Convertir a tensores expl√≠citamente para mejor control\n",
    "                    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "                    Y_tensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "\n",
    "                    # Crear dataset usando los tensores convertidos\n",
    "                    dataset = tf.data.Dataset.from_tensor_slices((X_tensor, Y_tensor))\n",
    "                    logger.info(f\"Dataset creado en CPU con batch_size={actual_batch_size}\")\n",
    "            else:\n",
    "                # Intentar crear el dataset con GPU\n",
    "                dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "                logger.info(f\"Dataset creado en GPU con batch_size={actual_batch_size}\")\n",
    "\n",
    "            # Configurar el dataset para entrenamiento con un buffer size adaptativo\n",
    "            # Usar buffer size m√°s peque√±o para reducir uso de memoria\n",
    "            samples = len(X)\n",
    "            buffer_size = min(samples, 1000)  # M√°ximo 1000 elementos en memoria\n",
    "\n",
    "            # Ajustar buffer size si la memoria est√° baja\n",
    "            if mem_info['available_gb'] < 2.0:  # Menos de 2GB disponibles\n",
    "                buffer_size = min(buffer_size, 100)  # Reducir a m√°ximo 100 elementos\n",
    "                logger.warning(f\"Memoria disponible baja ({mem_info['available_gb']:.1f}GB), buffer reducido a {buffer_size}\")\n",
    "\n",
    "            dataset = dataset.shuffle(buffer_size=buffer_size, seed=42)\n",
    "            dataset = dataset.batch(actual_batch_size)\n",
    "            dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "            # Probar que el dataset funciona extrayendo un batch\n",
    "            try:\n",
    "                for _ in dataset.take(1):\n",
    "                    pass  # Solo verificar que podemos iterar\n",
    "                logger.info(\"Dataset verificado correctamente\")\n",
    "            except tf.errors.ResourceExhaustedError as e:\n",
    "                raise e  # Relanzar para manejar en el bloque catch\n",
    "\n",
    "            return dataset\n",
    "\n",
    "        except (tf.errors.ResourceExhaustedError, tf.errors.InternalError, tf.errors.FailedPreconditionError,\n",
    "                tf.errors.AbortedError, tf.errors.OOM) as e:\n",
    "            # Si estamos en el √∫ltimo intento, reducir dr√°sticamente\n",
    "            if attempt == max_retries - 1:\n",
    "                logger.error(f\"Error cr√≠tico al crear dataset: {str(e)}\")\n",
    "\n",
    "                # √öltimo intento desesperado: m√≠nimo batch size y forzar CPU\n",
    "                logger.warning(\"Intento final con configuraci√≥n m√≠nima (batch=1, CPU)\")\n",
    "                with tf.device('/CPU:0'):\n",
    "                    logger.info(\"Creando dataset final con configuraci√≥n m√≠nima\")\n",
    "                    # Crear con el menor batch posible\n",
    "                    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "                    Y_tensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "                    dataset = tf.data.Dataset.from_tensor_slices((X_tensor, Y_tensor))\n",
    "                    dataset = dataset.batch(1)  # M√≠nimo batch size\n",
    "                    return dataset\n",
    "            else:\n",
    "                # Reducir batch size y forzar CPU en pr√≥ximo intento\n",
    "                prev_batch = actual_batch_size\n",
    "                actual_batch_size = max(1, actual_batch_size // 2)\n",
    "                actual_force_cpu = True\n",
    "\n",
    "                logger.warning(f\"Intento {attempt+1}/{max_retries}: Reduciendo batch size de {prev_batch} a {actual_batch_size} y forzando CPU\")\n",
    "\n",
    "                # Limpiar memoria antes del pr√≥ximo intento\n",
    "                clear_memory()\n",
    "                time.sleep(1)  # Peque√±a pausa para permitir que el sistema se estabilice\n",
    "\n",
    "# Funci√≥n gen√©rica para predecir en lotes\n",
    "def predict_in_batches(model, X, batch_size=32, verbose=0):\n",
    "    \"\"\"\n",
    "    Genera predicciones de cualquier modelo en lotes para evitar problemas de memoria\n",
    "\n",
    "    Args:\n",
    "        model: Modelo entrenado (Keras, TensorFlow, etc.)\n",
    "        X: Datos de entrada (numpy array)\n",
    "        batch_size: Tama√±o del lote para procesamiento\n",
    "        verbose: Nivel de verbosidad para las predicciones\n",
    "\n",
    "    Returns:\n",
    "        Array con predicciones\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "\n",
    "    # Si X es muy peque√±o, predecir directamente\n",
    "    if n_samples <= batch_size:\n",
    "        return model.predict(X, verbose=verbose)\n",
    "\n",
    "    # Ajustar batch_size seg√∫n memoria disponible\n",
    "    try:\n",
    "        mem_info = get_memory_info()\n",
    "        adaptive_batch = min(batch_size, max(8, int(mem_info['available_gb'] * 10)))\n",
    "        logger.info(f\"Generando predicciones en lotes de {adaptive_batch} muestras\")\n",
    "        batch_size = adaptive_batch\n",
    "    except:\n",
    "        # Si falla la adaptaci√≥n, usar el batch_size proporcionado\n",
    "        logger.info(f\"Generando predicciones en lotes de {batch_size} muestras\")\n",
    "\n",
    "    # Inferir la forma de salida del modelo haciendo una predicci√≥n en un √∫nico ejemplo\n",
    "    try:\n",
    "        sample_pred = model.predict(X[:1], verbose=0)\n",
    "        output_shape = sample_pred.shape[1:]  # Excluye la dimensi√≥n del batch\n",
    "    except:\n",
    "        # Si falla, asumir forma desconocida y manejarla despu√©s\n",
    "        output_shape = None\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # Procesar por lotes\n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        batch_X = X[start_idx:end_idx]\n",
    "\n",
    "        # Para mayor seguridad, comprobar si hay NaNs\n",
    "        has_nans = np.isnan(batch_X).any()\n",
    "        if has_nans:\n",
    "            logger.warning(f\"Detectados NaN en lote {start_idx}-{end_idx}, realizando imputaci√≥n\")\n",
    "            # Reemplazar NaNs con 0 para evitar errores\n",
    "            batch_X = np.nan_to_num(batch_X, nan=0.0)\n",
    "\n",
    "        # Predecir lote\n",
    "        try:\n",
    "            batch_preds = model.predict(batch_X, verbose=0 if start_idx > 0 else verbose)\n",
    "            predictions.append(batch_preds)\n",
    "\n",
    "            # Liberar memoria cada 5 lotes\n",
    "            if (start_idx // batch_size) % 5 == 0 and start_idx > 0:\n",
    "                # Liberar memoria expl√≠citamente\n",
    "                if 'gc' in sys.modules:\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al predecir lote {start_idx}-{end_idx}: {str(e)}\")\n",
    "            # Intentar con un batch m√°s peque√±o como √∫ltimo recurso\n",
    "            try:\n",
    "                smaller_batch = max(1, batch_size // 4)\n",
    "                logger.warning(f\"Reintentando con batch m√°s peque√±o: {smaller_batch}\")\n",
    "                mini_batch_preds = []\n",
    "                for mini_start in range(start_idx, end_idx, smaller_batch):\n",
    "                    mini_end = min(mini_start + smaller_batch, end_idx)\n",
    "                    mini_X = X[mini_start:mini_end]\n",
    "                    mini_pred = model.predict(mini_X, verbose=0)\n",
    "                    mini_batch_preds.append(mini_pred)\n",
    "                batch_preds = np.vstack(mini_batch_preds)\n",
    "                predictions.append(batch_preds)\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"Error en segundo intento de lote: {str(e2)}\")\n",
    "                # Si tambi√©n falla, rellenar con ceros\n",
    "                if output_shape:\n",
    "                    batch_size_curr = end_idx - start_idx\n",
    "                    zeros_shape = (batch_size_curr,) + output_shape\n",
    "                    logger.warning(f\"Rellenando con ceros de forma {zeros_shape}\")\n",
    "                    predictions.append(np.zeros(zeros_shape))\n",
    "                else:\n",
    "                    raise e2\n",
    "\n",
    "    # Concatenar resultados\n",
    "    try:\n",
    "        return np.vstack(predictions)\n",
    "    except:\n",
    "        # Si vstack falla (por ejemplo, formas inconsistentes), devolver una lista\n",
    "        logger.warning(\"No se pudo concatenar predicciones, devolviendo lista de arrays\")\n",
    "        return predictions\n",
    "\n",
    "# Funciones espec√≠ficas para XGBoost con optimizaci√≥n de memoria\n",
    "def predict_xgb_in_batches(model, X, batch_size=100):\n",
    "    \"\"\"\n",
    "    Genera predicciones XGBoost en lotes para evitar problemas de memoria\n",
    "\n",
    "    Args:\n",
    "        model: Modelo XGBoost entrenado\n",
    "        X: Datos de entrada (numpy array)\n",
    "        batch_size: Tama√±o del lote para procesamiento\n",
    "\n",
    "    Returns:\n",
    "        Array con predicciones\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    predictions = np.zeros(n_samples)\n",
    "\n",
    "    # Ajustar tama√±o de lote seg√∫n memoria disponible\n",
    "    mem_info = get_memory_info()\n",
    "    adaptive_batch = min(batch_size, max(10, int(mem_info['available_gb'] * 10)))\n",
    "\n",
    "    logger.info(f\"Generando predicciones XGBoost en lotes de {adaptive_batch} muestras\")\n",
    "\n",
    "    # Procesar por lotes\n",
    "    for start_idx in range(0, n_samples, adaptive_batch):\n",
    "        end_idx = min(start_idx + adaptive_batch, n_samples)\n",
    "        batch_X = X[start_idx:end_idx]\n",
    "\n",
    "        # Para mayor seguridad, comprobar si hay NaNs\n",
    "        has_nans = np.isnan(batch_X).any()\n",
    "        if has_nans:\n",
    "            logger.warning(f\"Detectados NaN en lote {start_idx}-{end_idx}, realizando imputaci√≥n\")\n",
    "            # Reemplazar NaNs con 0 para evitar errores\n",
    "            batch_X = np.nan_to_num(batch_X, nan=0.0)\n",
    "\n",
    "        # Predecir lote\n",
    "        batch_preds = model.predict(batch_X)\n",
    "        predictions[start_idx:end_idx] = batch_preds\n",
    "\n",
    "        # Liberar memoria cada 5 lotes\n",
    "        if (start_idx // adaptive_batch) % 5 == 0 and start_idx > 0:\n",
    "            if 'gc' in sys.modules:\n",
    "                import gc\n",
    "                gc.collect()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def train_xgb_with_memory_optimization(X_train, y_train, X_val=None, y_val=None, params=None):\n",
    "    \"\"\"\n",
    "    Entrena un modelo XGBoost con optimizaciones de memoria y velocidad\n",
    "\n",
    "    Args:\n",
    "        X_train: Datos de entrenamiento\n",
    "        y_train: Etiquetas de entrenamiento\n",
    "        X_val: Datos de validaci√≥n (opcional)\n",
    "        y_val: Etiquetas de validaci√≥n (opcional)\n",
    "        params: Par√°metros de XGBoost (diccionario)\n",
    "\n",
    "    Returns:\n",
    "        Modelo XGBoost entrenado\n",
    "    \"\"\"\n",
    "    # Par√°metros por defecto optimizados para velocidad y memoria\n",
    "    default_params = {\n",
    "        'n_estimators': 60,  # Reducido para mayor velocidad\n",
    "        'max_depth': 4,      # Reducido para mayor velocidad\n",
    "        'learning_rate': 0.2, # Aumentado para convergencia m√°s r√°pida\n",
    "        'subsample': 0.7,     # Reducido para mayor velocidad\n",
    "        'colsample_bytree': 0.7, # Reducido para mayor velocidad\n",
    "        'tree_method': 'hist',  # M√©todo m√°s eficiente en memoria\n",
    "        'predictor': 'cpu_predictor',  # Evitar problemas de GPU\n",
    "        'n_jobs': 1  # Un hilo por modelo para permitir paralelismo entre modelos\n",
    "    }\n",
    "\n",
    "    # Actualizar con par√°metros personalizados si se proporcionan\n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "\n",
    "    # Crear y entrenar modelo con early stopping si hay datos de validaci√≥n\n",
    "    if X_val is not None and y_val is not None:\n",
    "        model = XGBRegressor(**default_params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "    else:\n",
    "        # Sin early stopping si no hay datos de validaci√≥n\n",
    "        model = XGBRegressor(**default_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_xgb_horizon_predictions(meta_models, base_model_preds, cells, horizons=3):\n",
    "    \"\"\"\n",
    "    Genera predicciones por horizonte usando modelos XGBoost en metamodelado\n",
    "\n",
    "    Args:\n",
    "        meta_models: Lista de modelos XGBoost (uno por horizonte)\n",
    "        base_model_preds: Diccionario de predicciones de modelos base {modelo: predicciones}\n",
    "        cells: N√∫mero de celdas espaciales\n",
    "        horizons: N√∫mero de horizontes temporales\n",
    "\n",
    "    Returns:\n",
    "        Array de predicciones (muestras, horizontes, celdas)\n",
    "    \"\"\"\n",
    "    # Determinar n√∫mero de muestras del primer modelo base\n",
    "    first_model = list(base_model_preds.keys())[0]\n",
    "    n_samples = base_model_preds[first_model].shape[0]\n",
    "\n",
    "    # Inicializar array para predicciones\n",
    "    Y_pred = np.zeros((n_samples, horizons, cells))\n",
    "\n",
    "    # Procesar cada horizonte\n",
    "    for h in range(horizons):\n",
    "        logger.info(f\"Generando predicciones para horizonte {h+1}/{horizontes}\")\n",
    "\n",
    "        # Si no hay modelo para este horizonte, continuar al siguiente\n",
    "        if h >= len(meta_models):\n",
    "            logger.warning(f\"No hay modelo meta-XGB para horizonte {h+1}\")\n",
    "            continue\n",
    "\n",
    "        # Preparar caracter√≠sticas para este horizonte\n",
    "        X_meta_batches = []\n",
    "        batch_size = 100\n",
    "\n",
    "        # Procesar por lotes para evitar problemas de memoria\n",
    "        for start_idx in range(0, n_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "\n",
    "            # Preparar entradas para metamodelo\n",
    "            X_meta_batch_parts = []\n",
    "            for model_name in base_model_preds:\n",
    "                if h < base_model_preds[model_name].shape[1]:\n",
    "                    # Extraer predicciones del modelo base para este horizonte\n",
    "                    model_preds = base_model_preds[model_name][start_idx:end_idx, h, :]\n",
    "                    X_meta_batch_parts.append(model_preds.reshape(end_idx - start_idx, -1))\n",
    "\n",
    "            # Concatenar caracter√≠sticas de todos los modelos base\n",
    "            if X_meta_batch_parts:\n",
    "                X_meta_batch = np.hstack(X_meta_batch_parts)\n",
    "\n",
    "                # Predecir con el meta-modelo XGB para este lote\n",
    "                Y_pred[start_idx:end_idx, h, :] = meta_models[h].predict(X_meta_batch).reshape(-1, cells)\n",
    "\n",
    "            # Liberar memoria cada 5 lotes\n",
    "            if (start_idx // batch_size) % 5 == 0 and start_idx > 0:\n",
    "                # Liberar memoria expl√≠citamente\n",
    "                if 'gc' in sys.modules:\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "\n",
    "    return Y_pred\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Carga de datos con separaci√≥n expl√≠cita de caracter√≠sticas CEEMDAN y TFV-EMD\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\")\n",
    "ds_full = xr.open_dataset(FULL_NC)\n",
    "ds_fuse = xr.open_dataset(FUSION_NC)\n",
    "\n",
    "# precipitacion y variables\n",
    "prec = ds_full[\"total_precipitation\"].values  # (T, ny, nx)\n",
    "lags = sorted([v for v in ds_full.data_vars if \"_lag\" in v])\n",
    "da_lags = np.stack([ds_full[lag].values for lag in lags], axis=-1)  # (T, ny, nx, n_lags)\n",
    "\n",
    "# Separar caracter√≠sticas CEEMDAN y TFV-EMD para optimizar su fusi√≥n\n",
    "ceemdan_branches = [\"CEEMDAN_high\", \"CEEMDAN_medium\", \"CEEMDAN_low\"]\n",
    "tvfemd_branches = [\"TVFEMD_high\", \"TVFEMD_medium\", \"TVFEMD_low\"]\n",
    "fusion_branches = [\"FUSION_high\", \"FUSION_medium\", \"FUSION_low\"]\n",
    "\n",
    "# Cargar datos CEEMDAN\n",
    "da_ceemdan = np.stack([ds_fuse[branch].values for branch in ceemdan_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar datos TFV-EMD\n",
    "da_tvfemd = np.stack([ds_fuse[branch].values for branch in tvfemd_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar fusi√≥n predefinida (para referencia)\n",
    "da_fusion = np.stack([ds_fuse[branch].values for branch in fusion_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "\n",
    "# topograf√≠a y cluster\n",
    "elev = ds_full[\"elevation\"].values.ravel()  # (cells,)\n",
    "slope = ds_full[\"slope\"].values.ravel()\n",
    "\n",
    "# Manejar correctamente los valores de cluster (pueden ser texto)\n",
    "cluster_values = ds_full[\"cluster_elevation\"].values.ravel()\n",
    "# Verificar si los valores son strings o num√©ricos\n",
    "if isinstance(cluster_values[0], (str, np.str_)):\n",
    "    # Usar un LabelEncoder para convertir strings a enteros\n",
    "    le = LabelEncoder()\n",
    "    cluster = le.fit_transform(cluster_values)\n",
    "    logger.info(f\"Clusters codificados de texto a n√∫meros: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    # Si ya son num√©ricos, convertir a enteros\n",
    "    cluster = cluster_values.astype(int)\n",
    "\n",
    "# dimensiones\n",
    "lat = ds_full.latitude.values\n",
    "lon = ds_full.longitude.values\n",
    "ny, nx = len(lat), len(lon)\n",
    "cells = ny*nx\n",
    "T = prec.shape[0]\n",
    "\n",
    "logger.info(f\"Dimensiones: T={T}, ny={ny}, nx={nx}, cells={cells}\")\n",
    "logger.info(f\"Shapes: prec={prec.shape}, da_ceemdan={da_ceemdan.shape}, da_tvfemd={da_tvfemd.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Definir m√°scaras para los niveles de elevaci√≥n\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Definiendo m√°scaras para los niveles de elevaci√≥n...\")\n",
    "mask_nivel1 = elev < 957  # nivel_1: 58-956m\n",
    "mask_nivel2 = (elev >= 957) & (elev <= 2264)  # nivel_2: 957-2264m\n",
    "mask_nivel3 = elev > 2264  # nivel_3: 2264-4728m\n",
    "\n",
    "logger.info(f\"Distribuci√≥n de celdas por nivel de elevaci√≥n:\")\n",
    "logger.info(f\"  Nivel 1 (<957m): {np.sum(mask_nivel1)} celdas\")\n",
    "logger.info(f\"  Nivel 2 (957-2264m): {np.sum(mask_nivel2)} celdas\")\n",
    "logger.info(f\"  Nivel 3 (>2264m): {np.sum(mask_nivel3)} celdas\")\n",
    "\n",
    "# Crear diccionario de m√°scaras para facilitar el procesamiento\n",
    "elevation_masks = {\n",
    "    \"nivel_1\": mask_nivel1,\n",
    "    \"nivel_2\": mask_nivel2,\n",
    "    \"nivel_3\": mask_nivel3\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Implementar funci√≥n para optimizar fusi√≥n de CEEMDAN y TFV-EMD con XGBoost\n",
    "# -----------------------------------------------------------------------------\n",
    "import concurrent.futures\n",
    "import tqdm\n",
    "from functools import partial\n",
    "\n",
    "@trace(\"Optimizaci√≥n de fusi√≥n\")\n",
    "def optimize_fusion_with_xgboost(ceemdan_data, tvfemd_data, target_data, masks, test_size=0.2, force_retrain=False):\n",
    "    \"\"\"\n",
    "    Optimiza la fusi√≥n de CEEMDAN y TFV-EMD usando XGBoost para cada nivel de elevaci√≥n.\n",
    "    Implementa paralelismo adaptativo basado en CPU/GPU y memoria disponible.\n",
    "    \n",
    "    Args:\n",
    "        ceemdan_data: Array de caracter√≠sticas CEEMDAN (T, ny, nx, 3)\n",
    "        tvfemd_data: Array de caracter√≠sticas TFV-EMD (T, ny, nx, 3)\n",
    "        target_data: Array de valores objetivo (precipitaci√≥n) (T, ny, nx)\n",
    "        masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        test_size: Proporci√≥n del conjunto de prueba\n",
    "        force_retrain: Si es True, fuerza el reentrenamiento aunque existan modelos guardados\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary con modelos XGBoost para fusi√≥n por nivel y componente\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    # Comprobar si todos los modelos ya existen\n",
    "    all_models_exist = True\n",
    "    if not force_retrain:\n",
    "        for level_name in masks:\n",
    "            for component_idx in range(3):\n",
    "                if not model_exists('fusion', level_name, component_idx):\n",
    "                    all_models_exist = False\n",
    "                    break\n",
    "            if not all_models_exist:\n",
    "                break\n",
    "                \n",
    "        if all_models_exist:\n",
    "            logger.info(\"Todos los modelos de fusi√≥n existen. Cargando...\")\n",
    "            return load_all_fusion_models(masks)\n",
    "    \n",
    "    # Determinar recursos computacionales disponibles\n",
    "    mem_info = get_memory_info()\n",
    "    cpu_count = os.cpu_count()\n",
    "    \n",
    "    print(f\"\\nüñ•Ô∏è  Recursos detectados: {cpu_count} CPUs, {mem_info['total_gb']:.1f}GB RAM ({mem_info['available_gb']:.1f}GB disponible)\")\n",
    "    \n",
    "    # SOLUCI√ìN: Aumentar agresivamente el n√∫mero de trabajadores para forzar paralelismo\n",
    "    # y aprovechar mejor los recursos subutilizados\n",
    "    optimal_workers = max(3, min(cpu_count - 1, 8))  # M√≠nimo 3 workers, m√°ximo CPU-1 o 8\n",
    "    \n",
    "    # Verificar disponibilidad de GPU para tree_method\n",
    "    gpu_available = len(gpus) > 0\n",
    "    tree_method = 'gpu_hist' if gpu_available else 'hist'\n",
    "    \n",
    "    print(f\"üîß Configuraci√≥n optimizada: {optimal_workers} workers en paralelo FORZADOS, tree_method={tree_method}\")\n",
    "    print(f\"üß† Memoria disponible: {mem_info['available_gb']:.2f}GB ({mem_info['used_percent']:.1f}% usado)\")\n",
    "    \n",
    "    # Total de componentes a procesar\n",
    "    total_levels = len(masks)\n",
    "    total_components = total_levels * 3  # 3 componentes por nivel\n",
    "    \n",
    "    # Inicializar estructuras de datos para resultados\n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]  # Placeholder para los 3 componentes\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "    \n",
    "    # Barra de progreso global\n",
    "    print(f\"\\nüìä Iniciando entrenamiento acelerado de {total_components} componentes ({total_levels} niveles √ó 3 componentes)\")\n",
    "    \n",
    "    # Funci√≥n para procesar un componente espec√≠fico\n",
    "    def process_component(level_name, mask, component_idx):\n",
    "        # Verificar si el modelo ya existe (a menos que se fuerce reentrenamiento)\n",
    "        if not force_retrain and model_exists('fusion', level_name, component_idx):\n",
    "            print(f\"üîÑ Nivel {level_name}, componente {component_idx}: Cargando modelo existente...\")\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            if model and info:\n",
    "                weights = info.get('weights')\n",
    "                rmse = info.get('rmse', 0.0)\n",
    "                fit_time = info.get('fit_time', 0.0)\n",
    "                total_time = 0.1  # Tiempo m√≠nimo para evitar divisiones por cero\n",
    "                print(f\"‚úÖ {level_name}, comp{component_idx} (cargado): RMSE={rmse:.4f}, pesos=[CEEMDAN={weights[0]:.2f}, TFV-EMD={weights[1]:.2f}]\")\n",
    "                return {\n",
    "                    'level': level_name,\n",
    "                    'component': component_idx,\n",
    "                    'model': model,\n",
    "                    'weights': weights,\n",
    "                    'rmse': rmse,\n",
    "                    'fit_time': fit_time,\n",
    "                    'total_time': total_time,\n",
    "                    'loaded': True\n",
    "                }\n",
    "        \n",
    "        # Si llegamos aqu√≠, necesitamos entrenar el modelo\n",
    "        print(f\"‚ñ∂Ô∏è  Nivel {level_name}, componente {component_idx}: Iniciando entrenamiento r√°pido...\")\n",
    "        comp_start = time.time()\n",
    "        cells_in_level = np.sum(mask)\n",
    "        \n",
    "        # Reformatear los datos para el entrenamiento\n",
    "        X_ceemdan = ceemdan_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        X_tvfemd = tvfemd_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        y_target = target_data.reshape(T, -1)[:, mask]\n",
    "        \n",
    "        print(f\"   Datos: {X_ceemdan.shape[0]} muestras, {cells_in_level} celdas\")\n",
    "        \n",
    "        # Concatenar caracter√≠sticas\n",
    "        X_combined = np.column_stack([X_ceemdan, X_tvfemd])\n",
    "        \n",
    "        # Divisi√≥n simple para mayor velocidad (sin estratificaci√≥n que consume tiempo)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_target, test_size=test_size, random_state=42\n",
    "        )\n",
    "        split_method = \"simple (optimizado para velocidad)\"\n",
    "        \n",
    "        print(f\"   Split: {X_train.shape[0]} train, {X_test.shape[0]} test ({split_method})\")\n",
    "        \n",
    "        # SOLUCI√ìN: Optimizar hiperpar√°metros para mayor velocidad\n",
    "        n_samples, n_features = X_train.shape\n",
    "        # Reducir profundidad y n√∫mero de √°rboles para entrenamientos m√°s r√°pidos\n",
    "        max_depth = min(4, max(3, int(np.log2(n_features/2))))  # Profundidad reducida\n",
    "        n_estimators = min(60, max(30, int(30 + 5 * np.log(n_samples))))  # Menos √°rboles\n",
    "        learning_rate = min(0.3, max(0.08, 0.2))  # Learning rate m√°s alto para convergencia r√°pida\n",
    "        subsample = 0.7  # Usar menos datos por √°rbol\n",
    "        colsample = 0.7  # Usar menos columnas por √°rbol\n",
    "        \n",
    "        # Configurar modelo XGBoost con paralelismo m√°s eficiente\n",
    "        model = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample,\n",
    "            tree_method=tree_method,\n",
    "            n_jobs=1,  # Un hilo por modelo para maximizar paralelismo entre modelos\n",
    "            enable_categorical=False,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        # Entrenar modelo con mensaje de progreso\n",
    "        print(f\"   Entrenamiento ultra-r√°pido: {n_estimators} estimators, depth={max_depth}, lr={learning_rate:.3f}\")\n",
    "        fit_start = time.time()\n",
    "        \n",
    "        # Entrenamiento simplificado para mayor velocidad\n",
    "        model.fit(\n",
    "            X_train, y_train.ravel(),\n",
    "            eval_set=[(X_test, y_test.ravel())],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        fit_time = time.time() - fit_start\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test.ravel(), y_pred))\n",
    "        \n",
    "        # Extraer pesos de importancia para CEEMDAN vs TFV-EMD\n",
    "        importance = model.feature_importances_\n",
    "        cells_per_feature = cells_in_level\n",
    "        \n",
    "        # Promedio de importancia para cada fuente\n",
    "        ceemdan_importance = np.mean(importance[:cells_per_feature])\n",
    "        tvfemd_importance = np.mean(importance[cells_per_feature:])\n",
    "        \n",
    "        # Normalizar para que sumen 1\n",
    "        total_importance = ceemdan_importance + tvfemd_importance\n",
    "        ceemdan_weight = ceemdan_importance / total_importance\n",
    "        tvfemd_weight = tvfemd_importance / total_importance\n",
    "        \n",
    "        comp_time = time.time() - comp_start\n",
    "        \n",
    "        print(f\"‚úÖ {level_name}, comp{component_idx}: RMSE={rmse:.4f}, tiempo={comp_time:.1f}s, \"\n",
    "              f\"pesos=[CEEMDAN={ceemdan_weight:.2f}, TFV-EMD={tvfemd_weight:.2f}]\")\n",
    "        \n",
    "        weights = (ceemdan_weight, tvfemd_weight)\n",
    "        \n",
    "        # Guardar modelo para uso futuro con informaci√≥n adicional\n",
    "        info = {\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'training_date': datetime.datetime.now().strftime(timestamp_format),\n",
    "            'hyper_params': {\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'learning_rate': learning_rate,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        save_model(model, 'fusion', level_name, component_idx, info)\n",
    "        \n",
    "        # Devolver resultados\n",
    "        return {\n",
    "            'level': level_name,\n",
    "            'component': component_idx,\n",
    "            'model': model,\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'loaded': False\n",
    "        }\n",
    "    \n",
    "    # Procesar niveles y componentes usando paralelismo adaptativo\n",
    "    all_tasks = []\n",
    "    for level_name, mask in masks.items():\n",
    "        # Crear tareas para todos los componentes\n",
    "        for component_idx in range(3):\n",
    "            all_tasks.append((level_name, mask, component_idx))\n",
    "    \n",
    "    # SOLUCI√ìN: FORZAR paralelismo siempre\n",
    "    all_results = []\n",
    "    \n",
    "    # Mostrar mensaje claro sobre el modo paralelo\n",
    "    print(f\"\\n‚ö° Activando procesamiento paralelo forzado con {optimal_workers} workers para acelerar el entrenamiento\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=optimal_workers) as executor:\n",
    "        # Crear lista de futuros\n",
    "        futures = []\n",
    "        for level_name, mask, component_idx in all_tasks:\n",
    "            futures.append(executor.submit(\n",
    "                process_component, level_name, mask, component_idx\n",
    "            ))\n",
    "        \n",
    "        # Mostrar progreso mientras se completan las tareas\n",
    "        completed = 0\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            completed += 1\n",
    "            progress = completed / len(futures)\n",
    "            print(f\"‚è≥ Progreso global: {completed}/{len(futures)} componentes ({progress:.1%})\")\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en componente: {str(e)}\")\n",
    "    \n",
    "    # Organizar resultados por nivel y componente\n",
    "    for result in all_results:\n",
    "        level = result['level']\n",
    "        component = result['component']\n",
    "        \n",
    "        # Guardar modelo y pesos\n",
    "        fusion_models[level][component] = result['model']\n",
    "        fusion_weights[level][component] = result['weights']\n",
    "        \n",
    "        # Registrar m√©tricas para trazabilidad\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"rmse\", result['rmse'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"ceemdan_weight\", result['weights'][0])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"tvfemd_weight\", result['weights'][1])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"train_time\", result['fit_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"total_time\", result['total_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"loaded\", result.get('loaded', False))\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\nüèÅ Optimizaci√≥n de fusi√≥n completada:\")\n",
    "    for level_name, components in fusion_models.items():\n",
    "        valid_components = sum(1 for model in components if model is not None)\n",
    "        print(f\"  - {level_name}: {valid_components}/3 componentes entrenados\")\n",
    "    \n",
    "    tracker.add_checkpoint(\"Optimizaci√≥n de fusi√≥n completada\", \n",
    "                          {\"num_models\": sum(len(models) for models in fusion_models.values())})\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "def load_all_fusion_models(masks):\n",
    "    \"\"\"\n",
    "    Carga todos los modelos de fusi√≥n existentes\n",
    "    \n",
    "    Args:\n",
    "        masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fusion_models, fusion_weights)\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "        \n",
    "        # Cargar los tres modelos de componentes\n",
    "        for component_idx in range(3):\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            \n",
    "            if model is not None and info is not None:\n",
    "                fusion_models[level_name][component_idx] = model\n",
    "                fusion_weights[level_name][component_idx] = info['weights']\n",
    "                logger.info(f\"Modelo fusi√≥n {level_name}, componente {component_idx} cargado\")\n",
    "                \n",
    "                # Registrar m√©tricas para trazabilidad\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"rmse\", info.get('rmse', 0))\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"ceemdan_weight\", info['weights'][0])\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"tvfemd_weight\", info['weights'][1])\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"loaded\", True)\n",
    "            else:\n",
    "                logger.warning(f\"No se pudo cargar el modelo fusi√≥n {level_name}, componente {component_idx}\")\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Carga de datos con separaci√≥n expl√≠cita de caracter√≠sticas CEEMDAN y TFV-EMD\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\")\n",
    "ds_full = xr.open_dataset(FULL_NC)\n",
    "ds_fuse = xr.open_dataset(FUSION_NC)\n",
    "\n",
    "# precipitacion y variables\n",
    "prec = ds_full[\"total_precipitation\"].values  # (T, ny, nx)\n",
    "lags = sorted([v for v in ds_full.data_vars if \"_lag\" in v])\n",
    "da_lags = np.stack([ds_full[lag].values for lag in lags], axis=-1)  # (T, ny, nx, n_lags)\n",
    "\n",
    "# Separar caracter√≠sticas CEEMDAN y TFV-EMD para optimizar su fusi√≥n\n",
    "ceemdan_branches = [\"CEEMDAN_high\", \"CEEMDAN_medium\", \"CEEMDAN_low\"]\n",
    "tvfemd_branches = [\"TVFEMD_high\", \"TVFEMD_medium\", \"TVFEMD_low\"]\n",
    "fusion_branches = [\"FUSION_high\", \"FUSION_medium\", \"FUSION_low\"]\n",
    "\n",
    "# Cargar datos CEEMDAN\n",
    "da_ceemdan = np.stack([ds_fuse[branch].values for branch in ceemdan_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar datos TFV-EMD\n",
    "da_tvfemd = np.stack([ds_fuse[branch].values for branch in tvfemd_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar fusi√≥n predefinida (para referencia)\n",
    "da_fusion = np.stack([ds_fuse[branch].values for branch in fusion_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "\n",
    "# topograf√≠a y cluster\n",
    "elev = ds_full[\"elevation\"].values.ravel()  # (cells,)\n",
    "slope = ds_full[\"slope\"].values.ravel()\n",
    "\n",
    "# Manejar correctamente los valores de cluster (pueden ser texto)\n",
    "cluster_values = ds_full[\"cluster_elevation\"].values.ravel()\n",
    "# Verificar si los valores son strings o num√©ricos\n",
    "if isinstance(cluster_values[0], (str, np.str_)):\n",
    "    # Usar un LabelEncoder para convertir strings a enteros\n",
    "    le = LabelEncoder()\n",
    "    cluster = le.fit_transform(cluster_values)\n",
    "    logger.info(f\"Clusters codificados de texto a n√∫meros: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    # Si ya son num√©ricos, convertir a enteros\n",
    "    cluster = cluster_values.astype(int)\n",
    "\n",
    "# dimensiones\n",
    "lat = ds_full.latitude.values\n",
    "lon = ds_full.longitude.values\n",
    "ny, nx = len(lat), len(lon)\n",
    "cells = ny*nx\n",
    "T = prec.shape[0]\n",
    "\n",
    "logger.info(f\"Dimensiones: T={T}, ny={ny}, nx={nx}, cells={cells}\")\n",
    "logger.info(f\"Shapes: prec={prec.shape}, da_ceemdan={da_ceemdan.shape}, da_tvfemd={da_tvfemd.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Definir m√°scaras para los niveles de elevaci√≥n\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Definiendo m√°scaras para los niveles de elevaci√≥n...\")\n",
    "mask_nivel1 = elev < 957  # nivel_1: 58-956m\n",
    "mask_nivel2 = (elev >= 957) & (elev <= 2264)  # nivel_2: 957-2264m\n",
    "mask_nivel3 = elev > 2264  # nivel_3: 2264-4728m\n",
    "\n",
    "logger.info(f\"Distribuci√≥n de celdas por nivel de elevaci√≥n:\")\n",
    "logger.info(f\"  Nivel 1 (<957m): {np.sum(mask_nivel1)} celdas\")\n",
    "logger.info(f\"  Nivel 2 (957-2264m): {np.sum(mask_nivel2)} celdas\")\n",
    "logger.info(f\"  Nivel 3 (>2264m): {np.sum(mask_nivel3)} celdas\")\n",
    "\n",
    "# Crear diccionario de m√°scaras para facilitar el procesamiento\n",
    "elevation_masks = {\n",
    "    \"nivel_1\": mask_nivel1,\n",
    "    \"nivel_2\": mask_nivel2,\n",
    "    \"nivel_3\": mask_nivel3\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Implementar funci√≥n para optimizar fusi√≥n de CEEMDAN y TFV-EMD con XGBoost\n",
    "# -----------------------------------------------------------------------------\n",
    "import concurrent.futures\n",
    "import tqdm\n",
    "from functools import partial\n",
    "\n",
    "@trace(\"Optimizaci√≥n de fusi√≥n\")\n",
    "def optimize_fusion_with_xgboost(ceemdan_data, tvfemd_data, target_data, masks, test_size=0.2, force_retrain=False):\n",
    "    \"\"\"\n",
    "    Optimiza la fusi√≥n de CEEMDAN y TFV-EMD usando XGBoost para cada nivel de elevaci√≥n.\n",
    "    Implementa paralelismo adaptativo basado en CPU/GPU y memoria disponible.\n",
    "    \n",
    "    Args:\n",
    "        ceemdan_data: Array de caracter√≠sticas CEEMDAN (T, ny, nx, 3)\n",
    "        tvfemd_data: Array de caracter√≠sticas TFV-EMD (T, ny, nx, 3)\n",
    "        target_data: Array de valores objetivo (precipitaci√≥n) (T, ny, nx)\n",
    "        masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        test_size: Proporci√≥n del conjunto de prueba\n",
    "        force_retrain: Si es True, fuerza el reentrenamiento aunque existan modelos guardados\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary con modelos XGBoost para fusi√≥n por nivel y componente\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    # Comprobar si todos los modelos ya existen\n",
    "    all_models_exist = True\n",
    "    if not force_retrain:\n",
    "        for level_name in masks:\n",
    "            for component_idx in range(3):\n",
    "                if not model_exists('fusion', level_name, component_idx):\n",
    "                    all_models_exist = False\n",
    "                    break\n",
    "            if not all_models_exist:\n",
    "                break\n",
    "                \n",
    "        if all_models_exist:\n",
    "            logger.info(\"Todos los modelos de fusi√≥n existen. Cargando...\")\n",
    "            return load_all_fusion_models(masks)\n",
    "    \n",
    "    # Determinar recursos computacionales disponibles\n",
    "    mem_info = get_memory_info()\n",
    "    cpu_count = os.cpu_count()\n",
    "    \n",
    "    print(f\"\\nüñ•Ô∏è  Recursos detectados: {cpu_count} CPUs, {mem_info['total_gb']:.1f}GB RAM ({mem_info['available_gb']:.1f}GB disponible)\")\n",
    "    \n",
    "    # SOLUCI√ìN: Aumentar agresivamente el n√∫mero de trabajadores para forzar paralelismo\n",
    "    # y aprovechar mejor los recursos subutilizados\n",
    "    optimal_workers = max(3, min(cpu_count - 1, 8))  # M√≠nimo 3 workers, m√°ximo CPU-1 o 8\n",
    "    \n",
    "    # Verificar disponibilidad de GPU para tree_method\n",
    "    gpu_available = len(gpus) > 0\n",
    "    tree_method = 'gpu_hist' if gpu_available else 'hist'\n",
    "    \n",
    "    print(f\"üîß Configuraci√≥n optimizada: {optimal_workers} workers en paralelo FORZADOS, tree_method={tree_method}\")\n",
    "    print(f\"üß† Memoria disponible: {mem_info['available_gb']:.2f}GB ({mem_info['used_percent']:.1f}% usado)\")\n",
    "    \n",
    "    # Total de componentes a procesar\n",
    "    total_levels = len(masks)\n",
    "    total_components = total_levels * 3  # 3 componentes por nivel\n",
    "    \n",
    "    # Inicializar estructuras de datos para resultados\n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]  # Placeholder para los 3 componentes\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "    \n",
    "    # Barra de progreso global\n",
    "    print(f\"\\nüìä Iniciando entrenamiento acelerado de {total_components} componentes ({total_levels} niveles √ó 3 componentes)\")\n",
    "    \n",
    "    # Funci√≥n para procesar un componente espec√≠fico\n",
    "    def process_component(level_name, mask, component_idx):\n",
    "        # Verificar si el modelo ya existe (a menos que se fuerce reentrenamiento)\n",
    "        if not force_retrain and model_exists('fusion', level_name, component_idx):\n",
    "            print(f\"üîÑ Nivel {level_name}, componente {component_idx}: Cargando modelo existente...\")\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            if model and info:\n",
    "                weights = info.get('weights')\n",
    "                rmse = info.get('rmse', 0.0)\n",
    "                fit_time = info.get('fit_time', 0.0)\n",
    "                total_time = 0.1  # Tiempo m√≠nimo para evitar divisiones por cero\n",
    "                print(f\"‚úÖ {level_name}, comp{component_idx} (cargado): RMSE={rmse:.4f}, pesos=[CEEMDAN={weights[0]:.2f}, TFV-EMD={weights[1]:.2f}]\")\n",
    "                return {\n",
    "                    'level': level_name,\n",
    "                    'component': component_idx,\n",
    "                    'model': model,\n",
    "                    'weights': weights,\n",
    "                    'rmse': rmse,\n",
    "                    'fit_time': fit_time,\n",
    "                    'total_time': total_time,\n",
    "                    'loaded': True\n",
    "                }\n",
    "        \n",
    "        # Si llegamos aqu√≠, necesitamos entrenar el modelo\n",
    "        print(f\"‚ñ∂Ô∏è  Nivel {level_name}, componente {component_idx}: Iniciando entrenamiento r√°pido...\")\n",
    "        comp_start = time.time()\n",
    "        cells_in_level = np.sum(mask)\n",
    "        \n",
    "        # Reformatear los datos para el entrenamiento\n",
    "        X_ceemdan = ceemdan_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        X_tvfemd = tvfemd_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        y_target = target_data.reshape(T, -1)[:, mask]\n",
    "        \n",
    "        print(f\"   Datos: {X_ceemdan.shape[0]} muestras, {cells_in_level} celdas\")\n",
    "        \n",
    "        # Concatenar caracter√≠sticas\n",
    "        X_combined = np.column_stack([X_ceemdan, X_tvfemd])\n",
    "        \n",
    "        # Divisi√≥n simple para mayor velocidad (sin estratificaci√≥n que consume tiempo)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_target, test_size=test_size, random_state=42\n",
    "        )\n",
    "        split_method = \"simple (optimizado para velocidad)\"\n",
    "        \n",
    "        print(f\"   Split: {X_train.shape[0]} train, {X_test.shape[0]} test ({split_method})\")\n",
    "        \n",
    "        # SOLUCI√ìN: Optimizar hiperpar√°metros para mayor velocidad\n",
    "        n_samples, n_features = X_train.shape\n",
    "        # Reducir profundidad y n√∫mero de √°rboles para entrenamientos m√°s r√°pidos\n",
    "        max_depth = min(4, max(3, int(np.log2(n_features/2))))  # Profundidad reducida\n",
    "        n_estimators = min(60, max(30, int(30 + 5 * np.log(n_samples))))  # Menos √°rboles\n",
    "        learning_rate = min(0.3, max(0.08, 0.2))  # Learning rate m√°s alto para convergencia r√°pida\n",
    "        subsample = 0.7  # Usar menos datos por √°rbol\n",
    "        colsample = 0.7  # Usar menos columnas por √°rbol\n",
    "        \n",
    "        # Configurar modelo XGBoost con paralelismo m√°s eficiente\n",
    "        model = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample,\n",
    "            tree_method=tree_method,\n",
    "            n_jobs=1,  # Un hilo por modelo para maximizar paralelismo entre modelos\n",
    "            enable_categorical=False,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        # Entrenar modelo con mensaje de progreso\n",
    "        print(f\"   Entrenamiento ultra-r√°pido: {n_estimators} estimators, depth={max_depth}, lr={learning_rate:.3f}\")\n",
    "        fit_start = time.time()\n",
    "        \n",
    "        # Entrenamiento simplificado para mayor velocidad\n",
    "        model.fit(\n",
    "            X_train, y_train.ravel(),\n",
    "            eval_set=[(X_test, y_test.ravel())],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        fit_time = time.time() - fit_start\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test.ravel(), y_pred))\n",
    "        \n",
    "        # Extraer pesos de importancia para CEEMDAN vs TFV-EMD\n",
    "        importance = model.feature_importances_\n",
    "        cells_per_feature = cells_in_level\n",
    "        \n",
    "        # Promedio de importancia para cada fuente\n",
    "        ceemdan_importance = np.mean(importance[:cells_per_feature])\n",
    "        tvfemd_importance = np.mean(importance[cells_per_feature:])\n",
    "        \n",
    "        # Normalizar para que sumen 1\n",
    "        total_importance = ceemdan_importance + tvfemd_importance\n",
    "        ceemdan_weight = ceemdan_importance / total_importance\n",
    "        tvfemd_weight = tvfemd_importance / total_importance\n",
    "        \n",
    "        comp_time = time.time() - comp_start\n",
    "        \n",
    "        print(f\"‚úÖ {level_name}, comp{component_idx}: RMSE={rmse:.4f}, tiempo={comp_time:.1f}s, \"\n",
    "              f\"pesos=[CEEMDAN={ceemdan_weight:.2f}, TFV-EMD={tvfemd_weight:.2f}]\")\n",
    "        \n",
    "        weights = (ceemdan_weight, tvfemd_weight)\n",
    "        \n",
    "        # Guardar modelo para uso futuro con informaci√≥n adicional\n",
    "        info = {\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'training_date': datetime.datetime.now().strftime(timestamp_format),\n",
    "            'hyper_params': {\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'learning_rate': learning_rate,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        save_model(model, 'fusion', level_name, component_idx, info)\n",
    "        \n",
    "        # Devolver resultados\n",
    "        return {\n",
    "            'level': level_name,\n",
    "            'component': component_idx,\n",
    "            'model': model,\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'loaded': False\n",
    "        }\n",
    "    \n",
    "    # Procesar niveles y componentes usando paralelismo adaptativo\n",
    "    all_tasks = []\n",
    "    for level_name, mask in masks.items():\n",
    "        # Crear tareas para todos los componentes\n",
    "        for component_idx in range(3):\n",
    "            all_tasks.append((level_name, mask, component_idx))\n",
    "    \n",
    "    # SOLUCI√ìN: FORZAR paralelismo siempre\n",
    "    all_results = []\n",
    "    \n",
    "    # Mostrar mensaje claro sobre el modo paralelo\n",
    "    print(f\"\\n‚ö° Activando procesamiento paralelo forzado con {optimal_workers} workers para acelerar el entrenamiento\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=optimal_workers) as executor:\n",
    "        # Crear lista de futuros\n",
    "        futures = []\n",
    "        for level_name, mask, component_idx in all_tasks:\n",
    "            futures.append(executor.submit(\n",
    "                process_component, level_name, mask, component_idx\n",
    "            ))\n",
    "        \n",
    "        # Mostrar progreso mientras se completan las tareas\n",
    "        completed = 0\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            completed += 1\n",
    "            progress = completed / len(futures)\n",
    "            print(f\"‚è≥ Progreso global: {completed}/{len(futures)} componentes ({progress:.1%})\")\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en componente: {str(e)}\")\n",
    "    \n",
    "    # Organizar resultados por nivel y componente\n",
    "    for result in all_results:\n",
    "        level = result['level']\n",
    "        component = result['component']\n",
    "        \n",
    "        # Guardar modelo y pesos\n",
    "        fusion_models[level][component] = result['model']\n",
    "        fusion_weights[level][component] = result['weights']\n",
    "        \n",
    "        # Registrar m√©tricas para trazabilidad\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"rmse\", result['rmse'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"ceemdan_weight\", result['weights'][0])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"tvfemd_weight\", result['weights'][1])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"train_time\", result['fit_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"total_time\", result['total_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"loaded\", result.get('loaded', False))\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\nüèÅ Optimizaci√≥n de fusi√≥n completada:\")\n",
    "    for level_name, components in fusion_models.items():\n",
    "        valid_components = sum(1 for model in components if model is not None)\n",
    "        print(f\"  - {level_name}: {valid_components}/3 componentes entrenados\")\n",
    "    \n",
    "    tracker.add_checkpoint(\"Optimizaci√≥n de fusi√≥n completada\", \n",
    "                          {\"num_models\": sum(len(models) for models in fusion_models.values())})\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "def load_all_fusion_models(masks):\n",
    "    \"\"\"\n",
    "    Carga todos los modelos de fusi√≥n existentes\n",
    "    \n",
    "    Args:\n",
    "        masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fusion_models, fusion_weights)\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "        \n",
    "        # Cargar los tres modelos de componentes\n",
    "        for component_idx in range(3):\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            \n",
    "            if model is not None and info is not None:\n",
    "                fusion_models[level_name][component_idx] = model\n",
    "               \n",
    "                fusion_weights[level_name][component_idx] = info['weights']\n",
    "                logger.info(f\"Modelo fusi√≥n {level_name}, componente {component_idx} cargado\")\n",
    "                \n",
    "                # Registrar m√©tricas para trazabilidad\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"rmse\", info.get('rmse', 0))\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"ceemdan_weight\", info['weights'][0])\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"tvfemd_weight\", info['weights'][1])\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"loaded\", True)\n",
    "            else:\n",
    "                logger.warning(f\"No se pudo cargar el modelo fusi√≥n {level_name}, componente {component_idx}\")\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Carga de datos con separaci√≥n expl√≠cita de caracter√≠sticas CEEMDAN y TFV-EMD\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Cargando datasets y separando caracter√≠sticas CEEMDAN y TFV-EMD...\")\n",
    "ds_full = xr.open_dataset(FULL_NC)\n",
    "ds_fuse = xr.open_dataset(FUSION_NC)\n",
    "\n",
    "# precipitacion y variables\n",
    "prec = ds_full[\"total_precipitation\"].values  # (T, ny, nx)\n",
    "lags = sorted([v for v in ds_full.data_vars if \"_lag\" in v])\n",
    "da_lags = np.stack([ds_full[lag].values for lag in lags], axis=-1)  # (T, ny, nx, n_lags)\n",
    "\n",
    "# Separar caracter√≠sticas CEEMDAN y TFV-EMD para optimizar su fusi√≥n\n",
    "ceemdan_branches = [\"CEEMDAN_high\", \"CEEMDAN_medium\", \"CEEMDAN_low\"]\n",
    "tvfemd_branches = [\"TVFEMD_high\", \"TVFEMD_medium\", \"TVFEMD_low\"]\n",
    "fusion_branches = [\"FUSION_high\", \"FUSION_medium\", \"FUSION_low\"]\n",
    "\n",
    "# Cargar datos CEEMDAN\n",
    "da_ceemdan = np.stack([ds_fuse[branch].values for branch in ceemdan_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar datos TFV-EMD\n",
    "da_tvfemd = np.stack([ds_fuse[branch].values for branch in tvfemd_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar fusi√≥n predefinida (para referencia)\n",
    "da_fusion = np.stack([ds_fuse[branch].values for branch in fusion_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "\n",
    "# topograf√≠a y cluster\n",
    "elev = ds_full[\"elevation\"].values.ravel()  # (cells,)\n",
    "slope = ds_full[\"slope\"].values.ravel()\n",
    "\n",
    "# Manejar correctamente los valores de cluster (pueden ser texto)\n",
    "cluster_values = ds_full[\"cluster_elevation\"].values.ravel()\n",
    "# Verificar si los valores son strings o num√©ricos\n",
    "if isinstance(cluster_values[0], (str, np.str_)):\n",
    "    # Usar un LabelEncoder para convertir strings a enteros\n",
    "    le = LabelEncoder()\n",
    "    cluster = le.fit_transform(cluster_values)\n",
    "    logger.info(f\"Clusters codificados de texto a n√∫meros: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    # Si ya son num√©ricos, convertir a enteros\n",
    "    cluster = cluster_values.astype(int)\n",
    "\n",
    "# dimensiones\n",
    "lat = ds_full.latitude.values\n",
    "lon = ds_full.longitude.values\n",
    "ny, nx = len(lat), len(lon)\n",
    "cells = ny*nx\n",
    "T = prec.shape[0]\n",
    "\n",
    "logger.info(f\"Dimensiones: T={T}, ny={ny}, nx={nx}, cells={cells}\")\n",
    "logger.info(f\"Shapes: prec={prec.shape}, da_ceemdan={da_ceemdan.shape}, da_tvfemd={da_tvfemd.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Definir m√°scaras para los niveles de elevaci√≥n\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Definiendo m√°scaras para los niveles de elevaci√≥n...\")\n",
    "mask_nivel1 = elev < 957  # nivel_1: 58-956m\n",
    "mask_nivel2 = (elev >= 957) & (elev <= 2264)  # nivel_2: 957-2264m\n",
    "mask_nivel3 = elev > 2264  # nivel_3: 2264-4728m\n",
    "\n",
    "logger.info(f\"Distribuci√≥n de celdas por nivel de elevaci√≥n:\")\n",
    "logger.info(f\"  Nivel 1 (<957m): {np.sum(mask_nivel1)} celdas\")\n",
    "logger.info(f\"  Nivel 2 (957-2264m): {np.sum(mask_nivel2)} celdas\")\n",
    "logger.info(f\"  Nivel 3 (>2264m): {np.sum(mask_nivel3)} celdas\")\n",
    "\n",
    "# Crear diccionario de m√°scaras para facilitar el procesamiento\n",
    "elevation_masks = {\n",
    "    \"nivel_1\": mask_nivel1,\n",
    "    \"nivel_2\": mask_nivel2,\n",
    "    \"nivel_3\": mask_nivel3\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Implementar funci√≥n para optimizar fusi√≥n de CEEMDAN y TFV-EMD con XGBoost\n",
    "# -----------------------------------------------------------------------------\n",
    "import concurrent.futures\n",
    "import tqdm\n",
    "from functools import partial\n",
    "\n",
    "@trace(\"Optimizaci√≥n de fusi√≥n\")\n",
    "def optimize_fusion_with_xgboost(ceemdan_data, tvfemd_data, target_data, masks, test_size=0.2, force_retrain=False):\n",
    "    \"\"\"\n",
    "    Optimiza la fusi√≥n de CEEMDAN y TFV-EMD usando XGBoost para cada nivel de elevaci√≥n.\n",
    "    Implementa paralelismo adaptativo basado en CPU/GPU y memoria disponible.\n",
    "    \n",
    "    Args:\n",
    "        ceemdan_data: Array de caracter√≠sticas CEEMDAN (T, ny, nx, 3)\n",
    "        tvfemd_data: Array de caracter√≠sticas TFV-EMD (T, ny, nx, 3)\n",
    "        target_data: Array de valores objetivo (precipitaci√≥n) (T, ny, nx)\n",
    "        masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        test_size: Proporci√≥n del conjunto de prueba\n",
    "        force_retrain: Si es True, fuerza el reentrenamiento aunque existan modelos guardados\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary con modelos XGBoost para fusi√≥n por nivel y componente\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    # Comprobar si todos los modelos ya existen\n",
    "    all_models_exist = True\n",
    "    if not force_retrain:\n",
    "        for level_name in masks:\n",
    "            for component_idx in range(3):\n",
    "                if not model_exists('fusion', level_name, component_idx):\n",
    "                    all_models_exist = False\n",
    "                    break\n",
    "            if not all_models_exist:\n",
    "                break\n",
    "                \n",
    "        if all_models_exist:\n",
    "            logger.info(\"Todos los modelos de fusi√≥n existen. Cargando...\")\n",
    "            return load_all_fusion_models(masks)\n",
    "    \n",
    "    # Determinar recursos computacionales disponibles\n",
    "    mem_info = get_memory_info()\n",
    "    cpu_count = os.cpu_count()\n",
    "    \n",
    "    print(f\"\\nüñ•Ô∏è  Recursos detectados: {cpu_count} CPUs, {mem_info['total_gb']:.1f}GB RAM ({mem_info['available_gb']:.1f}GB disponible)\")\n",
    "    \n",
    "    # SOLUCI√ìN: Aumentar agresivamente el n√∫mero de trabajadores para forzar paralelismo\n",
    "    # y aprovechar mejor los recursos subutilizados\n",
    "    optimal_workers = max(3, min(cpu_count - 1, 8))  # M√≠nimo 3 workers, m√°ximo CPU-1 o 8\n",
    "    \n",
    "    # Verificar disponibilidad de GPU para tree_method\n",
    "    gpu_available = len(gpus) > 0\n",
    "    tree_method = 'gpu_hist' if gpu_available else 'hist'\n",
    "    \n",
    "    print(f\"üîß Configuraci√≥n optimizada: {optimal_workers} workers en paralelo FORZADOS, tree_method={tree_method}\")\n",
    "    print(f\"üß† Memoria disponible: {mem_info['available_gb']:.2f}GB ({mem_info['used_percent']:.1f}% usado)\")\n",
    "    \n",
    "    # Total de componentes a procesar\n",
    "    total_levels = len(masks)\n",
    "    total_components = total_levels * 3  # 3 componentes por nivel\n",
    "    \n",
    "    # Inicializar estructuras de datos para resultados\n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]  # Placeholder para los 3 componentes\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "    \n",
    "    # Barra de progreso global\n",
    "    print(f\"\\nüìä Iniciando entrenamiento acelerado de {total_components} componentes ({total_levels} niveles √ó 3 componentes)\")\n",
    "    \n",
    "    # Funci√≥n para procesar un componente espec√≠fico\n",
    "    def process_component(level_name, mask, component_idx):\n",
    "        # Verificar si el modelo ya existe (a menos que se fuerce reentrenamiento)\n",
    "        if not force_retrain and model_exists('fusion', level_name, component_idx):\n",
    "            print(f\"üîÑ Nivel {level_name}, componente {component_idx}: Cargando modelo existente...\")\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            if model and info:\n",
    "                weights = info.get('weights')\n",
    "                rmse = info.get('rmse', 0.0)\n",
    "                fit_time = info.get('fit_time', 0.0)\n",
    "                total_time = 0.1  # Tiempo m√≠nimo para evitar divisiones por cero\n",
    "                print(f\"‚úÖ {level_name}, comp{component_idx} (cargado): RMSE={rmse:.4f}, pesos=[CEEMDAN={weights[0]:.2f}, TFV-EMD={weights[1]:.2f}]\")\n",
    "                return {\n",
    "                    'level': level_name,\n",
    "                    'component': component_idx,\n",
    "                    'model': model,\n",
    "                    'weights': weights,\n",
    "                    'rmse': rmse,\n",
    "                    'fit_time': fit_time,\n",
    "                    'total_time': total_time,\n",
    "                    'loaded': True\n",
    "                }\n",
    "        \n",
    "        # Si llegamos aqu√≠, necesitamos entrenar el modelo\n",
    "        print(f\"‚ñ∂Ô∏è  Nivel {level_name}, componente {component_idx}: Iniciando entrenamiento r√°pido...\")\n",
    "        comp_start = time.time()\n",
    "        cells_in_level = np.sum(mask)\n",
    "        \n",
    "        # Reformatear los datos para el entrenamiento\n",
    "        X_ceemdan = ceemdan_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        X_tvfemd = tvfemd_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        y_target = target_data.reshape(T, -1)[:, mask]\n",
    "        \n",
    "        print(f\"   Datos: {X_ceemdan.shape[0]} muestras, {cells_in_level} celdas\")\n",
    "        \n",
    "        # Concatenar caracter√≠sticas\n",
    "        X_combined = np.column_stack([X_ceemdan, X_tvfemd])\n",
    "        \n",
    "        # Divisi√≥n simple para mayor velocidad (sin estratificaci√≥n que consume tiempo)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_target, test_size=test_size, random_state=42\n",
    "        )\n",
    "        split_method = \"simple (optimizado para velocidad)\"\n",
    "        \n",
    "        print(f\"   Split: {X_train.shape[0]} train, {X_test.shape[0]} test ({split_method})\")\n",
    "        \n",
    "        # SOLUCI√ìN: Optimizar hiperpar√°metros para mayor velocidad\n",
    "        n_samples, n_features = X_train.shape\n",
    "        # Reducir profundidad y n√∫mero de √°rboles para entrenamientos m√°s r√°pidos\n",
    "        max_depth = min(4, max(3, int(np.log2(n_features/2))))  # Profundidad reducida\n",
    "        n_estimators = min(60, max(30, int(30 + 5 * np.log(n_samples))))  # Menos √°rboles\n",
    "        learning_rate = min(0.3, max(0.08, 0.2))  # Learning rate m√°s alto para convergencia r√°pida\n",
    "        subsample = 0.7  # Usar menos datos por √°rbol\n",
    "        colsample = 0.7  # Usar menos columnas por √°rbol\n",
    "        \n",
    "        # Configurar modelo XGBoost con paralelismo m√°s eficiente\n",
    "        model = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample,\n",
    "            tree_method=tree_method,\n",
    "            n_jobs=1,  # Un hilo por modelo para maximizar paralelismo entre modelos\n",
    "            enable_categorical=False,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        # Entrenar modelo con mensaje de progreso\n",
    "        print(f\"   Entrenamiento ultra-r√°pido: {n_estimators} estimators, depth={max_depth}, lr={learning_rate:.3f}\")\n",
    "        fit_start = time.time()\n",
    "        \n",
    "        # Entrenamiento simplificado para mayor velocidad\n",
    "        model.fit(\n",
    "            X_train, y_train.ravel(),\n",
    "            eval_set=[(X_test, y_test.ravel())],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        fit_time = time.time() - fit_start\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test.ravel(), y_pred))\n",
    "        \n",
    "        # Extraer pesos de importancia para CEEMDAN vs TFV-EMD\n",
    "        importance = model.feature_importances_\n",
    "        cells_per_feature = cells_in_level\n",
    "        \n",
    "        # Promedio de importancia para cada fuente\n",
    "        ceemdan_importance = np.mean(importance[:cells_per_feature])\n",
    "        tvfemd_importance = np.mean(importance[cells_per_feature:])\n",
    "        \n",
    "        # Normalizar para que sumen 1\n",
    "        total_importance = ceemdan_importance + tvfemd_importance\n",
    "        ceemdan_weight = ceemdan_importance / total_importance\n",
    "        tvfemd_weight = tvfemd_importance / total_importance\n",
    "        \n",
    "        comp_time = time.time() - comp_start\n",
    "        \n",
    "        print(f\"‚úÖ {level_name}, comp{component_idx}: RMSE={rmse:.4f}, tiempo={comp_time:.1f}s, \"\n",
    "              f\"pesos=[CEEMDAN={ceemdan_weight:.2f}, TFV-EMD={tvfemd_weight:.2f}]\")\n",
    "        \n",
    "        weights = (ceemdan_weight, tvfemd_weight)\n",
    "        \n",
    "        # Guardar modelo para uso futuro con informaci√≥n adicional\n",
    "        info = {\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'training_date': datetime.datetime.now().strftime(timestamp_format),\n",
    "            'hyper_params': {\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'learning_rate': learning_rate,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        save_model(model, 'fusion', level_name, component_idx, info)\n",
    "        \n",
    "        # Devolver resultados\n",
    "        return {\n",
    "            'level': level_name,\n",
    "            'component': component_idx,\n",
    "            'model': model,\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'loaded': False\n",
    "        }\n",
    "    \n",
    "    # Procesar niveles y componentes usando paralelismo adaptativo\n",
    "    all_tasks = []\n",
    "    for level_name, mask in masks.items():\n",
    "        # Crear tareas para todos los componentes\n",
    "        for component_idx in range(3):\n",
    "            all_tasks.append((level_name, mask, component_idx))\n",
    "    \n",
    "    # SOLUCI√ìN: FORZAR paralelismo siempre\n",
    "    all_results = []\n",
    "    \n",
    "    # Mostrar mensaje claro sobre el modo paralelo\n",
    "    print(f\"\\n‚ö° Activando procesamiento paralelo forzado con {optimal_workers} workers para acelerar el entrenamiento\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=optimal_workers) as executor:\n",
    "        # Crear lista de futuros\n",
    "        futures = []\n",
    "        for level_name, mask, component_idx in all_tasks:\n",
    "            futures.append(executor.submit(\n",
    "                process_component, level_name, mask, component_idx\n",
    "            ))\n",
    "        \n",
    "        # Mostrar progreso mientras se completan las tareas\n",
    "        completed = 0\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            completed += 1\n",
    "            progress = completed / len(futures)\n",
    "            print(f\"‚è≥ Progreso global: {completed}/{len(futures)} componentes ({progress:.1%})\")\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en componente: {str(e)}\")\n",
    "    \n",
    "    # Organizar resultados por nivel y componente\n",
    "    for result in all_results:\n",
    "        level = result['level']\n",
    "        component = result['component']\n",
    "        \n",
    "        # Guardar modelo y pesos\n",
    "        fusion_models[level][component] = result['model']\n",
    "        fusion_weights[level][component] = result['weights']\n",
    "        \n",
    "        # Registrar m√©tricas para trazabilidad\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"rmse\", result['rmse'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"ceemdan_weight\", result['weights'][0])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"tvfemd_weight\", result['weights'][1])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"train_time\", result['fit_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"total_time\", result['total_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"loaded\", result.get('loaded', False))\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\nüèÅ Optimizaci√≥n de fusi√≥n completada:\")\n",
    "    for level_name, components in fusion_models.items():\n",
    "        valid_components = sum(1 for model in components if model is not None)\n",
    "        print(f\"  - {level_name}: {valid_components}/3 componentes entrenados\")\n",
    "    \n",
    "    tracker.add_checkpoint(\"Optimizaci√≥n de fusi√≥n completada\", \n",
    "                          {\"num_models\": sum(len(models) for models in fusion_models.values())})\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "# Funciones para evaluaci√≥n de modelos\n",
    "@trace(\"Evaluaci√≥n global de modelos\")\n",
    "def calculate_global_metrics(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas globales para todos los modelos.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Diccionario de predicciones por modelo\n",
    "        ground_truth: Valores reales de precipitaci√≥n\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con m√©tricas para cada modelo\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    metrics_list = []\n",
    "    \n",
    "    logger.info(\"Calculando m√©tricas globales para todos los modelos...\")\n",
    "    \n",
    "    for model_name, preds in predictions.items():\n",
    "        # Aplanar arrays para c√°lculo de m√©tricas globales\n",
    "        y_true = ground_truth.reshape(-1)\n",
    "        y_pred = preds.reshape(-1)\n",
    "        \n",
    "        # Filtrar NaN si existen\n",
    "        mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "        if np.sum(mask) < len(mask):\n",
    "            logger.warning(f\"Modelo {model_name}: {len(mask) - np.sum(mask)} valores NaN detectados y excluidos\")\n",
    "            y_true = y_true[mask]\n",
    "            y_pred = y_pred[mask]\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # MAPE con manejo de divisiones por cero\n",
    "        mask_nonzero = y_true != 0\n",
    "        if np.sum(mask_nonzero) > 0:\n",
    "            mape = 100 * np.mean(np.abs((y_true[mask_nonzero] - y_pred[mask_nonzero]) / y_true[mask_nonzero]))\n",
    "        else:\n",
    "            mape = np.nan\n",
    "        \n",
    "        metrics_list.append({\n",
    "            'Model': model_name,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'R¬≤': r2\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Modelo {model_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, MAPE={mape:.2f}%, R¬≤={r2:.4f}\")\n",
    "    \n",
    "    # Crear DataFrame con m√©tricas\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df\n",
    "\n",
    "@trace(\"Evaluaci√≥n por niveles de elevaci√≥n\")\n",
    "def calculate_metrics_by_elevation(predictions, ground_truth, elevation_masks):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas separadas por nivel de elevaci√≥n para todos los modelos.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Diccionario de predicciones por modelo\n",
    "        ground_truth: Valores reales de precipitaci√≥n\n",
    "        elevation_masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con m√©tricas para cada modelo y nivel de elevaci√≥n\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    metrics_list = []\n",
    "    \n",
    "    logger.info(\"Calculando m√©tricas por nivel de elevaci√≥n...\")\n",
    "    \n",
    "    # Para cada nivel de elevaci√≥n\n",
    "    for level_name, mask in elevation_masks.items():\n",
    "        # M√°scara a √≠ndices\n",
    "        level_indices = np.where(mask)[0]\n",
    "        \n",
    "        # Para cada modelo\n",
    "        for model_name, preds in predictions.items():\n",
    "            # Preparar datos para este nivel\n",
    "            y_true_level = []\n",
    "            y_pred_level = []\n",
    "            \n",
    "            # Recopilar predicciones para todos los timesteps y horizontes\n",
    "            for t in range(ground_truth.shape[0]):\n",
    "                for h in range(ground_truth.shape[1]):\n",
    "                    y_true_level.append(ground_truth[t, h, level_indices])\n",
    "                    y_pred_level.append(preds[t, h, level_indices])\n",
    "            \n",
    "            # Convertir a arrays y aplanar\n",
    "            y_true_level = np.concatenate(y_true_level)\n",
    "            y_pred_level = np.concatenate(y_pred_level)\n",
    "            \n",
    "            # Filtrar NaN si existen\n",
    "            mask_valid = ~np.isnan(y_true_level) & ~np.isnan(y_pred_level)\n",
    "            if np.sum(mask_valid) < len(mask_valid):\n",
    "                logger.warning(f\"Modelo {model_name}, nivel {level_name}: {len(mask_valid) - np.sum(mask_valid)} valores NaN detectados y excluidos\")\n",
    "                y_true_level = y_true_level[mask_valid]\n",
    "                y_pred_level = y_pred_level[mask_valid]\n",
    "            \n",
    "            # Calcular m√©tricas para este nivel\n",
    "            mae = mean_absolute_error(y_true_level, y_pred_level)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true_level, y_pred_level))\n",
    "            r2 = r2_score(y_true_level, y_pred_level)\n",
    "            \n",
    "            # MAPE con manejo de divisiones por cero\n",
    "            mask_nonzero = y_true_level != 0\n",
    "            if np.sum(mask_nonzero) > 0:\n",
    "                mape = 100 * np.mean(np.abs((y_true_level[mask_nonzero] - y_pred_level[mask_nonzero]) / y_true_level[mask_nonzero]))\n",
    "            else:\n",
    "                mape = np.nan\n",
    "            \n",
    "            metrics_list.append({\n",
    "                'Model': model_name,\n",
    "                'Elevation Level': level_name,\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'R¬≤': r2\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"Modelo {model_name}, nivel {level_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, MAPE={mape:.2f}%, R¬≤={r2:.4f}\")\n",
    "    \n",
    "    # Crear DataFrame con m√©tricas\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df\n",
    "\n",
    "@trace(\"Evaluaci√≥n por percentiles\")\n",
    "def calculate_metrics_by_percentiles(predictions, ground_truth, percentiles):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas separadas por rangos de percentiles para todos los modelos.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Diccionario de predicciones por modelo\n",
    "        ground_truth: Valores reales de precipitaci√≥n\n",
    "        percentiles: Lista de percentiles para definir los rangos\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con m√©tricas para cada modelo y rango de percentil\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    metrics_list = []\n",
    "    \n",
    "    logger.info(f\"Calculando m√©tricas por percentiles: {percentiles}\")\n",
    "    \n",
    "    # Calcular umbrales de percentiles en los datos reales\n",
    "    y_true_flat = ground_truth.reshape(-1)\n",
    "    y_true_nonzero = y_true_flat[y_true_flat > 0]  # Solo valores positivos\n",
    "    \n",
    "    if len(y_true_nonzero) == 0:\n",
    "        logger.warning(\"No hay valores positivos para calcular percentiles. Omitiendo c√°lculo por percentiles.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calcular umbrales\n",
    "    thresholds = [np.percentile(y_true_nonzero, p) for p in percentiles]\n",
    "    logger.info(f\"Umbrales de percentiles: {thresholds}\")\n",
    "    \n",
    "    # Para cada rango de percentiles\n",
    "    for i in range(len(percentiles)-1):\n",
    "        lower_pct = percentiles[i]\n",
    "        upper_pct = percentiles[i+1]\n",
    "        lower_val = thresholds[i]\n",
    "        upper_val = thresholds[i+1]\n",
    "        \n",
    "        range_name = f\"P{lower_pct}-P{upper_pct}\"\n",
    "        logger.info(f\"Calculando m√©tricas para rango {range_name}: {lower_val:.4f} - {upper_val:.4f}\")\n",
    "        \n",
    "        # Para cada modelo\n",
    "        for model_name, preds in predictions.items():\n",
    "            # Aplanar arrays\n",
    "            y_true = ground_truth.reshape(-1)\n",
    "            y_pred = preds.reshape(-1)\n",
    "            \n",
    "            # Filtrar por rango de percentiles\n",
    "            if i == len(percentiles)-2:  # √öltimo rango, incluir el valor superior\n",
    "                mask_range = (y_true >= lower_val) & (y_true <= upper_val)\n",
    "            else:\n",
    "                mask_range = (y_true >= lower_val) & (y_true < upper_val)\n",
    "            \n",
    "            # Si no hay datos en este rango, continuar\n",
    "            if np.sum(mask_range) == 0:\n",
    "                logger.warning(f\"No hay datos en rango {range_name} para modelo {model_name}\")\n",
    "                continue\n",
    "                \n",
    "            # Filtrar datos para este rango\n",
    "            y_true_range = y_true[mask_range]\n",
    "            y_pred_range = y_pred[mask_range]\n",
    "            \n",
    "            # Filtrar NaN si existen\n",
    "            mask_valid = ~np.isnan(y_true_range) & ~np.isnan(y_pred_range)\n",
    "            if np.sum(mask_valid) < len(mask_valid):\n",
    "                logger.warning(f\"Modelo {model_name}, rango {range_name}: {len(mask_valid) - np.sum(mask_valid)} valores NaN detectados y excluidos\")\n",
    "                y_true_range = y_true_range[mask_valid]\n",
    "                y_pred_range = y_pred_range[mask_valid]\n",
    "            \n",
    "            # Calcular m√©tricas para este rango\n",
    "            mae = mean_absolute_error(y_true_range, y_pred_range)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true_range, y_pred_range))\n",
    "            r2 = r2_score(y_true_range, y_pred_range)\n",
    "            \n",
    "            # MAPE con manejo de divisiones por cero\n",
    "            mask_nonzero = y_true_range != 0\n",
    "            if np.sum(mask_nonzero) > 0:\n",
    "                mape = 100 * np.mean(np.abs((y_true_range[mask_nonzero] - y_pred_range[mask_nonzero]) / y_true_range[mask_nonzero]))\n",
    "            else:\n",
    "                mape = np.nan\n",
    "            \n",
    "            metrics_list.append({\n",
    "                'Model': model_name,\n",
    "                'Percentile Range': range_name,\n",
    "                'Value Range': f\"{lower_val:.4f} - {upper_val:.4f}\",\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'R¬≤': r2,\n",
    "                'Samples': np.sum(mask_range)\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"Modelo {model_name}, rango {range_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, MAPE={mape:.2f}%, R¬≤={r2:.4f}\")\n",
    "    \n",
    "    # Crear DataFrame con m√©tricas\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df\n",
    "\n",
    "@trace(\"Visualizaci√≥n de mapas de predicci√≥n\")\n",
    "def plot_all_model_maps(predictions, ground_truth, lat, lon, example_idx=0, horizon_idx=0):\n",
    "    \"\"\"\n",
    "    Genera mapas de predicciones para todos los modelos en un horizonte espec√≠fico.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Diccionario de predicciones por modelo\n",
    "        ground_truth: Valores reales de precipitaci√≥n\n",
    "        lat: Latitudes para el mapa\n",
    "        lon: Longitudes para el mapa\n",
    "        example_idx: √çndice del ejemplo a visualizar\n",
    "        horizon_idx: √çndice del horizonte a visualizar\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.feature as cfeature\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    \n",
    "    # Configurar visualizaci√≥n\n",
    "    n_models = len(predictions) + 1  # +1 para ground truth\n",
    "    n_cols = min(3, n_models)\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Crear figura\n",
    "    fig = plt.figure(figsize=(n_cols * 5, n_rows * 4))\n",
    "    \n",
    "    # Encontrar l√≠mites de colorbar consistentes para todos los mapas\n",
    "    vmin = ground_truth[example_idx, horizon_idx].min()\n",
    "    vmax = ground_truth[example_idx, horizon_idx].max()\n",
    "    \n",
    "    # Crear mapa para ground truth primero\n",
    "    ax = plt.subplot(n_rows, n_cols, 1, projection=ccrs.PlateCarree())\n",
    "    \n",
    "    # Reshape de datos para mapeo\n",
    "    ny, nx = len(lat), len(lon)\n",
    "    truth_map = ground_truth[example_idx, horizon_idx].reshape(ny, nx)\n",
    "    \n",
    "    # Configurar colormap para precipitaci√≥n\n",
    "    cmap = plt.cm.YlGnBu\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    # A√±adir caracter√≠sticas del mapa\n",
    "    ax.coastlines(resolution='10m')\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.RIVERS, linestyle='-', alpha=0.5)\n",
    "    \n",
    "    # Plotear datos\n",
    "    im = ax.pcolormesh(lon, lat, truth_map, cmap=cmap, norm=norm, transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # A√±adir colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, axes_class=plt.Axes)\n",
    "    plt.colorbar(im, cax=cax, orientation=\"vertical\", label=\"Precipitation\")\n",
    "    \n",
    "    # T√≠tulo y configuraci√≥n\n",
    "    ax.set_title(f\"Ground Truth (H+{horizon_idx+1})\")\n",
    "    ax.gridlines(draw_labels=True, alpha=0.3)\n",
    "    \n",
    "    # Crear mapas para cada modelo\n",
    "    for i, (model_name, preds) in enumerate(predictions.items(), 2):\n",
    "        ax = plt.subplot(n_rows, n_cols, i, projection=ccrs.PlateCarree())\n",
    "        \n",
    "        # Reshape de datos para mapeo\n",
    "        pred_map = preds[example_idx, horizon_idx].reshape(ny, nx)\n",
    "        \n",
    "        # A√±adir caracter√≠sticas del mapa\n",
    "        ax.coastlines(resolution='10m')\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        ax.add_feature(cfeature.RIVERS, linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Plotear datos\n",
    "        im = ax.pcolormesh(lon, lat, pred_map, cmap=cmap, norm=norm, transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # A√±adir colorbar\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, axes_class=plt.Axes)\n",
    "        plt.colorbar(im, cax=cax, orientation=\"vertical\", label=\"Precipitation\")\n",
    "        \n",
    "        # T√≠tulo y configuraci√≥n\n",
    "        ax.set_title(f\"{model_name} (H+{horizon_idx+1})\")\n",
    "        ax.gridlines(draw_labels=True, alpha=0.3)\n",
    "    \n",
    "    # Ajustar dise√±o y guardar\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{BASE}/models/output/prediction_maps_horizon{horizonte_idx+1}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Mapas de predicci√≥n guardados para horizonte {horizonte_idx+1}\")\n",
    "\n",
    "def visualize_process_tracker_results():\n",
    "    \"\"\"Visualiza los resultados del tracker de proceso\"\"\"\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Crear directorio de salida si no existe\n",
    "    vis_dir = Path(f\"{BASE}/models/output/visualizations\")\n",
    "    vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Visualizar tiempo por secci√≥n\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    section_names = list(tracker.section_times.keys())\n",
    "    section_times = list(tracker.section_times.values())\n",
    "    \n",
    "    # Ordenar por tiempo\n",
    "    indices = np.argsort(section_times)\n",
    "    section_names = [section_names[i] for i in indices]\n",
    "    section_times = [section_times[i] for i in indices]\n",
    "    \n",
    "    sns.barplot(x=section_times, y=section_names)\n",
    "    plt.title('Tiempo de ejecuci√≥n por secci√≥n')\n",
    "    plt.xlabel('Tiempo (segundos)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{vis_dir}/section_times.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Visualizar uso de recursos a lo largo del tiempo\n",
    "    if tracker.resources:\n",
    "        times = [(r['timestamp'] - tracker.start_time) for r in tracker.resources]\n",
    "        mem_pcts = [r['memory_percent'] for r in tracker.resources]\n",
    "        cpu_pcts = [r['cpu_percent'] for r in tracker.resources]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(times, mem_pcts, label='Memoria (%)')\n",
    "        plt.plot(times, cpu_pcts, label='CPU (%)')\n",
    "        plt.axhline(y=90, color='r', linestyle='--', alpha=0.7, label='L√≠mite cr√≠tico (90%)')\n",
    "        \n",
    "        # A√±adir marcas de checkpoints\n",
    "        for cp in tracker.checkpoints:\n",
    "            plt.axvline(x=cp['elapsed_total'], color='g', alpha=0.5, linestyle='-.')\n",
    "        \n",
    "        plt.title('Uso de recursos durante la ejecuci√≥n')\n",
    "        plt.xlabel('Tiempo (segundos)')\n",
    "        plt.ylabel('Uso (%)')\n",
    "        plt.ylim(0, 100)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f\"{vis_dir}/resource_usage.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    logger.info(f\"Visualizaciones del proceso guardadas en {vis_dir}\")\n",
    "\n",
    "def display_log_summary():\n",
    "    \"\"\"Muestra un resumen del archivo de log\"\"\"\n",
    "    log_file = LOG_DIR / log_filename\n",
    "    if not log_file.exists():\n",
    "        logger.warning(f\"No se encontr√≥ el archivo de log: {log_file}\")\n",
    "        return\n",
    "    \n",
    "    # Leer √∫ltimas l√≠neas\n",
    "    try:\n",
    "        with open(log_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        # Mostrar stats b√°sicos\n",
    "        total_lines = len(lines)\n",
    "        errors = sum(1 for line in lines if \" ERROR \" in line)\n",
    "        warnings = sum(1 for line in lines if \" WARNING \" in line)\n",
    "        infos = sum(1 for line in lines if \" INFO \" in line)\n",
    "        \n",
    "        print(f\"\\nüìã Resumen del log ({log_file.name}):\")\n",
    "        print(f\"  Total l√≠neas: {total_lines}\")\n",
    "        print(f\"  Informaci√≥n: {infos}\")\n",
    "        print(f\"  Advertencias: {warnings}\")\n",
    "        print(f\"  Errores: {errors}\")\n",
    "        \n",
    "        # Mostrar √∫ltimos errores\n",
    "        if errors > 0:\n",
    "            print(\"\\n‚ö†Ô∏è √öltimos errores:\")\n",
    "            error_lines = [line.strip() for line in lines if \" ERROR \" in line]\n",
    "            for line in error_lines[-min(5, len(error_lines)):]:\n",
    "                print(f\"  {line}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error procesando archivo de log: {str(e)}\")\n",
    "        \n",
    "@trace(\"Generaci√≥n de fusi√≥n optimizada\")\n",
    "def generate_optimized_fusion(ceemdan_data, tvfemd_data, fusion_weights, elevation_masks):\n",
    "    \"\"\"\n",
    "    Genera features de fusi√≥n optimizadas basadas en los pesos de importancia aprendidos\n",
    "    \n",
    "    Args:\n",
    "        ceemdan_data: Array de caracter√≠sticas CEEMDAN (T, ny, nx, 3)\n",
    "        tvfemd_data: Array de caracter√≠sticas TFV-EMD (T, ny, nx, 3)\n",
    "        fusion_weights: Diccionario de pesos por nivel y componente\n",
    "        elevation_masks: Diccionario de m√°scaras por nivel de elevaci√≥n\n",
    "        \n",
    "    Returns:\n",
    "        Array de fusi√≥n optimizada (T, ny, nx, 3)\n",
    "    \"\"\"\n",
    "    T, ny, nx, n_components = ceemdan_data.shape\n",
    "    logger.info(f\"Generando fusi√≥n optimizada: shape={T}√ó{ny}√ó{nx}√ó{n_components}\")\n",
    "    \n",
    "    # Inicializar array para fusi√≥n optimizada\n",
    "    fusion_optimized = np.zeros_like(ceemdan_data)\n",
    "    \n",
    "    # Para cada nivel de elevaci√≥n\n",
    "    for level_name, mask in elevation_masks.items():\n",
    "        level_3d_mask = np.zeros((ny, nx), dtype=bool)\n",
    "        level_3d_mask = mask.reshape(ny, nx)\n",
    "        \n",
    "        # Para cada componente\n",
    "        for comp_idx in range(n_components):\n",
    "            # Obtener pesos para este nivel y componente\n",
    "            if level_name in fusion_weights and fusion_weights[level_name][component_idx] is not None:\n",
    "                ceemdan_weight, tvfemd_weight = fusion_weights[level_name][component_idx]\n",
    "            else:\n",
    "                logger.warning(f\"No hay pesos para {level_name}, componente {component_idx}. Usando pesos uniformes.\")\n",
    "                ceemdan_weight = tvfemd_weight = 0.5\n",
    "                \n",
    "            # Aplicar fusi√≥n ponderada para este nivel y componente\n",
    "            for t in range(T):\n",
    "                # Extraer solo las celdas para este nivel\n",
    "                fusion_optimized[t, level_3d_mask, comp_idx] = (\n",
    "                    ceemdan_weight * ceemdan_data[t, level_3d_mask, comp_idx] +\n",
    "                    tvfemd_weight * tvfemd_data[t, level_3d_mask, comp_idx]\n",
    "                )\n",
    "    \n",
    "    logger.info(f\"Fusi√≥n optimizada generada. Shape: {fusion_optimized.shape}\")\n",
    "    return fusion_optimized\n",
    "\n",
    "@trace(\"Entrenamiento de modelo por elevaci√≥n\")\n",
    "def train_elevation_model(level_name, mask, X_tr, Y_tr, X_va, Y_va, model_path, history_path, force_retrain=False):\n",
    "    \"\"\"\n",
    "    Entrena un modelo BiGRU para un nivel de elevaci√≥n espec√≠fico o carga uno existente\n",
    "    \n",
    "    Args:\n",
    "        level_name: Nombre del nivel de elevaci√≥n\n",
    "        mask: M√°scara para seleccionar celdas del nivel\n",
    "        X_tr: Datos de entrenamiento completos\n",
    "        Y_tr: Etiquetas de entrenamiento completas\n",
    "        X_va: Datos de validaci√≥n completos\n",
    "        Y_va: Etiquetas de validaci√≥n completas\n",
    "        model_path: Ruta para guardar/cargar el modelo\n",
    "        history_path: Ruta para guardar/cargar el historial\n",
    "        force_retrain: Si es True, se reentrenar√° aunque exista modelo\n",
    "        \n",
    "    Returns:\n",
    "        Tuple: (modelo, historial, predicciones)\n",
    "    \"\"\"\n",
    "    # Comprobar si el modelo ya existe\n",
    "    if os.path.exists(model_path) and not force_retrain:\n",
    "        logger.info(f\"Cargando modelo BiGRU existente para nivel {level_name}...\")\n",
    "        try:\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "            \n",
    "            # Intentar cargar historial\n",
    "            if os.path.exists(history_path):\n",
    "                history_data = np.load(history_path, allow_pickle=True)\n",
    "                history = history_data['history'].item()\n",
    "                logger.info(f\"Historial cargado para nivel {level_name}\")\n",
    "            else:\n",
    "                # Crear diccionario vac√≠o si no hay historial\n",
    "                history = {'loss': [], 'val_loss': []}\n",
    "                \n",
    "            # Generar predicciones\n",
    "            logger.info(f\"Generando predicciones para nivel {level_name} con modelo cargado\")\n",
    "            cells_in_level = np.sum(mask)\n",
    "            Y_pred = predict_in_batches(model, X_va)\n",
    "            \n",
    "            # Reorganizar predicciones\n",
    "            Y_pred_level = np.zeros((len(X_va), OUTPUT_HORIZON, cells_in_level))\n",
    "            for i in range(len(X_va)):\n",
    "                for h in range(OUTPUT_HORIZON):\n",
    "                    Y_pred_level[i, h] = Y_pred[i, h*cells_in_level:(h+1)*cells_in_level]\n",
    "            \n",
    "            logger.info(f\"Modelo {level_name} cargado y predicciones generadas: {Y_pred_level.shape}\")\n",
    "            tracker.log_metric(level_name, \"loaded_model\", True)\n",
    "            \n",
    "            return model, history, Y_pred_level\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cargando modelo {level_name}: {str(e)}\")\n",
    "            logger.info(f\"Entrenando nuevo modelo para {level_name}...\")\n",
    "    else:\n",
    "        logger.info(f\"{'Forzando reentrenamiento' if force_retrain else 'No existe modelo'} para nivel {level_name}. Entrenando...\")\n",
    "    \n",
    "    # Preparar datos espec√≠ficos para este nivel\n",
    "    cells_in_level = np.sum(mask)\n",
    "    logger.info(f\"Nivel {level_name}: {cells_in_level} celdas, {mask.shape}\")\n",
    "    \n",
    "    # Extraer solo las columnas relevantes para este nivel\n",
    "    X_tr_level = extract_level_features(X_tr, mask)\n",
    "    Y_tr_level = extract_level_targets(Y_tr, mask)\n",
    "    X_va_level = extract_level_features(X_va, mask)\n",
    "    Y_va_level = extract_level_targets(Y_va, mask)\n",
    "    \n",
    "    logger.info(f\"Shapes para {level_name}: X_tr={X_tr_level.shape}, Y_tr={Y_tr_level.shape}\")\n",
    "    \n",
    "    # Construir y entrenar modelo BiGRU\n",
    "    input_dim = X_tr_level.shape[-1]\n",
    "    output_length = OUTPUT_HORIZON\n",
    "    output_dim = cells_in_level\n",
    "    \n",
    "    # Configurar memoria limitada para evitar OOM\n",
    "    try:\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logger.info(\"Configuraci√≥n GPU aplicada para entrenamiento\")\n",
    "    except:\n",
    "        logger.warning(\"No se pudo aplicar configuraci√≥n espec√≠fica de GPU\")\n",
    "    \n",
    "    # Crear modelo BiGRU\n",
    "    model = create_bigru_model(\n",
    "        input_shape=(INPUT_WINDOW, input_dim),\n",
    "        output_length=output_length,\n",
    "        output_dim=output_dim,\n",
    "        level_name=level_name\n",
    "    )\n",
    "    \n",
    "    # Early stopping para prevenir overfitting\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Crear datasets eficientes\n",
    "    batch_size = min(32, len(X_tr_level) // 10 + 1)  # Batch adaptativo\n",
    "    train_dataset = create_tf_dataset(X_tr_level, Y_tr_level, batch_size)\n",
    "    val_dataset = create_tf_dataset(X_va_level, Y_va_level, batch_size)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    logger.info(f\"Entrenando modelo BiGRU para nivel {level_name}...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=100,  # N√∫mero m√°ximo de epochs\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    ).history\n",
    "    \n",
    "    # Guardar modelo e historial\n",
    "    model.save(model_path)\n",
    "    np.savez_compressed(history_path, history=history)\n",
    "    logger.info(f\"Modelo {level_name} guardado en {model_path}\")\n",
    "    \n",
    "    # Generar predicciones\n",
    "    logger.info(f\"Generando predicciones para nivel {level_name}\")\n",
    "    Y_pred = predict_in_batches(model, X_va_level)\n",
    "    \n",
    "    # Registrar m√©tricas\n",
    "    val_mask = ~np.isnan(Y_va_level) & ~np.isnan(Y_pred)\n",
    "    if np.all(val_mask):\n",
    "        rmse = np.sqrt(mean_squared_error(Y_va_level, Y_pred))\n",
    "        mae = mean_absolute_error(Y_va_level, Y_pred)\n",
    "        r2 = r2_score(Y_va_level.reshape(-1), Y_pred.reshape(-1))\n",
    "        \n",
    "        tracker.log_metric(level_name, \"rmse\", rmse)\n",
    "        tracker.log_metric(level_name, \"mae\", mae)\n",
    "        tracker.log_metric(level_name, \"r2\", r2)\n",
    "        logger.info(f\"Modelo {level_name}: RMSE={rmse:.4f}, MAE={mae:.4f}, R¬≤={r2:.4f}\")\n",
    "    \n",
    "    return model, history, Y_pred\n",
    "\n",
    "def extract_level_features(X, mask):\n",
    "    \"\"\"\n",
    "    Extrae caracter√≠sticas espec√≠ficas para un nivel de elevaci√≥n\n",
    "    \n",
    "    Args:\n",
    "        X: Array de caracter√≠sticas completo (samples, time_steps, features)\n",
    "        mask: M√°scara del nivel\n",
    "    \n",
    "    Returns:\n",
    "        Array con caracter√≠sticas para el nivel espec√≠fico\n",
    "    \"\"\"\n",
    "    # Extraer solo columnas para este nivel\n",
    "    n_samples = X.shape[0]\n",
    "    time_steps = X.shape[1]\n",
    "    cells_per_component = X.shape[2] // 3\n",
    "    cells_in_level = np.sum(mask)\n",
    "    \n",
    "    # Inicializar array para caracter√≠sticas del nivel\n",
    "    X_level = np.zeros((n_samples, time_steps, cells_in_level * 3))\n",
    "    \n",
    "    # Para cada componente\n",
    "    for comp in range(3):\n",
    "        # √çndices de inicio y fin para este componente en array original\n",
    "        start_idx = comp * cells_per_component\n",
    "        end_idx = (comp + 1) * cells_per_component\n",
    "        \n",
    "        # Extraer solo columnas para celdas en esta elevaci√≥n\n",
    "        comp_features = X[:, :, start_idx:end_idx]\n",
    "        comp_features_level = comp_features[:, :, mask]\n",
    "        \n",
    "        # Colocar en array de salida\n",
    "        level_start = comp * cells_in_level\n",
    "        level_end = (comp + 1) * cells_in_level\n",
    "        X_level[:, :, level_start:level_end] = comp_features_level\n",
    "    \n",
    "    return X_level\n",
    "\n",
    "def extract_level_targets(Y, mask):\n",
    "    \"\"\"\n",
    "    Extrae objetivos espec√≠ficos para un nivel de elevaci√≥n\n",
    "    \n",
    "    Args:\n",
    "        Y: Array de objetivos completo (samples, horizons, cells)\n",
    "        mask: M√°scara del nivel\n",
    "    \n",
    "    Returns:\n",
    "        Array con objetivos para el nivel espec√≠fico\n",
    "    \"\"\"\n",
    "    n_samples = Y.shape[0]\n",
    "    horizons = Y.shape[1]\n",
    "    cells_in_level = np.sum(mask)\n",
    "    \n",
    "    # Inicializar array para objetivos del nivel\n",
    "    Y_level = np.zeros((n_samples, horizons * cells_in_level))\n",
    "    \n",
    "    # Para cada horizonte\n",
    "    for h in range(horizons):\n",
    "        # Extraer datos para este horizonte\n",
    "        horizon_data = Y[:, h, :]\n",
    "        \n",
    "        # Extraer solo celdas para este nivel\n",
    "        horizon_level_data = horizon_data[:, mask]\n",
    "        \n",
    "        # Colocar en array de salida\n",
    "        start_idx = h * cells_in_level\n",
    "        end_idx = (h + 1) * cells_in_level\n",
    "        Y_level[:, start_idx:end_idx] = horizon_level_data\n",
    "    \n",
    "    return Y_level\n",
    "\n",
    "def create_bigru_model(input_shape, output_length, output_dim, level_name):\n",
    "    \"\"\"\n",
    "    Crea un modelo BiGRU con estructura de autoencoder-decoder\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Tupla con forma de entrada (timesteps, features)\n",
    "        output_length: N√∫mero de pasos de salida\n",
    "        output_dim: Dimensi√≥n de la salida (n√∫mero de celdas en el nivel)\n",
    "        level_name: Nombre del nivel de elevaci√≥n (para trazabilidad)\n",
    "    \n",
    "    Returns:\n",
    "        model: Modelo BiGRU compilado\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Bidirectional, GRU, TimeDistributed, Dense, Dropout, BatchNormalization\n",
    "    \n",
    "    model = Sequential(name=f\"BiGRU_{level_name}\")\n",
    "    \n",
    "    # Capa de entrada con ajuste de forma\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    # Encoder: BiGRU\n",
    "    model.add(Bidirectional(GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Bidirectional(GRU(32, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    \n",
    "    # Bottleneck: capa densa para compresi√≥n\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    \n",
    "    # Decoder: GRU unidireccional\n",
    "    model.add(RepeatVector(output_length))\n",
    "    model.add(GRU(32, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(TimeDistributed(Dense(output_dim)))\n",
    "    \n",
    "    # Compilaci√≥n del modelo\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    model.summary(print_fn=logger.info)  # Usar logger para imprimir resumen\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Funci√≥n principal para el proceso completo\n",
    "@trace(\"Proceso completo\")\n",
    "def main(force_retrain=False):\n",
    "    \"\"\"\n",
    "    Proceso completo de entrenamiento y evaluaci√≥n de TopoRain-NET\n",
    "    \n",
    "    Args:\n",
    "        force_retrain: Si es True, fuerza el reentrenamiento de todos los modelos\n",
    "        \n",
    "    Returns:\n",
    "        summary: Resumen final del proceso\n",
    "    \"\"\"\n",
    "    # 1. Preparaci√≥n de datos\n",
    "    tracker.start_section(\"Preparaci√≥n de datos\")\n",
    "    \n",
    "    # Ejecutar optimizaci√≥n de fusi√≥n para obtener pesos y modelos\n",
    "    logger.info(\"Optimizando fusi√≥n de CEEMDAN y TFV-EMD con XGBoost...\")\n",
    "    fusion_models, fusion_weights = optimize_fusion_with_xgboost(\n",
    "        ceemdan_data=da_ceemdan, \n",
    "        tvfemd_data=da_tvfemd, \n",
    "        target_data=prec, \n",
    "        masks=elevation_masks,\n",
    "        test_size=0.2,\n",
    "        force_retrain=force_retrain\n",
    "    )\n",
    "    \n",
    "    # Generar la fusi√≥n optimizada con los pesos aprendidos\n",
    "    logger.info(\"Generando fusi√≥n optimizada con pesos aprendidos...\")\n",
    "    da_fusion_optimized = generate_optimized_fusion(\n",
    "        ceemdan_data=da_ceemdan,\n",
    "        tvfemd_data=da_tvfemd,\n",
    "        fusion_weights=fusion_weights,\n",
    "        elevation_masks=elevation_masks\n",
    "    )\n",
    "    \n",
    "    # Registrar informaci√≥n sobre la fusi√≥n optimizada\n",
    "    tracker.add_checkpoint(\"Fusi√≥n optimizada generada\", {\n",
    "        \"shape\": da_fusion_optimized.shape,\n",
    "        \"min\": float(np.nanmin(da_fusion_optimized)),\n",
    "        \"max\": float(np.nanmax(da_fusion_optimized)),\n",
    "        \"nan_count\": int(np.isnan(da_fusion_optimized).sum())\n",
    "    })\n",
    "    \n",
    "    windows_prep_start = time.time()\n",
    "    \n",
    "    # Preparar datos de entrada y salida con ventanas deslizantes\n",
    "    logger.info(f\"Creando ventanas deslizantes: INPUT_WINDOW={INPUT_WINDOW}, OUTPUT_HORIZON={OUTPUT_HORIZON}\")\n",
    "    \n",
    "    # Obtener dimensiones\n",
    "    T, ny, nx, _ = da_fusion_optimized.shape\n",
    "    cells = ny * nx\n",
    "    \n",
    "    # Inicializar arrays para ventanas X (entrada) e Y (salida)\n",
    "    N = T - INPUT_WINDOW - OUTPUT_HORIZON + 1  # N√∫mero de ventanas v√°lidas\n",
    "    \n",
    "    # Inicializar arrays\n",
    "    X = np.zeros((N, INPUT_WINDOW, cells * 3))  # 3 componentes de fusi√≥n por celda\n",
    "    Y = np.zeros((N, OUTPUT_HORIZON, cells))\n",
    "    \n",
    "    # Para cada muestra\n",
    "    for i in range(N):\n",
    "        # Ventana de entrada: caracter√≠sticas de fusi√≥n optimizada\n",
    "        for t in range(INPUT_WINDOW):\n",
    "            # Reorganizar los datos: (T, ny, nx, 3) -> (T, INPUT_WINDOW, cells*3)\n",
    "            X[i, t, :] = da_fusion_optimized[i+t].reshape(-1)\n",
    "        \n",
    "        # Ventana de salida: precipitaci√≥n\n",
    "        for h in range(OUTPUT_HORIZON):\n",
    "            # Datos objetivo: (T, ny, nx) -> (T, OUTPUT_HORIZON, cells)\n",
    "            Y[i, h, :] = prec[i+INPUT_WINDOW+h].reshape(-1)\n",
    "    \n",
    "    windows_prep_time = time.time() - windows_prep_start\n",
    "    logger.info(f\"Preparaci√≥n de ventanas completada en {windows_prep_time:.2f} segundos\")\n",
    "    logger.info(f\"Ventanas v√°lidas totales: {N}\")\n",
    "    \n",
    "    # Escalado de features\n",
    "    scale_start = time.time()\n",
    "    logger.info(\"Escalado de features...\")\n",
    "    scX = StandardScaler()\n",
    "    Xf = scX.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "    scale_time = time.time() - scale_start\n",
    "    \n",
    "    # Train/val split\n",
    "    split = int(0.7*N)\n",
    "    X_tr = Xf[:split]\n",
    "    X_va = Xf[split:]\n",
    "    Y_tr = Y[:split]\n",
    "    Y_va = Y[split:]\n",
    "    logger.info(f\"Split train={len(X_tr)}, val={len(X_va)}\")\n",
    "    \n",
    "    tracker.end_section()\n",
    "    tracker.add_checkpoint(\"Ventanas y split preparados\", {\n",
    "        \"total_windows\": N,\n",
    "        \"train_size\": len(X_tr),\n",
    "        \"val_size\": len(X_va),\n",
    "        \"window_prep_time\": windows_prep_time\n",
    "    })\n",
    "    \n",
    "    # 4. Entrenar modelos por nivel de elevaci√≥n\n",
    "    tracker.start_section(\"Entrenamiento de modelos por nivel\")\n",
    "    elevation_models = {}\n",
    "    elevation_histories = {}\n",
    "    elevation_predictions = {}\n",
    "\n",
    "    for level_name, mask in elevation_masks.items():\n",
    "        model_path = TRAINED_DIR / f\"BiGRU_{level_name}_model.keras\"\n",
    "        history_path = HISTORY_DIR / f\"{level_name}_history.npz\"\n",
    "        \n",
    "        # Entrenar o cargar modelo para este nivel, respetando force_retrain\n",
    "        model, history, Y_pred_level = train_elevation_model(\n",
    "            level_name, mask, X_tr, Y_tr, X_va, Y_va, model_path, history_path, force_retrain\n",
    "        )\n",
    "        \n",
    "        # Almacenar resultados\n",
    "        elevation_models[level_name] = model\n",
    "        elevation_histories[level_name] = history\n",
    "        elevation_predictions[level_name] = Y_pred_level\n",
    "    \n",
    "    tracker.end_section()\n",
    "    tracker.add_checkpoint(\"Modelos por nivel entrenados\", {\n",
    "        \"num_models\": len(elevation_models),\n",
    "        \"models\": list(elevation_models.keys())\n",
    "    })\n",
    "    \n",
    "    # 5. Meta-modelo de fusi√≥n\n",
    "    tracker.start_section(\"Meta-modelo de fusi√≥n\")\n",
    "    \n",
    "    # Cargar o entrenar meta-modelo\n",
    "    meta_model_path = TRAINED_DIR / \"meta_fusion_model.pkl\"\n",
    "    meta_preds_path = PRED_DIR / \"meta_fusion_preds.npz\"\n",
    "\n",
    "    # Inicializar la variable Y_meta_va\n",
    "    Y_meta_va = None\n",
    "    meta_model_loaded = False\n",
    "\n",
    "    # Intentar cargar meta-modelo y predicciones desde disco si no se fuerza reentrenamiento\n",
    "    if os.path.exists(meta_model_path) and os.path.exists(meta_preds_path) and not force_retrain:\n",
    "        logger.info(\"Cargando meta-modelo y predicciones existentes...\")\n",
    "        load_start = time.time()\n",
    "        try:\n",
    "            meta_model, meta_info = load_model('meta', 'all')\n",
    "            \n",
    "            # Cargar predicciones\n",
    "            meta_preds_data = np.load(meta_preds_path)\n",
    "            Y_meta_va = meta_preds_data['predictions']\n",
    "            \n",
    "            load_time = time.time() - load_start\n",
    "            logger.info(f\"Meta-modelo y predicciones cargados correctamente en {load_time:.2f} segundos\")\n",
    "            \n",
    "            tracker.log_metric(\"meta_modelo\", \"loaded\", True)\n",
    "            tracker.log_metric(\"meta_modelo\", \"load_time\", load_time)\n",
    "            meta_model_loaded = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cargando meta-modelo o predicciones: {str(e)}\")\n",
    "            logger.warning(\"Entrenando nuevo meta-modelo...\")\n",
    "            meta_model_loaded = False\n",
    "    \n",
    "    @trace(\"Construcci√≥n de meta-modelo\")\n",
    "    def build_meta_fusion_model(base_preds, Y_true):\n",
    "        \"\"\"\n",
    "        Construye un meta-modelo que fusiona las predicciones de los modelos base\n",
    "        \n",
    "        Args:\n",
    "            base_preds: Diccionario {nombre_modelo: predicciones}\n",
    "                        donde predicciones tiene forma (samples, horizons, cells)\n",
    "            Y_true: Valores reales para entrenamiento (samples, horizons, cells)\n",
    "            \n",
    "        Returns:\n",
    "            meta_model: Modelo entrenado\n",
    "            X_meta: Caracter√≠sticas de entrada para meta-modelo\n",
    "            Y_meta_pred: Predicciones del meta-modelo\n",
    "        \"\"\"\n",
    "        logger.info(f\"Construyendo meta-modelo con {len(base_preds)} modelos base\")\n",
    "        n_samples = next(iter(base_preds.values())).shape[0]\n",
    "        n_horizons = next(iter(base_preds.values())).shape[1]\n",
    "        n_cells = next(iter(base_preds.values())).shape[2]\n",
    "        \n",
    "        # Crear meta-modelo para cada horizonte de predicci√≥n\n",
    "        meta_models = []\n",
    "        \n",
    "        # Preparar datos para meta-modelo\n",
    "        X_meta = []\n",
    "        for h in range(n_horizons):\n",
    "            # Para cada horizonte, concatenar predicciones de todos los modelos\n",
    "            X_h = []\n",
    "            for model_name, preds in base_preds.items():\n",
    "                X_h.append(preds[:, h, :])\n",
    "            \n",
    "            # Concatenar todas las predicciones para este horizonte\n",
    "            if X_h:\n",
    "                X_meta.append(np.hstack(X_h))\n",
    "        \n",
    "        # Entrenamiento de un meta-modelo por horizonte\n",
    "        logger.info(f\"Entrenando {n_horizons} meta-modelos XGBoost para fusi√≥n\")\n",
    "        \n",
    "        # Par√°metros para modelos meta-XGBoost\n",
    "        xgb_params = {\n",
    "            'tree_method': 'hist',\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'n_jobs': min(4, os.cpu_count() - 1)\n",
    "        }\n",
    "        \n",
    "        # Entrenar modelos por horizontes (paralelizable)\n",
    "        for h in range(n_horizons):\n",
    "            logger.info(f\"  Entrenando meta-modelo para horizonte {h+1}/{n_horizons}\")\n",
    "            \n",
    "            # Extraer datos para este horizonte\n",
    "            X_h = X_meta[h]\n",
    "            Y_h = Y_true[:, h, :].reshape(n_samples, -1)\n",
    "            \n",
    "            # Verificar datos\n",
    "            if np.isnan(X_h).any() or np.isnan(Y_h).any():\n",
    "                logger.warning(f\"Detectados NaN en los datos para horizonte {h}. Realizando imputaci√≥n.\")\n",
    "                X_h = np.nan_to_num(X_h, nan=0.0)\n",
    "                Y_h = np.nan_to_num(Y_h, nan=0.0)\n",
    "            \n",
    "            # Train-test split para este horizonte\n",
    "            X_h_train, X_h_test, Y_h_train, Y_h_test = train_test_split(X_h, Y_h, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # Entrenar modelo XGBoost para este horizonte\n",
    "            model_h = train_xgb_with_memory_optimization(X_h_train, Y_h_train, X_h_test, Y_h_test, xgb_params)\n",
    "            meta_models.append(model_h)\n",
    "            \n",
    "            # Evaluar modelo\n",
    "            y_h_pred = predict_xgb_in_batches(model_h, X_h_test)\n",
    "            rmse = np.sqrt(mean_squared_error(Y_h_test.ravel(), y_h_pred))\n",
    "            r2 = r2_score(Y_h_test.ravel(), y_h_pred)\n",
    "            logger.info(f\"  Meta-modelo horizonte {h+1}: RMSE={rmse:.4f}, R¬≤={r2:.4f}\")\n",
    "            \n",
    "            # Registrar m√©tricas\n",
    "            tracker.log_metric(f\"meta_modelo_h{h+1}\", \"rmse\", rmse)\n",
    "            tracker.log_metric(f\"meta_modelo_h{h+1}\", \"r2\", r2)\n",
    "        \n",
    "        # Generar predicciones meta-modelo\n",
    "        Y_meta_pred = np.zeros((n_samples, n_horizons, n_cells))\n",
    "        \n",
    "        logger.info(\"Generando predicciones del meta-modelo...\")\n",
    "        for h in range(n_horizons):\n",
    "            Y_meta_pred[:, h, :] = predict_xgb_in_batches(meta_models[h], X_meta[h]).reshape(n_samples, n_cells)\n",
    "        \n",
    "        # Guardar meta-modelo y predicciones\n",
    "        save_start = time.time()\n",
    "        \n",
    "        # Guardar modelo y predicciones\n",
    "        meta_info = {\n",
    "            'training_date': datetime.datetime.now().strftime(timestamp_format),\n",
    "            'base_models': list(base_preds.keys()),\n",
    "            'horizons': OUTPUT_HORIZON,\n",
    "            'input_shape': np.array(X_meta).shape\n",
    "        }\n",
    "        save_model(meta_models, 'meta', 'all', extra_info=meta_info)\n",
    "        \n",
    "        # Guardar predicciones\n",
    "        np.savez_compressed(meta_preds_path, predictions=Y_meta_pred)\n",
    "        save_time = time.time() - save_start\n",
    "        logger.info(f\"Meta-modelo y predicciones guardados en {save_time:.2f} segundos\")\n",
    "        \n",
    "        return meta_models, X_meta, Y_meta_pred\n",
    "    \n",
    "    # Si no se carg√≥ correctamente o se fuerza reentrenamiento, entrenar nuevo modelo\n",
    "    if not meta_model_loaded or force_retrain:\n",
    "        logger.info(\"Entrenando nuevo meta-modelo...\")\n",
    "        # Construir predicciones de modelos base para entrenamiento\n",
    "        base_preds = {}\n",
    "        for level_name, mask in elevation_masks.items():\n",
    "            if level_name in elevation_predictions:\n",
    "                # Reconstruir predicciones completas\n",
    "                preds = elevation_predictions[level_name]\n",
    "                complete_preds = np.zeros((len(X_va), OUTPUT_HORIZON, cells))\n",
    "                \n",
    "                for i in range(len(X_va)):\n",
    "                    for h in range(OUTPUT_HORIZON):\n",
    "                        complete_preds[i, h, mask] = preds[i, h]\n",
    "                \n",
    "                base_preds[f\"BiGRU_{level_name}\"] = complete_preds\n",
    "            \n",
    "        # Entrenar meta-modelo con predicciones y valores reales\n",
    "        if base_preds:\n",
    "            meta_model, X_meta, Y_meta_va = build_meta_fusion_model(base_preds, Y_va)\n",
    "        else:\n",
    "            logger.warning(\"No hay suficientes modelos base para entrenar meta-modelo\")\n",
    "            meta_model = None\n",
    "            \n",
    "    # 6. Evaluaci√≥n completa\n",
    "    tracker.start_section(\"Evaluaci√≥n de modelos\")\n",
    "    logger.info(\"Ejecutando evaluaci√≥n completa...\")\n",
    "    \n",
    "    # Reconstruir predicciones completas para cada nivel\n",
    "    elevation_preds_complete = {}\n",
    "    \n",
    "    for level_name, mask in elevation_masks.items():\n",
    "        if level_name not in elevation_predictions:\n",
    "            logger.warning(f\"No hay predicciones para el nivel {level_name}. Omitiendo.\")\n",
    "            continue\n",
    "            \n",
    "        preds = elevation_predictions[level_name]\n",
    "        complete_preds = np.zeros((len(X_va), OUTPUT_HORIZON, cells))\n",
    "        \n",
    "        for i in range(len(X_va)):\n",
    "            for h in range(OUTPUT_HORIZON):\n",
    "                complete_preds[i, h, mask] = preds[i, h]\n",
    "        \n",
    "        elevation_preds_complete[f\"BiGRU-{level_name}\"] = complete_preds\n",
    "    \n",
    "    # A√±adir meta-modelo a las predicciones\n",
    "    all_predictions = elevation_preds_complete.copy()\n",
    "    \n",
    "    if Y_meta_va is not None:\n",
    "        all_predictions[\"Meta-Fusion\"] = Y_meta_va\n",
    "    \n",
    "    # Verificar que hay predicciones para evaluar\n",
    "    if not all_predictions:\n",
    "        logger.error(\"No hay predicciones disponibles para evaluar. Abortando.\")\n",
    "    else:\n",
    "        # M√©tricas globales\n",
    "        metrics_global = calculate_global_metrics(all_predictions, Y_va)\n",
    "        metrics_global.to_csv(f\"{BASE}/models/output/elevation_models_metrics.csv\", index=False)\n",
    "        \n",
    "        # Registrar m√©tricas globales para trazabilidad\n",
    "        for _, row in metrics_global.iterrows():\n",
    "            model_name = row['Model']\n",
    "            for metric in ['MAE', 'RMSE', 'MAPE', 'R¬≤']:\n",
    "                if metric in row:\n",
    "                    tracker.log_metric(f\"global_{model_name}\", metric.lower(), row[metric])\n",
    "        \n",
    "        # M√©tricas por niveles de elevaci√≥n\n",
    "        metrics_elevation = calculate_metrics_by_elevation(all_predictions, Y_va, elevation_masks)\n",
    "        metrics_elevation.to_csv(f\"{BASE}/models/output/metrics_by_elevation_detailed.csv\", index=False)\n",
    "        \n",
    "        # M√©tricas por percentiles\n",
    "        percentiles = [0, 50, 90, 95, 99]\n",
    "        metrics_percentile = calculate_metrics_by_percentiles(all_predictions, Y_va, percentiles)\n",
    "        metrics_percentile.to_csv(f\"{BASE}/models/output/metrics_by_percentile.csv\", index=False)\n",
    "        \n",
    "        # Generar visualizaciones\n",
    "        logger.info(\"Generando visualizaciones...\")\n",
    "        \n",
    "        # Mapas de predicci√≥n para cada horizonte\n",
    "        for h in range(OUTPUT_HORIZON):\n",
    "            plot_all_model_maps(all_predictions, Y_va, lat, lon, example_idx=0, horizon_idx=h)\n",
    "        \n",
    "        # Scatter plots\n",
    "        compare_models_scatter(all_predictions, Y_va)\n",
    "        \n",
    "        # Gr√°ficos de barras de m√©tricas\n",
    "        for metric in ['MAE', 'RMSE', 'MAPE']:\n",
    "            plot_metrics_comparison(metrics_global, metric=metric)\n",
    "            plot_metrics_by_elevation(metrics_elevation, metric=metric)\n",
    "            plot_metrics_by_percentiles(metrics_percentile, metric=metric)\n",
    "    \n",
    "    tracker.end_section()\n",
    "    \n",
    "    # Generar resumen final del proceso\n",
    "    summary = tracker.summary()\n",
    "    \n",
    "    return summary\n",
    "# Asegurarnos de que todos los directorios necesarios existen\n",
    "for directory in [LOG_DIR, TRAINED_DIR, PRED_DIR, HISTORY_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Directorio asegurado: {directory}\")\n",
    "\n",
    "# Ejecutar el proceso completo con manejo de excepciones y trazabilidad mejorada\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Analizar argumentos de l√≠nea de comandos (si los hay)\n",
    "        import argparse\n",
    "        parser = argparse.ArgumentParser(description=\"Entrenamiento de TopoRain-NET\")\n",
    "        parser.add_argument('--force-retrain', action='store_true', \n",
    "                          help='Forzar reentrenamiento de todos los modelos')\n",
    "        \n",
    "        # En notebooks podemos capturar los argumentos si se ejecuta como script\n",
    "        try:\n",
    "            args = parser.parse_args()\n",
    "            force_retrain = args.force_retrain\n",
    "        except:\n",
    "            # Si falla (por ejemplo, en ejecuci√≥n de notebook interactivo)\n",
    "            force_retrain = False\n",
    "        \n",
    "        logger.info(f\"üöÄ Iniciando proceso TopoRain-NET{'(forzando reentrenamiento)' if force_retrain else ''}\")\n",
    "        process_summary = main(force_retrain=force_retrain)\n",
    "        logger.info(\"‚úÖ Proceso TopoRain-NET completado exitosamente\")\n",
    "        # Visualizar resultados de trazabilidad\n",
    "        visualize_process_tracker_results()\n",
    "        # Mostrar resumen del log\n",
    "        display_log_summary()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error en el proceso principal: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        try:\n",
    "            tracker.summary()  # Generar resumen incluso si hay error\n",
    "        except Exception as summary_error:\n",
    "            logger.error(f\"Error al generar resumen: {str(summary_error)}\")\n",
    "        raise\n",
    "\n",
    "# Asegurarnos de que todos los directorios necesarios existen\n",
    "for directory in [LOG_DIR, TRAINED_DIR, PRED_DIR, HISTORY_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Directorio asegurado: {directory}\")\n",
    "\n",
    "# Ejecutar el proceso completo con manejo de excepciones y trazabilidad mejorada\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"üöÄ Iniciando proceso TopoRain-NET...\")\n",
    "        process_summary = main()\n",
    "        logger.info(\"‚úÖ Proceso TopoRain-NET completado exitosamente\")\n",
    "        # Visualizar resultados de trazabilidad\n",
    "        visualize_process_tracker_results()\n",
    "        # Mostrar resumen del log\n",
    "        display_log_summary()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error en el proceso principal: {str(e)}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        try:\n",
    "            tracker.summary()  # Generar resumen incluso si hay error\n",
    "        except Exception as summary_error:\n",
    "            logger.error(f\"Error al generar resumen: {str(summary_error)}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
