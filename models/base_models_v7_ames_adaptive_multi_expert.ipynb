{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab-badge"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/main/models/base_models_v7_ames_adaptive_multi_expert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# V7-AMES: Adaptive Multi-Expert Ensemble System for Monthly Precipitation Prediction\n",
        "\n",
        "## Physics-Guided Mixture of Experts with 3-Stage Training\n",
        "\n",
        "**Version:** 7.0.0  \n",
        "**Date:** January 2026  \n",
        "**Last Updated:** 2026-01-18  \n",
        "**Author:** Manuel Ricardo Perez Reyes  \n",
        "**Institution:** UPTC - Doctoral Thesis in Engineering  \n",
        "\n",
        "**Changelog:**\n",
        "- v7.0.0 (2026-01-18): V7-AMES notebook aligned to V5 pipeline structure and features\n",
        "- v7.0.0 (2026-01-15): Initial V7-AMES architecture and 3-stage training\n",
        "\n",
        "---\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "V7-AMES combines three specialized experts with physics-guided routing and a physics-informed meta-learner:\n",
        "\n",
        "| Component | Description | Role |\n",
        "|-----------|-------------|------|\n",
        "| **Expert 1 (High Elevation)** | GNN-TAT specialist | Orographic precipitation |\n",
        "| **Expert 2 (Low Elevation)** | ConvLSTM specialist | Convective patterns |\n",
        "| **Expert 3 (Transition)** | Hybrid GNN + Conv | Mixed regime |\n",
        "| **Physics-Guided Gating** | Routing with orographic priors | Expert selection |\n",
        "| **Physics-Informed Meta** | Residual + physics correction | Final prediction |\n",
        "\n",
        "### Expected Performance (from V7-AMES README)\n",
        "\n",
        "| Metric | V4 Baseline | V7 Conservative | V7 Optimistic |\n",
        "|--------|-------------|----------------|--------------|\n",
        "| R2 | 0.597 | 0.67-0.70 | 0.72-0.75 |\n",
        "| RMSE (mm) | 84.40 | 76-79 | 72-75 |\n",
        "| Parameters | 98K | ~400K | ~400K |\n",
        "\n",
        "### Training Protocol\n",
        "\n",
        "1. Stage 1: Pre-train Experts (high, low, transition)\n",
        "2. Stage 2: Train Gating Network (experts frozen)\n",
        "3. Stage 3: Joint Fine-Tuning with physics-informed loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-1"
      },
      "source": [
        "## 1. Environment Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup-environment",
        "outputId": "d03b477e-2a32-43bb-df85-6072744aa153"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 1.1: ENVIRONMENT DETECTION AND SETUP\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Install required packages (match PyTorch/CUDA build)\n",
        "    import torch\n",
        "\n",
        "    torch_version = torch.__version__.split('+')[0]\n",
        "    cuda_version = torch.version.cuda\n",
        "    if cuda_version:\n",
        "        cuda_tag = f\"cu{cuda_version.replace('.', '')}\"\n",
        "    else:\n",
        "        cuda_tag = \"cpu\"\n",
        "    pyg_url = f\"https://data.pyg.org/whl/torch-{torch_version}+{cuda_tag}.html\"\n",
        "    print(f\"Installing PyG wheels from: {pyg_url}\")\n",
        "    # Removed -q for more verbose output to debug potential installation issues\n",
        "    !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f {pyg_url}\n",
        "    !pip install netCDF4 xarray dask h5netcdf\n",
        "\n",
        "    # Verify torch_geometric installation\n",
        "    try:\n",
        "        import torch_geometric\n",
        "        print(\"torch_geometric installed successfully.\")\n",
        "    except ImportError:\n",
        "        print(\"ERROR: torch_geometric failed to install. Please check the output above for errors.\")\n",
        "        # Optionally, sys.exit(1) or raise ImportError here if torch_geometric is critical\n",
        "\n",
        "    # Set base paths for Colab\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "    DRIVE_DATA_FILE = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "    LOCAL_DATA_FILE = Path('/content/complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc')\n",
        "    OUTPUT_ROOT = BASE_PATH / 'models' / 'output' / 'V7_AMES'\n",
        "\n",
        "    # Copy dataset to local for faster access\n",
        "    if not LOCAL_DATA_FILE.exists():\n",
        "        !cp \"{DRIVE_DATA_FILE}\" \"{LOCAL_DATA_FILE}\"\n",
        "        print(\"Dataset copied to local storage for faster access\")\n",
        "    DATA_FILE = LOCAL_DATA_FILE\n",
        "else:\n",
        "    # Local paths\n",
        "    BASE_PATH = Path('d:/github.com/ninja-marduk/ml_precipitation_prediction')\n",
        "    DATA_FILE = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'\n",
        "    OUTPUT_ROOT = BASE_PATH / 'models' / 'output' / 'V7_AMES'\n",
        "\n",
        "# Create output directories\n",
        "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output directory: {OUTPUT_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imports",
        "outputId": "7a2fa01e-ccc5-4922-af4f-474c9063080e"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 1.2: IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "import json\n",
        "import gc\n",
        "import copy\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "\n",
        "def ensure_matplotlib():\n",
        "    \"\"\"Ensures matplotlib is properly installed and importable.\"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        return plt, sns\n",
        "    except AttributeError as e:\n",
        "        if \"get_data_path\" in str(e):\n",
        "            print(\"[WARN] Matplotlib cache corrupted. Attempting fix...\")\n",
        "            import sys\n",
        "            import subprocess\n",
        "            import shutil\n",
        "            from pathlib import Path\n",
        "\n",
        "            # Clear matplotlib cache\n",
        "            try:\n",
        "                import matplotlib\n",
        "                cache_dir = Path(matplotlib.get_cachedir())\n",
        "                if cache_dir.exists():\n",
        "                    shutil.rmtree(cache_dir, ignore_errors=True)\n",
        "                    print(f\"[INFO] Cleared matplotlib cache: {cache_dir}\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # Reinstall matplotlib\n",
        "            try:\n",
        "                print(\"[INFO] Reinstalling matplotlib...\")\n",
        "                subprocess.check_call([\n",
        "                    sys.executable, \"-m\", \"pip\", \"install\",\n",
        "                    \"--force-reinstall\", \"--no-cache-dir\",\n",
        "                    \"matplotlib\", \"seaborn\"\n",
        "                ])\n",
        "                print(\"[INFO] Matplotlib reinstalled. Please restart the kernel.\")\n",
        "            except Exception as install_exc:\n",
        "                print(f\"[ERROR] Reinstall failed: {install_exc}\")\n",
        "\n",
        "            raise RuntimeError(\n",
        "                \"Matplotlib installation corrupted. \"\n",
        "                \"Please restart the kernel and rerun the cell.\"\n",
        "            )\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "plt, sns = ensure_matplotlib()\n",
        "\n",
        "try:\n",
        "    import geopandas as gpd\n",
        "except ImportError:\n",
        "    gpd = None\n",
        "\n",
        "try:\n",
        "    import imageio.v2 as imageio\n",
        "except ImportError:\n",
        "    imageio = None\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def ensure_torch_geometric() -> bool:\n",
        "    \"\"\"\n",
        "    Ensures torch_geometric is installed with compatible versions.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import torch_geometric  # noqa: F401\n",
        "        return True\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    import sys\n",
        "    import subprocess\n",
        "\n",
        "    torch_ver = torch.__version__.split(\"+\")[0]\n",
        "    cuda_ver = torch.version.cuda\n",
        "    if cuda_ver:\n",
        "        cuda_tag = \"cu\" + cuda_ver.replace(\".\", \"\")\n",
        "    else:\n",
        "        cuda_tag = \"cpu\"\n",
        "\n",
        "    url = f\"https://data.pyg.org/whl/torch-{torch_ver}+{cuda_tag}.html\"\n",
        "    pkgs = [\n",
        "        \"torch-scatter\",\n",
        "        \"torch-sparse\",\n",
        "        \"torch-cluster\",\n",
        "        \"torch-spline-conv\",\n",
        "        \"torch-geometric\",\n",
        "    ]\n",
        "\n",
        "    print(f\"[INFO] Installing PyG for torch={torch_ver}, cuda={cuda_tag}\")\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs + [\"-f\", url])\n",
        "        return True\n",
        "    except Exception as exc:\n",
        "        print(f\"[WARN] PyG install failed: {exc}\")\n",
        "\n",
        "    if cuda_tag != \"cpu\":\n",
        "        url_cpu = f\"https://data.pyg.org/whl/torch-{torch_ver}+cpu.html\"\n",
        "        print(f\"[INFO] Retrying CPU wheels: {url_cpu}\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs + [\"-f\", url_cpu])\n",
        "            print(\"[WARN] Installed CPU wheels; GPU will be disabled.\")\n",
        "            return True\n",
        "        except Exception as exc:\n",
        "            print(f\"[ERROR] PyG CPU install failed: {exc}\")\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "if not ensure_torch_geometric():\n",
        "    raise RuntimeError(\"torch_geometric not available. Restart runtime and rerun.\")\n",
        "\n",
        "# PyTorch Geometric\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
        "from torch_geometric.data import Data, Batch\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy import stats\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "def seed_everything(seed: int) -> bool:\n",
        "    \"\"\"\n",
        "    Sets random seeds for reproducibility across libraries.\n",
        "\n",
        "    Args:\n",
        "        seed: Random seed value\n",
        "\n",
        "    Returns:\n",
        "        bool: True if CUDA seeding succeeded, False otherwise\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    cuda_ok = False\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "            cuda_ok = True\n",
        "        except Exception as exc:\n",
        "            print(f\"[WARN] CUDA seed failed: {exc}\")\n",
        "\n",
        "    return cuda_ok\n",
        "\n",
        "# Initialize seeds and check CUDA status\n",
        "CUDA_READY = seed_everything(SEED)\n",
        "USE_CUDA = torch.cuda.is_available() and CUDA_READY\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if USE_CUDA:\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "elif torch.cuda.is_available() and not CUDA_READY:\n",
        "    print(\"[WARN] CUDA is available but in a bad state. Using CPU instead.\")\n",
        "    print(\"[WARN] Consider restarting the runtime if GPU is needed.\")\n",
        "else:\n",
        "    print(\"[INFO] CUDA not available. Using CPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-2"
      },
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "config",
        "outputId": "28051a57-ea8d-4a57-e246-2441f8a65008"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 2: V7-AMES CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class V7Config:\n",
        "    \"\"\"Complete V7-AMES configuration.\"\"\"\n",
        "\n",
        "    # Data configuration\n",
        "    input_window: int = 60      # Input months\n",
        "    horizon: int = 12           # Prediction months\n",
        "    train_val_split: float = 0.8\n",
        "\n",
        "    # Light mode for testing\n",
        "    light_mode: bool = False     # Set to False for full grid\n",
        "    light_grid_size: int = 5    # Grid subset size when light_mode=True\n",
        "\n",
        "    # Enabled horizons for experiments\n",
        "    enabled_horizons: List[int] = field(default_factory=lambda: [12])\n",
        "\n",
        "    # Grid shape (updated after dataset load)\n",
        "    n_lat: int = 61\n",
        "    n_lon: int = 65\n",
        "    n_nodes: int = 61 * 65\n",
        "\n",
        "    # Expert 2: ConvLSTM configuration (V2 Enhanced style)\n",
        "    convlstm_filters: List[int] = field(default_factory=lambda: [32, 16])\n",
        "    convlstm_kernel_size: int = 3\n",
        "    convlstm_attention: bool = True\n",
        "    convlstm_bidirectional: bool = True\n",
        "    convlstm_residual: bool = True\n",
        "    convlstm_output_dim: int = 64\n",
        "\n",
        "    # Expert 1: GNN-TAT configuration (V4 style)\n",
        "    gnn_type: str = 'GAT'           # GAT, SAGE, or GCN\n",
        "    gnn_hidden_dim: int = 64\n",
        "    gnn_num_layers: int = 3\n",
        "    gnn_num_heads: int = 4          # For GAT\n",
        "    gnn_temporal_heads: int = 4\n",
        "    gnn_lstm_hidden: int = 64\n",
        "    gnn_lstm_layers: int = 2\n",
        "    gnn_output_dim: int = 64\n",
        "    gnn_dropout: float = 0.1\n",
        "    use_temporal_attention: bool = True\n",
        "    temporal_num_heads: int = 4\n",
        "\n",
        "    # Expert 3: Hybrid configuration\n",
        "    hybrid_gnn_hidden: int = 32\n",
        "    hybrid_conv_hidden: int = 32\n",
        "    hybrid_conv_layers: int = 2\n",
        "    hybrid_dropout: float = 0.2\n",
        "\n",
        "    # Physics-guided gating\n",
        "    gating_hidden_dim: int = 32\n",
        "    gating_dropout: float = 0.2\n",
        "    physics_prior_weight_init: float = 0.3\n",
        "    gating_diversity_lambda: float = 0.01\n",
        "\n",
        "    # Meta-learner\n",
        "    meta_hidden_dim: int = 64\n",
        "    meta_hidden_dim2: int = 32\n",
        "    meta_dropout: float = 0.2\n",
        "    meta_use_context_features: bool = True\n",
        "\n",
        "    # Training\n",
        "    epochs_stage1: int = 50\n",
        "    epochs_stage2: int = 30\n",
        "    epochs_stage3: int = 50\n",
        "    batch_size: int = 8\n",
        "    lr_stage1: float = 0.001\n",
        "    lr_stage2: float = 0.001\n",
        "    lr_stage3: float = 0.0001\n",
        "    weight_decay: float = 1e-4\n",
        "    patience_stage1: int = 15\n",
        "    patience_stage2: int = 10\n",
        "    patience_stage3: int = 15\n",
        "    gradient_clip: float = 1.0\n",
        "    validate: bool = False\n",
        "\n",
        "    # Physics loss weights\n",
        "    lambda_mass_conservation: float = 0.05\n",
        "    lambda_orographic: float = 0.1\n",
        "\n",
        "    # Elevation thresholds (meters)\n",
        "    low_elev_threshold: float = 2000.0\n",
        "    high_elev_threshold: float = 3000.0\n",
        "    medium_elev_min: float = 2000.0\n",
        "    medium_elev_max: float = 3000.0\n",
        "\n",
        "    # Graph construction\n",
        "    edge_threshold: float = 0.3\n",
        "    max_neighbors: int = 8\n",
        "    use_distance_edges: bool = True\n",
        "    use_elevation_edges: bool = True\n",
        "    use_correlation_edges: bool = True\n",
        "    distance_scale_km: float = 10.0\n",
        "    elevation_weight: float = 0.3\n",
        "    correlation_weight: float = 0.5\n",
        "    elevation_scale_m: float = 500.0\n",
        "    min_edge_weight: float = 0.01\n",
        "    correlation_train_only: bool = True\n",
        "\n",
        "    # Export/visualization alignment\n",
        "    export_predictions: bool = True\n",
        "    generate_map_plots: bool = True\n",
        "    map_cycle_all_val_windows: bool = False\n",
        "    map_sample_index: int = -1\n",
        "    map_export_dpi: int = 300\n",
        "    map_gif_duration: float = 1.5\n",
        "    plot_graph_diagnostics: bool = True\n",
        "\n",
        "    # Quality checks / gates\n",
        "    max_bias_pct: float = 10.0\n",
        "    max_scale_ratio: float = 50.0\n",
        "    max_negative_frac: float = 0.001\n",
        "    min_expert_weight: float = 0.05\n",
        "    param_budget: int = 400000\n",
        "    enforce_quality_gates: bool = False\n",
        "    allow_missing_features: bool = False\n",
        "    allow_overlap_windows: bool = False\n",
        "\n",
        "    # Model sizing\n",
        "    compact_model: bool = False\n",
        "\n",
        "    # Device\n",
        "    device: torch.device = device\n",
        "\n",
        "# Initialize config\n",
        "CONFIG = V7Config()\n",
        "\n",
        "# Run-mode overrides\n",
        "RUN_FULL_DATASET = True  # Set False for quick smoke tests\n",
        "if RUN_FULL_DATASET:\n",
        "    CONFIG.light_mode = False\n",
        "    CONFIG.enabled_horizons = [12]\n",
        "    CONFIG.horizon = 12\n",
        "\n",
        "if CONFIG.compact_model:\n",
        "    CONFIG.convlstm_filters = [16, 8]\n",
        "    CONFIG.convlstm_output_dim = 32\n",
        "    CONFIG.gnn_hidden_dim = 32\n",
        "    CONFIG.gnn_num_heads = 2\n",
        "    CONFIG.gnn_temporal_heads = 2\n",
        "    CONFIG.gnn_lstm_hidden = 32\n",
        "    CONFIG.gnn_output_dim = 32\n",
        "    CONFIG.meta_hidden_dim = 48\n",
        "    CONFIG.meta_hidden_dim2 = 24\n",
        "    CONFIG.hybrid_gnn_hidden = 24\n",
        "    CONFIG.hybrid_conv_hidden = 24\n",
        "\n",
        "print(\"V7 Configuration initialized:\")\n",
        "print(f\"  - Light mode: {CONFIG.light_mode}\")\n",
        "print(f\"  - GNN type: {CONFIG.gnn_type}\")\n",
        "print(f\"  - Enabled horizons: {CONFIG.enabled_horizons}\")\n",
        "print(f\"  - Stage 1 epochs: {CONFIG.epochs_stage1}\")\n",
        "print(f\"  - Stage 2 epochs: {CONFIG.epochs_stage2}\")\n",
        "print(f\"  - Stage 3 epochs: {CONFIG.epochs_stage3}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-3"
      },
      "source": [
        "## 3. Data Loading and Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "data-loading",
        "outputId": "9f7a86d8-733e-43dc-d6d5-c8c1ca69c7da"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 3: DATA LOADING AND FEATURE EXTRACTION\n",
        "# =============================================================================\n",
        "\n",
        "# Feature set definitions\n",
        "FEATURE_SETS = {\n",
        "    'BASIC': [\n",
        "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
        "        'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
        "        'elevation', 'slope', 'aspect'\n",
        "    ],\n",
        "    'KCE': [\n",
        "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
        "        'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
        "        'elevation', 'slope', 'aspect',\n",
        "        'elev_high', 'elev_med', 'elev_low'\n",
        "    ],\n",
        "    'PAFC': [\n",
        "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
        "        'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',\n",
        "        'elevation', 'slope', 'aspect',\n",
        "        'elev_high', 'elev_med', 'elev_low',\n",
        "        'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag12'\n",
        "    ]\n",
        "}\n",
        "\n",
        "def _resolve_dim(ds: xr.Dataset, candidates: Tuple[str, ...]) -> str:\n",
        "    for name in candidates:\n",
        "        if name in ds.dims:\n",
        "            return name\n",
        "    raise ValueError(f\"Missing required dimension. Expected one of: {candidates}\")\n",
        "\n",
        "\n",
        "def _required_features(feature_sets: Dict[str, List[str]]) -> List[str]:\n",
        "    required = set()\n",
        "    for name in feature_sets['BASIC'] + feature_sets['KCE']:\n",
        "        if name.startswith('elev_'):\n",
        "            continue\n",
        "        required.add(name)\n",
        "    required.add('total_precipitation')\n",
        "    return sorted(required)\n",
        "\n",
        "\n",
        "def validate_dataset(ds: xr.Dataset, config: V7Config, feature_sets: Dict[str, List[str]]) -> Tuple[str, str]:\n",
        "    if 'time' not in ds.dims:\n",
        "        raise ValueError(\"Dataset missing required 'time' dimension\")\n",
        "\n",
        "    lat_dim = _resolve_dim(ds, ('latitude', 'lat'))\n",
        "    lon_dim = _resolve_dim(ds, ('longitude', 'lon'))\n",
        "\n",
        "    missing = [name for name in _required_features(feature_sets)\n",
        "               if name not in ds.data_vars and name not in ds.coords]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required variables: {missing}\")\n",
        "\n",
        "    n_time = int(ds.dims['time'])\n",
        "    if n_time < config.input_window + config.horizon:\n",
        "        raise ValueError(\n",
        "            f\"Not enough timesteps: {n_time} < {config.input_window + config.horizon}\"\n",
        "        )\n",
        "\n",
        "    return lat_dim, lon_dim\n",
        "\n",
        "\n",
        "def load_dataset(data_path: Path, config: V7Config) -> xr.Dataset:\n",
        "    \"\"\"Load and validate the NetCDF dataset.\"\"\"\n",
        "    print(f\"Loading dataset from: {data_path}\")\n",
        "    ds = xr.open_dataset(data_path)\n",
        "\n",
        "    lat_dim, lon_dim = validate_dataset(ds, config, FEATURE_SETS)\n",
        "\n",
        "    # Print dataset info\n",
        "    print(f\"\n",
        "Dataset dimensions:\")\n",
        "    for dim, size in ds.dims.items():\n",
        "        print(f\"  - {dim}: {size}\")\n",
        "\n",
        "    print(f\"\n",
        "Available variables: {list(ds.data_vars)}\")\n",
        "\n",
        "    # Apply light mode if enabled\n",
        "    if config.light_mode:\n",
        "        lat_slice = slice(0, config.light_grid_size)\n",
        "        lon_slice = slice(0, config.light_grid_size)\n",
        "        ds = ds.isel({lat_dim: lat_slice, lon_dim: lon_slice})\n",
        "        print(f\"\n",
        "Light mode enabled: using {config.light_grid_size}x{config.light_grid_size} grid\")\n",
        "\n",
        "    return ds\n",
        "\n",
        "\n",
        "def create_elevation_clusters(ds: xr.Dataset, n_clusters: int = 3) -> xr.Dataset:\n",
        "    \"\"\"Add elevation cluster features (KCE) to dataset.\"\"\"\n",
        "    elevation = ds['elevation'].values\n",
        "    elev_dims = ds['elevation'].dims\n",
        "\n",
        "    # Handle 3D elevation (time, lat, lon) by taking first timestep\n",
        "    if elevation.ndim == 3:\n",
        "        print(\"Handling 3D elevation: using first timestep for static clustering\")\n",
        "        elevation = elevation[0]\n",
        "        elev_dims = elev_dims[-2:]\n",
        "\n",
        "    valid_mask = ~np.isnan(elevation)\n",
        "\n",
        "    # Flatten for clustering\n",
        "    elev_flat = elevation[valid_mask].reshape(-1, 1)\n",
        "\n",
        "    # K-means clustering\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=SEED, n_init=10)\n",
        "    labels = np.full(elevation.shape, -1)\n",
        "    labels[valid_mask] = kmeans.fit_predict(elev_flat)\n",
        "\n",
        "    # Create one-hot encoded features\n",
        "    for i, name in enumerate(['elev_low', 'elev_med', 'elev_high']):\n",
        "        cluster_data = np.zeros_like(elevation)\n",
        "        cluster_data[labels == i] = 1.0\n",
        "        ds[name] = xr.DataArray(\n",
        "            data=cluster_data,\n",
        "            dims=elev_dims,\n",
        "            attrs={'description': f'Elevation cluster {name}'}\n",
        "        )\n",
        "\n",
        "    print(\"Added elevation clusters: elev_low, elev_med, elev_high\")\n",
        "    return ds\n",
        "\n",
        "\n",
        "def extract_features(ds: xr.Dataset, feature_names: List[str], config: V7Config) -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"Extract features from dataset into numpy array.\"\"\"\n",
        "    features = []\n",
        "    missing = []\n",
        "\n",
        "    for name in feature_names:\n",
        "        if name in ds.data_vars:\n",
        "            data = ds[name].values\n",
        "            # Handle different dimensions\n",
        "            if data.ndim == 2:  # (lat, lon) - static features\n",
        "                # Broadcast to (time, lat, lon)\n",
        "                data = np.broadcast_to(data, (ds.dims['time'], *data.shape))\n",
        "            features.append(data)\n",
        "        else:\n",
        "            missing.append(name)\n",
        "\n",
        "    if missing:\n",
        "        msg = f\"Missing features: {missing}\"\n",
        "        if config.allow_missing_features:\n",
        "            print(f\"Warning: {msg}\")\n",
        "        else:\n",
        "            raise ValueError(msg)\n",
        "\n",
        "    if not features:\n",
        "        raise ValueError(\"No features extracted; check dataset and feature list\")\n",
        "\n",
        "    # Stack features: (time, lat, lon, n_features)\n",
        "    features = np.stack(features, axis=-1)\n",
        "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    print(f\"Extracted features shape: {features.shape}\")\n",
        "\n",
        "    return features.astype(np.float32), missing\n",
        "\n",
        "\n",
        "def build_context_features(ds: xr.Dataset, lat_dim: str, lon_dim: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Build static context features (elevation, slope, aspect, lat/lon).\"\"\"\n",
        "    elevation = ds['elevation'].values\n",
        "    if elevation.ndim == 3:\n",
        "        elevation = elevation[0]\n",
        "\n",
        "    if 'slope' in ds:\n",
        "        slope = ds['slope'].values\n",
        "        if slope.ndim == 3:\n",
        "            slope = slope[0]\n",
        "    else:\n",
        "        grad_y, grad_x = np.gradient(elevation)\n",
        "        slope = np.sqrt(grad_x**2 + grad_y**2)\n",
        "\n",
        "    if 'aspect' in ds:\n",
        "        aspect = ds['aspect'].values\n",
        "        if aspect.ndim == 3:\n",
        "            aspect = aspect[0]\n",
        "    else:\n",
        "        grad_y, grad_x = np.gradient(elevation)\n",
        "        aspect = np.arctan2(grad_y, grad_x)\n",
        "\n",
        "    aspect_sin = np.sin(aspect)\n",
        "    aspect_cos = np.cos(aspect)\n",
        "\n",
        "    lat_vals = ds[lat_dim].values\n",
        "    lon_vals = ds[lon_dim].values\n",
        "    lat_grid, lon_grid = np.meshgrid(lat_vals, lon_vals, indexing='ij')\n",
        "\n",
        "    lat_norm = (lat_grid - np.mean(lat_grid)) / (np.std(lat_grid) + 1e-6)\n",
        "    lon_norm = (lon_grid - np.mean(lon_grid)) / (np.std(lon_grid) + 1e-6)\n",
        "\n",
        "    elev_norm = (elevation - np.mean(elevation)) / (np.std(elevation) + 1e-6)\n",
        "    slope_norm = (slope - np.mean(slope)) / (np.std(slope) + 1e-6)\n",
        "\n",
        "    context = np.stack(\n",
        "        [elev_norm, slope_norm, aspect_sin, aspect_cos, lat_norm, lon_norm],\n",
        "        axis=-1\n",
        "    ).astype(np.float32)\n",
        "\n",
        "    return context, elevation.astype(np.float32)\n",
        "\n",
        "# Load dataset\n",
        "DS = load_dataset(DATA_FILE, CONFIG)\n",
        "LAT_DIM = _resolve_dim(DS, ('latitude', 'lat'))\n",
        "LON_DIM = _resolve_dim(DS, ('longitude', 'lon'))\n",
        "DS = create_elevation_clusters(DS)\n",
        "\n",
        "CONFIG.n_lat = int(DS.dims[LAT_DIM])\n",
        "CONFIG.n_lon = int(DS.dims[LON_DIM])\n",
        "CONFIG.n_nodes = CONFIG.n_lat * CONFIG.n_lon\n",
        "\n",
        "# Build context features and elevation masks\n",
        "CONTEXT_STATIC, ELEVATION_GRID = build_context_features(DS, LAT_DIM, LON_DIM)\n",
        "CONTEXT_NODES = CONTEXT_STATIC.reshape(-1, CONTEXT_STATIC.shape[-1])\n",
        "ELEVATION_NODES = ELEVATION_GRID.reshape(-1)\n",
        "\n",
        "MASK_HIGH = (ELEVATION_GRID >= CONFIG.high_elev_threshold)\n",
        "MASK_LOW = (ELEVATION_GRID < CONFIG.low_elev_threshold)\n",
        "MASK_MED = (ELEVATION_GRID >= CONFIG.medium_elev_min) & (ELEVATION_GRID <= CONFIG.medium_elev_max)\n",
        "\n",
        "MASK_HIGH_NODES = MASK_HIGH.reshape(-1)\n",
        "MASK_LOW_NODES = MASK_LOW.reshape(-1)\n",
        "MASK_MED_NODES = MASK_MED.reshape(-1)\n",
        "\n",
        "V7_DATA_DIR = OUTPUT_ROOT / 'data'\n",
        "V7_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "np.save(V7_DATA_DIR / 'mask_high.npy', MASK_HIGH.astype(np.float32))\n",
        "np.save(V7_DATA_DIR / 'mask_medium.npy', MASK_MED.astype(np.float32))\n",
        "np.save(V7_DATA_DIR / 'mask_low.npy', MASK_LOW.astype(np.float32))\n",
        "np.save(V7_DATA_DIR / 'context_features_spatial.npy', CONTEXT_STATIC)\n",
        "\n",
        "print(\"Context features saved to:\", V7_DATA_DIR)\n",
        "print(\"Mask summary:\")\n",
        "print(f\"  High elevation: {int(MASK_HIGH.sum())} cells\")\n",
        "print(f\"  Medium elevation: {int(MASK_MED.sum())} cells\")\n",
        "print(f\"  Low elevation: {int(MASK_LOW.sum())} cells\")\n",
        "\n",
        "# Alias for compatibility with V5-style code\n",
        "ds = DS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-4"
      },
      "source": [
        "## 4. Graph Construction for GNN Experts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "graph-construction",
        "outputId": "1aa809d2-9d6a-40be-96e2-89a637c9485e"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 4: GRAPH CONSTRUCTION\n",
        "# =============================================================================\n",
        "\n",
        "class SpatialGraphBuilder:\n",
        "    \"\"\"Build spatial graph for GNN branch based on geographic and topographic similarity.\"\"\"\n",
        "\n",
        "    def __init__(self, config: V7Config):\n",
        "        self.config = config\n",
        "\n",
        "    @staticmethod\n",
        "    def _safe_correlation(ts_a: np.ndarray, ts_b: np.ndarray) -> float:\n",
        "        \"\"\"Compute correlation robustly (returns 0.0 for invalid cases).\"\"\"\n",
        "        mask = np.isfinite(ts_a) & np.isfinite(ts_b)\n",
        "        if mask.sum() < 2:\n",
        "            return 0.0\n",
        "        a = ts_a[mask]\n",
        "        b = ts_b[mask]\n",
        "        a = a - a.mean()\n",
        "        b = b - b.mean()\n",
        "        denom = np.sqrt(np.sum(a * a)) * np.sqrt(np.sum(b * b))\n",
        "        if denom < 1e-6:\n",
        "            return 0.0\n",
        "        corr = float(np.sum(a * b) / denom)\n",
        "        if not np.isfinite(corr):\n",
        "            return 0.0\n",
        "        return corr\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_coord(ds: xr.Dataset, names: List[str]) -> np.ndarray:\n",
        "        for name in names:\n",
        "            if name in ds.coords:\n",
        "                return ds.coords[name].values\n",
        "            if name in ds.variables:\n",
        "                return ds[name].values\n",
        "        raise KeyError(f\"Missing coordinate; tried {names}\")\n",
        "\n",
        "    def build_graph(self, ds: xr.Dataset, train_time_idx: Optional[int] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Build graph structure from dataset.\n",
        "\n",
        "        Returns:\n",
        "            edge_index: (2, num_edges) tensor of edge indices\n",
        "            edge_weight: (num_edges,) tensor of edge weights\n",
        "        \"\"\"\n",
        "        lat = self._get_coord(ds, ['lat', 'latitude'])\n",
        "        lon = self._get_coord(ds, ['lon', 'longitude'])\n",
        "        elevation = ds['elevation'].values\n",
        "\n",
        "        if elevation.ndim == 3:\n",
        "            elevation = elevation[0]\n",
        "\n",
        "        n_lat, n_lon = len(lat), len(lon)\n",
        "        n_nodes = n_lat * n_lon\n",
        "\n",
        "        print(f\"Building graph for {n_lat}x{n_lon} = {n_nodes} nodes\")\n",
        "\n",
        "        # Create node positions in km for distance scaling\n",
        "        lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')\n",
        "        mean_lat = float(np.mean(lat))\n",
        "        km_per_deg_lat = 111.32\n",
        "        km_per_deg_lon = 111.32 * np.cos(np.deg2rad(mean_lat))\n",
        "        positions = np.stack(\n",
        "            [lat_grid.flatten() * km_per_deg_lat, lon_grid.flatten() * km_per_deg_lon],\n",
        "            axis=1\n",
        "        )\n",
        "        elev_flat = elevation.flatten()\n",
        "\n",
        "        self.node_positions = positions\n",
        "        self.node_elevations = elev_flat\n",
        "        self.n_nodes = n_nodes\n",
        "\n",
        "        # Precompute precipitation time series for correlation edges\n",
        "        precip_flat = None\n",
        "        if self.config.use_correlation_edges:\n",
        "            precip = ds['total_precipitation'].values.astype(np.float32)\n",
        "            if train_time_idx is not None:\n",
        "                train_time_idx = max(2, min(train_time_idx, precip.shape[0]))\n",
        "                precip = precip[:train_time_idx]\n",
        "                print(f\"Correlation edges using first {train_time_idx} timesteps\")\n",
        "            precip_flat = precip.reshape(precip.shape[0], n_nodes)\n",
        "\n",
        "        edges = []\n",
        "        weights = []\n",
        "\n",
        "        distance_coeff = 1.0\n",
        "        if self.config.use_elevation_edges or self.config.use_correlation_edges:\n",
        "            distance_coeff = max(\n",
        "                0.0,\n",
        "                1.0 - self.config.elevation_weight - self.config.correlation_weight\n",
        "            )\n",
        "\n",
        "        for i in range(n_nodes):\n",
        "            # Calculate distances to all other nodes\n",
        "            distances = np.sqrt(np.sum((positions - positions[i])**2, axis=1))\n",
        "\n",
        "            # Get k nearest neighbors (excluding self)\n",
        "            distances[i] = np.inf\n",
        "            nearest_idx = np.argsort(distances)[:self.config.max_neighbors]\n",
        "            ts_i = precip_flat[:, i] if precip_flat is not None else None\n",
        "\n",
        "            for j in nearest_idx:\n",
        "                if distances[j] == np.inf:\n",
        "                    continue\n",
        "\n",
        "                component_weights = []\n",
        "\n",
        "                if self.config.use_distance_edges:\n",
        "                    dist_weight = np.exp(-distances[j] / self.config.distance_scale_km)\n",
        "                    component_weights.append((dist_weight, distance_coeff))\n",
        "\n",
        "                if self.config.use_elevation_edges:\n",
        "                    if not np.isnan(elev_flat[i]) and not np.isnan(elev_flat[j]):\n",
        "                        elev_diff = np.abs(elev_flat[i] - elev_flat[j])\n",
        "                        elev_weight = np.exp(-elev_diff / self.config.elevation_scale_m)\n",
        "                    else:\n",
        "                        elev_weight = 0.5\n",
        "                    component_weights.append((elev_weight, self.config.elevation_weight))\n",
        "\n",
        "                if self.config.use_correlation_edges:\n",
        "                    if precip_flat is not None:\n",
        "                        corr = self._safe_correlation(ts_i, precip_flat[:, j])\n",
        "                    else:\n",
        "                        corr = 0.0\n",
        "                    corr_weight = max(0.0, corr)\n",
        "                    component_weights.append((corr_weight, self.config.correlation_weight))\n",
        "\n",
        "                if not component_weights:\n",
        "                    continue\n",
        "\n",
        "                coeff_sum = sum(weight for _, weight in component_weights)\n",
        "                if coeff_sum <= 0:\n",
        "                    coeff_sum = len(component_weights)\n",
        "                    combined_weight = sum(val for val, _ in component_weights) / coeff_sum\n",
        "                else:\n",
        "                    combined_weight = sum(val * weight for val, weight in component_weights) / coeff_sum\n",
        "\n",
        "                threshold = max(self.config.edge_threshold, self.config.min_edge_weight)\n",
        "                if combined_weight >= threshold:\n",
        "                    edges.append([i, j])\n",
        "                    weights.append(combined_weight)\n",
        "\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        edge_weight = torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "        print(f\"Graph built: {n_nodes} nodes, {edge_index.shape[1]} edges\")\n",
        "        print(f\"Average edges per node: {edge_index.shape[1] / n_nodes:.1f}\")\n",
        "\n",
        "        return edge_index, edge_weight\n",
        "\n",
        "# Build graph\n",
        "graph_builder = SpatialGraphBuilder(CONFIG)\n",
        "train_time_idx = None\n",
        "if CONFIG.use_correlation_edges and CONFIG.correlation_train_only:\n",
        "    train_time_idx = int(ds.dims['time'] * CONFIG.train_val_split)\n",
        "edge_index, edge_weight = graph_builder.build_graph(ds, train_time_idx=train_time_idx)\n",
        "if edge_index.numel() == 0:\n",
        "    raise ValueError(\"Graph construction produced zero edges\")\n",
        "print(f\"Edge index shape: {edge_index.shape}\")\n",
        "print(f\"Edge weight shape: {edge_weight.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3lmX0x0oqvlL",
        "outputId": "c06893c9-ee12-4db5-be30-5d31b4ed3a3e"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 4.1: GRAPH DIAGNOSTICS (V4-COMPATIBLE)\n",
        "# =============================================================================\n",
        "\n",
        "def visualize_graph(graph_builder: SpatialGraphBuilder, edge_index: np.ndarray,\n",
        "                   edge_weight: np.ndarray, title: str = 'Spatial Graph'):\n",
        "    if edge_weight.size == 0:\n",
        "        print('Graph visualization skipped: empty edge list')\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    ax1 = axes[0]\n",
        "    pos = graph_builder.node_positions\n",
        "    n_edges_to_draw = min(500, len(edge_weight))\n",
        "    edge_indices = np.random.choice(len(edge_weight), n_edges_to_draw, replace=False)\n",
        "    for idx in edge_indices:\n",
        "        i, j = edge_index[0, idx], edge_index[1, idx]\n",
        "        ax1.plot([pos[i, 1], pos[j, 1]], [pos[i, 0], pos[j, 0]], 'b-', alpha=0.2, linewidth=0.5)\n",
        "\n",
        "    scatter = ax1.scatter(pos[:, 1], pos[:, 0], c=graph_builder.node_elevations,\n",
        "                         cmap='terrain', s=80, edgecolors='black', linewidth=0.4)\n",
        "    plt.colorbar(scatter, ax=ax1, label='Elevation (m)')\n",
        "    ax1.set_xlabel('Longitude')\n",
        "    ax1.set_ylabel('Latitude')\n",
        "    ax1.set_title('Graph Structure (colored by elevation)')\n",
        "\n",
        "    ax2 = axes[1]\n",
        "    ax2.hist(edge_weight, bins=50, color='steelblue', edgecolor='black')\n",
        "    ax2.set_xlabel('Edge Weight')\n",
        "    ax2.set_ylabel('Frequency')\n",
        "    ax2.set_title('Edge Weight Distribution')\n",
        "    ax2.axvline(np.mean(edge_weight), color='red', linestyle='--', label=f'Mean: {np.mean(edge_weight):.3f}')\n",
        "    ax2.legend()\n",
        "\n",
        "    ax3 = axes[2]\n",
        "    degrees = np.bincount(edge_index[0], minlength=graph_builder.n_nodes)\n",
        "    ax3.hist(degrees, bins=20, color='forestgreen', edgecolor='black')\n",
        "    ax3.set_xlabel('Node Degree')\n",
        "    ax3.set_ylabel('Frequency')\n",
        "    ax3.set_title('Node Degree Distribution')\n",
        "    ax3.axvline(np.mean(degrees), color='red', linestyle='--', label=f'Mean: {np.mean(degrees):.1f}')\n",
        "    ax3.legend()\n",
        "\n",
        "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig_path = OUTPUT_ROOT / 'graph_visualization_v7.png'\n",
        "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"Graph visualization saved to: {fig_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def analyze_graph_edges(graph_builder: SpatialGraphBuilder, edge_index: np.ndarray,\n",
        "                        edge_weight: np.ndarray, output_dir: Path):\n",
        "    if edge_weight.size == 0:\n",
        "        print('Graph edge analysis skipped: empty edge list')\n",
        "        return\n",
        "\n",
        "    max_nodes_for_adj = 6000\n",
        "    if graph_builder.n_nodes > max_nodes_for_adj:\n",
        "        print(f\"Skipping adjacency matrix plot: {graph_builder.n_nodes} nodes\")\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(7, 6))\n",
        "        axes = [ax]\n",
        "    else:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    if len(axes) > 1:\n",
        "        ax1 = axes[0]\n",
        "        adj_matrix = np.zeros((graph_builder.n_nodes, graph_builder.n_nodes), dtype=np.float32)\n",
        "        for idx in range(len(edge_weight)):\n",
        "            i, j = edge_index[0, idx], edge_index[1, idx]\n",
        "            adj_matrix[i, j] = edge_weight[idx]\n",
        "        im1 = ax1.imshow(adj_matrix, cmap='YlOrRd', aspect='auto')\n",
        "        plt.colorbar(im1, ax=ax1, label='Edge Weight')\n",
        "        ax1.set_xlabel('Node Index')\n",
        "        ax1.set_ylabel('Node Index')\n",
        "        ax1.set_title('Adjacency Matrix (Edge Weights)')\n",
        "        ax2 = axes[1]\n",
        "    else:\n",
        "        ax2 = axes[0]\n",
        "\n",
        "    threshold = np.percentile(edge_weight, 90)\n",
        "    strong_edges = edge_weight > threshold\n",
        "    pos = graph_builder.node_positions\n",
        "\n",
        "    scatter = ax2.scatter(pos[:, 1], pos[:, 0], c=graph_builder.node_elevations,\n",
        "                         cmap='terrain', s=80, edgecolors='black', linewidth=0.4, zorder=2)\n",
        "    plt.colorbar(scatter, ax=ax2, label='Elevation (m)')\n",
        "\n",
        "    for idx in np.where(strong_edges)[0]:\n",
        "        i, j = edge_index[0, idx], edge_index[1, idx]\n",
        "        ax2.plot([pos[i, 1], pos[j, 1]], [pos[i, 0], pos[j, 0]],\n",
        "                'r-', alpha=0.6, linewidth=edge_weight[idx] * 2, zorder=1)\n",
        "\n",
        "    ax2.set_xlabel('Longitude')\n",
        "    ax2.set_ylabel('Latitude')\n",
        "    ax2.set_title(f'Top 10% Strongest Edges (n={int(strong_edges.sum())})')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig_path = output_dir / 'graph_edge_analysis_v5.png'\n",
        "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"Edge analysis saved to: {fig_path}\")\n",
        "    plt.show()\n",
        "\n",
        "    print('Graph Statistics:')\n",
        "    print(f\"  Total nodes: {graph_builder.n_nodes}\")\n",
        "    print(f\"  Total edges: {len(edge_weight)}\")\n",
        "    print(f\"  Average edge weight: {np.mean(edge_weight):.4f}\")\n",
        "    print(f\"  Max edge weight: {np.max(edge_weight):.4f}\")\n",
        "    print(f\"  Strong edges (top 10%): {int(strong_edges.sum())}\")\n",
        "\n",
        "\n",
        "if getattr(CONFIG, 'plot_graph_diagnostics', True):\n",
        "    edge_index_np = edge_index.cpu().numpy()\n",
        "    edge_weight_np = edge_weight.cpu().numpy()\n",
        "    try:\n",
        "        visualize_graph(\n",
        "            graph_builder, edge_index_np, edge_weight_np,\n",
        "            title=f\"V7 Spatial Graph ({int(ds.dims[LAT_DIM])}x{int(ds.dims[LON_DIM])} grid)\"\n",
        "        )\n",
        "    except Exception as exc:\n",
        "        print(f\"Graph visualization skipped: {exc}\")\n",
        "    try:\n",
        "        analyze_graph_edges(graph_builder, edge_index_np, edge_weight_np, OUTPUT_ROOT)\n",
        "    except Exception as exc:\n",
        "        print(f\"Graph edge analysis skipped: {exc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-5"
      },
      "source": [
        "## 5. Data Preprocessing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "preprocessing",
        "outputId": "e6f39de9-46d6-4d7d-dddc-6e36bcb7780e"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 5: DATA PREPROCESSING AND DATASET CLASS\n",
        "# =============================================================================\n",
        "\n",
        "def create_temporal_windows(\n",
        "    features: np.ndarray,\n",
        "    target: np.ndarray,\n",
        "    input_window: int,\n",
        "    horizon: int\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Create temporal windows for sequence-to-sequence prediction.\n",
        "\n",
        "    Note: This is memory-heavy for full grids and kept for debugging only.\n",
        "    \"\"\"\n",
        "    n_time = features.shape[0]\n",
        "    n_samples = n_time - input_window - horizon + 1\n",
        "\n",
        "    if n_samples <= 0:\n",
        "        raise ValueError(f\"Not enough timesteps: {n_time} < {input_window + horizon}\")\n",
        "\n",
        "    X_list = []\n",
        "    Y_list = []\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        X_list.append(features[i:i+input_window])\n",
        "        Y_list.append(target[i+input_window:i+input_window+horizon])\n",
        "\n",
        "    X = np.stack(X_list, axis=0)\n",
        "    Y = np.stack(Y_list, axis=0)\n",
        "\n",
        "    print(f\"Created {n_samples} samples\")\n",
        "    print(f\"  X shape: {X.shape}\")\n",
        "    print(f\"  Y shape: {Y.shape}\")\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "class V7DualBranchDataset(Dataset):\n",
        "    \"\"\"Dataset for V7-AMES dual-branch input (grid + graph).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        features_grid: torch.Tensor,  # (time, lat, lon, features)\n",
        "        features_graph: torch.Tensor, # (time, lat, lon, features)\n",
        "        target: torch.Tensor,         # (time, lat, lon)\n",
        "        input_window: int,\n",
        "        horizon: int,\n",
        "        edge_index: torch.Tensor,\n",
        "        edge_weight: torch.Tensor,\n",
        "        context_nodes: torch.Tensor,  # (n_nodes, context_dim)\n",
        "        elevation_nodes: torch.Tensor, # (n_nodes,)\n",
        "        start_idx: int,\n",
        "        end_idx: int\n",
        "    ):\n",
        "        self.features_grid = features_grid\n",
        "        self.features_graph = features_graph\n",
        "        self.target = target\n",
        "        self.input_window = input_window\n",
        "        self.horizon = horizon\n",
        "        self.edge_index = edge_index\n",
        "        self.edge_weight = edge_weight\n",
        "        self.context_nodes = context_nodes\n",
        "        self.elevation_nodes = elevation_nodes\n",
        "        self.start_idx = start_idx\n",
        "        self.end_idx = end_idx\n",
        "\n",
        "        if end_idx <= start_idx:\n",
        "            raise ValueError(f\"Invalid window range: {start_idx} to {end_idx}\")\n",
        "\n",
        "        # Store grid dimensions for later use\n",
        "        self.n_lat = features_grid.shape[1]\n",
        "        self.n_lon = features_grid.shape[2]\n",
        "        self.n_nodes = self.n_lat * self.n_lon\n",
        "\n",
        "        max_start = target.shape[0] - input_window - horizon\n",
        "        if max_start < 0:\n",
        "            raise ValueError(\n",
        "                f\"Not enough timesteps: {target.shape[0]} < {input_window + horizon}\"\n",
        "            )\n",
        "        if self.start_idx < 0 or self.end_idx - 1 > max_start:\n",
        "            raise ValueError(\n",
        "                f\"Window range out of bounds: {self.start_idx}..{self.end_idx - 1} > {max_start}\"\n",
        "            )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.end_idx - self.start_idx\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i = self.start_idx + idx\n",
        "        x_grid = self.features_grid[i:i+self.input_window]\n",
        "        x_graph = self.features_graph[i:i+self.input_window]\n",
        "        y = self.target[i+self.input_window:i+self.input_window+self.horizon]\n",
        "\n",
        "        return {\n",
        "            'x_grid': x_grid,\n",
        "            'x_graph': x_graph.reshape(\n",
        "                self.input_window, -1, x_graph.shape[-1]\n",
        "            ),\n",
        "            'y': y,\n",
        "            'context': self.context_nodes,\n",
        "            'elevation': self.elevation_nodes,\n",
        "            'edge_index': self.edge_index,\n",
        "            'edge_weight': self.edge_weight\n",
        "        }\n",
        "\n",
        "\n",
        "def prepare_data(\n",
        "    ds: xr.Dataset,\n",
        "    config: V7Config,\n",
        "    edge_index: torch.Tensor,\n",
        "    edge_weight: torch.Tensor,\n",
        "    context_nodes: np.ndarray,\n",
        "    elevation_nodes: np.ndarray,\n",
        "    feature_set_grid: str = 'BASIC',\n",
        "    feature_set_graph: str = 'KCE'\n",
        "):\n",
        "    \"\"\"Prepare data for V7 training.\"\"\"\n",
        "    print(\"\n",
        "\" + \"=\"*60)\n",
        "    print(\"Preparing data for V7-AMES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Extract features for both branches\n",
        "    print(f\"\n",
        "Extracting {feature_set_grid} features (ConvLSTM branch):\")\n",
        "    features_grid, missing_grid = extract_features(ds, FEATURE_SETS[feature_set_grid], config)\n",
        "\n",
        "    print(f\"\n",
        "Extracting {feature_set_graph} features (GNN branch):\")\n",
        "    features_graph, missing_graph = extract_features(ds, FEATURE_SETS[feature_set_graph], config)\n",
        "\n",
        "    # Target variable\n",
        "    target = ds['total_precipitation'].values.astype(np.float32)\n",
        "    print(f\"\n",
        "Target shape: {target.shape}\")\n",
        "\n",
        "    # Handle NaN values\n",
        "    features_grid = np.nan_to_num(features_grid, nan=0.0)\n",
        "    features_graph = np.nan_to_num(features_graph, nan=0.0)\n",
        "    target = np.nan_to_num(target, nan=0.0)\n",
        "\n",
        "    # Window counts (avoid materializing all windows)\n",
        "    n_time = features_grid.shape[0]\n",
        "    max_start = n_time - config.input_window - config.horizon\n",
        "    if max_start < 0:\n",
        "        raise ValueError(f\"Not enough timesteps: {n_time} < {config.input_window + config.horizon}\")\n",
        "    n_samples = max_start + 1\n",
        "\n",
        "    print(\"\n",
        "Computed temporal windows:\")\n",
        "    print(f\"  Samples: {n_samples}\")\n",
        "    print(f\"  Input window: {config.input_window}\")\n",
        "    print(f\"  Horizon: {config.horizon}\")\n",
        "\n",
        "    # Train/val split by time BEFORE windowing (avoid leakage)\n",
        "    min_split = config.input_window + config.horizon\n",
        "    max_split = max_start\n",
        "    split_mode = 'time'\n",
        "    split_time_idx = int(n_time * config.train_val_split)\n",
        "\n",
        "    if min_split > max_split:\n",
        "        msg = (\n",
        "            f\"Not enough timesteps for leakage-free split: n_time={n_time}, \"\n",
        "            f\"need >= {2 * (config.input_window + config.horizon)}\"\n",
        "        )\n",
        "        if config.allow_overlap_windows:\n",
        "            print(f\"Warning: {msg}. Falling back to overlap-prone split.\")\n",
        "            split_mode = 'window'\n",
        "            n_train = int(n_samples * config.train_val_split)\n",
        "            n_val = n_samples - n_train\n",
        "            train_last_start = n_train - 1\n",
        "            val_start_idx = n_train\n",
        "            val_last_start = max_start\n",
        "            split_time_idx = val_start_idx\n",
        "        elif config.enforce_quality_gates:\n",
        "            raise ValueError(msg)\n",
        "        else:\n",
        "            raise ValueError(msg)\n",
        "    else:\n",
        "        if split_time_idx < min_split:\n",
        "            print(f\"Warning: split_time_idx={split_time_idx} < min_split={min_split}; clipping.\")\n",
        "            split_time_idx = min_split\n",
        "        if split_time_idx > max_split:\n",
        "            print(f\"Warning: split_time_idx={split_time_idx} > max_split={max_split}; clipping.\")\n",
        "            split_time_idx = max_split\n",
        "\n",
        "        train_last_start = split_time_idx - config.input_window - config.horizon\n",
        "        val_start_idx = split_time_idx\n",
        "        val_last_start = max_start\n",
        "        n_train = train_last_start + 1\n",
        "        n_val = val_last_start - val_start_idx + 1\n",
        "\n",
        "    if n_train <= 0 or n_val <= 0:\n",
        "        raise ValueError(f\"Invalid train/val split: {n_train}/{n_val}\")\n",
        "\n",
        "    print(\"\n",
        "Split configuration:\")\n",
        "    print(f\"  Split mode: {split_mode}\")\n",
        "    print(f\"  Split index: {split_time_idx}\")\n",
        "    print(f\"  Train window starts: 0..{train_last_start} ({n_train})\")\n",
        "    print(f\"  Val window starts: {val_start_idx}..{val_last_start} ({n_val})\")\n",
        "    gap = val_start_idx - (train_last_start + 1)\n",
        "    print(f\"  Gap windows: {gap}\")\n",
        "\n",
        "    # Overlap leakage check\n",
        "    last_train_end = train_last_start + config.input_window + config.horizon - 1\n",
        "    first_val_start = val_start_idx\n",
        "    overlap_leakage = last_train_end >= first_val_start\n",
        "    if overlap_leakage:\n",
        "        msg = \"Train/val windows overlap. Consider splitting before windowing.\"\n",
        "        if config.allow_overlap_windows:\n",
        "            print(f\"Warning: {msg}\")\n",
        "        elif config.enforce_quality_gates:\n",
        "            raise ValueError(msg)\n",
        "        else:\n",
        "            print(f\"Warning: {msg}\")\n",
        "\n",
        "    # Normalize using training input range only (avoid leakage)\n",
        "    print(\"\n",
        "Normalizing features using training split only:\")\n",
        "    train_input_end = train_last_start + config.input_window - 1\n",
        "    train_slice = slice(0, train_input_end + 1)\n",
        "\n",
        "    grid_mean = features_grid[train_slice].mean(axis=(0, 1, 2), keepdims=True)\n",
        "    grid_std = features_grid[train_slice].std(axis=(0, 1, 2), keepdims=True)\n",
        "    grid_std = np.where(grid_std > 1e-6, grid_std, 1.0)\n",
        "\n",
        "    graph_mean = features_graph[train_slice].mean(axis=(0, 1, 2), keepdims=True)\n",
        "    graph_std = features_graph[train_slice].std(axis=(0, 1, 2), keepdims=True)\n",
        "    graph_std = np.where(graph_std > 1e-6, graph_std, 1.0)\n",
        "\n",
        "    features_grid = ((features_grid - grid_mean) / grid_std).astype(np.float32)\n",
        "    features_graph = ((features_graph - graph_mean) / graph_std).astype(np.float32)\n",
        "\n",
        "    # Convert to torch once to avoid repeated conversion in __getitem__\n",
        "    features_grid_t = torch.from_numpy(features_grid)\n",
        "    features_graph_t = torch.from_numpy(features_graph)\n",
        "    target_t = torch.from_numpy(target)\n",
        "    context_nodes_t = torch.from_numpy(context_nodes).float()\n",
        "    elevation_nodes_t = torch.from_numpy(elevation_nodes).float()\n",
        "\n",
        "    train_dataset = V7DualBranchDataset(\n",
        "        features_grid_t, features_graph_t, target_t,\n",
        "        config.input_window, config.horizon,\n",
        "        edge_index, edge_weight,\n",
        "        context_nodes_t, elevation_nodes_t,\n",
        "        start_idx=0,\n",
        "        end_idx=n_train\n",
        "    )\n",
        "    val_dataset = V7DualBranchDataset(\n",
        "        features_grid_t, features_graph_t, target_t,\n",
        "        config.input_window, config.horizon,\n",
        "        edge_index, edge_weight,\n",
        "        context_nodes_t, elevation_nodes_t,\n",
        "        start_idx=val_start_idx,\n",
        "        end_idx=val_last_start + 1\n",
        "    )\n",
        "\n",
        "    feature_index_map = {\n",
        "        name: idx for idx, name in enumerate(FEATURE_SETS[feature_set_graph])\n",
        "    }\n",
        "\n",
        "    data_report = {\n",
        "        'n_samples': int(n_samples),\n",
        "        'n_train': int(n_train),\n",
        "        'n_val': int(n_val),\n",
        "        'n_time': int(n_time),\n",
        "        'feature_set_grid': feature_set_grid,\n",
        "        'feature_set_graph': feature_set_graph,\n",
        "        'split_mode': split_mode,\n",
        "        'split_time_idx': int(split_time_idx),\n",
        "        'train_last_start': int(train_last_start),\n",
        "        'val_start_idx': int(val_start_idx),\n",
        "        'val_end_idx': int(val_last_start),\n",
        "        'train_input_end': int(train_input_end),\n",
        "        'overlap_leakage': bool(overlap_leakage),\n",
        "        'missing_features_grid': missing_grid,\n",
        "        'missing_features_graph': missing_graph,\n",
        "        'context_dim': int(context_nodes.shape[-1])\n",
        "    }\n",
        "\n",
        "    print(f\"\n",
        "Dataset split:\")\n",
        "    print(f\"  Training samples: {len(train_dataset)}\")\n",
        "    print(f\"  Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    return train_dataset, val_dataset, features_grid.shape[-1], features_graph.shape[-1], data_report, feature_index_map\n",
        "\n",
        "# Prepare data\n",
        "DEFAULT_FEATURE_SET_GRID = 'BASIC'\n",
        "DEFAULT_FEATURE_SET_GRAPH = 'KCE'\n",
        "\n",
        "train_dataset, val_dataset, n_features_grid, n_features_graph, data_report, FEATURE_INDEX = prepare_data(\n",
        "    DS, CONFIG, edge_index, edge_weight,\n",
        "    context_nodes=CONTEXT_NODES,\n",
        "    elevation_nodes=ELEVATION_NODES,\n",
        "    feature_set_grid=DEFAULT_FEATURE_SET_GRID,\n",
        "    feature_set_graph=DEFAULT_FEATURE_SET_GRAPH\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG.batch_size, shuffle=False)\n",
        "\n",
        "print(f\"\n",
        "Data loaders created:\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-6"
      },
      "source": [
        "## 6. V7-AMES Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "convlstm-branch",
        "outputId": "38609370-d9e9-4ebe-fb25-c8b1ff0891e6"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 6.1: EXPERT 2 - CONVLSTM (LOW ELEVATION)\n",
        "# =============================================================================\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    \"\"\"Single ConvLSTM cell with spatial convolutions.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, kernel_size: int):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        padding = kernel_size // 2\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=input_dim + hidden_dim,\n",
        "            out_channels=4 * hidden_dim,  # i, f, o, g gates\n",
        "            kernel_size=kernel_size,\n",
        "            padding=padding,\n",
        "            bias=True\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]):\n",
        "        h, c = state\n",
        "        combined = torch.cat([x, h], dim=1)\n",
        "        gates = self.conv(combined)\n",
        "\n",
        "        i, f, o, g = torch.split(gates, self.hidden_dim, dim=1)\n",
        "        i = torch.sigmoid(i)\n",
        "        f = torch.sigmoid(f)\n",
        "        o = torch.sigmoid(o)\n",
        "        g = torch.tanh(g)\n",
        "\n",
        "        c_new = f * c + i * g\n",
        "        h_new = o * torch.tanh(c_new)\n",
        "\n",
        "        return h_new, c_new\n",
        "\n",
        "    def init_hidden(self, batch_size: int, height: int, width: int, device: torch.device):\n",
        "        return (\n",
        "            torch.zeros(batch_size, self.hidden_dim, height, width, device=device),\n",
        "            torch.zeros(batch_size, self.hidden_dim, height, width, device=device)\n",
        "        )\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    \"\"\"Spatial attention mechanism for ConvLSTM output.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        attention = torch.sigmoid(self.conv(x))\n",
        "        return x * attention\n",
        "\n",
        "class ConvLSTMBranch(nn.Module):\n",
        "    \"\"\"Branch 1: ConvLSTM encoder for Euclidean spatial patterns.\n",
        "\n",
        "    Based on V2 Enhanced architecture with attention and residual connections.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: V7Config, n_features: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Input projection\n",
        "        self.input_proj = nn.Conv2d(\n",
        "            n_features, config.convlstm_filters[0],\n",
        "            kernel_size=1\n",
        "        )\n",
        "\n",
        "        # ConvLSTM layers\n",
        "        self.convlstm_cells_fw = nn.ModuleList()\n",
        "        self.convlstm_cells_bw = nn.ModuleList() if config.convlstm_bidirectional else None\n",
        "        in_dim = config.convlstm_filters[0]\n",
        "        for out_dim in config.convlstm_filters:\n",
        "            self.convlstm_cells_fw.append(\n",
        "                ConvLSTMCell(in_dim, out_dim, config.convlstm_kernel_size)\n",
        "            )\n",
        "            if config.convlstm_bidirectional:\n",
        "                self.convlstm_cells_bw.append(\n",
        "                    ConvLSTMCell(in_dim, out_dim, config.convlstm_kernel_size)\n",
        "                )\n",
        "            in_dim = out_dim\n",
        "\n",
        "        # Spatial attention\n",
        "        attn_in_dim = config.convlstm_filters[-1] * (2 if config.convlstm_bidirectional else 1)\n",
        "        if config.convlstm_attention:\n",
        "            self.attention = SpatialAttention(attn_in_dim)\n",
        "        else:\n",
        "            self.attention = None\n",
        "\n",
        "        # Output projection to match GNN branch\n",
        "        self.output_proj = nn.Conv2d(\n",
        "            config.convlstm_filters[-1] * (2 if config.convlstm_bidirectional else 1),\n",
        "            config.convlstm_output_dim,\n",
        "            kernel_size=1\n",
        "        )\n",
        "\n",
        "        # Residual connection\n",
        "        if config.convlstm_residual:\n",
        "            self.residual_proj = nn.Conv2d(n_features, config.convlstm_output_dim, kernel_size=1)\n",
        "        else:\n",
        "            self.residual_proj = None\n",
        "\n",
        "    def _run_convlstm(self, x: torch.Tensor, cells: nn.ModuleList) -> torch.Tensor:\n",
        "        \"\"\"Run ConvLSTM stack and return last hidden state.\"\"\"\n",
        "        batch_size, seq_len, _, h, w = x.shape\n",
        "        layer_input = x\n",
        "        for layer_idx, cell in enumerate(cells):\n",
        "            h_state, c_state = cell.init_hidden(batch_size, h, w, x.device)\n",
        "            outputs = []\n",
        "            for t in range(seq_len):\n",
        "                x_t = layer_input[:, t]\n",
        "                if layer_idx == 0:\n",
        "                    x_t = self.input_proj(x_t)\n",
        "                h_state, c_state = cell(x_t, (h_state, c_state))\n",
        "                outputs.append(h_state)\n",
        "            layer_input = torch.stack(outputs, dim=1)\n",
        "        return h_state\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, lat, lon, features)\n",
        "\n",
        "        Returns:\n",
        "            output: (batch, lat, lon, output_dim)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, h, w, _ = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # Reshape for Conv2d: (batch, seq, features, h, w)\n",
        "        x_reshaped = x.permute(0, 1, 4, 2, 3)\n",
        "\n",
        "        # Store residual\n",
        "        if self.residual_proj is not None:\n",
        "            residual = self.residual_proj(x_reshaped[:, -1])  # Use last timestep\n",
        "\n",
        "        # Forward direction\n",
        "        h_forward = self._run_convlstm(x_reshaped, self.convlstm_cells_fw)\n",
        "\n",
        "        if self.config.convlstm_bidirectional:\n",
        "            # Backward direction\n",
        "            x_backward = x_reshaped.flip(1)\n",
        "            h_backward = self._run_convlstm(x_backward, self.convlstm_cells_bw)\n",
        "            output = torch.cat([h_forward, h_backward], dim=1)\n",
        "        else:\n",
        "            output = h_forward\n",
        "\n",
        "        # Apply attention\n",
        "        if self.attention is not None:\n",
        "            output = self.attention(output)\n",
        "\n",
        "        # Project to output dimension\n",
        "        output = self.output_proj(output)  # (batch, output_dim, h, w)\n",
        "\n",
        "        # Add residual\n",
        "        if self.residual_proj is not None:\n",
        "            output = output + residual\n",
        "\n",
        "        # Reshape to (batch, h, w, output_dim)\n",
        "        output = output.permute(0, 2, 3, 1)\n",
        "\n",
        "        return output\n",
        "\n",
        "print(\"ConvLSTM Branch defined successfully\")\n",
        "\n",
        "\n",
        "class Expert2_LowElevation(nn.Module):\n",
        "    \"\"\"Expert 2: ConvLSTM specialist for low elevation.\"\"\"\n",
        "\n",
        "    def __init__(self, config: V7Config, n_features: int, n_lat: int, n_lon: int, horizon: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_lat = n_lat\n",
        "        self.n_lon = n_lon\n",
        "        self.horizon = horizon\n",
        "\n",
        "        self.encoder = ConvLSTMBranch(config, n_features)\n",
        "        self.head = nn.Conv2d(config.convlstm_output_dim, horizon, kernel_size=1)\n",
        "\n",
        "    def forward(self, x_grid, x_graph=None, edge_index=None, edge_weight=None):\n",
        "        # x_grid: (batch, seq, lat, lon, features)\n",
        "        features = self.encoder(x_grid)  # (batch, lat, lon, dim)\n",
        "        pred_grid = self.head(features.permute(0, 3, 1, 2))  # (batch, horizon, lat, lon)\n",
        "        pred_nodes = pred_grid.permute(0, 2, 3, 1).reshape(x_grid.size(0), -1, self.horizon)\n",
        "        return pred_nodes\n",
        "\n",
        "print(\"Expert 2 (ConvLSTM) defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-6-2"
      },
      "source": [
        "### 6.2 Expert 1: GNN-TAT (High Elevation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eb0188b-289a-4b2b-bdba-8f1a5af2da5f",
        "outputId": "1fa3a08e-4dbd-4077-ed61-41dc5adc6c5a"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 6.2: EXPERT 1 - GNN-TAT (HIGH ELEVATION)\n",
        "# =============================================================================\n",
        "# This is the corrected and optimized GNNBranch class for Cell 18 in\n",
        "# base_models_gnn_convlstm_stacking_v5.ipynb\n",
        "#\n",
        "# CRITICAL IMPROVEMENTS:\n",
        "# 1.  Per-layer GNN validation (after each graph convolution)\n",
        "# 2.  Message passing numerical stability checks\n",
        "# 3.  Temporal attention validation\n",
        "# 4.  Edge weight normalization validation\n",
        "# 5.  Gradient-safe operations\n",
        "# 6.  Detailed error messages with layer-specific diagnostics\n",
        "# 7.  Optional validation toggle\n",
        "# 8.  Memory-efficient implementation\n",
        "# 9.  FIXED: Handles 4D input (Batch, Time, Nodes, Features)\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def ensure_pyg_import():\n",
        "    try:\n",
        "        return GCNConv, GATConv, SAGEConv\n",
        "    except ModuleNotFoundError:\n",
        "        print(\"[WARN] torch_geometric not found. Installing...\")\n",
        "        import sys\n",
        "        import subprocess\n",
        "        torch_ver = torch.__version__.split(\"+\")[0]\n",
        "        cuda_ver = torch.version.cuda\n",
        "        cuda_tag = \"cpu\"\n",
        "        if cuda_ver:\n",
        "            cuda_tag = \"cu\" + cuda_ver.replace(\".\", \"\")\n",
        "        url = f\"https://data.pyg.org/whl/torch-{torch_ver}+{cuda_tag}.html\"\n",
        "        pkgs = [\n",
        "            \"torch-scatter\",\n",
        "            \"torch-sparse\",\n",
        "            \"torch-cluster\",\n",
        "            \"torch-spline-conv\",\n",
        "            \"torch-geometric\",\n",
        "        ]\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs + [\"-f\", url])\n",
        "        return GCNConv, GATConv, SAGEConv\n",
        "\n",
        "GCNConv, GATConv, SAGEConv = ensure_pyg_import()\n",
        "from typing import Tuple, Optional, Literal\n",
        "\n",
        "class GNNBranch(nn.Module):\n",
        "    \"\"\"Graph Neural Network branch for non-Euclidean spatial processing.\n",
        "\n",
        "    Architecture:\n",
        "        Processes precipitation data as graph:\n",
        "        - Nodes: Grid locations (lat, lon) with features\n",
        "        - Edges: Spatial relationships (distance-based or learned)\n",
        "        - Message passing: Aggregates neighbor information\n",
        "\n",
        "        Flow:\n",
        "        1. Input embedding: Project features to hidden dim\n",
        "        2. GNN layers: Multiple graph convolutions with residual connections\n",
        "        3. Temporal attention: Capture temporal dependencies\n",
        "        4. Output: Node features ready for fusion\n",
        "\n",
        "    Key Features:\n",
        "        - Multiple GNN architectures (GCN, GAT, SAGE)\n",
        "        - Residual connections for gradient flow\n",
        "        - Layer-wise validation (prevents error propagation)\n",
        "        - Numerical stability (gradient clipping, normalization)\n",
        "        - Handling of temporal sequences (4D input)\n",
        "\n",
        "    Improvements over Original:\n",
        "         Per-layer validation (catches errors early)\n",
        "         Message passing stability checks\n",
        "         Edge weight normalization validation\n",
        "         Temporal attention numerical stability\n",
        "         Gradient-safe aggregations\n",
        "         Detailed layer-specific error messages\n",
        "         4D Input Support (Batch, Time, Nodes, Features)\n",
        "\n",
        "    Args:\n",
        "        config: V7Config with GNN hyperparameters\n",
        "        gnn_type: Type of GNN ('GCN', 'GAT', or 'SAGE')\n",
        "        validate: Enable validation checks (default: True)\n",
        "\n",
        "    Input Shapes:\n",
        "        x: (batch, time, n_nodes, input_dim) - Node features over time\n",
        "        edge_index: (2, num_edges) - Graph connectivity\n",
        "        edge_weight: Optional (num_edges,) - Edge weights\n",
        "\n",
        "    Output Shape:\n",
        "        output: (batch, n_nodes, gnn_hidden_dim) - Processed node features (time-aggregated)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config,\n",
        "        n_features: int,\n",
        "        n_nodes: int,\n",
        "        gnn_type: str = None,\n",
        "        validate: bool = True\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_nodes = n_nodes\n",
        "        self.gnn_type = gnn_type or config.gnn_type\n",
        "        # Use config.validate if available, otherwise use parameter\n",
        "        self.validate = getattr(config, 'validate', validate)\n",
        "\n",
        "        # Cache for edge_index to avoid repeated CPU->GPU transfers\n",
        "        self._cached_edge_index = None\n",
        "        self._cached_edge_weight = None\n",
        "        self._cache_device = None\n",
        "\n",
        "        # Extract config parameters\n",
        "        input_dim = n_features\n",
        "        hidden_dim = config.gnn_hidden_dim\n",
        "        num_layers = config.gnn_num_layers\n",
        "        dropout = config.gnn_dropout\n",
        "        use_residual = True\n",
        "\n",
        "        # Store expected dimensions\n",
        "        self.expected_input_dim = input_dim\n",
        "        self.expected_hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.use_residual = use_residual\n",
        "\n",
        "        # =================================================================\n",
        "        # INPUT EMBEDDING\n",
        "        # =================================================================\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # =================================================================\n",
        "        # GNN LAYERS\n",
        "        # =================================================================\n",
        "        self.gnn_layers = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            if self.gnn_type == 'GCN':\n",
        "                self.gnn_layers.append(GCNConv(hidden_dim, hidden_dim))\n",
        "            elif self.gnn_type == 'GAT':\n",
        "                # GAT with 4 heads, concatenate outputs\n",
        "                num_heads = 4\n",
        "                head_out_dim = hidden_dim // num_heads\n",
        "                self.gnn_layers.append(\n",
        "                    GATConv(hidden_dim, head_out_dim, heads=num_heads, dropout=dropout, concat=True)\n",
        "                )\n",
        "            elif self.gnn_type == 'SAGE':\n",
        "                self.gnn_layers.append(SAGEConv(hidden_dim, hidden_dim))\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown GNN type: {self.gnn_type}. Choose from ['GCN', 'GAT', 'SAGE']\")\n",
        "\n",
        "        # Layer normalization for each GNN layer\n",
        "        self.layer_norms = nn.ModuleList([\n",
        "            nn.LayerNorm(hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # =================================================================\n",
        "        # TEMPORAL ATTENTION (Optional)\n",
        "        # =================================================================\n",
        "        if config.use_temporal_attention:\n",
        "            self.temporal_attn = nn.MultiheadAttention(\n",
        "                embed_dim=hidden_dim,\n",
        "                num_heads=config.temporal_num_heads,\n",
        "                dropout=dropout,\n",
        "                batch_first=True\n",
        "            )\n",
        "            self.temporal_norm = nn.LayerNorm(hidden_dim)\n",
        "        else:\n",
        "            self.temporal_attn = None\n",
        "\n",
        "    def _validate_tensor(\n",
        "        self,\n",
        "        tensor: torch.Tensor,\n",
        "        name: str,\n",
        "        expected_shape: Tuple[int, ...],\n",
        "        check_numerical: bool = True\n",
        "    ) -> None:\n",
        "        \"\"\"Validate tensor shape and numerical stability.\"\"\"\n",
        "        if not self.validate:\n",
        "            return\n",
        "\n",
        "        # Check shape (allow variable batch and nodes)\n",
        "        if len(tensor.shape) != len(expected_shape):\n",
        "            raise ValueError(\n",
        "                f\"[GNN VALIDATION] {name} has wrong number of dimensions!\\n\"\n",
        "                f\"  Expected: {len(expected_shape)} dimensions {expected_shape}\\n\"\n",
        "                f\"  Got: {len(tensor.shape)} dimensions {tensor.shape}\"\n",
        "            )\n",
        "\n",
        "        # Check last dimension (feature dimension)\n",
        "        if expected_shape[-1] != -1 and tensor.shape[-1] != expected_shape[-1]:\n",
        "            raise ValueError(\n",
        "                f\"[GNN VALIDATION] {name} feature dimension mismatch!\\n\"\n",
        "                f\"  Expected last dim: {expected_shape[-1]}\\n\"\n",
        "                f\"  Got: {tensor.shape[-1]}\\n\"\n",
        "                f\"  Full shape: {tensor.shape}\"\n",
        "            )\n",
        "\n",
        "        # Check numerical stability\n",
        "        if check_numerical:\n",
        "            if torch.isnan(tensor).any():\n",
        "                nan_count = torch.isnan(tensor).sum().item()\n",
        "                raise ValueError(\n",
        "                    f\"[GNN VALIDATION] {name} contains NaN!\\n\"\n",
        "                    f\"  Shape: {tensor.shape}\\n\"\n",
        "                    f\"  NaN count: {nan_count} / {tensor.numel()} ({100*nan_count/tensor.numel():.2f}%)\\n\"\n",
        "                    f\"  Recommendation: Check graph convolution or attention for numerical instability\"\n",
        "                )\n",
        "\n",
        "            if torch.isinf(tensor).any():\n",
        "                inf_count = torch.isinf(tensor).sum().item()\n",
        "                raise ValueError(\n",
        "                    f\"[GNN VALIDATION] {name} contains Inf!\\n\"\n",
        "                    f\"  Shape: {tensor.shape}\\n\"\n",
        "                    f\"  Inf count: {inf_count} / {tensor.numel()} ({100*inf_count/tensor.numel():.2f}%)\\n\"\n",
        "                    f\"  Recommendation: Check for division by zero in normalization or aggregation overflow\"\n",
        "                )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,  # (batch, time, n_nodes, input_dim)\n",
        "        edge_index: torch.Tensor,  # (2, num_edges)\n",
        "        edge_weight: Optional[torch.Tensor] = None  # (num_edges,)\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Process node features through GNN layers with temporal handling.\n",
        "\n",
        "        Args:\n",
        "            x: Node features (batch, time, n_nodes, input_dim)\n",
        "            edge_index: Graph connectivity (2, num_edges)\n",
        "            edge_weight: Optional edge weights (num_edges,)\n",
        "\n",
        "        Returns:\n",
        "            output: (batch, n_nodes, gnn_hidden_dim)\n",
        "        \"\"\"\n",
        "        # Handle 4D input (Batch, Time, Nodes, Features)\n",
        "        if x.dim() == 4:\n",
        "            batch_size, seq_len, n_nodes, input_dim = x.shape\n",
        "            # Flatten Batch and Time for GNN (GNN is spatial-only)\n",
        "            # (B, T, N, F) -> (B*T, N, F)\n",
        "            x_flat = x.view(batch_size * seq_len, n_nodes, input_dim)\n",
        "            is_sequence = True\n",
        "        else:\n",
        "            # Handle 3D input (Batch, Nodes, Features)\n",
        "            batch_size, n_nodes, input_dim = x.shape\n",
        "            x_flat = x\n",
        "            seq_len = 1\n",
        "            is_sequence = False\n",
        "\n",
        "        # =====================================================================\n",
        "        # =====================================================================\n",
        "        if self.validate:\n",
        "            try:\n",
        "                # Validate node features\n",
        "                self._validate_tensor(x_flat, \"gnn_input_flat\", (batch_size * seq_len, n_nodes, self.expected_input_dim))\n",
        "\n",
        "                # Validate edge_index\n",
        "                if edge_index.dim() != 2 or edge_index.shape[0] != 2:\n",
        "                    raise ValueError(f\"[GNN VALIDATION] edge_index must be (2, num_edges), got {edge_index.shape}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n{'='*80}\\n GNN INPUT VALIDATION FAILED\\n{'='*80}\\nError: {e}\\n\")\n",
        "                raise\n",
        "\n",
        "        # =====================================================================\n",
        "        # SPATIAL PROCESSING (GNN)\n",
        "        # =====================================================================\n",
        "        # Cache edge_index on correct device to avoid repeated transfers\n",
        "        device = x_flat.device\n",
        "        if self._cached_edge_index is None or self._cache_device != device:\n",
        "            self._cached_edge_index = edge_index.to(device)\n",
        "            self._cached_edge_weight = edge_weight.to(device) if edge_weight is not None else None\n",
        "            self._cache_device = device\n",
        "\n",
        "        edge_index = self._cached_edge_index\n",
        "        edge_weight = self._cached_edge_weight\n",
        "\n",
        "        # Project to hidden dimension\n",
        "        h = self.input_proj(x_flat)  # (B*T, N, hidden)\n",
        "\n",
        "        for layer_idx, (gnn_layer, norm_layer) in enumerate(zip(self.gnn_layers, self.layer_norms)):\n",
        "            h_input = h\n",
        "            try:\n",
        "                # Batched GNN processing\n",
        "                # Note: For strict correctness with disjoint graphs, we should use Batch object,\n",
        "                # but here we iterate or rely on broadcast if edges are shared.\n",
        "                # Since edges are shared across all samples (static graph), we can simply process.\n",
        "                # However, PyG layers usually expect (TotalNodes, Features) and (2, TotalEdges).\n",
        "                # Here h is (Batch, Nodes, Features).\n",
        "                # We loop over batch dim to avoid constructing giant diagonal adjacency matrix.\n",
        "\n",
        "                current_batch_size = h.shape[0]\n",
        "                h_list = []\n",
        "                for b in range(current_batch_size):\n",
        "                    h_b = h[b]\n",
        "                    if edge_weight is not None:\n",
        "                        h_out = gnn_layer(h_b, edge_index, edge_weight)\n",
        "                    else:\n",
        "                        h_out = gnn_layer(h_b, edge_index)\n",
        "                    h_list.append(h_out)\n",
        "                h = torch.stack(h_list, dim=0)\n",
        "\n",
        "                # Activation, Norm, Dropout, Residual\n",
        "                h = torch.relu(h)\n",
        "                h = norm_layer(h)\n",
        "                h = self.dropout(h)\n",
        "                if self.use_residual and layer_idx > 0:\n",
        "                    h = h + h_input\n",
        "\n",
        "            except Exception as e:\n",
        "                if self.validate:\n",
        "                    print(f\"[GNN ERROR] Layer {layer_idx} failed: {e}\")\n",
        "                raise\n",
        "\n",
        "        # =====================================================================\n",
        "        # TEMPORAL PROCESSING\n",
        "        # =====================================================================\n",
        "        if is_sequence:\n",
        "            # Reshape back to (B, T, N, H)\n",
        "            h = h.view(batch_size, seq_len, n_nodes, self.expected_hidden_dim)\n",
        "\n",
        "            if self.temporal_attn is not None:\n",
        "                try:\n",
        "                    # Prepare for MultiheadAttention: (Batch*Nodes, Time, Hidden)\n",
        "                    # Permute: (B, T, N, H) -> (B, N, T, H) -> (B*N, T, H)\n",
        "                    h_time = h.permute(0, 2, 1, 3).reshape(batch_size * n_nodes, seq_len, self.expected_hidden_dim)\n",
        "\n",
        "                    # Self-Attention over time\n",
        "                    # h_time: (Sequence, Features) logic for MHA\n",
        "                    # attn_out: (B*N, T, H)\n",
        "                    h_attn, _ = self.temporal_attn(h_time, h_time, h_time)\n",
        "\n",
        "                    # Residual + Norm\n",
        "                    h_time = h_time + h_attn\n",
        "                    h_time = self.temporal_norm(h_time)\n",
        "\n",
        "                    # Take last timestep as representation (or pool)\n",
        "                    # (B*N, H)\n",
        "                    h_final = h_time[:, -1, :]\n",
        "\n",
        "                    # Reshape back to (B, N, H)\n",
        "                    h = h_final.view(batch_size, n_nodes, self.expected_hidden_dim)\n",
        "\n",
        "                except Exception as e:\n",
        "                    if self.validate:\n",
        "                        print(f\"[GNN ERROR] Temporal Attention failed: {e}\")\n",
        "                    raise\n",
        "            else:\n",
        "                # If no temporal attention, just take last timestep or mean\n",
        "                h = h[:, -1, :, :]\n",
        "\n",
        "        # =====================================================================\n",
        "        # FINAL VALIDATION\n",
        "        # =====================================================================\n",
        "        if self.validate:\n",
        "            self._validate_tensor(h, \"gnn_final_output\", (batch_size, n_nodes, self.expected_hidden_dim))\n",
        "\n",
        "        return h\n",
        "\n",
        "print(\" Enhanced GNNBranch with 4D Input Support defined successfully\")\n",
        "\n",
        "class Expert1_HighElevation(nn.Module):\n",
        "    \"\"\"Expert 1: GNN-TAT specialist for high elevation.\"\"\"\n",
        "\n",
        "    def __init__(self, config: V7Config, n_features: int, n_nodes: int, horizon: int):\n",
        "        super().__init__()\n",
        "        self.gnn = GNNBranch(config, n_features, n_nodes, gnn_type=config.gnn_type, validate=config.validate)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(config.gnn_hidden_dim, max(8, config.gnn_hidden_dim // 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.gnn_dropout),\n",
        "            nn.Linear(max(8, config.gnn_hidden_dim // 2), horizon)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_grid, x_graph, edge_index, edge_weight):\n",
        "        # x_graph: (batch, seq, n_nodes, features)\n",
        "        gnn_out = self.gnn(x_graph, edge_index, edge_weight)  # (batch, n_nodes, hidden)\n",
        "        preds = self.head(gnn_out)\n",
        "        return preds\n",
        "\n",
        "print(\"Expert 1 (GNN-TAT) defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-6-3"
      },
      "source": [
        "### 6.3 Expert 3: Hybrid Transition Specialist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8f0176b-9931-4a73-8432-4a7022c2c125",
        "outputId": "e0298276-41e1-4e62-a804-44d8e866e0e2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    GATConv\n",
        "except NameError:\n",
        "    def ensure_pyg_import():\n",
        "        try:\n",
        "            from torch_geometric.nn import GATConv\n",
        "            return GATConv\n",
        "        except ModuleNotFoundError:\n",
        "            print(\"[WARN] torch_geometric not found. Installing...\")\n",
        "            import sys\n",
        "            import subprocess\n",
        "            torch_ver = torch.__version__.split(\"+\")[0]\n",
        "            cuda_ver = torch.version.cuda\n",
        "            cuda_tag = \"cpu\"\n",
        "            if cuda_ver:\n",
        "                cuda_tag = \"cu\" + cuda_ver.replace(\".\", \"\")\n",
        "            url = f\"https://data.pyg.org/whl/torch-{torch_ver}+{cuda_tag}.html\"\n",
        "            pkgs = [\n",
        "                \"torch-scatter\",\n",
        "                \"torch-sparse\",\n",
        "                \"torch-cluster\",\n",
        "                \"torch-spline-conv\",\n",
        "                \"torch-geometric\",\n",
        "            ]\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs + [\"-f\", url])\n",
        "            from torch_geometric.nn import GATConv\n",
        "            return GATConv\n",
        "    GATConv = ensure_pyg_import()\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 6.3: EXPERT 3 - HYBRID (TRANSITION ZONE)\n",
        "# =============================================================================\n",
        "\n",
        "class Expert3_Transition(nn.Module):\n",
        "    \"\"\"Expert 3: Hybrid GNN + Conv for transition zone.\"\"\"\n",
        "\n",
        "    def __init__(self, config: V7Config, n_features_grid: int, n_features_graph: int,\n",
        "                 n_lat: int, n_lon: int, horizon: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_lat = n_lat\n",
        "        self.n_lon = n_lon\n",
        "        self.horizon = horizon\n",
        "\n",
        "        # Lightweight GNN branch\n",
        "        self.gat = GATConv(\n",
        "            in_channels=n_features_graph,\n",
        "            out_channels=config.hybrid_gnn_hidden,\n",
        "            heads=1,\n",
        "            concat=False,\n",
        "            dropout=config.hybrid_dropout\n",
        "        )\n",
        "\n",
        "        # Lightweight Conv branch\n",
        "        conv_layers = [\n",
        "            nn.Conv2d(n_features_grid, config.hybrid_conv_hidden, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.hybrid_dropout)\n",
        "        ]\n",
        "        for _ in range(config.hybrid_conv_layers - 1):\n",
        "            conv_layers.extend([\n",
        "                nn.Conv2d(config.hybrid_conv_hidden, config.hybrid_conv_hidden, kernel_size=3, padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(config.hybrid_dropout)\n",
        "            ])\n",
        "        self.conv = nn.Sequential(*conv_layers)\n",
        "\n",
        "        # Fusion and output\n",
        "        self.fusion = nn.Linear(config.hybrid_gnn_hidden + config.hybrid_conv_hidden,\n",
        "                                config.hybrid_gnn_hidden)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.hybrid_gnn_hidden, horizon)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_grid, x_graph, edge_index, edge_weight=None):\n",
        "        batch_size = x_grid.size(0)\n",
        "        n_nodes = self.n_lat * self.n_lon\n",
        "\n",
        "        # Graph branch (use last timestep)\n",
        "        x_graph_last = x_graph[:, -1]  # (batch, n_nodes, features)\n",
        "        x_flat = x_graph_last.reshape(batch_size * n_nodes, -1)\n",
        "        gnn_out = self.gat(x_flat, edge_index, edge_weight)\n",
        "        gnn_out = F.elu(gnn_out).view(batch_size, n_nodes, -1)\n",
        "\n",
        "        # Conv branch (use last timestep)\n",
        "        x_grid_last = x_grid[:, -1].permute(0, 3, 1, 2)\n",
        "        conv_out = self.conv(x_grid_last)  # (batch, hidden, lat, lon)\n",
        "        conv_nodes = conv_out.permute(0, 2, 3, 1).reshape(batch_size, n_nodes, -1)\n",
        "\n",
        "        # Fusion\n",
        "        fused = torch.cat([gnn_out, conv_nodes], dim=-1)\n",
        "        fused = self.fusion(fused)\n",
        "        preds = self.head(fused)\n",
        "\n",
        "        return preds\n",
        "\n",
        "print(\"Expert 3 (Hybrid) defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-6-4"
      },
      "source": [
        "### 6.4 Physics-Guided Gating\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d05611ab-0e93-43ce-8d81-44f7a660b2e3",
        "outputId": "7d8d30d7-b7cd-4f7e-cf69-5e3b64146dde"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 6.4: PHYSICS-GUIDED GATING NETWORK\n",
        "# =============================================================================\n",
        "\n",
        "class PhysicsGuidedGating(nn.Module):\n",
        "    \"\"\"Physics-guided gating network for expert routing.\"\"\"\n",
        "\n",
        "    def __init__(self, config: V7Config, context_dim: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        init_alpha = min(max(config.physics_prior_weight_init, 1e-3), 1 - 1e-3)\n",
        "        self.alpha_logit = nn.Parameter(torch.log(torch.tensor(init_alpha / (1 - init_alpha))))\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(context_dim, config.gating_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.gating_dropout),\n",
        "            nn.Linear(config.gating_hidden_dim, 3)\n",
        "        )\n",
        "\n",
        "    def compute_physics_priors(self, elevation: torch.Tensor) -> torch.Tensor:\n",
        "        # elevation: (batch, n_nodes)\n",
        "        w1 = torch.sigmoid((elevation - self.config.high_elev_threshold) / 300.0)\n",
        "        w2 = torch.sigmoid((self.config.low_elev_threshold - elevation) / 300.0)\n",
        "\n",
        "        center = (self.config.medium_elev_min + self.config.medium_elev_max) / 2.0\n",
        "        sigma = max(1.0, (self.config.medium_elev_max - self.config.medium_elev_min) / 4.0)\n",
        "        w3 = torch.exp(-((elevation - center) ** 2) / (2 * sigma ** 2))\n",
        "\n",
        "        priors = torch.stack([w1, w2, w3], dim=-1)\n",
        "        priors = F.softmax(priors, dim=-1)\n",
        "        return priors\n",
        "\n",
        "    def forward(self, elevation: torch.Tensor, context: torch.Tensor):\n",
        "        data_logits = self.mlp(context)\n",
        "        data_weights = F.softmax(data_logits, dim=-1)\n",
        "        physics_weights = self.compute_physics_priors(elevation)\n",
        "\n",
        "        alpha = torch.sigmoid(self.alpha_logit)\n",
        "        weights = alpha * physics_weights + (1.0 - alpha) * data_weights\n",
        "        weights = weights / (weights.sum(dim=-1, keepdim=True) + 1e-6)\n",
        "\n",
        "        return weights, alpha\n",
        "\n",
        "print(\"Physics-guided gating network defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-6-5"
      },
      "source": [
        "### 6.5 Physics-Informed Meta-Learner and Full V7 Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5-model",
        "outputId": "cbf9ffa9-f84e-4f2a-b868-33bce95df986"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 6.5: META-LEARNER + COMPLETE V7-AMES\n",
        "# =============================================================================\n",
        "\n",
        "class PhysicsInformedMetaLearner(nn.Module):\n",
        "    \"\"\"Physics-informed meta-learner for expert fusion.\"\"\"\n",
        "\n",
        "    def __init__(self, config: V7Config, context_dim: int, horizon: int):\n",
        "        super().__init__()\n",
        "        self.horizon = horizon\n",
        "        self.residual_mlp = nn.Sequential(\n",
        "            nn.Linear(horizon * 3 + context_dim, config.meta_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.meta_dropout),\n",
        "            nn.Linear(config.meta_hidden_dim, config.meta_hidden_dim2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.meta_dropout),\n",
        "            nn.Linear(config.meta_hidden_dim2, horizon)\n",
        "        )\n",
        "\n",
        "        self.orographic_enhancement = nn.Parameter(torch.tensor(0.1))\n",
        "        self.rain_shadow_suppression = nn.Parameter(torch.tensor(0.1))\n",
        "\n",
        "    def forward(self, expert_preds, gating_weights, context):\n",
        "        # expert_preds: list of (batch, n_nodes, horizon)\n",
        "        stacked = torch.stack(expert_preds, dim=-1)  # (batch, n_nodes, horizon, 3)\n",
        "        weighted = (stacked * gating_weights.unsqueeze(2)).sum(dim=-1)\n",
        "\n",
        "        preds_flat = torch.cat(expert_preds, dim=-1)  # (batch, n_nodes, horizon*3)\n",
        "        residual_in = torch.cat([preds_flat, context], dim=-1)\n",
        "        residual = self.residual_mlp(residual_in)\n",
        "\n",
        "        # Simple physics correction using normalized elevation and slope if available\n",
        "        elev_norm = context[..., 0]\n",
        "        if context.size(-1) > 1:\n",
        "            slope_norm = context[..., 1]\n",
        "        else:\n",
        "            slope_norm = torch.zeros_like(elev_norm)\n",
        "\n",
        "        oro = torch.relu(elev_norm) * torch.relu(slope_norm)\n",
        "        shadow = torch.relu(-slope_norm) * torch.relu(elev_norm)\n",
        "        correction = (self.orographic_enhancement * oro - self.rain_shadow_suppression * shadow).unsqueeze(-1)\n",
        "        correction = correction.expand(-1, -1, self.horizon)\n",
        "\n",
        "        return weighted + residual + correction\n",
        "\n",
        "\n",
        "class V7_AMES(nn.Module):\n",
        "    \"\"\"Complete V7-AMES model.\"\"\"\n",
        "\n",
        "    def __init__(self, config: V7Config, n_features_grid: int, n_features_graph: int,\n",
        "                 context_dim: int, n_lat: int, n_lon: int, horizon: int):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n_lat = n_lat\n",
        "        self.n_lon = n_lon\n",
        "        self.n_nodes = n_lat * n_lon\n",
        "        self.horizon = horizon\n",
        "\n",
        "        self.expert1 = Expert1_HighElevation(config, n_features_graph, self.n_nodes, horizon)\n",
        "        self.expert2 = Expert2_LowElevation(config, n_features_grid, n_lat, n_lon, horizon)\n",
        "        self.expert3 = Expert3_Transition(config, n_features_grid, n_features_graph, n_lat, n_lon, horizon)\n",
        "\n",
        "        self.gating = PhysicsGuidedGating(config, context_dim)\n",
        "        self.meta = PhysicsInformedMetaLearner(config, context_dim, horizon)\n",
        "\n",
        "    def _nodes_to_grid(self, preds_nodes: torch.Tensor) -> torch.Tensor:\n",
        "        # preds_nodes: (batch, n_nodes, horizon)\n",
        "        pred_grid = preds_nodes.view(preds_nodes.size(0), self.n_lat, self.n_lon, self.horizon)\n",
        "        pred_grid = pred_grid.permute(0, 3, 1, 2)  # (batch, horizon, lat, lon)\n",
        "        return pred_grid\n",
        "\n",
        "    def forward(self, x_grid, x_graph, edge_index, edge_weight, context, elevation, stage='full'):\n",
        "        pred1 = self.expert1(x_grid, x_graph, edge_index, edge_weight)\n",
        "        pred2 = self.expert2(x_grid, x_graph, edge_index, edge_weight)\n",
        "        pred3 = self.expert3(x_grid, x_graph, edge_index, edge_weight)\n",
        "\n",
        "        if stage == 'stage1':\n",
        "            return {'expert1': pred1, 'expert2': pred2, 'expert3': pred3}\n",
        "\n",
        "        gating_weights, alpha = self.gating(elevation, context)\n",
        "\n",
        "        if stage == 'stage2':\n",
        "            stacked = torch.stack([pred1, pred2, pred3], dim=-1)\n",
        "            weighted = (stacked * gating_weights.unsqueeze(2)).sum(dim=-1)\n",
        "            return self._nodes_to_grid(weighted), {'gating_weights': gating_weights, 'alpha': alpha}\n",
        "\n",
        "        final_nodes = self.meta([pred1, pred2, pred3], gating_weights, context)\n",
        "        return self._nodes_to_grid(final_nodes), {'gating_weights': gating_weights, 'alpha': alpha}\n",
        "\n",
        "    def freeze_experts(self):\n",
        "        for param in self.expert1.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.expert2.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.expert3.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_all(self):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def count_parameters(self) -> Dict[str, int]:\n",
        "        counts = {\n",
        "            'expert1': sum(p.numel() for p in self.expert1.parameters()),\n",
        "            'expert2': sum(p.numel() for p in self.expert2.parameters()),\n",
        "            'expert3': sum(p.numel() for p in self.expert3.parameters()),\n",
        "            'gating': sum(p.numel() for p in self.gating.parameters()),\n",
        "            'meta': sum(p.numel() for p in self.meta.parameters())\n",
        "        }\n",
        "        counts['total'] = sum(counts.values())\n",
        "        return counts\n",
        "\n",
        "\n",
        "def physics_informed_loss(pred_nodes, target_nodes, elevation, config: V7Config):\n",
        "    \"\"\"Compute physics-informed loss.\"\"\"\n",
        "    mse_loss = F.mse_loss(pred_nodes, target_nodes)\n",
        "\n",
        "    pred_sum = pred_nodes.sum(dim=(1, 2))\n",
        "    target_sum = target_nodes.sum(dim=(1, 2))\n",
        "    mass_loss = torch.abs(pred_sum - target_sum) / (target_sum + 1e-6)\n",
        "    mass_loss = mass_loss.mean()\n",
        "\n",
        "    if elevation.dim() == 2:\n",
        "        high_mask = elevation[0] >= config.high_elev_threshold\n",
        "    else:\n",
        "        high_mask = elevation >= config.high_elev_threshold\n",
        "\n",
        "    if high_mask.any():\n",
        "        pred_high = pred_nodes[:, high_mask, :]\n",
        "        target_high = target_nodes[:, high_mask, :]\n",
        "        oro_loss = F.relu(target_high - pred_high).mean()\n",
        "    else:\n",
        "        oro_loss = torch.tensor(0.0, device=pred_nodes.device)\n",
        "\n",
        "    total = mse_loss + config.lambda_mass_conservation * mass_loss + config.lambda_orographic * oro_loss\n",
        "    components = {\n",
        "        'mse': mse_loss.item(),\n",
        "        'mass': mass_loss.item(),\n",
        "        'orographic': oro_loss.item()\n",
        "    }\n",
        "    return total, components\n",
        "\n",
        "print(\"V7-AMES model defined successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-7"
      },
      "source": [
        "## 7. Training Infrastructure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trainer",
        "outputId": "80ff8b3e-8d72-4ecb-ea2b-882e5d40e056"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 7: TRAINING INFRASTRUCTURE\n",
        "# =============================================================================\n",
        "\n",
        "def build_context_batch(x_graph: torch.Tensor, context_static: torch.Tensor, feature_index_map: Dict[str, int]):\n",
        "    \"\"\"Combine static context with temporal seasonality features from x_graph.\"\"\"\n",
        "    context = context_static\n",
        "    if feature_index_map is None:\n",
        "        return context\n",
        "\n",
        "    if 'month_sin' in feature_index_map and 'month_cos' in feature_index_map:\n",
        "        idx_sin = feature_index_map['month_sin']\n",
        "        idx_cos = feature_index_map['month_cos']\n",
        "        month_sin = x_graph[:, :, :, idx_sin].mean(dim=1)\n",
        "        month_cos = x_graph[:, :, :, idx_cos].mean(dim=1)\n",
        "        temporal = torch.stack([month_sin, month_cos], dim=-1)\n",
        "        context = torch.cat([context_static, temporal], dim=-1)\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "def reshape_targets_to_nodes(y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Reshape grid targets to node format.\"\"\"\n",
        "    # y: (batch, horizon, lat, lon)\n",
        "    return y.permute(0, 2, 3, 1).reshape(y.size(0), -1, y.size(1))\n",
        "\n",
        "\n",
        "def apply_grid_mask(x_grid: torch.Tensor, mask_grid: torch.Tensor) -> torch.Tensor:\n",
        "    if mask_grid is None:\n",
        "        return x_grid\n",
        "    return x_grid * mask_grid.view(1, 1, mask_grid.size(0), mask_grid.size(1), 1)\n",
        "\n",
        "\n",
        "def apply_node_mask(x_graph: torch.Tensor, mask_nodes: torch.Tensor) -> torch.Tensor:\n",
        "    if mask_nodes is None:\n",
        "        return x_graph\n",
        "    x_graph = x_graph.clone()\n",
        "    x_graph[:, :, ~mask_nodes, :] = 0\n",
        "    return x_graph\n",
        "\n",
        "\n",
        "def masked_mse_loss(preds: torch.Tensor, targets: torch.Tensor, mask_nodes: torch.Tensor) -> torch.Tensor:\n",
        "    if mask_nodes is None:\n",
        "        return F.mse_loss(preds, targets)\n",
        "    if mask_nodes.sum() == 0:\n",
        "        return F.mse_loss(preds, targets)\n",
        "    preds_m = preds[:, mask_nodes, :]\n",
        "    targets_m = targets[:, mask_nodes, :]\n",
        "    return F.mse_loss(preds_m, targets_m)\n",
        "\n",
        "\n",
        "def _get_edges(batch, device: torch.device):\n",
        "    edge_index = batch['edge_index'][0].to(device)\n",
        "    edge_weight = batch['edge_weight'][0].to(device)\n",
        "    return edge_index, edge_weight\n",
        "\n",
        "\n",
        "def train_expert_stage1(\n",
        "    expert: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    mask_nodes: np.ndarray,\n",
        "    config: V7Config,\n",
        "    device: torch.device,\n",
        "    output_dir: Path,\n",
        "    name: str\n",
        "):\n",
        "    \"\"\"Train a single expert with elevation mask.\"\"\"\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        expert.parameters(),\n",
        "        lr=config.lr_stage1,\n",
        "        weight_decay=config.weight_decay\n",
        "    )\n",
        "    best_val = float('inf')\n",
        "    patience = 0\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    mask_nodes_t = None\n",
        "    mask_grid_t = None\n",
        "    if mask_nodes is not None:\n",
        "        mask_nodes_t = torch.as_tensor(mask_nodes, device=device)\n",
        "        mask_grid_t = mask_nodes_t.view(config.n_lat, config.n_lon)\n",
        "\n",
        "    for epoch in range(config.epochs_stage1):\n",
        "        expert.train()\n",
        "        total_loss = 0.0\n",
        "        n_batches = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            x_grid = batch['x_grid'].to(device)\n",
        "            x_graph = batch['x_graph'].to(device)\n",
        "            y = batch['y'].to(device)\n",
        "            edge_index, edge_weight = _get_edges(batch, device)\n",
        "\n",
        "            if mask_grid_t is not None:\n",
        "                x_grid = apply_grid_mask(x_grid, mask_grid_t)\n",
        "            if mask_nodes_t is not None:\n",
        "                x_graph = apply_node_mask(x_graph, mask_nodes_t)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = expert(x_grid, x_graph, edge_index, edge_weight)\n",
        "            targets = reshape_targets_to_nodes(y)\n",
        "            loss = masked_mse_loss(preds, targets, mask_nodes_t)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(expert.parameters(), config.gradient_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "\n",
        "        avg_train = total_loss / max(1, n_batches)\n",
        "        history['train_loss'].append(avg_train)\n",
        "\n",
        "        # Validation\n",
        "        expert.eval()\n",
        "        val_loss = 0.0\n",
        "        val_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                x_grid = batch['x_grid'].to(device)\n",
        "                x_graph = batch['x_graph'].to(device)\n",
        "                y = batch['y'].to(device)\n",
        "                edge_index, edge_weight = _get_edges(batch, device)\n",
        "\n",
        "                if mask_grid_t is not None:\n",
        "                    x_grid = apply_grid_mask(x_grid, mask_grid_t)\n",
        "                if mask_nodes_t is not None:\n",
        "                    x_graph = apply_node_mask(x_graph, mask_nodes_t)\n",
        "\n",
        "                preds = expert(x_grid, x_graph, edge_index, edge_weight)\n",
        "                targets = reshape_targets_to_nodes(y)\n",
        "                loss = masked_mse_loss(preds, targets, mask_nodes_t)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_val = val_loss / max(1, val_batches)\n",
        "        history['val_loss'].append(avg_val)\n",
        "\n",
        "        print(f\"{name} Epoch {epoch+1}: Train={avg_train:.4f} Val={avg_val:.4f}\")\n",
        "\n",
        "        if avg_val < best_val:\n",
        "            best_val = avg_val\n",
        "            patience = 0\n",
        "            ckpt = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': expert.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': avg_train,\n",
        "                'val_loss': avg_val\n",
        "            }\n",
        "            torch.save(ckpt, output_dir / f'{name}_best.pt')\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= config.patience_stage1:\n",
        "                print(f\"Early stopping {name} at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return history, best_val\n",
        "\n",
        "\n",
        "def train_stage2_gating(\n",
        "    model: V7_AMES,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    config: V7Config,\n",
        "    device: torch.device,\n",
        "    output_dir: Path,\n",
        "    feature_index_map: Dict[str, int]\n",
        "):\n",
        "    \"\"\"Train gating network with experts frozen.\"\"\"\n",
        "    model.freeze_experts()\n",
        "    for param in model.meta.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.gating.parameters(),\n",
        "        lr=config.lr_stage2,\n",
        "        weight_decay=config.weight_decay\n",
        "    )\n",
        "    best_val = float('inf')\n",
        "    patience = 0\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(config.epochs_stage2):\n",
        "        model.gating.train()\n",
        "        model.expert1.eval()\n",
        "        model.expert2.eval()\n",
        "        model.expert3.eval()\n",
        "        model.meta.eval()\n",
        "        total_loss = 0.0\n",
        "        n_batches = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            x_grid = batch['x_grid'].to(device)\n",
        "            x_graph = batch['x_graph'].to(device)\n",
        "            y = batch['y'].to(device)\n",
        "            context_static = batch['context'].to(device)\n",
        "            elevation = batch['elevation'].to(device)\n",
        "            edge_index, edge_weight = _get_edges(batch, device)\n",
        "\n",
        "            context = build_context_batch(x_graph, context_static, feature_index_map)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds, aux = model(x_grid, x_graph, edge_index, edge_weight, context, elevation, stage='stage2')\n",
        "            loss = F.mse_loss(preds, y)\n",
        "\n",
        "            # Diversity penalty (encourage non-collapsed routing)\n",
        "            weights = aux['gating_weights']\n",
        "            entropy = -(weights * torch.log(weights + 1e-8)).sum(dim=-1).mean()\n",
        "            loss = loss - config.gating_diversity_lambda * entropy\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.gating.parameters(), config.gradient_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "\n",
        "        avg_train = total_loss / max(1, n_batches)\n",
        "        history['train_loss'].append(avg_train)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                x_grid = batch['x_grid'].to(device)\n",
        "                x_graph = batch['x_graph'].to(device)\n",
        "                y = batch['y'].to(device)\n",
        "                context_static = batch['context'].to(device)\n",
        "                elevation = batch['elevation'].to(device)\n",
        "                edge_index, edge_weight = _get_edges(batch, device)\n",
        "\n",
        "                context = build_context_batch(x_graph, context_static, feature_index_map)\n",
        "                preds, _ = model(x_grid, x_graph, edge_index, edge_weight, context, elevation, stage='stage2')\n",
        "                loss = F.mse_loss(preds, y)\n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_val = val_loss / max(1, val_batches)\n",
        "        history['val_loss'].append(avg_val)\n",
        "        print(f\"Stage2 Epoch {epoch+1}: Train={avg_train:.4f} Val={avg_val:.4f}\")\n",
        "\n",
        "        if avg_val < best_val:\n",
        "            best_val = avg_val\n",
        "            patience = 0\n",
        "            ckpt = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': avg_train,\n",
        "                'val_loss': avg_val\n",
        "            }\n",
        "            torch.save(ckpt, output_dir / 'v7_stage2_best.pt')\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= config.patience_stage2:\n",
        "                print(f\"Early stopping Stage 2 at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return history, best_val\n",
        "\n",
        "\n",
        "def train_stage3_joint(\n",
        "    model: V7_AMES,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    config: V7Config,\n",
        "    device: torch.device,\n",
        "    output_dir: Path,\n",
        "    feature_index_map: Dict[str, int]\n",
        "):\n",
        "    \"\"\"Joint fine-tuning with physics-informed loss.\"\"\"\n",
        "    model.unfreeze_all()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=config.lr_stage3,\n",
        "        weight_decay=config.weight_decay\n",
        "    )\n",
        "    best_val = float('inf')\n",
        "    patience = 0\n",
        "    history = {'train_loss': [], 'val_loss': [], 'train_mse': [], 'train_mass': [], 'train_oro': []}\n",
        "\n",
        "    for epoch in range(config.epochs_stage3):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        total_mse = 0.0\n",
        "        total_mass = 0.0\n",
        "        total_oro = 0.0\n",
        "        n_batches = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            x_grid = batch['x_grid'].to(device)\n",
        "            x_graph = batch['x_graph'].to(device)\n",
        "            y = batch['y'].to(device)\n",
        "            context_static = batch['context'].to(device)\n",
        "            elevation = batch['elevation'].to(device)\n",
        "            edge_index, edge_weight = _get_edges(batch, device)\n",
        "\n",
        "            context = build_context_batch(x_graph, context_static, feature_index_map)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds, _ = model(x_grid, x_graph, edge_index, edge_weight, context, elevation, stage='full')\n",
        "            pred_nodes = reshape_targets_to_nodes(preds)\n",
        "            target_nodes = reshape_targets_to_nodes(y)\n",
        "\n",
        "            loss, components = physics_informed_loss(pred_nodes, target_nodes, elevation, config)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_mse += components['mse']\n",
        "            total_mass += components['mass']\n",
        "            total_oro += components['orographic']\n",
        "            n_batches += 1\n",
        "\n",
        "        avg_train = total_loss / max(1, n_batches)\n",
        "        history['train_loss'].append(avg_train)\n",
        "        history['train_mse'].append(total_mse / max(1, n_batches))\n",
        "        history['train_mass'].append(total_mass / max(1, n_batches))\n",
        "        history['train_oro'].append(total_oro / max(1, n_batches))\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_batches = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                x_grid = batch['x_grid'].to(device)\n",
        "                x_graph = batch['x_graph'].to(device)\n",
        "                y = batch['y'].to(device)\n",
        "                context_static = batch['context'].to(device)\n",
        "                elevation = batch['elevation'].to(device)\n",
        "                edge_index, edge_weight = _get_edges(batch, device)\n",
        "\n",
        "                context = build_context_batch(x_graph, context_static, feature_index_map)\n",
        "                preds, _ = model(x_grid, x_graph, edge_index, edge_weight, context, elevation, stage='full')\n",
        "                pred_nodes = reshape_targets_to_nodes(preds)\n",
        "                target_nodes = reshape_targets_to_nodes(y)\n",
        "\n",
        "                loss, _ = physics_informed_loss(pred_nodes, target_nodes, elevation, config)\n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_val = val_loss / max(1, val_batches)\n",
        "        history['val_loss'].append(avg_val)\n",
        "\n",
        "        print(\n",
        "            f\"Stage3 Epoch {epoch+1}: Train={avg_train:.4f} Val={avg_val:.4f} \"\n",
        "            f\"(MSE={history['train_mse'][-1]:.4f}, Mass={history['train_mass'][-1]:.4f}, Oro={history['train_oro'][-1]:.4f})\"\n",
        "        )\n",
        "\n",
        "        if avg_val < best_val:\n",
        "            best_val = avg_val\n",
        "            patience = 0\n",
        "            ckpt = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': avg_train,\n",
        "                'val_loss': avg_val\n",
        "            }\n",
        "            torch.save(ckpt, output_dir / 'v7_final_best.pt')\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= config.patience_stage3:\n",
        "                print(f\"Early stopping Stage 3 at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return history, best_val\n",
        "\n",
        "print(\"Training infrastructure defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "metrics-evaluation",
        "outputId": "e1cd4fa9-1aeb-4383-dd6c-cc3f290b94d3"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 7.2: METRICS EVALUATION AND EXPORTS\n",
        "# =============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(\n",
        "    model: V7_AMES,\n",
        "    data_loader: DataLoader,\n",
        "    device: torch.device,\n",
        "    feature_index_map: Dict[str, int] = None,\n",
        "    return_arrays: bool = False\n",
        ") -> Dict:\n",
        "    \"\"\"Evaluate model and compute metrics per horizon.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_weights = []\n",
        "    all_alpha = []\n",
        "\n",
        "    for batch in data_loader:\n",
        "        x_grid = batch['x_grid'].to(device)\n",
        "        x_graph = batch['x_graph'].to(device)\n",
        "        y = batch['y'].to(device)\n",
        "        context_static = batch['context'].to(device)\n",
        "        elevation = batch['elevation'].to(device)\n",
        "        edge_index, edge_weight = _get_edges(batch, device)\n",
        "\n",
        "        context = build_context_batch(x_graph, context_static, feature_index_map)\n",
        "        preds, aux = model(x_grid, x_graph, edge_index, edge_weight, context, elevation, stage='full')\n",
        "\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_targets.append(y.cpu().numpy())\n",
        "        all_weights.append(aux['gating_weights'].cpu().numpy())\n",
        "        all_alpha.append(float(aux['alpha'].detach().cpu().item()))\n",
        "\n",
        "    preds = np.concatenate(all_preds, axis=0).astype(np.float32)\n",
        "    targets = np.concatenate(all_targets, axis=0).astype(np.float32)\n",
        "    weights = np.concatenate(all_weights, axis=0)\n",
        "\n",
        "    # Global quality stats\n",
        "    pred_finite = np.isfinite(preds)\n",
        "    target_finite = np.isfinite(targets)\n",
        "    neg_pred_frac = float(np.mean(preds < 0))\n",
        "    nan_pred_frac = float(np.mean(~pred_finite))\n",
        "    nan_target_frac = float(np.mean(~target_finite))\n",
        "\n",
        "    # Compute metrics per horizon\n",
        "    metrics = {}\n",
        "    n_horizons = preds.shape[1]\n",
        "\n",
        "    for h in range(n_horizons):\n",
        "        pred_h = preds[:, h].flatten()\n",
        "        target_h = targets[:, h].flatten()\n",
        "\n",
        "        # Remove NaN values\n",
        "        valid_mask = np.isfinite(pred_h) & np.isfinite(target_h)\n",
        "        pred_h = pred_h[valid_mask]\n",
        "        target_h = target_h[valid_mask]\n",
        "\n",
        "        if len(pred_h) == 0:\n",
        "            continue\n",
        "\n",
        "        # RMSE\n",
        "        rmse = np.sqrt(np.mean((pred_h - target_h) ** 2))\n",
        "\n",
        "        # MAE\n",
        "        mae = np.mean(np.abs(pred_h - target_h))\n",
        "\n",
        "        # R2\n",
        "        ss_res = np.sum((target_h - pred_h) ** 2)\n",
        "        ss_tot = np.sum((target_h - np.mean(target_h)) ** 2)\n",
        "        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0\n",
        "\n",
        "        # Bias\n",
        "        mean_true = np.mean(target_h)\n",
        "        mean_pred = np.mean(pred_h)\n",
        "        total_true = float(np.sum(target_h))\n",
        "        total_pred = float(np.sum(pred_h))\n",
        "        bias_mm = mean_pred - mean_true\n",
        "        bias_pct = 100 * bias_mm / mean_true if mean_true != 0 else 0.0\n",
        "        scale_ratio = abs(mean_pred) / max(abs(mean_true), 1e-6)\n",
        "\n",
        "        metrics[f'H{h+1}'] = {\n",
        "            'RMSE': float(rmse),\n",
        "            'MAE': float(mae),\n",
        "            'R^2': float(r2),\n",
        "            'Mean_True_mm': float(mean_true),\n",
        "            'Mean_Pred_mm': float(mean_pred),\n",
        "            'mean_bias_mm': float(bias_mm),\n",
        "            'mean_bias_pct': float(bias_pct),\n",
        "            'scale_ratio': float(scale_ratio),\n",
        "            'TotalPrecipitation': float(total_true),\n",
        "            'TotalPrecipitation_Pred': float(total_pred)\n",
        "        }\n",
        "\n",
        "    # Average expert weights\n",
        "    avg_weights = weights.mean(axis=(0, 1))\n",
        "    metrics['expert_weights'] = {\n",
        "        'w_expert1': float(avg_weights[0]),\n",
        "        'w_expert2': float(avg_weights[1]),\n",
        "        'w_expert3': float(avg_weights[2])\n",
        "    }\n",
        "    metrics['alpha'] = float(np.mean(all_alpha))\n",
        "\n",
        "    metrics['quality'] = {\n",
        "        'neg_pred_frac': neg_pred_frac,\n",
        "        'nan_pred_frac': nan_pred_frac,\n",
        "        'nan_target_frac': nan_target_frac\n",
        "    }\n",
        "\n",
        "    if return_arrays:\n",
        "        return metrics, preds, targets\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def run_quality_checks(\n",
        "    metrics: Dict,\n",
        "    config: V7Config,\n",
        "    param_counts: Dict[str, int],\n",
        "    data_report: Dict = None\n",
        ") -> Dict:\n",
        "    issues = []\n",
        "    max_abs_bias = 0.0\n",
        "    max_scale_ratio = 0.0\n",
        "\n",
        "    for h_key, h_metrics in metrics.items():\n",
        "        if not h_key.startswith('H'):\n",
        "            continue\n",
        "        max_abs_bias = max(max_abs_bias, abs(h_metrics.get('mean_bias_pct', 0.0)))\n",
        "        max_scale_ratio = max(max_scale_ratio, h_metrics.get('scale_ratio', 0.0))\n",
        "\n",
        "    neg_pred_frac = metrics.get('quality', {}).get('neg_pred_frac', 0.0)\n",
        "    nan_pred_frac = metrics.get('quality', {}).get('nan_pred_frac', 0.0)\n",
        "    nan_target_frac = metrics.get('quality', {}).get('nan_target_frac', 0.0)\n",
        "\n",
        "    if max_abs_bias > config.max_bias_pct:\n",
        "        issues.append('bias_pct')\n",
        "    if max_scale_ratio > config.max_scale_ratio:\n",
        "        issues.append('scale_ratio')\n",
        "    if neg_pred_frac > config.max_negative_frac:\n",
        "        issues.append('neg_pred')\n",
        "    if nan_pred_frac > 0 or nan_target_frac > 0:\n",
        "        issues.append('nan_values')\n",
        "\n",
        "    weights = metrics.get('expert_weights', {})\n",
        "    min_weight = min(weights.values()) if weights else 0.5\n",
        "    if min_weight < config.min_expert_weight:\n",
        "        issues.append('expert_collapse')\n",
        "\n",
        "    param_budget_ok = param_counts.get('total', 0) <= config.param_budget\n",
        "    if not param_budget_ok:\n",
        "        issues.append('param_budget')\n",
        "\n",
        "    overlap_leakage = False\n",
        "    missing_features = []\n",
        "    if data_report:\n",
        "        overlap_leakage = bool(data_report.get('overlap_leakage', False))\n",
        "        missing_features = data_report.get('missing_features_grid', []) + data_report.get('missing_features_graph', [])\n",
        "    if overlap_leakage:\n",
        "        issues.append('overlap_windows')\n",
        "    if missing_features:\n",
        "        issues.append('missing_features')\n",
        "\n",
        "    return {\n",
        "        'issues': issues,\n",
        "        'max_abs_bias_pct': float(max_abs_bias),\n",
        "        'max_scale_ratio': float(max_scale_ratio),\n",
        "        'neg_pred_frac': float(neg_pred_frac),\n",
        "        'nan_pred_frac': float(nan_pred_frac),\n",
        "        'nan_target_frac': float(nan_target_frac),\n",
        "        'min_expert_weight': float(min_weight),\n",
        "        'param_budget_ok': bool(param_budget_ok),\n",
        "        'overlap_leakage': bool(overlap_leakage),\n",
        "        'missing_features': missing_features\n",
        "    }\n",
        "\n",
        "\n",
        "def _forecast_dates(ds: xr.Dataset, val_indices: List[int], input_window: int, horizon: int) -> List[List[str]]:\n",
        "    if 'time' not in ds.coords and 'time' not in ds.dims:\n",
        "        return [[] for _ in val_indices]\n",
        "    times = pd.to_datetime(ds['time'].values)\n",
        "    dates = []\n",
        "    for start in val_indices:\n",
        "        base = start + input_window\n",
        "        horizon_dates = []\n",
        "        for h in range(horizon):\n",
        "            idx = base + h\n",
        "            if idx < len(times):\n",
        "                horizon_dates.append(pd.Timestamp(times[idx]).strftime('%Y-%m'))\n",
        "        dates.append(horizon_dates)\n",
        "    return dates\n",
        "\n",
        "\n",
        "def export_predictions_for_maps(\n",
        "    preds: np.ndarray,\n",
        "    targets: np.ndarray,\n",
        "    output_root: Path,\n",
        "    horizon: int,\n",
        "    exp_name: str,\n",
        "    model_name: str,\n",
        "    config: V7Config,\n",
        "    ds: xr.Dataset,\n",
        "    data_report: Dict\n",
        ") -> Path:\n",
        "    \"\"\"Export predictions in V2/V3-compatible map format.\"\"\"\n",
        "    export_dir = output_root / 'map_exports' / f'H{horizon}' / exp_name / model_name\n",
        "    export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save predictions and targets\n",
        "    np.save(export_dir / 'predictions.npy', preds)\n",
        "    np.save(export_dir / 'targets.npy', targets)\n",
        "\n",
        "    # Metadata\n",
        "    val_start_idx = data_report.get('val_start_idx')\n",
        "    val_end_idx = data_report.get('val_end_idx')\n",
        "    val_indices = list(range(val_start_idx, val_end_idx + 1)) if val_start_idx is not None else []\n",
        "\n",
        "    forecast_dates = _forecast_dates(ds, val_indices, config.input_window, horizon)\n",
        "\n",
        "    metadata = {\n",
        "        'model': model_name,\n",
        "        'experiment': exp_name,\n",
        "        'horizon': int(horizon),\n",
        "        'input_window': int(config.input_window),\n",
        "        'val_start_idx': int(val_start_idx) if val_start_idx is not None else None,\n",
        "        'val_end_idx': int(val_end_idx) if val_end_idx is not None else None,\n",
        "        'val_indices': val_indices,\n",
        "        'forecast_dates': forecast_dates,\n",
        "        'shape': list(preds.shape),\n",
        "        'generated_at': datetime.now().isoformat(),\n",
        "        'rmse_mean': float(np.sqrt(np.mean((preds - targets) ** 2))),\n",
        "        'framework': 'V7-AMES'\n",
        "    }\n",
        "    (export_dir / 'metadata.json').write_text(json.dumps(metadata, indent=2))\n",
        "\n",
        "    print(f\"Exported predictions to: {export_dir}\")\n",
        "    return export_dir\n",
        "\n",
        "print(\"Metrics evaluation functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-8"
      },
      "source": [
        "## 8. Main Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "main-training",
        "outputId": "634f3b6d-f69b-4bba-bba5-eb1ee32c41d4"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 8: MAIN TRAINING LOOP\n",
        "# =============================================================================\n",
        "\n",
        "# Default experiment list (aligns with V5 structure)\n",
        "EXPERIMENTS = [\n",
        "    {'name': 'BASIC_KCE', 'grid_set': 'BASIC', 'graph_set': 'KCE'},\n",
        "]\n",
        "RUN_MIXED_ABLATION = False\n",
        "\n",
        "\n",
        "def run_v7_experiments(\n",
        "    ds: xr.Dataset,\n",
        "    config: V7Config,\n",
        "    edge_index: torch.Tensor,\n",
        "    edge_weight: torch.Tensor,\n",
        "    output_root: Path,\n",
        "    experiments: List[Dict]\n",
        "):\n",
        "    \"\"\"Run V7 experiments across multiple horizons.\"\"\"\n",
        "    all_metrics = []\n",
        "    experiment_state = {\n",
        "        'config': asdict(config),\n",
        "        'experiments': experiments,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'results': {}\n",
        "    }\n",
        "\n",
        "    for exp in experiments:\n",
        "        exp_name = exp['name']\n",
        "        feature_set_grid = exp['grid_set']\n",
        "        feature_set_graph = exp['graph_set']\n",
        "\n",
        "        experiment_state['results'][exp_name] = {}\n",
        "\n",
        "        for horizon in config.enabled_horizons:\n",
        "            print(\"\n",
        "\" + \"=\"*70)\n",
        "            print(f\"TRAINING V7-AMES - {exp_name} - H{horizon}\")\n",
        "            print(\"=\"*70)\n",
        "\n",
        "            current_config = V7Config(**{\n",
        "                **asdict(config),\n",
        "                'horizon': horizon\n",
        "            })\n",
        "\n",
        "            # Prepare data\n",
        "            train_dataset, val_dataset, n_grid, n_graph, data_report, feature_index_map = prepare_data(\n",
        "                ds, current_config, edge_index, edge_weight,\n",
        "                context_nodes=CONTEXT_NODES,\n",
        "                elevation_nodes=ELEVATION_NODES,\n",
        "                feature_set_grid=feature_set_grid,\n",
        "                feature_set_graph=feature_set_graph\n",
        "            )\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=current_config.batch_size, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=current_config.batch_size, shuffle=False)\n",
        "\n",
        "            # Context dimension (static + optional temporal)\n",
        "            context_dim = train_dataset.context_nodes.shape[1]\n",
        "            if 'month_sin' in feature_index_map and 'month_cos' in feature_index_map:\n",
        "                context_dim += 2\n",
        "\n",
        "            # Initialize model\n",
        "            model = V7_AMES(\n",
        "                config=current_config,\n",
        "                n_features_grid=n_grid,\n",
        "                n_features_graph=n_graph,\n",
        "                context_dim=context_dim,\n",
        "                n_lat=train_dataset.n_lat,\n",
        "                n_lon=train_dataset.n_lon,\n",
        "                horizon=horizon\n",
        "            ).to(device)\n",
        "\n",
        "            param_counts = model.count_parameters()\n",
        "            if param_counts['total'] > current_config.param_budget:\n",
        "                print(\"Warning: parameter budget exceeded\")\n",
        "\n",
        "            horizon_dir = output_root / f'h{horizon}' / exp_name\n",
        "            ckpt_dir = horizon_dir / 'checkpoints'\n",
        "            metrics_dir = horizon_dir / 'training_metrics'\n",
        "            ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "            metrics_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # Stage 1: Train experts\n",
        "            hist_e1, best_e1 = train_expert_stage1(\n",
        "                model.expert1, train_loader, val_loader, MASK_HIGH_NODES,\n",
        "                current_config, device, ckpt_dir, 'expert1'\n",
        "            )\n",
        "            hist_e2, best_e2 = train_expert_stage1(\n",
        "                model.expert2, train_loader, val_loader, MASK_LOW_NODES,\n",
        "                current_config, device, ckpt_dir, 'expert2'\n",
        "            )\n",
        "            hist_e3, best_e3 = train_expert_stage1(\n",
        "                model.expert3, train_loader, val_loader, MASK_MED_NODES,\n",
        "                current_config, device, ckpt_dir, 'expert3'\n",
        "            )\n",
        "\n",
        "            # Reload best expert checkpoints\n",
        "            for name, expert in [('expert1', model.expert1), ('expert2', model.expert2), ('expert3', model.expert3)]:\n",
        "                ckpt_path = ckpt_dir / f'{name}_best.pt'\n",
        "                if ckpt_path.exists():\n",
        "                    expert.load_state_dict(torch.load(ckpt_path, map_location=device)['model_state_dict'])\n",
        "\n",
        "            # Stage 2: Train gating network\n",
        "            hist_stage2, best_stage2 = train_stage2_gating(\n",
        "                model, train_loader, val_loader, current_config, device, ckpt_dir, feature_index_map\n",
        "            )\n",
        "\n",
        "            # Reload best stage2 checkpoint if available\n",
        "            stage2_path = ckpt_dir / 'v7_stage2_best.pt'\n",
        "            if stage2_path.exists():\n",
        "                model.load_state_dict(torch.load(stage2_path, map_location=device)['model_state_dict'])\n",
        "\n",
        "            # Stage 3: Joint fine-tuning\n",
        "            hist_stage3, best_stage3 = train_stage3_joint(\n",
        "                model, train_loader, val_loader, current_config, device, ckpt_dir, feature_index_map\n",
        "            )\n",
        "\n",
        "            # Reload best final checkpoint if available\n",
        "            final_path = ckpt_dir / 'v7_final_best.pt'\n",
        "            if final_path.exists():\n",
        "                model.load_state_dict(torch.load(final_path, map_location=device)['model_state_dict'])\n",
        "\n",
        "            # Evaluate\n",
        "            return_arrays = bool(current_config.export_predictions or current_config.generate_map_plots)\n",
        "            if return_arrays:\n",
        "                metrics, preds, targets = evaluate_model(model, val_loader, device, feature_index_map, return_arrays=True)\n",
        "            else:\n",
        "                metrics = evaluate_model(model, val_loader, device, feature_index_map)\n",
        "                preds, targets = None, None\n",
        "\n",
        "            quality = run_quality_checks(metrics, current_config, param_counts, data_report)\n",
        "            metrics['quality_checks'] = quality\n",
        "\n",
        "            if quality['issues']:\n",
        "                print(f\"Warning: quality checks flagged {quality['issues']}\")\n",
        "                if current_config.enforce_quality_gates:\n",
        "                    raise ValueError(f\"Quality gates failed: {quality['issues']}\")\n",
        "\n",
        "            map_export_dir = None\n",
        "            if current_config.export_predictions and preds is not None:\n",
        "                map_export_dir = export_predictions_for_maps(\n",
        "                    preds, targets, output_root, horizon,\n",
        "                    exp_name=exp_name, model_name='V7_AMES',\n",
        "                    config=current_config, ds=ds, data_report=data_report\n",
        "                )\n",
        "\n",
        "            # Save training history\n",
        "            history_data = {\n",
        "                'model_name': 'V7_AMES',\n",
        "                'experiment': exp_name,\n",
        "                'feature_set_grid': feature_set_grid,\n",
        "                'feature_set_graph': feature_set_graph,\n",
        "                'horizon': horizon,\n",
        "                'stage1': {\n",
        "                    'expert1': hist_e1,\n",
        "                    'expert2': hist_e2,\n",
        "                    'expert3': hist_e3\n",
        "                },\n",
        "                'stage2': hist_stage2,\n",
        "                'stage3': hist_stage3,\n",
        "                'parameters': param_counts['total'],\n",
        "                'data_report': data_report,\n",
        "                'quality_checks': quality,\n",
        "                'map_export_dir': str(map_export_dir) if map_export_dir else None\n",
        "            }\n",
        "\n",
        "            with open(metrics_dir / 'v7_history.json', 'w') as f:\n",
        "                json.dump(history_data, f, indent=2)\n",
        "\n",
        "            # Save training log CSV (Stage 3)\n",
        "            log_df = pd.DataFrame({\n",
        "                'epoch': range(1, len(hist_stage3['train_loss']) + 1),\n",
        "                'train_loss': hist_stage3['train_loss'],\n",
        "                'val_loss': hist_stage3['val_loss'],\n",
        "                'train_mse': hist_stage3['train_mse'],\n",
        "                'train_mass': hist_stage3['train_mass'],\n",
        "                'train_oro': hist_stage3['train_oro']\n",
        "            })\n",
        "            log_df.to_csv(metrics_dir / f'v7_training_log_h{horizon}.csv', index=False)\n",
        "\n",
        "            # Collect metrics for CSV\n",
        "            for h_key, h_metrics in metrics.items():\n",
        "                if h_key.startswith('H'):\n",
        "                    h_num = int(h_key[1:])\n",
        "                    row = {\n",
        "                        'TotalHorizon': horizon,\n",
        "                        'Experiment': exp_name,\n",
        "                        'Feature_Set_Grid': feature_set_grid,\n",
        "                        'Feature_Set_Graph': feature_set_graph,\n",
        "                        'Model': 'V7_AMES',\n",
        "                        'H': h_num,\n",
        "                        **h_metrics\n",
        "                    }\n",
        "                    all_metrics.append(row)\n",
        "\n",
        "            # Store results\n",
        "            experiment_state['results'][exp_name][f'H{horizon}'] = {\n",
        "                'metrics': metrics,\n",
        "                'history': history_data,\n",
        "                'param_counts': param_counts,\n",
        "                'data_report': data_report,\n",
        "                'quality_checks': quality,\n",
        "                'map_export_dir': str(map_export_dir) if map_export_dir else None\n",
        "            }\n",
        "\n",
        "            # Print summary\n",
        "            print(f\"\n",
        "{exp_name} H{horizon} Results Summary:\")\n",
        "            for h_key, h_metrics in metrics.items():\n",
        "                if h_key.startswith('H'):\n",
        "                    print(f\"  {h_key}: RMSE={h_metrics['RMSE']:.2f}mm, \"\n",
        "                          f\"MAE={h_metrics['MAE']:.2f}mm, R2={h_metrics['R^2']:.4f}\")\n",
        "            if 'expert_weights' in metrics:\n",
        "                w = metrics['expert_weights']\n",
        "                print(f\"  Expert weights: E1={w['w_expert1']:.2%}, E2={w['w_expert2']:.2%}, E3={w['w_expert3']:.2%}\")\n",
        "            if quality['issues']:\n",
        "                print(f\"  Quality issues: {', '.join(quality['issues'])}\")\n",
        "\n",
        "            # Cleanup\n",
        "            del model\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Save consolidated metrics CSV\n",
        "    if all_metrics:\n",
        "        metrics_df = pd.DataFrame(all_metrics)\n",
        "        metrics_df.to_csv(output_root / 'metrics_spatial_v7_all_horizons.csv', index=False)\n",
        "        print(f\"\n",
        "Consolidated metrics saved to: {output_root / 'metrics_spatial_v7_all_horizons.csv'}\")\n",
        "    else:\n",
        "        metrics_df = None\n",
        "\n",
        "    # Save experiment state\n",
        "    with open(output_root / 'experiment_state_v7.json', 'w') as f:\n",
        "        json.dump(experiment_state, f, indent=2, default=str)\n",
        "\n",
        "    return experiment_state, metrics_df\n",
        "\n",
        "print(\"Main training function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "run-training",
        "outputId": "c4523e63-1522-4d78-93df-03f535d69528"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 8.2: EXECUTE TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "experiment_state, metrics_df = run_v7_experiments(\n",
        "    DS, CONFIG, edge_index, edge_weight, OUTPUT_ROOT, EXPERIMENTS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-9"
      },
      "source": [
        "## 9. Results Export and Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "visualization",
        "outputId": "d19caa62-42fa-4243-f17f-5dff3b5bb61e"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 9.1: RESULTS OVERVIEW\n",
        "# =============================================================================\n",
        "\n",
        "COMP_DIR = OUTPUT_ROOT / 'comparisons'\n",
        "COMP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def plot_v7_results(metrics_df: pd.DataFrame, experiment_state: Dict, output_dir: Path):\n",
        "    if metrics_df is None or len(metrics_df) == 0:\n",
        "        print(\"No metrics to plot\")\n",
        "        return\n",
        "\n",
        "    horizons = sorted(metrics_df['H'].unique())\n",
        "    rmse_by_h = metrics_df.groupby('H')['RMSE'].mean()\n",
        "    r2_by_h = metrics_df.groupby('H')['R^2'].mean()\n",
        "    bias_by_h = metrics_df.groupby('H')['mean_bias_mm'].mean()\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.plot(horizons, rmse_by_h.values, marker='o', color='steelblue')\n",
        "    ax1.set_xlabel('Horizon (H)')\n",
        "    ax1.set_ylabel('RMSE (mm)')\n",
        "    ax1.set_title('RMSE by Horizon')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.plot(horizons, r2_by_h.values, marker='o', color='forestgreen')\n",
        "    ax2.set_xlabel('Horizon (H)')\n",
        "    ax2.set_ylabel('R2')\n",
        "    ax2.set_title('R2 by Horizon')\n",
        "    ax2.axhline(y=0.597, color='red', linestyle='--', label='V4 Baseline (0.597)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    ax3 = axes[1, 0]\n",
        "    ax3.plot(horizons, bias_by_h.values, marker='o', color='coral')\n",
        "    ax3.axhline(0, color='red', linestyle='--')\n",
        "    ax3.set_xlabel('Horizon (H)')\n",
        "    ax3.set_ylabel('Mean Bias (mm)')\n",
        "    ax3.set_title('Bias by Horizon')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    ax4 = axes[1, 1]\n",
        "    weights_list = []\n",
        "    for exp_name, exp_results in experiment_state.get('results', {}).items():\n",
        "        for h_key, results in exp_results.items():\n",
        "            metrics = results.get('metrics', {})\n",
        "            if 'expert_weights' in metrics:\n",
        "                w = metrics['expert_weights']\n",
        "                weights_list.append([w['w_expert1'], w['w_expert2'], w['w_expert3']])\n",
        "\n",
        "    if weights_list:\n",
        "        avg_weights = np.mean(weights_list, axis=0)\n",
        "        ax4.bar(['Expert1', 'Expert2', 'Expert3'], avg_weights, color=['#4a86e8', '#e69138', '#6aa84f'])\n",
        "        ax4.set_ylim(0, 1)\n",
        "        ax4.set_ylabel('Average Weight')\n",
        "        ax4.set_title('Average Expert Weights')\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, 'No expert weights available', ha='center', va='center')\n",
        "        ax4.set_axis_off()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig_path = output_dir / 'v7_results_summary.png'\n",
        "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Results plot saved to: {fig_path}\")\n",
        "\n",
        "\n",
        "plot_v7_results(metrics_df, experiment_state, OUTPUT_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "RGvybhtiqvlU",
        "outputId": "ea216c31-9537-454a-8add-95803bc69e94"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 9.1: METRICS EVOLUTION BY HORIZON\n",
        "# =============================================================================\n",
        "\n",
        "COMP_DIR = OUTPUT_ROOT / 'comparisons'\n",
        "COMP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def plot_v7_evolution_by_horizon(metrics_df: pd.DataFrame, output_dir: Path):\n",
        "    if metrics_df is None or metrics_df.empty:\n",
        "        print('No metrics to plot for evolution panels')\n",
        "        return\n",
        "\n",
        "    metrics = ['RMSE', 'MAE', 'R^2']\n",
        "    has_tp = 'TotalPrecipitation' in metrics_df.columns and 'TotalPrecipitation_Pred' in metrics_df.columns\n",
        "    if has_tp:\n",
        "        metrics.append('TotalPrecipitation')\n",
        "\n",
        "    for total_h in sorted(metrics_df['TotalHorizon'].unique()):\n",
        "        sub_df = metrics_df[metrics_df['TotalHorizon'] == total_h].copy()\n",
        "        fig, axes = plt.subplots(1, len(metrics), figsize=(6 * len(metrics), 6))\n",
        "        if len(metrics) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for metric, ax in zip(metrics, axes):\n",
        "            if metric == 'TotalPrecipitation':\n",
        "                ax.plot(sub_df['H'], sub_df['TotalPrecipitation'], label='True', marker='o')\n",
        "                ax.plot(sub_df['H'], sub_df['TotalPrecipitation_Pred'], label='Pred', marker='o')\n",
        "                ax.set_ylabel('Total Precipitation (mm)')\n",
        "            else:\n",
        "                ax.plot(sub_df['H'], sub_df[metric], marker='o')\n",
        "                ax.set_ylabel(metric)\n",
        "            ax.set_xlabel('Horizon (H)')\n",
        "            ax.set_title(f'{metric} by Horizon (Total H={total_h})')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            if metric == 'TotalPrecipitation':\n",
        "                ax.legend()\n",
        "\n",
        "        fig_path = output_dir / f'v7_evolution_h{total_h}.png'\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f\"Evolution plot saved to: {fig_path}\")\n",
        "\n",
        "\n",
        "if metrics_df is not None and len(metrics_df) > 0:\n",
        "    plot_v7_evolution_by_horizon(metrics_df, COMP_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "Ayt6nPwQqvlU",
        "outputId": "d4e9b092-5c36-4009-fab2-02472b982ecb"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 9.2: METRICS TABLE\n",
        "# =============================================================================\n",
        "\n",
        "COMP_DIR = OUTPUT_ROOT / 'comparisons'\n",
        "COMP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def plot_v7_metrics_table(metrics_df: pd.DataFrame, output_dir: Path):\n",
        "    if metrics_df is None or metrics_df.empty:\n",
        "        print('No metrics available to build summary table')\n",
        "        return\n",
        "\n",
        "    has_tp = 'TotalPrecipitation' in metrics_df.columns and 'TotalPrecipitation_Pred' in metrics_df.columns\n",
        "\n",
        "    summary_data = []\n",
        "    experiments = metrics_df['Experiment'].unique()\n",
        "    models = metrics_df['Model'].unique()\n",
        "    headers = ['Experiment', 'Model', 'RMSE', 'MAE', 'R^2', 'Total Pcp (True)', 'Total Pcp (Pred)', 'Best H']\n",
        "\n",
        "    for exp in experiments:\n",
        "        for model in models:\n",
        "            sub = metrics_df[(metrics_df['Experiment'] == exp) & (metrics_df['Model'] == model)]\n",
        "            if sub.empty:\n",
        "                continue\n",
        "            avg_rmse = sub['RMSE'].mean()\n",
        "            avg_mae = sub['MAE'].mean()\n",
        "            avg_r2 = sub['R^2'].mean()\n",
        "            if has_tp:\n",
        "                avg_tp_t = sub['TotalPrecipitation'].mean()\n",
        "                avg_tp_p = sub['TotalPrecipitation_Pred'].mean()\n",
        "            else:\n",
        "                avg_tp_t = float('nan')\n",
        "                avg_tp_p = float('nan')\n",
        "            best_h = sub.loc[sub['RMSE'].idxmin(), 'H']\n",
        "            summary_data.append([\n",
        "                exp, model,\n",
        "                f'{avg_rmse:.4f}', f'{avg_mae:.4f}', f'{avg_r2:.4f}',\n",
        "                f'{avg_tp_t:.1f}' if has_tp else 'n/a',\n",
        "                f'{avg_tp_p:.1f}' if has_tp else 'n/a',\n",
        "                f'H={best_h}'\n",
        "            ])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(17, 10))\n",
        "    ax.axis('off')\n",
        "    table = ax.table(cellText=summary_data, colLabels=headers, cellLoc='center', loc='center', bbox=[0, 0, 1, 0.85])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1.15, 1.8)\n",
        "\n",
        "    for j in range(len(headers)):\n",
        "        table[(0, j)].set_facecolor('#4a86e8')\n",
        "        table[(0, j)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "    fig.suptitle('Metrics Summary by Model and Experiment', fontsize=16, fontweight='bold', y=0.95)\n",
        "    plt.subplots_adjust(top=0.85)\n",
        "    fig_path = output_dir / 'v7_metrics_summary_table.png'\n",
        "    plt.savefig(fig_path, dpi=150, bbox_inches='tight', pad_inches=0.5)\n",
        "    plt.show()\n",
        "\n",
        "    if summary_data:\n",
        "        print('Metrics summary table saved to:', fig_path)\n",
        "\n",
        "\n",
        "if metrics_df is not None and len(metrics_df) > 0:\n",
        "    plot_v7_metrics_table(metrics_df, COMP_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hewtd5BRqvlU",
        "outputId": "3f6b26ca-0a31-4733-9bc5-39a46ac68765"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 9.3: MODEL COMPARISON\n",
        "# =============================================================================\n",
        "\n",
        "COMP_DIR = OUTPUT_ROOT / 'comparisons'\n",
        "COMP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def plot_model_comparison_v7(res_df: pd.DataFrame, output_dir: Path):\n",
        "    if res_df is None or res_df.empty:\n",
        "        print('No results to plot for model comparison')\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    ax1 = axes[0, 0]\n",
        "    pivot_rmse = res_df.groupby(['Model', 'Experiment'])['RMSE'].mean().unstack()\n",
        "    pivot_rmse.plot(kind='bar', ax=ax1, width=0.8)\n",
        "    ax1.set_ylabel('RMSE (mm)')\n",
        "    ax1.set_title('RMSE by Model and Feature Set')\n",
        "    ax1.legend(title='Feature Set')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    ax2 = axes[0, 1]\n",
        "    pivot_r2 = res_df.groupby(['Model', 'Experiment'])['R^2'].mean().unstack()\n",
        "    pivot_r2.plot(kind='bar', ax=ax2, width=0.8)\n",
        "    ax2.set_ylabel('R^2')\n",
        "    ax2.set_title('R^2 by Model and Feature Set')\n",
        "    ax2.legend(title='Feature Set')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    ax2.axhline(y=0.597, color='red', linestyle='--', label='V4 Baseline (0.597)')\n",
        "\n",
        "    ax3 = axes[1, 0]\n",
        "    for model in res_df['Model'].unique():\n",
        "        model_data = res_df[res_df['Model'] == model]\n",
        "        for exp in model_data['Experiment'].unique():\n",
        "            exp_data = model_data[model_data['Experiment'] == exp]\n",
        "            ax3.plot(exp_data['H'], exp_data['RMSE'], marker='o', label=f\"{model} ({exp})\")\n",
        "    ax3.set_xlabel('Horizon (H)')\n",
        "    ax3.set_ylabel('RMSE (mm)')\n",
        "    ax3.set_title('RMSE Degradation by Horizon')\n",
        "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    ax4 = axes[1, 1]\n",
        "    for model in res_df['Model'].unique():\n",
        "        model_bias = res_df[res_df['Model'] == model]['mean_bias_mm']\n",
        "        ax4.hist(model_bias, bins=20, alpha=0.5, label=model)\n",
        "    ax4.axvline(x=0, color='red', linestyle='--')\n",
        "    ax4.set_xlabel('Mean Bias (mm)')\n",
        "    ax4.set_ylabel('Frequency')\n",
        "    ax4.set_title('Bias Distribution by Model')\n",
        "    ax4.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig_path = output_dir / 'model_comparison_v7_ames.png'\n",
        "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"Comparison plot saved to: {fig_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if metrics_df is not None and len(metrics_df) > 0:\n",
        "    plot_model_comparison_v7(metrics_df, COMP_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7vCTj5ZqvlV",
        "outputId": "34351f57-1d7d-488d-83e9-a07953fe6bc2"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 9.4: MAP EXPORTS\n",
        "# =============================================================================\n",
        "\n",
        "SHAPE_DIR = BASE_PATH / 'data' / 'input' / 'shapes'\n",
        "\n",
        "\n",
        "def _load_boyaca_shape(shape_dir: Path):\n",
        "    if gpd is None:\n",
        "        print('geopandas not available; overlay disabled')\n",
        "        return None, None\n",
        "    if not shape_dir.exists():\n",
        "        print(f\"[WARN] Shape directory not found: {shape_dir}\")\n",
        "        return None, None\n",
        "\n",
        "    shp_path = shape_dir / 'MGN_Departamento.shp'\n",
        "    try:\n",
        "        dept_gdf = gpd.read_file(shp_path)\n",
        "    except Exception as exc:\n",
        "        print(f\"[WARN] Failed to read shapefile {shp_path}: {exc}\")\n",
        "        return None, None\n",
        "\n",
        "    if dept_gdf is None or dept_gdf.empty:\n",
        "        print(f\"[WARN] Shapefile is empty: {shp_path}\")\n",
        "        return dept_gdf, None\n",
        "\n",
        "    col_candidates = [c for c in dept_gdf.columns if c.lower() in {\n",
        "        'nombre_dpt', 'nombre_dept', 'nombre', 'departamen', 'dpto'\n",
        "    }]\n",
        "    boyaca = None\n",
        "    if col_candidates:\n",
        "        name_col = col_candidates[0]\n",
        "        boyaca = dept_gdf[dept_gdf[name_col].astype(str).str.upper().str.contains('BOYACA', na=False)].copy()\n",
        "    elif len(dept_gdf) == 1:\n",
        "        boyaca = dept_gdf.copy()\n",
        "        print('[INFO] No name column found; assuming single feature is Boyaca')\n",
        "    else:\n",
        "        print(f\"[WARN] Could not identify Boyaca geometry; columns present: {list(dept_gdf.columns)}\")\n",
        "\n",
        "    if boyaca is not None and boyaca.empty:\n",
        "        print('[WARN] Boyaca geometry not found in shapefile; overlay disabled')\n",
        "        boyaca = None\n",
        "\n",
        "    return dept_gdf, boyaca\n",
        "\n",
        "\n",
        "DEPT_GDF, BOYACA = _load_boyaca_shape(SHAPE_DIR)\n",
        "\n",
        "MAP_OUT_ROOT = OUTPUT_ROOT / 'map_exports'\n",
        "PLOT_HORIZONS = CONFIG.enabled_horizons or [CONFIG.horizon]\n",
        "PLOT_EXPERIMENTS = [exp['name'] for exp in EXPERIMENTS] if 'EXPERIMENTS' in globals() else ['BASIC_KCE']\n",
        "PLOT_MODELS = ['V7_AMES']\n",
        "MAP_SAMPLE_INDEX = CONFIG.map_sample_index\n",
        "CYCLE_ALL_VAL_WINDOWS = CONFIG.map_cycle_all_val_windows\n",
        "EXPORT_DPI = CONFIG.map_export_dpi\n",
        "GIF_FRAME_DURATION = CONFIG.map_gif_duration\n",
        "MAP_MAX_WINDOWS = None\n",
        "\n",
        "lats = DS[LAT_DIM].values\n",
        "lons = DS[LON_DIM].values\n",
        "EXTENT = [float(lons.min()), float(lons.max()), float(lats.min()), float(lats.max())]\n",
        "\n",
        "\n",
        "def _load_exports(exp_name: str, model_name: str, horizon: int):\n",
        "    export_dir = MAP_OUT_ROOT / f'H{horizon}' / exp_name / model_name\n",
        "    pred_f = export_dir / 'predictions.npy'\n",
        "    targ_f = export_dir / 'targets.npy'\n",
        "    meta_f = export_dir / 'metadata.json'\n",
        "    if not (pred_f.exists() and targ_f.exists() and meta_f.exists()):\n",
        "        return None, None, None, None\n",
        "    y_pred = np.load(pred_f)\n",
        "    y_true = np.load(targ_f)\n",
        "    meta = json.loads(meta_f.read_text())\n",
        "    if y_pred.ndim == 5:\n",
        "        y_pred = y_pred[..., 0]\n",
        "    if y_true.ndim == 5:\n",
        "        y_true = y_true[..., 0]\n",
        "    return y_pred, y_true, meta, export_dir\n",
        "\n",
        "\n",
        "def _plot_triplet(ax, data, title, cmap, vmin=None, vmax=None):\n",
        "    im = ax.imshow(data, origin='lower', extent=EXTENT, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "    ax.set_title(title, fontsize=11)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    return im\n",
        "\n",
        "\n",
        "def plot_maps_v7():\n",
        "    if not MAP_OUT_ROOT.exists():\n",
        "        print(f\"Map export root not found: {MAP_OUT_ROOT}\")\n",
        "        return\n",
        "    if imageio is None:\n",
        "        print('imageio not available; GIF export disabled')\n",
        "\n",
        "    for horizon in PLOT_HORIZONS:\n",
        "        for exp_name in PLOT_EXPERIMENTS:\n",
        "            for model_name in PLOT_MODELS:\n",
        "                y_pred, y_true, meta, export_dir = _load_exports(exp_name, model_name, horizon)\n",
        "                if y_pred is None:\n",
        "                    print(f\"[SKIP] {exp_name} - {model_name} - H{horizon}: exports not found\")\n",
        "                    continue\n",
        "\n",
        "                if CYCLE_ALL_VAL_WINDOWS:\n",
        "                    indices_to_plot = range(len(y_true))\n",
        "                else:\n",
        "                    idx = MAP_SAMPLE_INDEX if MAP_SAMPLE_INDEX >= 0 else (len(y_true) - 1)\n",
        "                    idx = max(min(idx, len(y_true) - 1), 0)\n",
        "                    indices_to_plot = [idx]\n",
        "\n",
        "                if MAP_MAX_WINDOWS is not None:\n",
        "                    indices_to_plot = list(indices_to_plot)[:MAP_MAX_WINDOWS]\n",
        "\n",
        "                forecast_dates = meta.get('forecast_dates', [])\n",
        "                if not forecast_dates and meta.get('val_indices'):\n",
        "                    forecast_dates = _forecast_dates(DS, meta['val_indices'], CONFIG.input_window, horizon)\n",
        "\n",
        "                frame_count = 0\n",
        "                gif_path = export_dir / f'{model_name}_H{horizon}.gif'\n",
        "                writer = None\n",
        "\n",
        "                for idx in indices_to_plot:\n",
        "                    sample_dates = forecast_dates[idx] if idx < len(forecast_dates) else []\n",
        "                    for h in range(min(horizon, len(sample_dates) or horizon)):\n",
        "                        real = y_true[idx, h]\n",
        "                        pred = y_pred[idx, h]\n",
        "                        err = np.abs(pred - real) / (np.abs(real) + 1e-6) * 100.0\n",
        "\n",
        "                        fig, axes = plt.subplots(1, 3, figsize=(14, 4.5), constrained_layout=True)\n",
        "                        date_label = sample_dates[h] if h < len(sample_dates) else f'H{h + 1}'\n",
        "                        im0 = _plot_triplet(axes[0], real, f'Real {date_label}', 'Blues')\n",
        "                        im1 = _plot_triplet(axes[1], pred, f'{model_name} H{h + 1} {date_label}', 'Blues')\n",
        "                        im2 = _plot_triplet(axes[2], np.clip(err, 0, 100), f'MAPE% {model_name} H{h + 1} {date_label}', 'Reds', vmin=0, vmax=100)\n",
        "                        if BOYACA is not None:\n",
        "                            try:\n",
        "                                BOYACA.boundary.plot(ax=axes[0], color='k', linewidth=0.8)\n",
        "                                BOYACA.boundary.plot(ax=axes[1], color='k', linewidth=0.8)\n",
        "                                BOYACA.boundary.plot(ax=axes[2], color='k', linewidth=0.8)\n",
        "                            except Exception as exc:\n",
        "                                print(f\"[WARN] shapefile overlay failed: {exc}\")\n",
        "\n",
        "                        cbar0 = fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
        "                        cbar0.set_label('Precipitation (mm)', rotation=270, labelpad=12)\n",
        "                        cbar1 = fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "                        cbar1.set_label('Precipitation (mm)', rotation=270, labelpad=12)\n",
        "                        cbar2 = fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
        "                        cbar2.set_label('MAPE (%)', rotation=270, labelpad=12)\n",
        "\n",
        "                        title = f'{exp_name} | {model_name} | H{horizon} | {date_label}'\n",
        "                        fig.suptitle(title, fontsize=12)\n",
        "\n",
        "                        export_dir.mkdir(parents=True, exist_ok=True)\n",
        "                        png_path = export_dir / f'{model_name}_H{horizon}_{date_label}.png'\n",
        "                        fig.savefig(png_path, dpi=EXPORT_DPI)\n",
        "                        plt.close(fig)\n",
        "\n",
        "                        if imageio is not None:\n",
        "                            frame = imageio.imread(png_path)\n",
        "                            if writer is None:\n",
        "                                writer = imageio.get_writer(gif_path, mode='I', duration=GIF_FRAME_DURATION, loop=0)\n",
        "                            writer.append_data(frame)\n",
        "                            del frame\n",
        "\n",
        "                        frame_count += 1\n",
        "                        del real, pred, err\n",
        "\n",
        "                if writer is not None:\n",
        "                    writer.close()\n",
        "\n",
        "                print(f\"[OK] {exp_name} - {model_name} - H{horizon}: saved {frame_count} frames to {export_dir}\")\n",
        "\n",
        "\n",
        "if getattr(CONFIG, 'generate_map_plots', True):\n",
        "    plot_maps_v7()\n",
        "else:\n",
        "    print('Map plot generation disabled')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "training-curves",
        "outputId": "abed88ed-4892-44a2-8e4f-cf2f63f520e4"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 9.5: TRAINING CURVES VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "def plot_training_curves(experiment_state: Dict, output_dir: Path):\n",
        "    \"\"\"Plot training curves for all horizons.\"\"\"\n",
        "\n",
        "    entries = []\n",
        "    for exp_name, exp_results in experiment_state.get('results', {}).items():\n",
        "        for h_key, results in exp_results.items():\n",
        "            entries.append((exp_name, h_key, results))\n",
        "\n",
        "    if not entries:\n",
        "        print(\"No training results to plot\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    colors = ['steelblue', 'coral', 'forestgreen', 'purple']\n",
        "\n",
        "    for idx, (exp_name, h_key, results) in enumerate(entries[:4]):\n",
        "        ax = axes[idx]\n",
        "\n",
        "        h_num = h_key.replace('H', '')\n",
        "        log_path = output_dir / f'h{h_num}' / exp_name / 'training_metrics' / f'v7_training_log_h{h_num}.csv'\n",
        "\n",
        "        if log_path.exists():\n",
        "            log_df = pd.read_csv(log_path)\n",
        "\n",
        "            ax.plot(log_df['epoch'], log_df['train_loss'], label='Train Loss', color=colors[idx], alpha=0.8)\n",
        "            ax.plot(log_df['epoch'], log_df['val_loss'], label='Val Loss', color=colors[idx], linestyle='--', alpha=0.8)\n",
        "\n",
        "            best_epoch = log_df['val_loss'].idxmin() + 1\n",
        "            best_val = log_df['val_loss'].min()\n",
        "            ax.scatter([best_epoch], [best_val], color='red', s=100, zorder=5, marker='*', label=f'Best: {best_val:.4f}')\n",
        "\n",
        "        ax.set_xlabel('Epoch', fontsize=10)\n",
        "        ax.set_ylabel('Loss', fontsize=10)\n",
        "        ax.set_title(f'Training Curves - {exp_name} {h_key}', fontsize=12, fontweight='bold')\n",
        "        ax.legend(fontsize=8)\n",
        "        ax.grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    fig_path = output_dir / 'v7_training_curves.png'\n",
        "    fig.savefig(fig_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Training curves saved to: {fig_path}\")\n",
        "\n",
        "# Plot training curves\n",
        "plot_training_curves(experiment_state, OUTPUT_ROOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section-10"
      },
      "source": [
        "## 10. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "summary",
        "outputId": "a3bd367a-827f-4103-e96d-df88d5ec3807"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 10: SUMMARY AND RESULTS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\n",
        "\" + \"=\"*70)\n",
        "print(\"V7-AMES - EXPERIMENT SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Print configuration\n",
        "print(\"\n",
        "[Configuration]\")\n",
        "print(f\"  Light Mode: {CONFIG.light_mode}\")\n",
        "if CONFIG.light_mode:\n",
        "    print(f\"  Grid Size: {CONFIG.light_grid_size}x{CONFIG.light_grid_size}\")\n",
        "print(f\"  GNN Type: {CONFIG.gnn_type}\")\n",
        "print(f\"  Horizons: {CONFIG.enabled_horizons}\")\n",
        "print(f\"  Stage 1 Epochs: {CONFIG.epochs_stage1}\")\n",
        "print(f\"  Stage 2 Epochs: {CONFIG.epochs_stage2}\")\n",
        "print(f\"  Stage 3 Epochs: {CONFIG.epochs_stage3}\")\n",
        "print(f\"  Batch Size: {CONFIG.batch_size}\")\n",
        "if 'EXPERIMENTS' in globals():\n",
        "    exp_names = [exp['name'] for exp in EXPERIMENTS]\n",
        "    print(f\"  Experiments: {', '.join(exp_names)}\")\n",
        "if 'data_report' in globals() and data_report:\n",
        "    print(f\"  Split Mode: {data_report.get('split_mode', 'unknown')}\")\n",
        "    print(f\"  Split Index: {data_report.get('split_time_idx', 'unknown')}\")\n",
        "\n",
        "# Print results summary\n",
        "if metrics_df is not None and len(metrics_df) > 0:\n",
        "    print(\"\n",
        "[Results Summary]\")\n",
        "    print(\"-\"*70)\n",
        "    print(metrics_df[['TotalHorizon', 'H', 'RMSE', 'MAE', 'R^2', 'mean_bias_mm']].to_string(index=False))\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    # Overall averages\n",
        "    print(\"\n",
        "[Overall Averages]\")\n",
        "    print(f\"  RMSE: {metrics_df['RMSE'].mean():.2f} mm\")\n",
        "    print(f\"  MAE: {metrics_df['MAE'].mean():.2f} mm\")\n",
        "    print(f\"  R2: {metrics_df['R^2'].mean():.4f}\")\n",
        "    print(f\"  Bias: {metrics_df['mean_bias_mm'].mean():.2f} mm\")\n",
        "\n",
        "# Expert weights\n",
        "print(\"\n",
        "[Expert Weights (Gating)]\")\n",
        "for exp_name, exp_results in experiment_state.get('results', {}).items():\n",
        "    for h_key, results in exp_results.items():\n",
        "        metrics = results.get('metrics', {})\n",
        "        if 'expert_weights' in metrics:\n",
        "            w = metrics['expert_weights']\n",
        "            print(f\"  {exp_name} {h_key}: E1={w['w_expert1']:.1%}, E2={w['w_expert2']:.1%}, E3={w['w_expert3']:.1%}\")\n",
        "        if 'alpha' in metrics:\n",
        "            print(f\"  {exp_name} {h_key}: alpha={metrics['alpha']:.3f}\")\n",
        "\n",
        "# Quality checks\n",
        "print(\"\n",
        "[Quality Checks]\")\n",
        "for exp_name, exp_results in experiment_state.get('results', {}).items():\n",
        "    for h_key, results in exp_results.items():\n",
        "        qc = results.get('quality_checks', {})\n",
        "        issues = qc.get('issues', [])\n",
        "        status = 'OK' if not issues else f\"WARN: {', '.join(issues)}\"\n",
        "        print(f\"  {exp_name} {h_key}: {status}\")\n",
        "\n",
        "# Output files\n",
        "print(\"\n",
        "[Output Files]\")\n",
        "print(f\"  Output Directory: {OUTPUT_ROOT}\")\n",
        "for path in OUTPUT_ROOT.glob('*'):\n",
        "    if path.is_file():\n",
        "        print(f\"  - {path.name}\")\n",
        "\n",
        "# Next steps\n",
        "print(\"\n",
        "[Next Steps]\")\n",
        "if CONFIG.light_mode:\n",
        "    print(\"  1. Set light_mode=False for full grid validation (61x65)\")\n",
        "    print(\"  2. Run full experiments on Colab GPU\")\n",
        "print(\"  3. Run ablation studies (disable physics loss or gating)\")\n",
        "print(\"  4. Compare results with V4 baseline (R2=0.597, RMSE=84.40mm)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cleanup",
        "outputId": "80f40b70-44cc-47ff-b64b-4ec6f3f2ca56"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# SECTION 10.2: CLEANUP\n",
        "# =============================================================================\n",
        "\n",
        "# Memory cleanup\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"GPU Memory freed. Current usage: {torch.cuda.memory_allocated() / 1e6:.1f} MB\")\n",
        "\n",
        "print(\"\n",
        "Notebook execution completed successfully.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}