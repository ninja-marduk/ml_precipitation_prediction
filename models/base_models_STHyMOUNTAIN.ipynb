{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4e7925",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/main/models/base_models_STHyMOUNTAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3744f4",
   "metadata": {
    "id": "af3744f4"
   },
   "source": [
    "# üìò Entrenamiento de Modelos Baseline para Predicci√≥n Espaciotemporal de Precipitaci√≥n Mensual STHyMOUNTAIN\n",
    "\n",
    "Este notebook implementa modelos baseline para la predicci√≥n de precipitaciones usando datos espaciotemporales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994be36",
   "metadata": {},
   "source": [
    "## üîç Implementaci√≥n de Modelos Avanzados y T√©cnicas de Validaci√≥n\n",
    "\n",
    "Adem√°s de los modelos tabulares baseline, implementaremos:\n",
    "\n",
    "1. **Optimizaci√≥n avanzada con Optuna** para los modelos tabulares XGBoost y LightGBM\n",
    "2. **Validaci√≥n robusta** mediante:\n",
    "   - Hold-Out Validation (ya implementada)\n",
    "   - Cross-Validation (k=5)\n",
    "   - Bootstrapping (100 muestras)\n",
    "3. **Modelos de Deep Learning** para capturar patrones espaciales y temporales:\n",
    "   - Redes CNN para patrones espaciales\n",
    "   - Redes ConvLSTM para patrones espaciotemporales\n",
    "\n",
    "El objetivo es proporcionar una evaluaci√≥n completa de diferentes enfoques de modelado para la predicci√≥n de precipitaci√≥n en regiones monta√±osas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Xjn1C7PKysIw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xjn1C7PKysIw",
    "outputId": "dd29e4f7-1612-4094-c1c6-06c9f06ccf11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06416284",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06416284",
    "outputId": "a8e4c864-34e9-41b2-d5c3-e6ccaaf3699e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ml_precipitation_prediction'...\n",
      "fatal: could not read Username for 'https://github.com': No such device or address\n",
      "[Errno 2] No such file or directory: 'ml_precipitation_prediction'\n",
      "/content\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (2025.1.2)\n",
      "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.0.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from xarray) (24.2)\n",
      "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.2.2)\n",
      "Requirement already satisfied: cftime in /usr/local/lib/python3.11/dist-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2025.1.31)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.14.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
      "Entorno configurado. Usando ruta base: /content/drive/MyDrive/ml_precipitation_prediction\n",
      "Directorio para salida de modelos creado: /content/drive/MyDrive/ml_precipitation_prediction/models/output\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n del entorno (compatible con Colab y local)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detectar si estamos en Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Si estamos en Colab, clonar el repositorio\n",
    "    !git clone https://github.com/username/ml_precipitation_prediction.git\n",
    "    %cd ml_precipitation_prediction\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn\n",
    "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
    "else:\n",
    "    # Si estamos en local, usar la ruta actual\n",
    "    if '/models' in os.getcwd():\n",
    "        BASE_PATH = Path('..')\n",
    "    else:\n",
    "        BASE_PATH = Path('.')\n",
    "\n",
    "print(f\"Entorno configurado. Usando ruta base: {BASE_PATH}\")\n",
    "\n",
    "# Si BASE_PATH viene como string, lo convertimos\n",
    "BASE_PATH = Path(BASE_PATH)\n",
    "\n",
    "# Ahora puedes concatenar correctamente\n",
    "model_output_dir = BASE_PATH / 'models' / 'output'\n",
    "model_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directorio para salida de modelos creado: {model_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462e5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizaci√≥n adaptativa de memoria RAM para Optuna (compatible con Colab y Local)\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "def get_available_memory():\n",
    "    \"\"\"Detecta la memoria RAM disponible en el sistema actual (Colab o local)\"\"\"\n",
    "    try:\n",
    "        # Obtener memoria disponible usando psutil\n",
    "        mem_info = psutil.virtual_memory()\n",
    "        available_mb = mem_info.available / (1024 * 1024)  # Convertir a MB\n",
    "        total_mb = mem_info.total / (1024 * 1024)  # Convertir a MB\n",
    "        \n",
    "        # Detectar si estamos en Colab\n",
    "        in_colab = 'google.colab' in sys.modules\n",
    "        if in_colab:\n",
    "            # En Colab, limitar el uso a un porcentaje conservador\n",
    "            print(\"Entorno detectado: Google Colab\")\n",
    "            max_usage_percent = 70  # Usar m√°ximo 70% de la memoria disponible en Colab\n",
    "        else:\n",
    "            # En local, tambi√©n ser conservador pero un poco menos\n",
    "            print(\"Entorno detectado: Local\")\n",
    "            max_usage_percent = 80  # Usar m√°ximo 80% de la memoria disponible en local\n",
    "        \n",
    "        # Calcular memoria m√°xima a usar\n",
    "        max_memory_mb = available_mb * (max_usage_percent / 100)\n",
    "        \n",
    "        print(f\"Memoria total del sistema: {total_mb:.0f} MB\")\n",
    "        print(f\"Memoria disponible: {available_mb:.0f} MB\")\n",
    "        print(f\"Memoria m√°xima a utilizar: {max_memory_mb:.0f} MB ({max_usage_percent}% de la disponible)\")\n",
    "        \n",
    "        return max_memory_mb\n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener informaci√≥n de memoria: {e}\")\n",
    "        # Valor conservador por defecto\n",
    "        return 2000  # 2GB por defecto si no se puede detectar\n",
    "\n",
    "def calculate_optimal_batch_size(mem_per_sample_mb, max_memory_mb, min_batch=10):\n",
    "    \"\"\"Calcula el tama√±o √≥ptimo de lote basado en la memoria disponible\"\"\"\n",
    "    # Estimar cu√°nta memoria usa cada muestra (con margen de seguridad)\n",
    "    optimal_batch = int(max_memory_mb / (mem_per_sample_mb * 1.5))\n",
    "    return max(optimal_batch, min_batch)\n",
    "\n",
    "def configure_optuna_for_memory_efficiency(max_memory_mb, model_name):\n",
    "    \"\"\"Configura par√°metros para Optuna basados en la memoria disponible\"\"\"\n",
    "    # Estimar n√∫mero de trials y paralelismo basado en la memoria\n",
    "    if max_memory_mb < 1000:  # Menos de 1GB disponible\n",
    "        n_trials = 15  # Reducir n√∫mero de trials\n",
    "        n_jobs = 1     # Sin paralelismo\n",
    "        use_sqlite = True  # Usar SQLite para ahorrar memoria\n",
    "    elif max_memory_mb < 4000:  # Menos de 4GB\n",
    "        n_trials = 25  # N√∫mero moderado de trials\n",
    "        n_jobs = min(2, os.cpu_count() or 2)  # Paralelismo limitado\n",
    "        use_sqlite = True  # Usar SQLite\n",
    "    else:  # 4GB o m√°s\n",
    "        n_trials = 30  # N√∫mero normal de trials\n",
    "        n_jobs = min(3, os.cpu_count() or 2)  # Mejor paralelismo\n",
    "        use_sqlite = True  # Mantener SQLite para estabilidad\n",
    "    \n",
    "    # Definir configuraci√≥n espec√≠fica por modelo\n",
    "    model_config = {\n",
    "        'RandomForest': {\n",
    "            'n_estimators_max': min(500, int(50 + max_memory_mb / 100)),  # Adaptar a memoria\n",
    "            'max_depth_max': min(50, int(10 + max_memory_mb / 200))\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'tree_method': 'hist',  # M√©todo eficiente en memoria\n",
    "            'max_depth_max': min(12, 3 + int(max_memory_mb / 1000))\n",
    "            'grow_policy': 'lossguide'  # M√°s eficiente en memoria\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'max_bin': min(255, 63 + int(max_memory_mb / 100)),  # Adaptar precisi√≥n\n",
    "            'num_leaves_max': min(150, 31 + int(max_memory_mb / 100))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Crear almacenamiento SQLite si es necesario\n",
    "    storage = None\n",
    "    if use_sqlite:\n",
    "        # Crear directorio para almacenamiento Optuna si no existe\n",
    "        os.makedirs(model_output_dir / 'optuna_storage', exist_ok=True)\n",
    "        storage = f\"sqlite:///{model_output_dir}/optuna_storage/optuna_{model_name}_{int(time.time())}.db\"\n",
    "    \n",
    "    # Devolver toda la configuraci√≥n optimizada\n",
    "    return {\n",
    "        'n_trials': n_trials,\n",
    "        'n_jobs': n_jobs,\n",
    "        'storage': storage,\n",
    "        'model_config': model_config.get(model_name, {})\n",
    "    }\n",
    "\n",
    "def memory_efficient_objective_factory(model_name, X_train, y_train, max_memory_mb, model_config):\n",
    "    \"\"\"Crea una funci√≥n objetivo para Optuna optimizada para memoria\"\"\"\n",
    "    # Configurar espacio de b√∫squeda adaptado a la memoria disponible\n",
    "    def objective(trial):\n",
    "        # Limpiar memoria antes de cada trial\n",
    "        gc.collect()\n",
    "        \n",
    "        # Configuraci√≥n de par√°metros espec√≠ficos para cada modelo, adaptados a la memoria\n",
    "        if model_name == 'RandomForest':\n",
    "            n_estimators_max = model_config.get('n_estimators_max', 500)\n",
    "            max_depth_max = model_config.get('max_depth_max', 50)\n",
    "            \n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, n_estimators_max),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, max_depth_max),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'random_state': 42,\n",
    "                # Par√°metros para optimizar memoria\n",
    "                'verbose': 0,\n",
    "                'n_jobs': 1 if max_memory_mb < 2000 else 2  # Limitar paralelismo seg√∫n memoria\n",
    "            }\n",
    "            model = RandomForestRegressor(**params)\n",
    "        \n",
    "        elif model_name == 'XGBoost':\n",
    "            max_depth_max = model_config.get('max_depth_max', 12)\n",
    "            tree_method = model_config.get('tree_method', 'hist')\n",
    "            grow_policy = model_config.get('grow_policy', 'lossguide')\n",
    "            \n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, max_depth_max),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "                'random_state': 42,\n",
    "                # Optimizaci√≥n de memoria\n",
    "                'tree_method': tree_method,\n",
    "                'grow_policy': grow_policy,\n",
    "                'verbosity': 0,\n",
    "                'nthread': 1 if max_memory_mb < 2000 else 2  # Adaptar seg√∫n memoria\n",
    "            }\n",
    "            model = XGBRegressor(**params)\n",
    "        \n",
    "        elif model_name == 'LightGBM':\n",
    "            num_leaves_max = model_config.get('num_leaves_max', 150)\n",
    "            max_bin = model_config.get('max_bin', 63)\n",
    "            \n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, num_leaves_max),\n",
    "                'max_depth': trial.suggest_int('max_depth', -1, 15),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "                'random_state': 42,\n",
    "                # Par√°metros para optimizar memoria\n",
    "                'verbose': -1,\n",
    "                'max_bin': max_bin,\n",
    "                'num_threads': 1 if max_memory_mb < 2000 else 2\n",
    "            }\n",
    "            model = LGBMRegressor(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Modelo no soportado: {model_name}\")\n",
    "        \n",
    "        # Evaluaci√≥n con validaci√≥n cruzada adaptada a memoria disponible\n",
    "        try:\n",
    "            # Reducir el n√∫mero de folds si hay poca memoria\n",
    "            n_folds = 3 if max_memory_mb < 2000 else 5\n",
    "            from sklearn.model_selection import KFold\n",
    "            \n",
    "            kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "            scores = []\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X_train):\n",
    "                X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "                y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                \n",
    "                # Entrenar modelo\n",
    "                model.fit(X_fold_train, y_fold_train)\n",
    "                \n",
    "                # Evaluar y liberar memoria inmediatamente\n",
    "                y_pred = model.predict(X_fold_val)\n",
    "                rmse = np.sqrt(mean_squared_error(y_fold_val, y_pred))\n",
    "                scores.append(rmse)\n",
    "                \n",
    "                # Limpieza agresiva de memoria\n",
    "                del X_fold_train, X_fold_val, y_fold_train, y_fold_val, y_pred\n",
    "                gc.collect()\n",
    "            \n",
    "            # Liberar modelo tambi√©n\n",
    "            del model\n",
    "            gc.collect()\n",
    "            \n",
    "            return np.mean(scores)\n",
    "        except Exception as e:\n",
    "            print(f\"Error en evaluaci√≥n: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    return objective\n",
    "\n",
    "def run_memory_efficient_optimization(model_name='XGBoost'):\n",
    "    \"\"\"Ejecuta optimizaci√≥n con Optuna adaptada a la memoria disponible\"\"\"\n",
    "    print(f\"\\nüß† Iniciando optimizaci√≥n adaptativa de memoria para {model_name}...\")\n",
    "    \n",
    "    # 1. Detectar memoria disponible\n",
    "    max_memory_mb = get_available_memory()\n",
    "    \n",
    "    # 2. Configurar Optuna seg√∫n memoria disponible\n",
    "    optuna_config = configure_optuna_for_memory_efficiency(max_memory_mb, model_name)\n",
    "    \n",
    "    # 3. Mostrar configuraci√≥n\n",
    "    print(f\"\\nConfiguraci√≥n para {model_name}:\")\n",
    "    print(f\"- Trials: {optuna_config['n_trials']}\")\n",
    "    print(f\"- Paralelismo: {optuna_config['n_jobs']} jobs\")\n",
    "    print(f\"- Almacenamiento: {'SQLite' if optuna_config['storage'] else 'En memoria'}\")\n",
    "    print(f\"- Configuraci√≥n espec√≠fica: {optuna_config['model_config']}\")\n",
    "    \n",
    "    # 4. Crear estudio Optuna\n",
    "    from optuna.pruners import MedianPruner\n",
    "    pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        pruner=pruner,\n",
    "        storage=optuna_config['storage'],\n",
    "        study_name=f\"{model_name}_memory_optimized\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    # 5. Crear funci√≥n objetivo eficiente en memoria\n",
    "    objective = memory_efficient_objective_factory(\n",
    "        model_name, \n",
    "        X_train_scaled, \n",
    "        y_train, \n",
    "        max_memory_mb,\n",
    "        optuna_config['model_config']\n",
    "    )\n",
    "    \n",
    "    # 6. Callback para liberar memoria peri√≥dicamente\n",
    "    def gc_callback(study, trial):\n",
    "        if trial.number % 3 == 0:  # Cada 3 trials\n",
    "            gc.collect()\n",
    "    \n",
    "    # 7. Visualizaci√≥n inicial\n",
    "    color_map = {\n",
    "        'RandomForest': '#e6f3ff',  # Azul claro\n",
    "        'XGBoost': '#fff0e6',      # Naranja claro\n",
    "        'LightGBM': '#e6fff2'      # Verde claro\n",
    "    }\n",
    "    display(HTML(f'<div style=\"background-color:{color_map.get(model_name, \"#f5f5f5\")}; padding:10px; border-radius:5px;\">' +\n",
    "                f'<h3>üîç Optimizaci√≥n adaptativa de memoria - {model_name}</h3>' +\n",
    "                f'<div>Memoria m√°xima a utilizar: {max_memory_mb:.0f} MB</div>' +\n",
    "                f'<div>Trials planeados: {optuna_config[\"n_trials\"]}</div>' +\n",
    "                f'</div>'))\n",
    "    \n",
    "    # 8. Iniciar optimizaci√≥n\n",
    "    callback = OptimizationProgressCallback(\n",
    "        optuna_config['n_trials'], f\"{model_name} (RAM optimizada)\"\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=optuna_config['n_trials'],\n",
    "        n_jobs=optuna_config['n_jobs'],\n",
    "        gc_after_trial=True,\n",
    "        callbacks=[callback, gc_callback]\n",
    "    )\n",
    "    \n",
    "    # 9. Obtener mejores par√°metros\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value\n",
    "    \n",
    "    # 10. Visualizar resultados\n",
    "    display(HTML(f'<div style=\"background-color:{color_map.get(model_name, \"#f5f5f5\")}; padding:10px; border-radius:5px;\">' +\n",
    "                f'<h3>‚úÖ Optimizaci√≥n completada - {model_name}</h3>' +\n",
    "                f'<div><b>Mejor RMSE:</b> {best_value:.4f}</div>' +\n",
    "                f'<div><b>Mejores par√°metros:</b> {str(best_params)}</div>' +\n",
    "                f'</div>'))\n",
    "    \n",
    "    # 11. Entrenar modelo final con los mejores par√°metros\n",
    "    print(\"\\nEntrenando modelo final con los mejores par√°metros...\")\n",
    "    \n",
    "    if model_name == 'RandomForest':\n",
    "        best_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "    elif model_name == 'XGBoost':\n",
    "        best_model = XGBRegressor(**best_params, random_state=42)\n",
    "    elif model_name == 'LightGBM':\n",
    "        best_model = LGBMRegressor(**best_params, random_state=42)\n",
    "    \n",
    "    # 12. Evaluar modelo final\n",
    "    better_model, metrics = entrenar_y_evaluar_modelo(\n",
    "        best_model, f'{model_name}_RAM_Opt', X_train_scaled, y_train, X_test_scaled, y_test\n",
    "    )\n",
    "    \n",
    "    # 13. Guardar resultados\n",
    "    resultados_base[f'{model_name}_RAM_Opt'] = metrics\n",
    "    modelo_file = guardar_modelo(better_model, f'{model_name}_RAM_Opt')\n",
    "    modelos_guardados[f'{model_name}_RAM_Opt'] = modelo_file\n",
    "    \n",
    "    return best_params, better_model, metrics\n",
    "\n",
    "# Ejemplo de uso (descomenta para ejecutar):\n",
    "\"\"\"\n",
    "# Para ejecutar la optimizaci√≥n adaptativa solo de un modelo:\n",
    "xgb_params, xgb_model, xgb_metrics = run_memory_efficient_optimization('XGBoost')\n",
    "\n",
    "# Para ejecutar todos los modelos con optimizaci√≥n de memoria RAM:\n",
    "print(\"\\nüöÄ Iniciando optimizaci√≥n adaptativa para todos los modelos...\")\n",
    "\n",
    "# Optimizar RandomForest con control de memoria adaptativo\n",
    "rf_params, rf_model, rf_metrics = run_memory_efficient_optimization('RandomForest')\n",
    "gc.collect()  # Liberar memoria entre modelos\n",
    "\n",
    "# Optimizar XGBoost con control de memoria adaptativo\n",
    "xgb_params, xgb_model, xgb_metrics = run_memory_efficient_optimization('XGBoost')\n",
    "gc.collect()  # Liberar memoria entre modelos\n",
    "\n",
    "# Optimizar LightGBM con control de memoria adaptativo\n",
    "lgbm_params, lgbm_model, lgbm_metrics = run_memory_efficient_optimization('LightGBM')\n",
    "gc.collect()  # Liberar memoria final\n",
    "\"\"\"\n",
    "\n",
    "# Ejecutar solo esta celda para iniciar la optimizaci√≥n adaptativa de memoria para XGBoost\n",
    "print(\"\\nüîç Ejecutando optimizaci√≥n adaptativa de memoria RAM para XGBoost...\")\n",
    "xgb_params, xgb_model, xgb_metrics = run_memory_efficient_optimization('XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313434be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones adicionales para Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, save_model, load_model\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Conv2D, Conv3D, ConvLSTM2D, BatchNormalization, \n",
    "                                   MaxPooling2D, Flatten, Input, concatenate, Reshape, TimeDistributed)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(\"TensorFlow versi√≥n:\", tf.__version__)\n",
    "\n",
    "# Configurar GPU si est√° disponible\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(f\"GPU disponible: {physical_devices}\")\n",
    "    # Permitir crecimiento de memoria seg√∫n sea necesario\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "else:\n",
    "    print(\"No se detect√≥ GPU. Usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fb555",
   "metadata": {
    "id": "e47fb555"
   },
   "outputs": [],
   "source": [
    "# 1. Importaciones necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import optuna\n",
    "import pickle\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importaciones para barras de progreso y mejora de visualizaci√≥n\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import time\n",
    "\n",
    "# Configurar visualizaci√≥n m√°s atractiva\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26215d90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "26215d90",
    "outputId": "ccb06926-e151-453f-a306-999f15566bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando archivo en: /content/drive/MyDrive/ml_precipitation_prediction/data/output/complete_dataset_with_features.nc\n",
      "Intentando cargar el archivo: /content/drive/MyDrive/ml_precipitation_prediction/data/output/complete_dataset_with_features.nc\n",
      "Archivo cargado exitosamente con xarray\n",
      "\n",
      "Informaci√≥n del dataset:\n",
      "xarray.Dataset {\n",
      "dimensions:\n",
      "\ttime = 530 ;\n",
      "\tlatitude = 62 ;\n",
      "\tlongitude = 66 ;\n",
      "\n",
      "variables:\n",
      "\tdatetime64[ns] time(time) ;\n",
      "\tfloat32 latitude(latitude) ;\n",
      "\tfloat32 longitude(longitude) ;\n",
      "\tfloat32 total_precipitation(time, latitude, longitude) ;\n",
      "\tfloat32 max_daily_precipitation(time, latitude, longitude) ;\n",
      "\tfloat32 min_daily_precipitation(time, latitude, longitude) ;\n",
      "\tfloat32 daily_precipitation_std(time, latitude, longitude) ;\n",
      "\tfloat32 month_sin(time, latitude, longitude) ;\n",
      "\tfloat32 month_cos(time, latitude, longitude) ;\n",
      "\tfloat32 doy_sin(time, latitude, longitude) ;\n",
      "\tfloat32 doy_cos(time, latitude, longitude) ;\n",
      "\tfloat64 elevation(latitude, longitude) ;\n",
      "\tfloat32 slope(latitude, longitude) ;\n",
      "\tfloat32 aspect(latitude, longitude) ;\n",
      "\n",
      "// global attributes:\n",
      "\t:description = ST-HyMOUNTAIN-Net ready dataset with CHIRPS monthly precipitation and DEM variables ;\n",
      "\t:source = CHIRPS v2.0 & DEM Boyac√° ;\n",
      "\t:created_at = 2025-04-27 19:02:24 ;\n",
      "}None\n",
      "\n",
      "Variables disponibles:\n",
      "- total_precipitation: (530, 62, 66)\n",
      "- max_daily_precipitation: (530, 62, 66)\n",
      "- min_daily_precipitation: (530, 62, 66)\n",
      "- daily_precipitation_std: (530, 62, 66)\n",
      "- month_sin: (530, 62, 66)\n",
      "- month_cos: (530, 62, 66)\n",
      "- doy_sin: (530, 62, 66)\n",
      "- doy_cos: (530, 62, 66)\n",
      "- elevation: (62, 66)\n",
      "- slope: (62, 66)\n",
      "- aspect: (62, 66)\n",
      "Dataset cargado con √©xito. Dimensiones: (2168760, 14)\n",
      "\n",
      "Primeras filas del DataFrame:\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"    print(\\\"No se pudo cargar el dataset\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1981-01-01 00:00:00\",\n        \"max\": \"1981-01-01 00:00:00\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1981-01-01 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latitude\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4.3249969482421875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"longitude\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -74.92500305175781\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_precipitation\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          40.750823974609375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max_daily_precipitation\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          21.819194793701172\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_daily_precipitation\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"daily_precipitation_std\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5.019045352935791\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month_sin\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month_cos\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.8660253882408142\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doy_sin\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.01720157451927662\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doy_cos\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9998520612716675\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"elevation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 123.34058798530309,\n        \"min\": 248.7760453427361,\n        \"max\": 519.7501066909579,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          519.7501066909579\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          89.86701965332031\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"aspect\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          73.48167419433594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-15081ce3-b18d-4749-b97a-17789f6d32ca\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>total_precipitation</th>\n",
       "      <th>max_daily_precipitation</th>\n",
       "      <th>min_daily_precipitation</th>\n",
       "      <th>daily_precipitation_std</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>doy_sin</th>\n",
       "      <th>doy_cos</th>\n",
       "      <th>elevation</th>\n",
       "      <th>slope</th>\n",
       "      <th>aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.975006</td>\n",
       "      <td>47.381050</td>\n",
       "      <td>24.706928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.825776</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>493.784552</td>\n",
       "      <td>89.539551</td>\n",
       "      <td>102.044502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.925003</td>\n",
       "      <td>40.750824</td>\n",
       "      <td>21.819195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.019045</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>519.750107</td>\n",
       "      <td>89.867020</td>\n",
       "      <td>73.481674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.875008</td>\n",
       "      <td>46.338623</td>\n",
       "      <td>26.092327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.740223</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>248.776045</td>\n",
       "      <td>89.722221</td>\n",
       "      <td>65.916817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.825005</td>\n",
       "      <td>48.779938</td>\n",
       "      <td>29.421450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.611738</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>351.415728</td>\n",
       "      <td>86.986130</td>\n",
       "      <td>140.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.775002</td>\n",
       "      <td>38.932945</td>\n",
       "      <td>18.483061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.733574</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>278.261922</td>\n",
       "      <td>88.273293</td>\n",
       "      <td>18.439939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15081ce3-b18d-4749-b97a-17789f6d32ca')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-15081ce3-b18d-4749-b97a-17789f6d32ca button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-15081ce3-b18d-4749-b97a-17789f6d32ca');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-68be0e13-4bc8-4d50-8f9a-2810d0d15626\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-68be0e13-4bc8-4d50-8f9a-2810d0d15626')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-68be0e13-4bc8-4d50-8f9a-2810d0d15626 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "        time  latitude  longitude  total_precipitation  \\\n",
       "0 1981-01-01  4.324997 -74.975006            47.381050   \n",
       "1 1981-01-01  4.324997 -74.925003            40.750824   \n",
       "2 1981-01-01  4.324997 -74.875008            46.338623   \n",
       "3 1981-01-01  4.324997 -74.825005            48.779938   \n",
       "4 1981-01-01  4.324997 -74.775002            38.932945   \n",
       "\n",
       "   max_daily_precipitation  min_daily_precipitation  daily_precipitation_std  \\\n",
       "0                24.706928                      0.0                 5.825776   \n",
       "1                21.819195                      0.0                 5.019045   \n",
       "2                26.092327                      0.0                 5.740223   \n",
       "3                29.421450                      0.0                 5.611738   \n",
       "4                18.483061                      0.0                 3.733574   \n",
       "\n",
       "   month_sin  month_cos   doy_sin   doy_cos   elevation      slope      aspect  \n",
       "0        0.5   0.866025  0.017202  0.999852  493.784552  89.539551  102.044502  \n",
       "1        0.5   0.866025  0.017202  0.999852  519.750107  89.867020   73.481674  \n",
       "2        0.5   0.866025  0.017202  0.999852  248.776045  89.722221   65.916817  \n",
       "3        0.5   0.866025  0.017202  0.999852  351.415728  86.986130  140.916000  \n",
       "4        0.5   0.866025  0.017202  0.999852  278.261922  88.273293   18.439939  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Cargar el dataset NetCDF\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Carga un archivo NetCDF y lo convierte a pandas DataFrame\"\"\"\n",
    "    try:\n",
    "        # Cargar el archivo NetCDF con xarray\n",
    "        print(f\"Intentando cargar el archivo: {file_path}\")\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        print(\"Archivo cargado exitosamente con xarray\")\n",
    "\n",
    "        # Mostrar informaci√≥n del dataset cargado\n",
    "        print(\"\\nInformaci√≥n del dataset:\")\n",
    "        print(ds.info())\n",
    "        print(\"\\nVariables disponibles:\")\n",
    "        for var_name in ds.data_vars:\n",
    "            print(f\"- {var_name}: {ds[var_name].shape}\")\n",
    "\n",
    "        # Convertir a DataFrame\n",
    "        df = ds.to_dataframe().reset_index()\n",
    "        return df, ds\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el archivo NetCDF: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Ruta al dataset\n",
    "data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features.nc'\n",
    "print(f\"Buscando archivo en: {data_file}\")\n",
    "\n",
    "# Cargar el dataset\n",
    "df, ds_original = load_dataset(data_file)\n",
    "\n",
    "# Verificar si se carg√≥ correctamente\n",
    "if df is not None:\n",
    "    print(f\"Dataset cargado con √©xito. Dimensiones: {df.shape}\")\n",
    "    print(\"\\nPrimeras filas del DataFrame:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"No se pudo cargar el dataset. Verificar la ruta y el formato del archivo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f0aebbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f0aebbc",
    "outputId": "48e82987-ff47-41e8-9f40-57e53cf90cf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna objetivo identificada: total_precipitation\n",
      "Filas antes de eliminar NaN: 2168760\n",
      "Filas despu√©s de eliminar NaN: 2168760\n",
      "\n",
      "Features seleccionadas (12):\n",
      "['latitude', 'longitude', 'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos', 'elevation', 'slope', 'aspect']\n",
      "\n",
      "Variable objetivo: total_precipitation\n"
     ]
    }
   ],
   "source": [
    "# 3. Preparaci√≥n de los datos\n",
    "if df is not None:\n",
    "    # Identificar la columna objetivo (precipitaci√≥n)\n",
    "    target_column = 'total_precipitation'  # Ajustar si tiene otro nombre en tu dataset\n",
    "\n",
    "    # Ver si existe 'precip_target' o usar 'total_precipitation'\n",
    "    if 'total_precipitation' in df.columns:\n",
    "        target_column = 'total_precipitation'\n",
    "\n",
    "    print(f\"Columna objetivo identificada: {target_column}\")\n",
    "\n",
    "    # Separar variables predictoras y variable objetivo\n",
    "    feature_cols = [col for col in df.columns if col != target_column and not pd.isna(df[col]).all()]\n",
    "\n",
    "    # Eliminar columnas no num√©ricas para los modelos (como fechas o coordenadas si no se usan como features)\n",
    "    non_feature_cols = ['time', 'spatial_ref']\n",
    "    feature_cols = [col for col in feature_cols if col not in non_feature_cols]\n",
    "\n",
    "    # Eliminar filas con valores NaN\n",
    "    print(f\"Filas antes de eliminar NaN: {df.shape[0]}\")\n",
    "    df_clean = df.dropna(subset=[target_column] + feature_cols)\n",
    "    print(f\"Filas despu√©s de eliminar NaN: {df_clean.shape[0]}\")\n",
    "\n",
    "    # Separar features y target\n",
    "    X = df_clean[feature_cols]\n",
    "    y = df_clean[target_column]\n",
    "\n",
    "    print(f\"\\nFeatures seleccionadas ({len(feature_cols)}):\\n{feature_cols}\")\n",
    "    print(f\"\\nVariable objetivo: {target_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da222af5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da222af5",
    "outputId": "579fbd57-a790-42dd-807a-e61fc1daacbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del conjunto de entrenamiento: (1735008, 12)\n",
      "Dimensiones del conjunto de prueba: (433752, 12)\n",
      "Escalador guardado en models/output/scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# 4. Divisi√≥n del conjunto de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dimensiones del conjunto de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Dimensiones del conjunto de prueba: {X_test.shape}\")\n",
    "\n",
    "# 5. Estandarizaci√≥n de variables predictoras\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Guardar el scaler para uso futuro\n",
    "with open(model_output_dir / 'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Escalador guardado en models/output/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba053b",
   "metadata": {
    "id": "c5ba053b"
   },
   "outputs": [],
   "source": [
    "# 6. Funciones de evaluaci√≥n y entrenamiento\n",
    "def evaluar_modelo(y_true, y_pred):\n",
    "    \"\"\"Eval√∫a el rendimiento de un modelo usando m√∫ltiples m√©tricas\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "def entrenar_y_evaluar_modelo(modelo, nombre, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Entrena un modelo y eval√∫a su rendimiento con visualizaci√≥n del progreso\"\"\"\n",
    "    # Crear widget para mostrar informaci√≥n del proceso\n",
    "    display(HTML(f'<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\">' +\n",
    "                 f'<h3>üîÑ Entrenando modelo: {nombre}</h3>' +\n",
    "                 f'<div id=\"status_{nombre}\">Estado: Iniciando entrenamiento...</div>' +\n",
    "                 f'</div>'))\n",
    "    \n",
    "    # Tiempo de inicio\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Entrenar el modelo con seguimiento visual seg√∫n el tipo\n",
    "    if hasattr(modelo, 'fit_generator') or nombre in ['XGBoost', 'XGBoost_Optuna', 'LightGBM', 'LightGBM_Optuna']:\n",
    "        # Para modelos que soportan entrenamiento por lotes como XGBoost, LightGBM\n",
    "        print(f\"Entrenando {nombre} con visualizaci√≥n de progreso...\")\n",
    "        if hasattr(modelo, 'n_estimators'):\n",
    "            n_estimators = modelo.n_estimators\n",
    "            for i in tqdm(range(n_estimators), desc=f\"Entrenando {nombre}\"):\n",
    "                if i == 0:\n",
    "                    # Primera iteraci√≥n, ajuste inicial\n",
    "                    if nombre.startswith('LightGBM'):\n",
    "                        # LightGBM tiene par√°metro verbose\n",
    "                        temp_modelo = type(modelo)(n_estimators=1, **{k:v for k,v in modelo.get_params().items() \n",
    "                                                                 if k != 'n_estimators' and k != 'verbose'}, verbose=-1)\n",
    "                    else:\n",
    "                        temp_modelo = type(modelo)(n_estimators=1, **{k:v for k,v in modelo.get_params().items() \n",
    "                                                                if k != 'n_estimators'})\n",
    "                    temp_modelo.fit(X_train, y_train)\n",
    "                elif i == n_estimators - 1:\n",
    "                    # √öltima iteraci√≥n, ajuste completo\n",
    "                    modelo.fit(X_train, y_train)\n",
    "                \n",
    "                # Actualizar progreso visual\n",
    "                if i % max(1, n_estimators // 10) == 0:\n",
    "                    clear_output(wait=True)\n",
    "                    display(HTML(f'<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\">' +\n",
    "                                f'<h3>üîÑ Entrenando modelo: {nombre}</h3>' +\n",
    "                                f'<div id=\"status_{nombre}\">Estado: Progreso {i+1}/{n_estimators} estimadores ({((i+1)/n_estimators*100):.1f}%)</div>' +\n",
    "                                f'</div>'))\n",
    "                    time.sleep(0.1)  # Peque√±a pausa para actualizaci√≥n visual\n",
    "        else:\n",
    "            # Si no tiene n_estimators, entrenamiento directo\n",
    "            modelo.fit(X_train, y_train)\n",
    "    else:\n",
    "        # Para modelos est√°ndar como RandomForest\n",
    "        modelo.fit(X_train, y_train)\n",
    "    \n",
    "    # Tiempo de entrenamiento\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Visualizar tiempo de entrenamiento\n",
    "    display(HTML(f'<div style=\"background-color:#e6ffe6; padding:10px; border-radius:5px;\">' +\n",
    "                f'<h3>‚úÖ Entrenamiento completado: {nombre}</h3>' +\n",
    "                f'<div>Tiempo de entrenamiento: {training_time:.2f} segundos</div>' +\n",
    "                f'</div>'))\n",
    "    \n",
    "    print(f\"Evaluando rendimiento de {nombre}...\")\n",
    "    predicciones = modelo.predict(X_test)\n",
    "    rmse, mae, r2 = evaluar_modelo(y_test, predicciones)\n",
    "    \n",
    "    # Visualizar m√©tricas con estilo\n",
    "    display(HTML(f'<div style=\"background-color:#f5f5dc; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
    "                f'<h3>üìä M√©tricas para {nombre}</h3>' +\n",
    "                f'<table style=\"width:100%; text-align:left;\">' +\n",
    "                f'<tr><th>M√©trica</th><th>Valor</th></tr>' +\n",
    "                f'<tr><td>RMSE</td><td>{rmse:.4f}</td></tr>' +\n",
    "                f'<tr><td>MAE</td><td>{mae:.4f}</td></tr>' +\n",
    "                f'<tr><td>R¬≤</td><td>{r2:.4f}</td></tr>' +\n",
    "                f'</table></div>'))\n",
    "    \n",
    "    return modelo, (rmse, mae, r2)\n",
    "\n",
    "def guardar_modelo(modelo, nombre):\n",
    "    \"\"\"Guarda un modelo entrenado en disco\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{nombre}_{timestamp}.pkl\"\n",
    "    with open(model_output_dir / filename, 'wb') as f:\n",
    "        pickle.dump(modelo, f)\n",
    "    \n",
    "    # Visualizar confirmaci√≥n de guardado\n",
    "    display(HTML(f'<div style=\"background-color:#e6ffee; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
    "                f'<h3>üíæ Modelo guardado</h3>' +\n",
    "                f'<div>Modelo <b>{nombre}</b> guardado como: {filename}</div>' +\n",
    "                f'</div>'))\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Entrenamiento de modelos base sin optimizaci√≥n\n",
    "\n",
    "# Inicializar diccionarios para almacenar resultados y modelos\n",
    "resultados_base = {}  # Para almacenar m√©tricas (RMSE, MAE, R2)\n",
    "modelos_base = {}     # Para almacenar instancias de modelos\n",
    "modelos_guardados = {} # Para almacenar nombres de archivos guardados\n",
    "\n",
    "print(\"\\nüîç Entrenando modelos baseline sin optimizaci√≥n de hiperpar√°metros...\")\n",
    "\n",
    "# 1. Modelo RandomForest b√°sico\n",
    "print(\"\\nEntrenando RandomForest base...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model, rf_metrics = entrenar_y_evaluar_modelo(\n",
    "    rf_model, 'RandomForest', X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "resultados_base['RandomForest'] = rf_metrics\n",
    "modelos_base['RandomForest'] = rf_model\n",
    "modelo_file = guardar_modelo(rf_model, 'RandomForest')\n",
    "modelos_guardados['RandomForest'] = modelo_file\n",
    "\n",
    "# Visualizar importancia de caracter√≠sticas para RandomForest\n",
    "plt.figure(figsize=(12, 6))\n",
    "feat_importances = rf_model.feature_importances_\n",
    "indices = np.argsort(feat_importances)[::-1]\n",
    "plt.bar(range(len(indices)), feat_importances[indices], color='skyblue')\n",
    "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
    "plt.title('Importancia de Caracter√≠sticas - RandomForest')\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'randomforest_feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Modelo XGBoost b√°sico\n",
    "print(\"\\nEntrenando XGBoost base...\")\n",
    "xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb_model, xgb_metrics = entrenar_y_evaluar_modelo(\n",
    "    xgb_model, 'XGBoost', X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "resultados_base['XGBoost'] = xgb_metrics\n",
    "modelos_base['XGBoost'] = xgb_model\n",
    "modelo_file = guardar_modelo(xgb_model, 'XGBoost')\n",
    "modelos_guardados['XGBoost'] = modelo_file\n",
    "\n",
    "# Visualizar importancia de caracter√≠sticas para XGBoost\n",
    "plt.figure(figsize=(12, 6))\n",
    "feat_importances = xgb_model.feature_importances_\n",
    "indices = np.argsort(feat_importances)[::-1]\n",
    "plt.bar(range(len(indices)), feat_importances[indices], color='coral')\n",
    "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
    "plt.title('Importancia de Caracter√≠sticas - XGBoost')\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'xgboost_feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Modelo LightGBM b√°sico\n",
    "print(\"\\nEntrenando LightGBM base...\")\n",
    "lgbm_model = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "lgbm_model, lgbm_metrics = entrenar_y_evaluar_modelo(\n",
    "    lgbm_model, 'LightGBM', X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "resultados_base['LightGBM'] = lgbm_metrics\n",
    "modelos_base['LightGBM'] = lgbm_model\n",
    "modelo_file = guardar_modelo(lgbm_model, 'LightGBM')\n",
    "modelos_guardados['LightGBM'] = modelo_file\n",
    "\n",
    "# Visualizar importancia de caracter√≠sticas para LightGBM\n",
    "plt.figure(figsize=(12, 6))\n",
    "feat_importances = lgbm_model.feature_importances_\n",
    "indices = np.argsort(feat_importances)[::-1]\n",
    "plt.bar(range(len(indices)), feat_importances[indices], color='lightgreen')\n",
    "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
    "plt.title('Importancia de Caracter√≠sticas - LightGBM')\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'lightgbm_feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# Comparar resultados de modelos base\n",
    "print(\"\\nüîç Comparaci√≥n de modelos base sin optimizaci√≥n:\")\n",
    "temp_df = pd.DataFrame(resultados_base, index=['RMSE', 'MAE', 'R2']).T\n",
    "print(\"\\nOrdenados por RMSE (menor es mejor):\")\n",
    "display(temp_df.sort_values('RMSE'))\n",
    "\n",
    "# Visualizar comparaci√≥n de RMSE\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=temp_df.index, y=temp_df['RMSE'])\n",
    "plt.title('Comparaci√≥n de RMSE - Modelos Base')\n",
    "plt.ylabel('RMSE (menor es mejor)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'baseline_rmse_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d90151",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c1d90151",
    "outputId": "3564d17f-5103-4615-e806-bbaa1457ef7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 01:29:40,112] A new study created in memory with name: no-name-f2097522-b917-45e3-a523-4d5b98a66044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando optimizaci√≥n de hiperpar√°metros para RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 01:58:10,879] Trial 0 finished with value: 36.8066452846521 and parameters: {'n_estimators': 343, 'max_depth': 34, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 0 with value: 36.8066452846521.\n",
      "[I 2025-04-28 02:18:38,335] Trial 1 finished with value: 41.63606143990411 and parameters: {'n_estimators': 304, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 0 with value: 36.8066452846521.\n",
      "[W 2025-04-28 02:18:39,457] Trial 2 failed with parameters: {'n_estimators': 371, 'max_depth': 49, 'min_samples_split': 7, 'min_samples_leaf': 10, 'max_features': 'auto'} because of the following error: ValueError('\\nAll the 5 fits failed.\\nIt is very likely that your model is misconfigured.\\nYou can try to debug the error by setting error_score=\\'raise\\'.\\n\\nBelow are more details about the failures:\\n--------------------------------------------------------------------------------\\n3 fits failed with the following error:\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\\n    estimator.fit(X_train, y_train, **fit_params)\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\\n    estimator._validate_params()\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\\n    validate_parameter_constraints(\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\\n    raise InvalidParameterError(\\nsklearn.utils._param_validation.InvalidParameterError: The \\'max_features\\' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {\\'log2\\', \\'sqrt\\'} or None. Got \\'auto\\' instead.\\n\\n--------------------------------------------------------------------------------\\n2 fits failed with the following error:\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\\n    estimator.fit(X_train, y_train, **fit_params)\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\\n    estimator._validate_params()\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\\n    validate_parameter_constraints(\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\\n    raise InvalidParameterError(\\nsklearn.utils._param_validation.InvalidParameterError: The \\'max_features\\' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {\\'sqrt\\', \\'log2\\'} or None. Got \\'auto\\' instead.\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"<ipython-input-25-63084c155e09>\", line 13, in objective_rf\n",
      "    score = cross_val_score(model, X_train_scaled, y_train,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 684, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 431, in cross_validate\n",
      "    _warn_or_raise_about_fit_failures(results, error_score)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 517, in _warn_or_raise_about_fit_failures\n",
      "    raise ValueError(all_fits_failed_message)\n",
      "ValueError: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "[W 2025-04-28 02:18:39,463] Trial 2 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n    estimator._validate_params()\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n    estimator._validate_params()\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-63084c155e09>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iniciando optimizaci√≥n de hiperpar√°metros para RandomForest...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mstudy_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mstudy_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nMejores hiperpar√°metros encontrados:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-63084c155e09>\u001b[0m in \u001b[0;36mobjective_rf\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     11\u001b[0m     }\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     score = cross_val_score(model, X_train_scaled, y_train, \n\u001b[0m\u001b[1;32m     14\u001b[0m                           scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1)\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Devolvemos negativo porque optimizamos minimizando\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;31m# For callable scoring, the return type is only know after calling. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n    estimator._validate_params()\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n    estimator._validate_params()\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n"
     ]
    }
   ],
   "source": [
    "# 8. Optimizaci√≥n de hiperpar√°metros con Optuna (Refactorizado para los tres modelos)\n",
    "\n",
    "# Clase para mostrar el progreso de la optimizaci√≥n con Optuna\n",
    "class OptimizationProgressCallback:\n",
    "    def __init__(self, n_trials, desc=\"Optimizaci√≥n\"):\n",
    "        self.pbar = tqdm(total=n_trials, desc=desc)\n",
    "        self.best_value = float('inf')\n",
    "        self.n_trials = n_trials\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def __call__(self, study, trial):\n",
    "        # Actualizar la mejor m√©trica si hay mejora\n",
    "        if study.best_value < self.best_value:\n",
    "            self.best_value = study.best_value\n",
    "            best_params_str = \", \".join([f\"{k}={v}\" for k, v in study.best_params.items()])\n",
    "            elapsed = time.time() - self.start_time\n",
    "            minutes, seconds = divmod(elapsed, 60)\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f'<div style=\"background-color:#e6f3ff; padding:10px; border-radius:5px;\">' +\n",
    "                        f'<h3>üîç Optimizaci√≥n de hiperpar√°metros en progreso</h3>' +\n",
    "                        f'<div><b>Mejor RMSE:</b> {self.best_value:.4f}</div>' +\n",
    "                        f'<div><b>Mejores par√°metros:</b> {best_params_str}</div>' +\n",
    "                        f'<div><b>Prueba actual:</b> {trial.number+1}/{self.n_trials}</div>' +\n",
    "                        f'<div><b>Tiempo transcurrido:</b> {int(minutes)}m {int(seconds)}s</div>' +\n",
    "                        f'<div><b>Progreso:</b></div>' +\n",
    "                        f'</div>'))\n",
    "        \n",
    "        # Actualizar la barra de progreso\n",
    "        self.pbar.update(1)\n",
    "        self.pbar.set_postfix({\"mejor_rmse\": f\"{study.best_value:.4f}\"})\n",
    "        \n",
    "        # Si es la √∫ltima iteraci√≥n, cerramos la barra\n",
    "        if trial.number + 1 == self.n_trials:\n",
    "            self.pbar.close()\n",
    "\n",
    "def optimize_model_hyperparams(model_name, n_trials=30):\n",
    "    \"\"\"Funci√≥n para optimizar hiperpar√°metros de diferentes modelos usando Optuna\"\"\"\n",
    "    # Definir espacio de b√∫squeda seg√∫n el modelo\n",
    "    def objective(trial):\n",
    "        # Configuraci√≥n de par√°metros espec√≠ficos para cada modelo\n",
    "        if model_name == 'RandomForest':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            model = RandomForestRegressor(**params)\n",
    "        \n",
    "        elif model_name == 'XGBoost':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            model = XGBRegressor(**params)\n",
    "        \n",
    "        elif model_name == 'LightGBM':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "                'max_depth': trial.suggest_int('max_depth', -1, 15),  # -1 significa sin restricci√≥n\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            model = LGBMRegressor(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Modelo no soportado: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            score = cross_val_score(model, X_train_scaled, y_train,\n",
    "                              scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1,\n",
    "                              error_score='raise')\n",
    "            return -np.mean(score)  # Devolvemos negativo porque optimizamos minimizando\n",
    "        except Exception as e:\n",
    "            print(f\"Error en prueba de hiperpar√°metros para {model_name}: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    # Crear y configurar estudio de Optuna\n",
    "    print(f\"\\nIniciando optimizaci√≥n de hiperpar√°metros para {model_name}...\")\n",
    "    \n",
    "    # Definir colores para cada modelo\n",
    "    color_map = {\n",
    "        'RandomForest': '#e6f3ff',  # Azul claro\n",
    "        'XGBoost': '#fff0e6',      # Naranja claro\n",
    "        'LightGBM': '#e6fff2'      # Verde claro\n",
    "    }\n",
    "    \n",
    "    # Visualizaci√≥n inicial\n",
    "    display(HTML(f'<div style=\"background-color:{color_map[model_name]}; padding:10px; border-radius:5px;\">' +\n",
    "                f'<h3>üîç Iniciando optimizaci√≥n de hiperpar√°metros - {model_name}</h3>' +\n",
    "                f'<div>Pruebas totales: {n_trials}</div>' +\n",
    "                f'<div>M√©trica objetivo: RMSE (menor es mejor)</div>' +\n",
    "                f'</div>'))\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    callback = OptimizationProgressCallback(n_trials, f\"{model_name} Optimization\")\n",
    "    exceptions = (ValueError,) if model_name == 'RandomForest' else None\n",
    "    study.optimize(objective, n_trials=n_trials, catch=exceptions, callbacks=[callback])\n",
    "    \n",
    "    # Visualizar resultados finales de optimizaci√≥n\n",
    "    best_rmse = study.best_value\n",
    "    best_params = study.best_params\n",
    "    best_params_str = \", \".join([f\"{k}={v}\" for k, v in best_params.items()])\n",
    "    \n",
    "    display(HTML(f'<div style=\"background-color:{color_map[model_name]}; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
    "                f'<h3>‚úÖ Optimizaci√≥n completada - {model_name}</h3>' +\n",
    "                f'<div><b>Mejor RMSE:</b> {best_rmse:.4f}</div>' +\n",
    "                f'<div><b>Mejores par√°metros:</b> {best_params_str}</div>' +\n",
    "                f'</div>'))\n",
    "    \n",
    "    # Crear y entrenar el mejor modelo con los par√°metros optimizados\n",
    "    if model_name == 'RandomForest':\n",
    "        best_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "    elif model_name == 'XGBoost':\n",
    "        best_model = XGBRegressor(**best_params, random_state=42)\n",
    "    elif model_name == 'LightGBM':\n",
    "        best_model = LGBMRegressor(**best_params, random_state=42)\n",
    "    \n",
    "    # Entrenar y evaluar el modelo optimizado\n",
    "    mejor_modelo, metricas = entrenar_y_evaluar_modelo(\n",
    "        best_model, f'{model_name}_Optuna', X_train_scaled, y_train, X_test_scaled, y_test\n",
    "    )\n",
    "    \n",
    "    # Guardar los resultados y el modelo\n",
    "    resultados_base[f'{model_name}_Optuna'] = metricas\n",
    "    modelo_file = guardar_modelo(mejor_modelo, f'{model_name}_Optuna')\n",
    "    modelos_guardados[f'{model_name}_Optuna'] = modelo_file\n",
    "    \n",
    "    # Visualizar importancia de caracter√≠sticas\n",
    "    feature_importances = mejor_modelo.feature_importances_\n",
    "    indices = np.argsort(feature_importances)[::-1]\n",
    "    \n",
    "    # Definir colores para gr√°ficas por modelo\n",
    "    plot_color_map = {\n",
    "        'RandomForest': 'skyblue',\n",
    "        'XGBoost': 'coral',\n",
    "        'LightGBM': 'lightgreen'\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(indices)), feature_importances[indices], color=plot_color_map[model_name])\n",
    "    plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
    "    plt.title(f'Importancia de Caracter√≠sticas - {model_name} Optimizado')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(model_output_dir / f'{model_name.lower()}_feature_importance.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return best_params, mejor_modelo, metricas\n",
    "\n",
    "# Optimizar cada uno de los modelos\n",
    "n_trials_per_model = 30  # N√∫mero de pruebas para cada modelo\n",
    "\n",
    "# Optimizar RandomForest\n",
    "print(\"Iniciando optimizaci√≥n de hiperpar√°metros para RandomForest...\")\n",
    "rf_best_params, mejor_rf, rf_metrics = optimize_model_hyperparams('RandomForest', n_trials_per_model)\n",
    "\n",
    "# Optimizar XGBoost\n",
    "print(\"Iniciando optimizaci√≥n de hiperpar√°metros para XGBoost...\")\n",
    "xgb_best_params, mejor_xgb, xgb_metrics = optimize_model_hyperparams('XGBoost', n_trials_per_model)\n",
    "\n",
    "# Optimizar LightGBM\n",
    "print(\"Iniciando optimizaci√≥n de hiperpar√°metros para LightGBM...\")\n",
    "lgbm_best_params, mejor_lgbm, lgbm_metrics = optimize_model_hyperparams('LightGBM', n_trials_per_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e1a844",
   "metadata": {
    "id": "a8e1a844"
   },
   "outputs": [],
   "source": [
    "# 9. Visualizaci√≥n de resultados\n",
    "resultados_df = pd.DataFrame(resultados_base, index=['RMSE', 'MAE', 'R2']).T\n",
    "\n",
    "# Visualizar resultados en tabla\n",
    "display(resultados_df)\n",
    "\n",
    "# Gr√°ficas de comparaci√≥n\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=resultados_df.index, y=resultados_df['RMSE'])\n",
    "plt.title('Comparaci√≥n de RMSE entre modelos')\n",
    "plt.ylabel('RMSE (menor es mejor)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'rmse_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# Gr√°fica de R¬≤\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=resultados_df.index, y=resultados_df['R2'])\n",
    "plt.title('Comparaci√≥n de R¬≤ entre modelos')\n",
    "plt.ylabel('R¬≤ (mayor es mejor)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'r2_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b378c2",
   "metadata": {
    "id": "95b378c2"
   },
   "outputs": [],
   "source": [
    "# 10. Exportar resultados finales\n",
    "# Crear un dataset de xarray con resultados\n",
    "def create_results_dataset(resultados_df):\n",
    "    \"\"\"Crea un dataset de xarray con los resultados para exportar como NetCDF\"\"\"\n",
    "    modelo_names = list(resultados_df.index)\n",
    "    metrics = ['RMSE', 'MAE', 'R2']\n",
    "\n",
    "    # Crear arrays para cada m√©trica\n",
    "    rmse_values = resultados_df['RMSE'].values\n",
    "    mae_values = resultados_df['MAE'].values\n",
    "    r2_values = resultados_df['R2'].values\n",
    "\n",
    "    # Crear dataset\n",
    "    ds = xr.Dataset(\n",
    "        data_vars={\n",
    "            'RMSE': (['model'], rmse_values),\n",
    "            'MAE': (['model'], mae_values),\n",
    "            'R2': (['model'], r2_values)\n",
    "        },\n",
    "        coords={\n",
    "            'model': modelo_names,\n",
    "        },\n",
    "        attrs={\n",
    "            'description': 'Resultados de modelos de predicci√≥n de precipitaci√≥n STHyMOUNTAIN',\n",
    "            'created': datetime.datetime.now().isoformat(),\n",
    "            'features_used': ', '.join(feature_cols)\n",
    "        }\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "# Crear dataset de resultados\n",
    "results_ds = create_results_dataset(resultados_df)\n",
    "\n",
    "# Guardar resultados como NetCDF\n",
    "results_file = model_output_dir / 'model_results.nc'\n",
    "results_ds.to_netcdf(results_file)\n",
    "print(f\"Resultados guardados como NetCDF en: {results_file}\")\n",
    "\n",
    "# Tambi√©n guardar como CSV para f√°cil visualizaci√≥n\n",
    "csv_file = model_output_dir / 'model_results.csv'\n",
    "resultados_df.to_csv(csv_file)\n",
    "print(f\"Resultados guardados como CSV en: {csv_file}\")\n",
    "\n",
    "# Crear un diccionario con informaci√≥n del modelo para uso futuro\n",
    "model_info = {\n",
    "    'date_trained': datetime.datetime.now().isoformat(),\n",
    "    'feature_columns': feature_cols,\n",
    "    'target_column': target_column,\n",
    "    'models_saved': modelos_guardados,\n",
    "    'results': resultados_df.to_dict(),\n",
    "    'best_model': resultados_df['RMSE'].idxmin(),\n",
    "    'scaler': 'scaler.pkl'\n",
    "}\n",
    "\n",
    "# Guardar informaci√≥n del modelo\n",
    "with open(model_output_dir / 'model_info.pkl', 'wb') as f:\n",
    "    pickle.dump(model_info, f)\n",
    "print(f\"Informaci√≥n del modelo guardada en: {model_output_dir / 'model_info.pkl'}\")\n",
    "\n",
    "print(\"\\nüî• Entrenamiento completado con √©xito! El mejor modelo es:\", resultados_df['RMSE'].idxmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Preparaci√≥n de datos para modelos espaciales y espaciotemporales\n",
    "\n",
    "def reorganize_data_for_dl(df_clean, ds_original, target_column, feature_cols, time_steps=3):\n",
    "    \"\"\"\n",
    "    Reorganiza los datos del DataFrame en formato espacial o espaciotemporal para modelos CNN y ConvLSTM\n",
    "    \n",
    "    Args:\n",
    "        df_clean: DataFrame con datos limpios\n",
    "        ds_original: Dataset xarray original\n",
    "        target_column: Nombre de la columna objetivo\n",
    "        feature_cols: Lista de columnas de caracter√≠sticas\n",
    "        time_steps: N√∫mero de pasos temporales para secuencias (ConvLSTM)\n",
    "    \n",
    "    Returns:\n",
    "        Datos para CNN y ConvLSTM en formato adecuado\n",
    "    \"\"\"\n",
    "    print(\"Preparando datos para modelos espaciales y espaciotemporales...\")\n",
    "    \n",
    "    # 1. Extraer informaci√≥n espacial del dataset original\n",
    "    lats = ds_original.lat.values\n",
    "    lons = ds_original.lon.values\n",
    "    times = np.sort(df_clean['time'].unique())\n",
    "    \n",
    "    n_times = len(times)\n",
    "    n_lats = len(lats)\n",
    "    n_lons = len(lons)\n",
    "    n_features = len(feature_cols)\n",
    "    \n",
    "    print(f\"Dimensiones: tiempos={n_times}, lats={n_lats}, lons={n_lons}, features={n_features}\")\n",
    "    \n",
    "    # Para CNN necesitamos: [samples, lat, lon, features]\n",
    "    # Para ConvLSTM necesitamos: [samples, time_steps, lat, lon, features]\n",
    "    \n",
    "    # 2. Crear arrays vac√≠os para almacenar datos reestructurados\n",
    "    # Para CNN (formato espacial)\n",
    "    X_spatial = np.zeros((n_times, n_lats, n_lons, n_features))\n",
    "    y_spatial = np.zeros((n_times, n_lats, n_lons, 1))\n",
    "    \n",
    "    # 3. Llenar arrays para CNN\n",
    "    for i, t in enumerate(times):\n",
    "        # Filtrar datos para este tiempo\n",
    "        df_time = df_clean[df_clean['time'] == t]\n",
    "        \n",
    "        # Para cada lat/lon, extraer features y target\n",
    "        for lat_idx, lat in enumerate(lats):\n",
    "            for lon_idx, lon in enumerate(lons):\n",
    "                # Obtener el registro para estas coordenadas\n",
    "                df_point = df_time[(df_time['lat'] == lat) & (df_time['lon'] == lon)]\n",
    "                \n",
    "                if not df_point.empty:\n",
    "                    # Extraer caracter√≠sticas\n",
    "                    X_spatial[i, lat_idx, lon_idx, :] = df_point[feature_cols].values[0]\n",
    "                    # Extraer target\n",
    "                    y_spatial[i, lat_idx, lon_idx, 0] = df_point[target_column].values[0]\n",
    "                \n",
    "    print(f\"Datos espaciales para CNN preparados. Forma X: {X_spatial.shape}, y: {y_spatial.shape}\")\n",
    "    \n",
    "    # 4. Crear secuencias para ConvLSTM\n",
    "    # Necesitamos [samples, time_steps, lat, lon, features]\n",
    "    X_spatiotemporal = []\n",
    "    y_spatiotemporal = []\n",
    "    \n",
    "    # Solo procesamos si hay suficientes pasos de tiempo\n",
    "    if n_times > time_steps:\n",
    "        for i in range(n_times - time_steps):\n",
    "            # Secuencia de entrada: time_steps pasos consecutivos\n",
    "            X_seq = X_spatial[i:i+time_steps, :, :, :]\n",
    "            # Target: el valor del siguiente paso de tiempo\n",
    "            y_seq = y_spatial[i+time_steps, :, :, :]\n",
    "            \n",
    "            X_spatiotemporal.append(X_seq)\n",
    "            y_spatiotemporal.append(y_seq)\n",
    "        \n",
    "        # Convertir a arrays numpy\n",
    "        X_spatiotemporal = np.array(X_spatiotemporal)\n",
    "        y_spatiotemporal = np.array(y_spatiotemporal)\n",
    "        \n",
    "        print(f\"Datos espaciotemporales para ConvLSTM preparados. Forma X: {X_spatiotemporal.shape}, y: {y_spatiotemporal.shape}\")\n",
    "        \n",
    "        # Divisi√≥n en entrenamiento/prueba para CNN\n",
    "        X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
    "            X_spatial, y_spatial, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Divisi√≥n en entrenamiento/prueba para ConvLSTM\n",
    "        X_train_convlstm, X_test_convlstm, y_train_convlstm, y_test_convlstm = train_test_split(\n",
    "            X_spatiotemporal, y_spatiotemporal, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Normalizar datos, solo las caracter√≠sticas\n",
    "        # Para CNN\n",
    "        for i in range(n_features):\n",
    "            # Calcular media y desviaci√≥n est√°ndar en datos de entrenamiento\n",
    "            feature_mean = np.mean(X_train_cnn[:, :, :, i])\n",
    "            feature_std = np.std(X_train_cnn[:, :, :, i])\n",
    "            \n",
    "            # Normalizar train y test\n",
    "            X_train_cnn[:, :, :, i] = (X_train_cnn[:, :, :, i] - feature_mean) / (feature_std + 1e-8)\n",
    "            X_test_cnn[:, :, :, i] = (X_test_cnn[:, :, :, i] - feature_mean) / (feature_std + 1e-8)\n",
    "        \n",
    "        # Para ConvLSTM (normalizar cada caracter√≠stica en cada paso de tiempo)\n",
    "        for i in range(n_features):\n",
    "            for t in range(time_steps):\n",
    "                # Calcular media y desviaci√≥n est√°ndar en datos de entrenamiento\n",
    "                feature_mean = np.mean(X_train_convlstm[:, t, :, :, i])\n",
    "                feature_std = np.std(X_train_convlstm[:, t, :, :, i])\n",
    "                \n",
    "                # Normalizar train y test\n",
    "                X_train_convlstm[:, t, :, :, i] = (X_train_convlstm[:, t, :, :, i] - feature_mean) / (feature_std + 1e-8)\n",
    "                X_test_convlstm[:, t, :, :, i] = (X_test_convlstm[:, t, :, :, i] - feature_mean) / (feature_std + 1e-8)\n",
    "        \n",
    "        return (\n",
    "            (X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn),\n",
    "            (X_train_convlstm, X_test_convlstm, y_train_convlstm, y_test_convlstm),\n",
    "            (lats, lons, times)\n",
    "        )\n",
    "    else:\n",
    "        print(\"No hay suficientes datos temporales para crear secuencias ConvLSTM.\")\n",
    "        # Divisi√≥n en entrenamiento/prueba solo para CNN\n",
    "        X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
    "            X_spatial, y_spatial, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Normalizar datos CNN\n",
    "        for i in range(n_features):\n",
    "            feature_mean = np.mean(X_train_cnn[:, :, :, i])\n",
    "            feature_std = np.std(X_train_cnn[:, :, :, i])\n",
    "            \n",
    "            X_train_cnn[:, :, :, i] = (X_train_cnn[:, :, :, i] - feature_mean) / (feature_std + 1e-8)\n",
    "            X_test_cnn[:, :, :, i] = (X_test_cnn[:, :, :, i] - feature_mean) / (feature_std + 1e-8)\n",
    "        \n",
    "        return (\n",
    "            (X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn),\n",
    "            None,\n",
    "            (lats, lons, times)\n",
    "        )\n",
    "\n",
    "# Preparar datos para CNN y ConvLSTM\n",
    "try:\n",
    "    time_steps = 3  # N√∫mero de pasos temporales para secuencias\n",
    "    spatial_data, spatiotemporal_data, spatial_coords = reorganize_data_for_dl(\n",
    "        df_clean, ds_original, target_column, feature_cols, time_steps)\n",
    "    \n",
    "    # Desempaquetar datos espaciales (CNN)\n",
    "    X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = spatial_data\n",
    "    \n",
    "    # Verificar si hay datos espaciotemporales (ConvLSTM)\n",
    "    if spatiotemporal_data is not None:\n",
    "        X_train_convlstm, X_test_convlstm, y_train_convlstm, y_test_convlstm = spatiotemporal_data\n",
    "        print(\"Datos para CNN y ConvLSTM preparados exitosamente.\")\n",
    "    else:\n",
    "        print(\"Solo se prepararon datos para CNN. No hay suficientes datos temporales para ConvLSTM.\")\n",
    "    \n",
    "    # Recuperar coordenadas espaciales\n",
    "    lats, lons, times = spatial_coords\n",
    "except Exception as e:\n",
    "    print(f\"Error al preparar datos para modelos espaciales: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Implementaci√≥n de modelo CNN para patrones espaciales\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    \"\"\"\n",
    "    Crea una arquitectura CNN para capturar patrones espaciales.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Dimensiones de los datos de entrada (lat, lon, features)\n",
    "    \n",
    "    Returns:\n",
    "        Modelo compilado de Keras\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Primera capa convolucional\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Segunda capa convolucional\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Tercera capa convolucional\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Flatten para conectar a capas densas\n",
    "        Flatten(),\n",
    "        \n",
    "        # Capas densas\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),  # Regularizaci√≥n para prevenir sobreajuste\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Capa de salida: Reshape para obtener predicciones espaciales\n",
    "        Dense(np.prod(input_shape[:2])),\n",
    "        Reshape(input_shape[:2] + (1,))  # Reshape a [lat, lon, 1]\n",
    "    ])\n",
    "    \n",
    "    # Compilar modelo con optimizador Adam\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',  # Error cuadr√°tico medio para regresi√≥n\n",
    "        metrics=['mae']  # Error absoluto medio como m√©trica adicional\n",
    "    )\n",
    "    \n",
    "    print(f\"Modelo CNN creado con input_shape={input_shape}\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "try:\n",
    "    # Obtener dimensiones de entrada a partir de los datos de entrenamiento\n",
    "    input_shape_cnn = X_train_cnn.shape[1:]  # [lat, lon, features]\n",
    "    print(f\"Forma de datos de entrada CNN: {input_shape_cnn}\")\n",
    "    \n",
    "    # Crear y entrenar modelo CNN\n",
    "    model_cnn = create_cnn_model(input_shape_cnn)\n",
    "    \n",
    "    # Callbacks para entrenamiento\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ModelCheckpoint(filepath=str(model_output_dir / 'cnn_model.h5'), \n",
    "                       save_best_only=True, monitor='val_loss')\n",
    "    ]\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"\\nEntrenando modelo CNN...\")\n",
    "    history_cnn = model_cnn.fit(\n",
    "        X_train_cnn, \n",
    "        y_train_cnn,\n",
    "        batch_size=16,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluar modelo en conjunto de prueba\n",
    "    print(\"\\nEvaluando modelo CNN en conjunto de prueba...\")\n",
    "    y_pred_cnn = model_cnn.predict(X_test_cnn)\n",
    "    \n",
    "    # Aplanar para evaluaci√≥n\n",
    "    y_test_flat = y_test_cnn.reshape(-1)\n",
    "    y_pred_flat = y_pred_cnn.reshape(-1)\n",
    "    \n",
    "    # Eliminar valores NaN si existen\n",
    "    mask = ~np.isnan(y_test_flat) & ~np.isnan(y_pred_flat)\n",
    "    y_test_valid = y_test_flat[mask]\n",
    "    y_pred_valid = y_pred_flat[mask]\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    rmse_cnn = np.sqrt(mean_squared_error(y_test_valid, y_pred_valid))\n",
    "    mae_cnn = mean_absolute_error(y_test_valid, y_pred_valid)\n",
    "    r2_cnn = r2_score(y_test_valid, y_pred_valid)\n",
    "    \n",
    "    print(f\"CNN - RMSE: {rmse_cnn:.4f}, MAE: {mae_cnn:.4f}, R¬≤: {r2_cnn:.4f}\")\n",
    "    \n",
    "    # A√±adir resultados al DataFrame de resultados\n",
    "    resultados_base['CNN'] = (rmse_cnn, mae_cnn, r2_cnn)\n",
    "    \n",
    "    # Guardar modelo en formato H5\n",
    "    model_cnn_file = model_output_dir / 'cnn_model.h5'\n",
    "    model_cnn.save(model_cnn_file)\n",
    "    print(f\"Modelo CNN guardado en: {model_cnn_file}\")\n",
    "    \n",
    "    # Visualizar historia de entrenamiento\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Gr√°fica de p√©rdida\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_cnn.history['loss'], label='Train')\n",
    "    plt.plot(history_cnn.history['val_loss'], label='Validation')\n",
    "    plt.title('P√©rdida CNN')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Gr√°fica de MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_cnn.history['mae'], label='Train')\n",
    "    plt.plot(history_cnn.history['val_mae'], label='Validation')\n",
    "    plt.title('MAE CNN')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(model_output_dir / 'cnn_training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Crear visualizaci√≥n espacial del error\n",
    "    # Elegir un ejemplo aleatorio del conjunto de prueba para visualizaci√≥n\n",
    "    sample_idx = np.random.randint(0, len(X_test_cnn))\n",
    "    sample_true = y_test_cnn[sample_idx, :, :, 0]\n",
    "    sample_pred = y_pred_cnn[sample_idx, :, :, 0]\n",
    "    sample_error = np.abs(sample_true - sample_pred)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Mapa de precipitaci√≥n real\n",
    "    im0 = axes[0].imshow(sample_true, cmap='Blues')\n",
    "    axes[0].set_title('Precipitaci√≥n Real')\n",
    "    plt.colorbar(im0, ax=axes[0], label='mm')\n",
    "    \n",
    "    # Mapa de precipitaci√≥n predicha\n",
    "    im1 = axes[1].imshow(sample_pred, cmap='Blues')\n",
    "    axes[1].set_title('Precipitaci√≥n Predicha (CNN)')\n",
    "    plt.colorbar(im1, ax=axes[1], label='mm')\n",
    "    \n",
    "    # Mapa de error\n",
    "    im2 = axes[2].imshow(sample_error, cmap='Reds')\n",
    "    axes[2].set_title('Error Absoluto')\n",
    "    plt.colorbar(im2, ax=axes[2], label='mm')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(model_output_dir / 'cnn_spatial_error.png')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error al implementar modelo CNN: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86974d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Implementaci√≥n de modelo ConvLSTM para patrones espaciotemporales\n",
    "\n",
    "def create_convlstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Crea una arquitectura ConvLSTM para capturar patrones espaciotemporales.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Dimensiones de los datos de entrada (time_steps, lat, lon, features)\n",
    "    \n",
    "    Returns:\n",
    "        Modelo compilado de Keras\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Primera capa ConvLSTM\n",
    "        ConvLSTM2D(filters=32, kernel_size=(3, 3), padding='same',\n",
    "                   return_sequences=True, activation='tanh',\n",
    "                   recurrent_activation='hard_sigmoid',\n",
    "                   input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Segunda capa ConvLSTM\n",
    "        ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same',\n",
    "                   return_sequences=False, activation='tanh',\n",
    "                   recurrent_activation='hard_sigmoid'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Capas convolucionales para procesamiento final\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Capa de salida para predicci√≥n\n",
    "        Conv2D(filters=1, kernel_size=(3, 3), padding='same', activation='linear')\n",
    "    ])\n",
    "    \n",
    "    # Compilar modelo con optimizador Adam\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',  # Error cuadr√°tico medio para regresi√≥n\n",
    "        metrics=['mae']  # Error absoluto medio como m√©trica adicional\n",
    "    )\n",
    "    \n",
    "    print(f\"Modelo ConvLSTM creado con input_shape={input_shape}\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Creamos una clase para visualizar el progreso de entrenamiento con tqdm\n",
    "class TqdmCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epochs, metrics=['loss', 'val_loss']):\n",
    "        self.epochs = epochs\n",
    "        self.metrics = metrics\n",
    "        self.tqdm_progress = None\n",
    "        self.epoch_count = 0\n",
    "        self.training_start = None\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.training_start = time.time()\n",
    "        display(HTML(f'<div style=\"background-color:#e6f2ff; padding:10px; border-radius:5px;\">' +\n",
    "                     f'<h3>üß† Iniciando entrenamiento de ConvLSTM</h3>' +\n",
    "                     f'<div>Total √©pocas: {self.epochs}</div>' +\n",
    "                     f'<div>M√©tricas monitorizadas: {\", \".join(self.metrics)}</div>' +\n",
    "                     f'</div>'))\n",
    "        self.tqdm_progress = tqdm(total=self.epochs, desc=\"Entrenando ConvLSTM\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch_count += 1\n",
    "        \n",
    "        # Recoger m√©tricas para mostrar\n",
    "        metrics_str = \", \".join([f\"{m}: {logs.get(m, 0):.4f}\" for m in self.metrics if m in logs])\n",
    "        self.tqdm_progress.set_postfix_str(metrics_str)\n",
    "        self.tqdm_progress.update(1)\n",
    "        \n",
    "        # Cada 5 √©pocas (o en la √∫ltima), mostramos un resumen m√°s completo\n",
    "        if epoch % 5 == 0 or epoch == self.epochs - 1:\n",
    "            elapsed = time.time() - self.training_start\n",
    "            minutes, seconds = divmod(elapsed, 60)\n",
    "            hours, minutes = divmod(minutes, 60)\n",
    "            \n",
    "            # Crear un resumen de progreso bonito\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f'<div style=\"background-color:#e6f2ff; padding:10px; border-radius:5px;\">' +\n",
    "                         f'<h3>üß† Entrenando ConvLSTM - Progreso</h3>' +\n",
    "                         f'<div><b>√âpoca:</b> {epoch+1}/{self.epochs} ({((epoch+1)/self.epochs*100):.1f}%)</div>' +\n",
    "                         f'<div><b>Tiempo transcurrido:</b> {int(hours)}h {int(minutes)}m {int(seconds)}s</div>' +\n",
    "                         f'<div><b>M√©tricas actuales:</b> {metrics_str}</div>' +\n",
    "                         f'<div><b>Mejor val_loss:</b> {min([logs.get(\"val_loss\", float(\"inf\"))] + [logs.get(\"val_loss\", float(\"inf\")) for logs in self.model.history.history.get(\"val_loss\", []) or []]):.4f}</div>' +\n",
    "                         f'</div>'))\n",
    "            self.tqdm_progress = tqdm(total=self.epochs, initial=self.epoch_count, desc=\"Entrenando ConvLSTM\")\n",
    "            \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.tqdm_progress.close()\n",
    "        elapsed = time.time() - self.training_start\n",
    "        minutes, seconds = divmod(elapsed, 60)\n",
    "        hours, minutes = divmod(minutes, 60)\n",
    "        \n",
    "        display(HTML(f'<div style=\"background-color:#e6ffe6; padding:10px; border-radius:5px;\">' +\n",
    "                     f'<h3>‚úÖ Entrenamiento de ConvLSTM completado</h3>' +\n",
    "                     f'<div><b>Tiempo total:</b> {int(hours)}h {int(minutes)}m {int(seconds)}s</div>' +\n",
    "                     f'</div>'))\n",
    "\n",
    "try:\n",
    "    # Verificar si tenemos datos espaciotemporales para ConvLSTM\n",
    "    if 'spatiotemporal_data' in locals() and spatiotemporal_data is not None:\n",
    "        # Crear y entrenar modelo ConvLSTM\n",
    "        input_shape_convlstm = X_train_convlstm.shape[1:]  # [time_steps, lat, lon, features]\n",
    "        print(f\"Forma de datos de entrada ConvLSTM: {input_shape_convlstm}\")\n",
    "        \n",
    "        # Crear modelo\n",
    "        model_convlstm = create_convlstm_model(input_shape_convlstm)\n",
    "        \n",
    "        # Callbacks para entrenamiento\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            filepath=str(model_output_dir / 'convlstm_model.h5'),\n",
    "            save_best_only=True,\n",
    "            monitor='val_loss'\n",
    "        )\n",
    "        tqdm_callback = TqdmCallback(epochs=50, metrics=['loss', 'val_loss', 'mae', 'val_mae'])\n",
    "        \n",
    "        # Entrenar modelo con barra de progreso personalizada\n",
    "        print(\"\\nEntrenando modelo ConvLSTM...\")\n",
    "        history_convlstm = model_convlstm.fit(\n",
    "            X_train_convlstm,\n",
    "            y_train_convlstm,\n",
    "            batch_size=16,\n",
    "            epochs=50,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stopping, model_checkpoint, tqdm_callback],\n",
    "            verbose=0  # No mostrar la barra de progreso est√°ndar, usamos nuestra personalizada\n",
    "        )\n",
    "        \n",
    "        # Evaluar modelo en conjunto de prueba\n",
    "        print(\"\\nEvaluando modelo ConvLSTM en conjunto de prueba...\")\n",
    "        y_pred_convlstm = model_convlstm.predict(X_test_convlstm)\n",
    "        \n",
    "        # Aplanar para evaluaci√≥n\n",
    "        y_test_flat_convlstm = y_test_convlstm.reshape(-1)\n",
    "        y_pred_flat_convlstm = y_pred_convlstm.reshape(-1)\n",
    "        \n",
    "        # Eliminar valores NaN si existen\n",
    "        mask_convlstm = ~np.isnan(y_test_flat_convlstm) & ~np.isnan(y_pred_flat_convlstm)\n",
    "        y_test_valid_convlstm = y_test_flat_convlstm[mask_convlstm]\n",
    "        y_pred_valid_convlstm = y_pred_flat_convlstm[mask_convlstm]\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        rmse_convlstm = np.sqrt(mean_squared_error(y_test_valid_convlstm, y_pred_valid_convlstm))\n",
    "        mae_convlstm = mean_absolute_error(y_test_valid_convlstm, y_pred_valid_convlstm)\n",
    "        r2_convlstm = r2_score(y_test_valid_convlstm, y_pred_valid_convlstm)\n",
    "        \n",
    "        print(f\"ConvLSTM - RMSE: {rmse_convlstm:.4f}, MAE: {mae_convlstm:.4f}, R¬≤: {r2_convlstm:.4f}\")\n",
    "        \n",
    "        # Visualizar resultados con estilo\n",
    "        display(HTML(f'<div style=\"background-color:#f5f5dc; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
    "                    f'<h3>üìä M√©tricas para ConvLSTM</h3>' +\n",
    "                    f'<table style=\"width:100%; text-align:left;\">' +\n",
    "                    f'<tr><th>M√©trica</th><th>Valor</th></tr>' +\n",
    "                    f'<tr><td>RMSE</td><td>{rmse_convlstm:.4f}</td></tr>' +\n",
    "                    f'<tr><td>MAE</td><td>{mae_convlstm:.4f}</td></tr>' +\n",
    "                    f'<tr><td>R¬≤</td><td>{r2_convlstm:.4f}</td></tr>' +\n",
    "                    f'</table></div>'))\n",
    "        \n",
    "        # A√±adir resultados al DataFrame de resultados\n",
    "        resultados_base['ConvLSTM'] = (rmse_convlstm, mae_convlstm, r2_convlstm)\n",
    "        \n",
    "        # Guardar modelo en formato H5\n",
    "        model_convlstm_file = model_output_dir / 'convlstm_model.h5'\n",
    "        model_convlstm.save(model_convlstm_file)\n",
    "        print(f\"Modelo ConvLSTM guardado en: {model_convlstm_file}\")\n",
    "        \n",
    "        # Visualizar historia de entrenamiento\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Gr√°fica de p√©rdida\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history_convlstm.history['loss'], label='Train', linewidth=2)\n",
    "        plt.plot(history_convlstm.history['val_loss'], label='Validation', linewidth=2, linestyle='--')\n",
    "        plt.title('P√©rdida ConvLSTM', fontsize=14)\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        \n",
    "        # Gr√°fica de MAE\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history_convlstm.history['mae'], label='Train', linewidth=2)\n",
    "        plt.plot(history_convlstm.history['val_mae'], label='Validation', linewidth=2, linestyle='--')\n",
    "        plt.title('MAE ConvLSTM', fontsize=14)\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('MAE', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(model_output_dir / 'convlstm_training_history.png')\n",
    "        plt.show()\n",
    "        \n",
    "        # Visualizar predicciones vs valores reales\n",
    "        # Elegir un ejemplo aleatorio del conjunto de prueba para visualizaci√≥n\n",
    "        sample_idx_convlstm = np.random.randint(0, len(X_test_convlstm))\n",
    "        sample_true_convlstm = y_test_convlstm[sample_idx_convlstm, :, :, 0]\n",
    "        sample_pred_convlstm = y_pred_convlstm[sample_idx_convlstm, :, :, 0]\n",
    "        sample_error_convlstm = np.abs(sample_true_convlstm - sample_pred_convlstm)\n",
    "        \n",
    "        # Crear gr√°ficas con mejor aspecto visual\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # Mapa de precipitaci√≥n real\n",
    "        im0 = axes[0].imshow(sample_true_convlstm, cmap='Blues', interpolation='nearest')\n",
    "        axes[0].set_title('Precipitaci√≥n Real', fontsize=14)\n",
    "        plt.colorbar(im0, ax=axes[0], label='mm')\n",
    "        axes[0].grid(False)\n",
    "        \n",
    "        # Mapa de precipitaci√≥n predicha\n",
    "        im1 = axes[1].imshow(sample_pred_convlstm, cmap='Blues', interpolation='nearest')\n",
    "        axes[1].set_title('Predicci√≥n ConvLSTM', fontsize=14)\n",
    "        plt.colorbar(im1, ax=axes[1], label='mm')\n",
    "        axes[1].grid(False)\n",
    "        \n",
    "        # Mapa de error\n",
    "        im2 = axes[2].imshow(sample_error_convlstm, cmap='Reds', interpolation='nearest')\n",
    "        axes[2].set_title('Error Absoluto', fontsize=14)\n",
    "        plt.colorbar(im2, ax=axes[2], label='mm')\n",
    "        axes[2].grid(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(model_output_dir / 'convlstm_predictions.png')\n",
    "        plt.show()\n",
    "        \n",
    "        # Resumen final de resultados\n",
    "        print(\"\\nüåü Modelos entrenados y evaluados:\")\n",
    "        display(HTML(f'<div style=\"background-color:#f0f8ff; padding:15px; border-radius:8px; margin-top:20px;\">' +\n",
    "                    f'<h2>üèÜ Resumen de Resultados</h2>' +\n",
    "                    f'<p>Se han entrenado tanto modelos de machine learning (RandomForest, XGBoost, LightGBM) ' +\n",
    "                    f'como modelos de deep learning (CNN, ConvLSTM).</p>' +\n",
    "                    f'<h3>Ranking seg√∫n RMSE (menor es mejor):</h3>' +\n",
    "                    f'</div>'))\n",
    "        \n",
    "        # Ranking de modelos\n",
    "        resultados_df_final = pd.DataFrame(resultados_base, index=['RMSE', 'MAE', 'R2']).T\n",
    "        resultados_ordenados = resultados_df_final.sort_values('RMSE')\n",
    "        display(resultados_ordenados)\n",
    "        \n",
    "        # Gr√°fica de resumen\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x=resultados_ordenados.index, y=resultados_ordenados['RMSE'], palette='viridis')\n",
    "        plt.title('RMSE por Modelo (menor es mejor)', fontsize=14)\n",
    "        plt.ylabel('RMSE (mm)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.grid(True, axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x=resultados_ordenados.index, y=resultados_ordenados['R2'], palette='plasma')\n",
    "        plt.title('R¬≤ por Modelo (mayor es mejor)', fontsize=14)\n",
    "        plt.ylabel('Coeficiente R¬≤', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.grid(True, axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(model_output_dir / 'modelos_ranking_final.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No se pudieron preparar los datos espaciotemporales para el modelo ConvLSTM.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al implementar modelo ConvLSTM: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed0729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Validaci√≥n cruzada (Cross-Validation) para evaluar modelos\n",
    "\n",
    "def evaluate_model_with_cv(modelo, nombre, X, y, n_splits=5):\n",
    "    \"\"\"\n",
    "    Eval√∫a el modelo usando validaci√≥n cruzada (k-fold)\n",
    "    \n",
    "    Args:\n",
    "        modelo: Modelo a evaluar\n",
    "        nombre: Nombre del modelo\n",
    "        X: Caracter√≠sticas\n",
    "        y: Variable objetivo\n",
    "        n_splits: N√∫mero de divisiones (folds)\n",
    "        \n",
    "    Returns:\n",
    "        Promedio de m√©tricas y lista de m√©tricas por fold\n",
    "    \"\"\"\n",
    "    print(f\"\\nRealizando validaci√≥n cruzada para {nombre} con {n_splits} folds...\")\n",
    "    \n",
    "    # Crear instancia de KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Listas para almacenar m√©tricas de cada fold\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    r2_scores = []\n",
    "    \n",
    "    # Iterar sobre cada fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"Fold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        # Dividir datos para este fold\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        modelo.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Predecir\n",
    "        y_pred_fold = modelo.predict(X_val_fold)\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "        mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "        r2 = r2_score(y_val_fold, y_pred_fold)\n",
    "        \n",
    "        # Almacenar m√©tricas\n",
    "        rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        r2_scores.append(r2)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R¬≤: {r2:.4f}\")\n",
    "    \n",
    "    # Calcular promedios\n",
    "    mean_rmse = np.mean(rmse_scores)\n",
    "    mean_mae = np.mean(mae_scores)\n",
    "    mean_r2 = np.mean(r2_scores)\n",
    "    \n",
    "    # Calcular desviaciones est√°ndar\n",
    "    std_rmse = np.std(rmse_scores)\n",
    "    std_mae = np.std(mae_scores)\n",
    "    std_r2 = np.std(r2_scores)\n",
    "    \n",
    "    print(f\"\\nResumen de validaci√≥n cruzada para {nombre}:\")\n",
    "    print(f\"RMSE: {mean_rmse:.4f} ¬± {std_rmse:.4f}\")\n",
    "    print(f\"MAE: {mean_mae:.4f} ¬± {std_mae:.4f}\")\n",
    "    print(f\"R¬≤: {mean_r2:.4f} ¬± {std_r2:.4f}\")\n",
    "    \n",
    "    # Crear diccionario con resultados\n",
    "    cv_results = {\n",
    "        'mean_rmse': mean_rmse,\n",
    "        'mean_mae': mean_mae,\n",
    "        'mean_r2': mean_r2,\n",
    "        'std_rmse': std_rmse,\n",
    "        'std_mae': std_mae,\n",
    "        'std_r2': std_r2,\n",
    "        'rmse_scores': rmse_scores,\n",
    "        'mae_scores': mae_scores,\n",
    "        'r2_scores': r2_scores\n",
    "    }\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "# Aplicar validaci√≥n cruzada a los modelos tabulares\n",
    "print(\"\\nüîÑ Ejecutando validaci√≥n cruzada para modelos tabulares...\")\n",
    "cv_results = {}\n",
    "\n",
    "# Lista de modelos a evaluar con validaci√≥n cruzada\n",
    "models_for_cv = {\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Solo tomamos el mejor modelo de cada categor√≠a para CV\n",
    "if 'RandomForest_Optuna' in resultados_base:\n",
    "    models_for_cv['RandomForest_Optuna'] = RandomForestRegressor(**rf_best_params, random_state=42)\n",
    "if 'XGBoost_Optuna' in resultados_base:\n",
    "    models_for_cv['XGBoost_Optuna'] = XGBRegressor(**xgb_best_params, random_state=42)\n",
    "if 'LightGBM_Optuna' in resultados_base:\n",
    "    models_for_cv['LightGBM_Optuna'] = LGBMRegressor(**lgbm_best_params, random_state=42)\n",
    "\n",
    "# Ejecutar validaci√≥n cruzada para cada modelo\n",
    "for nombre, modelo in models_for_cv.items():\n",
    "    cv_results[nombre] = evaluate_model_with_cv(modelo, nombre, X_train_scaled, y_train, n_splits=5)\n",
    "\n",
    "# Visualizar resultados de validaci√≥n cruzada\n",
    "plt.figure(figsize=(12, 6))\n",
    "modelo_names = list(cv_results.keys())\n",
    "mean_rmses = [cv_results[model]['mean_rmse'] for model in modelo_names]\n",
    "std_rmses = [cv_results[model]['std_rmse'] for model in modelo_names]\n",
    "\n",
    "# Crear barplot con barras de error\n",
    "bars = plt.bar(modelo_names, mean_rmses, yerr=std_rmses, capsize=5, color='skyblue', alpha=0.7)\n",
    "plt.title('Comparaci√≥n de RMSE con Validaci√≥n Cruzada (5-fold)')\n",
    "plt.ylabel('RMSE (menor es mejor)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for bar, rmse, std in zip(bars, mean_rmses, std_rmses):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + std + 0.05,\n",
    "             f'{rmse:.3f}¬±{std:.3f}', ha='center', va='bottom', rotation=0, size=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'cross_validation_rmse.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c517aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Bootstrapping para evaluar intervalos de confianza\n",
    "\n",
    "def bootstrap_evaluate_model(model, X, y, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Eval√∫a el modelo usando bootstrapping para obtener intervalos de confianza\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado a evaluar\n",
    "        X: Caracter√≠sticas\n",
    "        y: Variable objetivo\n",
    "        n_iterations: N√∫mero de iteraciones de bootstrap\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con m√©tricas e intervalos de confianza\n",
    "    \"\"\"\n",
    "    print(f\"Realizando bootstrapping con {n_iterations} iteraciones...\")\n",
    "    \n",
    "    # Convertir a numpy arrays si no lo son ya\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = X.values\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = y.values\n",
    "        \n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Listas para almacenar m√©tricas de cada iteraci√≥n\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    r2_scores = []\n",
    "    \n",
    "    # Realizar iteraciones de bootstrap\n",
    "    for i in range(n_iterations):\n",
    "        if i % 20 == 0:  # Solo imprimir cada 20 iteraciones para no saturar la salida\n",
    "            print(f\"Iteraci√≥n bootstrap {i+1}/{n_iterations}\")\n",
    "            \n",
    "        # Muestreo con reemplazo\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        \n",
    "        # Crear muestra bootstrap\n",
    "        X_boot = X[indices, :]\n",
    "        y_boot = y[indices]\n",
    "        \n",
    "        # Predecir\n",
    "        y_pred = model.predict(X_boot)\n",
    "        \n",
    "        # Calcular m√©tricas\n",
    "        rmse = np.sqrt(mean_squared_error(y_boot, y_pred))\n",
    "        mae = mean_absolute_error(y_boot, y_pred)\n",
    "        r2 = r2_score(y_boot, y_pred)\n",
    "        \n",
    "        # Almacenar m√©tricas\n",
    "        rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        r2_scores.append(r2)\n",
    "    \n",
    "    # Calcular estad√≠sticas de bootstrapping\n",
    "    # Mediana como estimaci√≥n central y percentiles 2.5 y 97.5 para intervalo de confianza del 95%\n",
    "    rmse_median = np.median(rmse_scores)\n",
    "    rmse_lower = np.percentile(rmse_scores, 2.5)\n",
    "    rmse_upper = np.percentile(rmse_scores, 97.5)\n",
    "    \n",
    "    mae_median = np.median(mae_scores)\n",
    "    mae_lower = np.percentile(mae_scores, 2.5)\n",
    "    mae_upper = np.percentile(mae_scores, 97.5)\n",
    "    \n",
    "    r2_median = np.median(r2_scores)\n",
    "    r2_lower = np.percentile(r2_scores, 2.5)\n",
    "    r2_upper = np.percentile(r2_scores, 97.5)\n",
    "    \n",
    "    print(\"\\nResultados de bootstrapping:\")\n",
    "    print(f\"RMSE: {rmse_median:.4f} (IC 95%: [{rmse_lower:.4f}, {rmse_upper:.4f}])\")\n",
    "    print(f\"MAE: {mae_median:.4f} (IC 95%: [{mae_lower:.4f}, {mae_upper:.4f}])\")\n",
    "    print(f\"R¬≤: {r2_median:.4f} (IC 95%: [{r2_lower:.4f}, {r2_upper:.4f}])\")\n",
    "    \n",
    "    # Crear diccionario con resultados\n",
    "    bootstrap_results = {\n",
    "        'rmse_median': rmse_median,\n",
    "        'rmse_ci': (rmse_lower, rmse_upper),\n",
    "        'mae_median': mae_median,\n",
    "        'mae_ci': (mae_lower, mae_upper),\n",
    "        'r2_median': r2_median,\n",
    "        'r2_ci': (r2_lower, r2_upper),\n",
    "        'rmse_scores': rmse_scores,\n",
    "        'mae_scores': mae_scores,\n",
    "        'r2_scores': r2_scores\n",
    "    }\n",
    "    \n",
    "    return bootstrap_results\n",
    "\n",
    "# Realizar bootstrapping para el mejor modelo basado en RMSE\n",
    "print(\"\\nüîÑ Ejecutando evaluaci√≥n por bootstrapping para el mejor modelo...\")\n",
    "\n",
    "# Encontrar el mejor modelo basado en RMSE\n",
    "best_model_name = min(resultados_base.items(), key=lambda x: x[1][0])[0]\n",
    "print(f\"Mejor modelo seg√∫n RMSE: {best_model_name}\")\n",
    "\n",
    "# Conseguir instancia del mejor modelo\n",
    "if 'XGBoost_Optuna' in best_model_name:\n",
    "    best_model = mejor_xgb\n",
    "elif 'LightGBM_Optuna' in best_model_name:\n",
    "    best_model = mejor_lgbm\n",
    "elif 'RandomForest_Optuna' in best_model_name:\n",
    "    best_model = mejor_rf\n",
    "elif 'CNN' in best_model_name:\n",
    "    print(\"No se puede aplicar bootstrapping directamente al modelo CNN, solo a modelos tabulares.\")\n",
    "    best_model = None\n",
    "elif 'ConvLSTM' in best_model_name:\n",
    "    print(\"No se puede aplicar bootstrapping directamente al modelo ConvLSTM, solo a modelos tabulares.\")\n",
    "    best_model = None\n",
    "else:\n",
    "    # Obtener el modelo original de modelos_base\n",
    "    model_class = best_model_name.split('_')[0]\n",
    "    if model_class == 'RandomForest':\n",
    "        best_model = modelos_base['RandomForest']\n",
    "    elif model_class == 'XGBoost':\n",
    "        best_model = modelos_base['XGBoost']\n",
    "    elif model_class == 'LightGBM':\n",
    "        best_model = modelos_base['LightGBM']\n",
    "        \n",
    "# Aplicar bootstrapping al mejor modelo si es tabular\n",
    "if best_model is not None:\n",
    "    bootstrap_results = bootstrap_evaluate_model(best_model, X_test_scaled, y_test.values, n_iterations=100)\n",
    "    \n",
    "    # Visualizar distribuci√≥n de RMSE por bootstrapping\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Histograma de RMSE\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(bootstrap_results['rmse_scores'], bins=20, alpha=0.7, color='skyblue')\n",
    "    plt.axvline(bootstrap_results['rmse_median'], color='red', linestyle='--', label=f\"Mediana: {bootstrap_results['rmse_median']:.3f}\")\n",
    "    plt.axvline(bootstrap_results['rmse_ci'][0], color='green', linestyle=':', label=f\"IC 95%: [{bootstrap_results['rmse_ci'][0]:.3f}, {bootstrap_results['rmse_ci'][1]:.3f}]\")\n",
    "    plt.axvline(bootstrap_results['rmse_ci'][1], color='green', linestyle=':')\n",
    "    plt.title(f'Distribuci√≥n de RMSE - {best_model_name}')\n",
    "    plt.xlabel('RMSE')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Histograma de MAE\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(bootstrap_results['mae_scores'], bins=20, alpha=0.7, color='lightgreen')\n",
    "    plt.axvline(bootstrap_results['mae_median'], color='red', linestyle='--', label=f\"Mediana: {bootstrap_results['mae_median']:.3f}\")\n",
    "    plt.axvline(bootstrap_results['mae_ci'][0], color='green', linestyle=':', label=f\"IC 95%: [{bootstrap_results['mae_ci'][0]:.3f}, {bootstrap_results['mae_ci'][1]:.3f}]\")\n",
    "    plt.axvline(bootstrap_results['mae_ci'][1], color='green', linestyle=':')\n",
    "    plt.title(f'Distribuci√≥n de MAE - {best_model_name}')\n",
    "    plt.xlabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Histograma de R¬≤\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist(bootstrap_results['r2_scores'], bins=20, alpha=0.7, color='salmon')\n",
    "    plt.axvline(bootstrap_results['r2_median'], color='red', linestyle='--', label=f\"Mediana: {bootstrap_results['r2_median']:.3f}\")\n",
    "    plt.axvline(bootstrap_results['r2_ci'][0], color='green', linestyle=':', label=f\"IC 95%: [{bootstrap_results['r2_ci'][0]:.3f}, {bootstrap_results['r2_ci'][1]:.3f}]\")\n",
    "    plt.axvline(bootstrap_results['r2_ci'][1], color='green', linestyle=':')\n",
    "    plt.title(f'Distribuci√≥n de R¬≤ - {best_model_name}')\n",
    "    plt.xlabel('R¬≤')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(model_output_dir / 'bootstrap_distributions.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
