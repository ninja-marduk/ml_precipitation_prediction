{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "827c19f7",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/hybrid_models_enconders_layering_w3_ST-HybridWaveStack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abece5",
   "metadata": {},
   "source": [
    "# Meta-Modelo Convolucional: ElevClusConvPrecipMetaNet\n",
    "\n",
    "Este modelo implementa una arquitectura convolucional avanzada para la predicci√≥n espaciotemporal de precipitaci√≥n con horizonte de 12 meses.\n",
    "\n",
    "## Arquitectura\n",
    "\n",
    "1. **Entradas**:\n",
    "   - Mapas de predicci√≥n de los modelos base (ConvBiGRU-AE y ConvLSTM-AE) para cada horizonte (12 meses)\n",
    "   - Informaci√≥n de elevaci√≥n y clusters para condicionamiento (FiLM)\n",
    "\n",
    "2. **Reducci√≥n Temprana de Canales**:\n",
    "   - Reduce los 24 canales (2 modelos √ó 12 horizontes) a 16 para optimizar memoria\n",
    "   - Aplica Conv2D(1√ó1) para mezclar informaci√≥n sin perder resoluci√≥n espacial\n",
    "\n",
    "3. **Bloques Residuales Multiescala**:\n",
    "   - Bloques depthwise-separables con distintas dilataciones (1,2,4)\n",
    "   - Captura patrones a diferentes escalas espaciales sin incrementar par√°metros\n",
    "\n",
    "4. **Atenci√≥n Espacial por Cluster**:\n",
    "   - FiLM (Feature-wise Linear Modulation): Œ≥_cluster ‚äó F + Œ≤_cluster\n",
    "   - Adapta el comportamiento seg√∫n el r√©gimen orogr√°fico\n",
    "\n",
    "5. **U-Net Compacto**:\n",
    "   - Arquitectura de encoder-decoder con skip connections\n",
    "   - Solo 2 niveles de downsampling para preservar detalle\n",
    "\n",
    "6. **Agrupamiento de Horizontes**:\n",
    "   - Conv3D para procesar conjuntamente la dimensi√≥n temporal de horizontes\n",
    "   - Permite aprender relaciones entre meses consecutivos\n",
    "\n",
    "7. **Salida Multi-Horizonte**:\n",
    "   - Genera los 12 mapas refinados de predicci√≥n\n",
    "\n",
    "8. **Estrategias Memory-Friendly**:\n",
    "   - Mixed precision (float16)\n",
    "   - Gradient checkpointing\n",
    "   - Acumulaci√≥n de gradientes\n",
    "   - Entrenamiento por etapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicci√≥n Espaciotemporal de Precipitaci√≥n Mensual - Notebook Completo\n",
    "\n",
    "# 0) Configuraci√≥n del entorno, rutas y dependencias\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import joblib  # Para persistir scalers\n",
    "import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def log_and_print(msg):\n",
    "    logger.info(msg)\n",
    "    print(msg)\n",
    "\n",
    "# Detectar entorno (Colab o local)\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "log_and_print(f\"Ejecutando en Colab: {IN_COLAB}\")\n",
    "\n",
    "# Definir rutas base\n",
    "desired_repo = 'ml_precipitation_prediction'\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    BASE_PATH = Path('/content/drive/MyDrive') / desired_repo\n",
    "    if not Path(desired_repo).exists():\n",
    "        log_and_print(\"Clonando repositorio...\")\n",
    "        get_ipython().system('git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git')\n",
    "    os.chdir(desired_repo)\n",
    "    get_ipython().system('pip install -q xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas joblib')\n",
    "else:\n",
    "    # Instalaci√≥n m√°s simple sin PyEMD\n",
    "    %pip install -q xarray netCDF4 scikit-image\n",
    "    current = Path.cwd()\n",
    "    for p in [current] + list(current.parents):\n",
    "        if (p / '.git').is_dir() or (p / 'requirements.txt').is_file() or (p / 'README.md').is_file():\n",
    "            BASE_PATH = p\n",
    "            break\n",
    "    else:\n",
    "        BASE_PATH = current\n",
    "    log_and_print(f\"Ejecutando en local. Base path: {BASE_PATH}\")\n",
    "\n",
    "# Rutas de datos y modelos\n",
    "DATA_OUTPUT   = BASE_PATH / 'data' / 'output'\n",
    "MODELS_OUTPUT = BASE_PATH / 'models' / 'output'\n",
    "PREDS_DIR     = MODELS_OUTPUT / 'base_model_predictions'\n",
    "SHP_PATH      = BASE_PATH / 'data' / 'input' / 'shapes' / 'MGN_Departamento.shp'\n",
    "\n",
    "# Crear directorios\n",
    "MODELS_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "PREDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Par√°metros generales\n",
    "INPUT_WINDOW   = 60\n",
    "OUTPUT_HORIZON = 12  # 12 meses\n",
    "BATCH_SIZE     = 16\n",
    "MAX_EPOCHS     = 300\n",
    "PATIENCE       = 50\n",
    "LR             = 1e-3\n",
    "\n",
    "import torch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log_and_print(f\"Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "\n",
    "# 1) Imports adicionales y utilidades\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature  # A√±adido para caracter√≠sticas cartogr√°ficas\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler # Mixed precision training\n",
    "from torch.utils.checkpoint import checkpoint # Gradient checkpointing\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  # A√±adido para learning rate adaptativo\n",
    "from IPython.display import clear_output  # Para actualizar gr√°ficos durante entrenamiento\n",
    "import pywt\n",
    "from scipy.signal import hilbert\n",
    "from skimage.restoration import denoise_wavelet\n",
    "\n",
    "# 2) Carga y preprocesamiento de datos - Versi√≥n simplificada\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    log_and_print(\"Cargando datos...\")\n",
    "    \n",
    "    # Cargar datos completos\n",
    "    ds_full = xr.open_dataset(DATA_OUTPUT / 'complete_dataset_with_features_with_clusters_elevation_with_windows.nc')\n",
    "    log_and_print(f\"Dataset completo cargado, dims: {ds_full.dims}\")\n",
    "    \n",
    "    # Cargar componentes directamente de archivos espec√≠ficos\n",
    "    ds_ceemdan = xr.open_dataset(MODELS_OUTPUT / 'features_CEEMDAN.nc')\n",
    "    log_and_print(f\"Dataset CEEMDAN cargado, dims: {ds_ceemdan.dims}\")\n",
    "    log_and_print(f\"Variables disponibles en CEEMDAN: {list(ds_ceemdan.data_vars.keys())}\")\n",
    "    \n",
    "    ds_tvfemd = xr.open_dataset(MODELS_OUTPUT / 'features_TVFEMD.nc')\n",
    "    log_and_print(f\"Dataset TVF-EMD cargado, dims: {ds_tvfemd.dims}\")\n",
    "    log_and_print(f\"Variables disponibles en TVF-EMD: {list(ds_tvfemd.data_vars.keys())}\")\n",
    "    \n",
    "    # Cargar shapefile para visualizaciones\n",
    "    gdf = gpd.read_file(SHP_PATH)\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(epsg=4326)\n",
    "    elif gdf.crs.to_epsg() != 4326:\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "    log_and_print(\"Shapefile cargado y CRS validado.\")\n",
    "\n",
    "    # Extraer informaci√≥n temporal\n",
    "    times = ds_full.time.values.astype('datetime64[M]')\n",
    "    REF = np.datetime64('2024-02','M')\n",
    "    idx_ref = int(np.where(times==REF)[0][0])\n",
    "    log_and_print(f\"Referencia (REF) = {REF}, index={idx_ref}\")\n",
    "    \n",
    "    return ds_full, ds_ceemdan, ds_tvfemd, gdf, times, REF, idx_ref\n",
    "\n",
    "# Ejecutar carga/preproc con los archivos espec√≠ficos\n",
    "ds_full, ds_ceemdan, ds_tvfemd, gdf, times, REF, idx_ref = load_and_preprocess_data()\n",
    "\n",
    "# Mantenemos s√≥lo las funciones de preprocesamiento que a√∫n se necesitan\n",
    "def calculate_afc(signal, lags=[1, 3, 6]):\n",
    "    \"\"\"Calcula la Funci√≥n de Autocorrelaci√≥n para los lags dados.\"\"\"\n",
    "    afc = [np.correlate(signal[lag:], signal[:-lag], mode='valid') for lag in lags]\n",
    "    return afc\n",
    "\n",
    "# Funciones de preprocesamiento adicionales\n",
    "def wavelet_denoise(data, wavelet='db4', level=3):\n",
    "    \"\"\"Aplica denoising wavelet a los datos.\"\"\"\n",
    "    try:\n",
    "        # API moderna de scikit-image (0.19+)\n",
    "        return denoise_wavelet(\n",
    "            data, \n",
    "            wavelet=wavelet, \n",
    "            mode='soft',\n",
    "            method='BayesShrink',\n",
    "            channel_axis=None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error en wavelet denoising: {str(e)}. Intentando m√©todo alternativo.\")\n",
    "        # Si falla, intentar solo con par√°metros b√°sicos\n",
    "        return denoise_wavelet(data, wavelet=wavelet)\n",
    "\n",
    "# 3) Definici√≥n de Datasets PyTorch\n",
    "class PrecipitationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para datos de precipitaci√≥n que maneja la reducci√≥n\n",
    "    de dimensionalidad y garantiza la compatibilidad dimensional.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, target, seq_length):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.target = torch.from_numpy(target).float()\n",
    "        self.seq_length = seq_length\n",
    "        self.target_shape = target.shape[1:]  # Guardar la forma objetivo (height, width)\n",
    "        \n",
    "        print(f\"Dataset inicializado - Forma de datos: {self.data.shape}\")\n",
    "        print(f\"Dataset inicializado - Forma de targets: {self.target.shape}\")\n",
    "        print(f\"Forma objetivo almacenada: {self.target_shape}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Obtener secuencia de datos\n",
    "        inputs = self.data[idx:idx+self.seq_length]\n",
    "        labels = self.target[idx:idx+self.seq_length]\n",
    "        \n",
    "        # Para debugging, imprimir formas solo para el primer elemento\n",
    "        if idx == 0:\n",
    "            print(f\"Ejemplo de entrada - forma original: {inputs.shape}\")\n",
    "            \n",
    "        # Reducir dimensionalidad de las caracter√≠sticas y transformar a tensor 2D\n",
    "        if len(inputs.shape) == 2:  # [seq_length, features]\n",
    "            # Crear un tensor 3D con forma [channels, height, width] \n",
    "            # con dimensiones espaciales adecuadas para redes convolucionales\n",
    "            n_features = inputs.shape[1]\n",
    "            feature_dim = min(int(np.sqrt(n_features)), 16)  # Limitar a un tama√±o razonable\n",
    "            \n",
    "            # Seleccionar primeros feature_dim¬≤ caracter√≠sticas para crear mapa 2D\n",
    "            n_features_to_use = min(feature_dim * feature_dim, n_features)\n",
    "            flattened_features = inputs.mean(dim=0)[:n_features_to_use]\n",
    "            \n",
    "            # Reshape a forma [1, feature_dim, feature_dim] para canal √∫nico\n",
    "            padding = torch.zeros(feature_dim * feature_dim - n_features_to_use) if n_features_to_use < feature_dim * feature_dim else None\n",
    "            if padding is not None:\n",
    "                flattened_features = torch.cat([flattened_features, padding])\n",
    "            \n",
    "            # Crear mapa 2D con canal √∫nico [1, H, W]\n",
    "            spatial_features = flattened_features.reshape(1, feature_dim, feature_dim)\n",
    "            inputs = spatial_features\n",
    "            \n",
    "        # Para debugging\n",
    "        if idx == 0:\n",
    "            print(f\"Ejemplo de entrada - forma final: {inputs.shape}\")\n",
    "            print(f\"Ejemplo de etiqueta: {labels.shape}\")\n",
    "            \n",
    "        # CORREGIDO: Devolver s√≥lo inputs y labels, omitir target_shape para compatibilidad\n",
    "        return inputs, labels\n",
    "\n",
    "# 4) Modelos H√≠bridos: ConvBiGRU-AE y ConvLSTM-AE\n",
    "class ConvBiGRU_AE(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim, num_layers, output_channels, seq_length, target_shape=(61, 65), kernel_size=3, padding=1):\n",
    "        super(ConvBiGRU_AE, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_channels = output_channels\n",
    "        self.target_shape = target_shape  # Guardar la forma objetivo\n",
    "        \n",
    "        # Encoder - Convoluciones 2D para cada paso de tiempo\n",
    "        # En lugar de esperar m√∫ltiples canales, procesaremos cada paso de tiempo independientemente\n",
    "        self.conv1 = nn.Conv2d(1, hidden_dim, kernel_size=kernel_size, padding=padding)\n",
    "        self.norm1 = nn.InstanceNorm2d(hidden_dim)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=padding)\n",
    "        self.norm2 = nn.InstanceNorm2d(hidden_dim)\n",
    "        \n",
    "        # GRU para procesar la secuencia comprimida\n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(hidden_dim * 2, hidden_dim, kernel_size=kernel_size, padding=padding)\n",
    "        self.dnorm1 = nn.InstanceNorm2d(hidden_dim)\n",
    "        self.deconv2 = nn.ConvTranspose2d(hidden_dim, output_channels * seq_length, kernel_size=kernel_size, padding=padding)\n",
    "        \n",
    "    def forward(self, x, target_shape=None):\n",
    "        # Usar la forma objetivo proporcionada o la predeterminada\n",
    "        if target_shape is None:\n",
    "            target_shape = self.target_shape\n",
    "        \n",
    "        # x tiene forma [batch_size, seq_length, height, width]\n",
    "        batch_size, seq_len, H, W = x.size()\n",
    "        \n",
    "        # Procesar cada paso de tiempo individualmente\n",
    "        processed_features = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Obtener el paso de tiempo actual y a√±adir dimensi√≥n de canal\n",
    "            x_t = x[:, t].unsqueeze(1)  # [batch_size, 1, height, width]\n",
    "            \n",
    "            # Aplicar convoluciones\n",
    "            x_t = F.relu(self.norm1(self.conv1(x_t)))\n",
    "            x_t = F.relu(self.norm2(self.conv2(x_t)))\n",
    "            \n",
    "            # Extraer caracter√≠sticas globales\n",
    "            # Promedio espacial para reducir a [batch_size, hidden_dim]\n",
    "            x_t_features = x_t.mean(dim=(2, 3))\n",
    "            \n",
    "            # Almacenar caracter√≠sticas para este paso de tiempo\n",
    "            processed_features.append(x_t_features)\n",
    "        \n",
    "        # Concatenar caracter√≠sticas para todos los pasos de tiempo\n",
    "        sequence_features = torch.stack(processed_features, dim=1)  # [batch_size, seq_length, hidden_dim]\n",
    "        \n",
    "        # Aplicar GRU a la secuencia\n",
    "        output, _ = self.gru(sequence_features)  # [batch_size, seq_length, hidden_dim*2]\n",
    "        \n",
    "        # Tomar el √∫ltimo estado y preparar para deconv\n",
    "        output = output[:, -1, :].view(batch_size, self.hidden_dim*2, 1, 1)\n",
    "        output = F.interpolate(output, size=(H, W), mode='nearest')\n",
    "        \n",
    "        # Decoder\n",
    "        output = F.relu(self.dnorm1(self.deconv1(output)))\n",
    "        output = self.deconv2(output)  # [batch_size, seq_length*output_channels, H, W]\n",
    "        \n",
    "        # Reorganizar para obtener [batch_size, seq_length, output_channels, H, W]\n",
    "        output = output.view(batch_size, self.seq_length, self.output_channels, H, W)\n",
    "        \n",
    "        # Redimensionar a la forma objetivo usando interpolaci√≥n bilineal\n",
    "        # Primero reorganizamos para [batch_size*seq_length, output_channels, H, W]\n",
    "        output_reshaped = output.view(batch_size * self.seq_length, self.output_channels, H, W)\n",
    "        output_resized = F.interpolate(output_reshaped, size=target_shape, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Reorganizar de nuevo a [batch_size, seq_length, output_channels, target_H, target_W]\n",
    "        output = output_resized.view(batch_size, self.seq_length, self.output_channels, target_shape[0], target_shape[1])\n",
    "        \n",
    "        return output\n",
    "\n",
    "class ConvBiLSTM_AE(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_dim, num_layers, output_channels, seq_length, target_shape=(61, 65), kernel_size=3, padding=1):\n",
    "        super(ConvBiLSTM_AE, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_channels = output_channels\n",
    "        self.target_shape = target_shape  # Guardar la forma objetivo\n",
    "        \n",
    "        # Encoder para procesar cada paso de tiempo individualmente\n",
    "        self.conv1 = nn.Conv2d(1, hidden_dim, kernel_size=kernel_size, padding=padding)\n",
    "        self.norm1 = nn.InstanceNorm2d(hidden_dim)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=padding)\n",
    "        self.norm2 = nn.InstanceNorm2d(hidden_dim)\n",
    "        \n",
    "        # LSTM para procesar la secuencia\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # Decoder\n",
    "        self.deconv1 = nn.ConvTranspose2d(hidden_dim * 2, hidden_dim, kernel_size=kernel_size, padding=padding)\n",
    "        self.dnorm1 = nn.InstanceNorm2d(hidden_dim)\n",
    "        self.deconv2 = nn.ConvTranspose2d(hidden_dim, output_channels * seq_length, kernel_size=kernel_size, padding=padding)\n",
    "        \n",
    "    def forward(self, x, target_shape=None):\n",
    "        # Usar la forma objetivo proporcionada o la predeterminada\n",
    "        if target_shape is None:\n",
    "            target_shape = self.target_shape\n",
    "            \n",
    "        # x tiene forma [batch_size, seq_length, height, width]\n",
    "        batch_size, seq_len, H, W = x.size()\n",
    "        \n",
    "        # Procesar cada paso de tiempo individualmente\n",
    "        processed_features = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Obtener el paso de tiempo actual y a√±adir dimensi√≥n de canal\n",
    "            x_t = x[:, t].unsqueeze(1)  # [batch_size, 1, height, width]\n",
    "            \n",
    "            # Aplicar convoluciones\n",
    "            x_t = F.relu(self.norm1(self.conv1(x_t)))\n",
    "            x_t = F.relu(self.norm2(self.conv2(x_t)))\n",
    "            \n",
    "            # Extraer caracter√≠sticas globales\n",
    "            # Promedio espacial para reducir a [batch_size, hidden_dim]\n",
    "            x_t_features = x_t.mean(dim=(2, 3))\n",
    "            \n",
    "            # Almacenar caracter√≠sticas para este paso de tiempo\n",
    "            processed_features.append(x_t_features)\n",
    "        \n",
    "        # Concatenar caracter√≠sticas para todos los pasos de tiempo\n",
    "        sequence_features = torch.stack(processed_features, dim=1)  # [batch_size, seq_length, hidden_dim]\n",
    "        \n",
    "        # Aplicar LSTM a la secuencia\n",
    "        output, _ = self.lstm(sequence_features)  # [batch_size, seq_length, hidden_dim*2]\n",
    "        \n",
    "        # Tomar el √∫ltimo estado y preparar para deconv\n",
    "        output = output[:, -1, :].view(batch_size, self.hidden_dim*2, 1, 1)\n",
    "        output = F.interpolate(output, size=(H, W), mode='nearest')\n",
    "        \n",
    "        # Decoder\n",
    "        output = F.relu(self.dnorm1(self.deconv1(output)))\n",
    "        output = self.deconv2(output)  # [batch_size, seq_length*output_channels, H, W]\n",
    "        \n",
    "        # Reorganizar para obtener [batch_size, seq_length, output_channels, H, W]\n",
    "        output = output.view(batch_size, self.seq_length, self.output_channels, H, W)\n",
    "        \n",
    "        # Redimensionar a la forma objetivo usando interpolaci√≥n bilineal\n",
    "        output_reshaped = output.view(batch_size * self.seq_length, self.output_channels, H, W)\n",
    "        output_resized = F.interpolate(output_reshaped, size=target_shape, mode='bilinear', align_corners=False)\n",
    "        output = output_resized.view(batch_size, self.seq_length, self.output_channels, target_shape[0], target_shape[1])\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 5) Funciones de entrenamiento y evaluaci√≥n\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=500, patience=50):\n",
    "    \"\"\"\n",
    "    Entrena el modelo usando los loaders proporcionados, con soporte para scheduler y m√°s m√©tricas.\n",
    "    Compatible con diferentes versiones de PyTorch.\n",
    "    \"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    # Guardar ruta para el modelo\n",
    "    model_path = MODELS_OUTPUT / 'best_model_temp.pth'\n",
    "    \n",
    "    # Verificar la versi√≥n de PyTorch para usar los par√°metros adecuados al guardar modelos\n",
    "    import torch\n",
    "    torch_version = torch.__version__\n",
    "    print(f\"Versi√≥n de PyTorch detectada: {torch_version}\")\n",
    "    \n",
    "    # Funci√≥n para guardar modelo compatible con diferentes versiones de PyTorch\n",
    "    def save_model(model, path):\n",
    "        try:\n",
    "            torch.save(model.state_dict(), path, weights_only=True)\n",
    "            print(\"Modelo guardado con par√°metro weights_only=True\")\n",
    "        except TypeError:\n",
    "            try:\n",
    "                torch.save(model.state_dict(), path, _use_new_zipfile_serialization=True)\n",
    "                print(\"Modelo guardado con par√°metro _use_new_zipfile_serialization=True\")\n",
    "            except TypeError:\n",
    "                torch.save(model.state_dict(), path)\n",
    "                print(\"Modelo guardado sin par√°metros adicionales\")\n",
    "    \n",
    "    # Funci√≥n para cargar modelo compatible con diferentes versiones\n",
    "    def load_model(model, path):\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(path, weights_only=True))\n",
    "            print(\"Modelo cargado con par√°metro weights_only=True\")\n",
    "        except TypeError:\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "                print(\"Modelo cargado con par√°metro map_location\")\n",
    "            except TypeError:\n",
    "                model.load_state_dict(torch.load(path))\n",
    "                print(\"Modelo cargado sin par√°metros adicionales\")\n",
    "    \n",
    "    # Para graficar el progreso durante el entrenamiento\n",
    "    def plot_progress():\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.axhline(y=best_val_loss, color='r', linestyle='--', label=f'Best: {best_val_loss:.2f}')\n",
    "        plt.title(f'Loss vs. Epochs (Current: {val_losses[-1]:.2f})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        rel_loss = [l/train_losses[0] for l in train_losses]\n",
    "        rel_val_loss = [l/val_losses[0] for l in val_losses]\n",
    "        plt.plot(rel_loss, label='Train')\n",
    "        plt.plot(rel_val_loss, label='Val')\n",
    "        plt.title(f'Relative Loss (% of initial loss)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Relative Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        batch_train_losses = []\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            # Debug info en primera iteraci√≥n\n",
    "            if epoch == 0 and len(batch_train_losses) == 0:\n",
    "                print(f\"Batch entrenamiento - inputs: {inputs.shape}, targets: {targets.shape}\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Asegurar que outputs y targets tienen formas comparables para calcular la p√©rdida\n",
    "            if len(outputs.shape) == 5 and len(targets.shape) <= 4:\n",
    "                outputs = outputs.squeeze(2)  # eliminar dim C si es 1\n",
    "                \n",
    "            if len(outputs.shape) != len(targets.shape):\n",
    "                if len(outputs.shape) == 5 and len(targets.shape) == 3:\n",
    "                    targets = targets.unsqueeze(1).unsqueeze(2).repeat(1, outputs.shape[1], 1, 1, 1)\n",
    "                elif len(outputs.shape) == 5 and len(targets.shape) == 4:\n",
    "                    targets = targets.unsqueeze(2)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            # Gradient clipping para estabilidad\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_train_losses.append(loss.item())\n",
    "        \n",
    "        train_loss = np.mean(batch_train_losses)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        batch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Ajustar dimensiones si es necesario\n",
    "                if len(outputs.shape) == 5 and len(targets.shape) <= 4:\n",
    "                    outputs = outputs.squeeze(2)\n",
    "                \n",
    "                if len(outputs.shape) != len(targets.shape):\n",
    "                    if len(outputs.shape) == 5 and len(targets.shape) == 3:\n",
    "                        targets = targets.unsqueeze(1).unsqueeze(2).repeat(1, outputs.shape[1], 1, 1, 1)\n",
    "                    elif len(outputs.shape) == 5 and len(targets.shape) == 4:\n",
    "                        targets = targets.unsqueeze(2)\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                batch_val_losses.append(loss.item())\n",
    "        \n",
    "        val_loss = np.mean(batch_val_losses)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Usar scheduler si est√° disponible\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        # Mostrar progreso cada 10 √©pocas o en la √∫ltima\n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            print(f'Epoca {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            plot_progress()  # Actualizar el gr√°fico\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Usar la funci√≥n save_model compatible con diferentes versiones\n",
    "            save_model(model, model_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print('Early stopping triggered')\n",
    "                break\n",
    "    \n",
    "    # Cargar el mejor modelo con la funci√≥n load_model compatible\n",
    "    load_model(model, model_path)\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Tambi√©n necesitamos corregir la funci√≥n al guardar los modelos finales al final del entrenamiento\n",
    "def save_model_compatible(model, path):\n",
    "    \"\"\"\n",
    "    Guarda un modelo de manera compatible con diferentes versiones de PyTorch.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        torch.save(model.state_dict(), path, weights_only=True)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            torch.save(model.state_dict(), path, _use_new_zipfile_serialization=True)\n",
    "        except TypeError:\n",
    "            torch.save(model.state_dict(), path)\n",
    "    print(f\"Modelo guardado en {path}\")\n",
    "\n",
    "def save_model(model, path, epoch=0, val_loss=0.0):\n",
    "    \"\"\"\n",
    "    Guarda un modelo PyTorch con metadatos, de forma compatible con diferentes versiones PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo PyTorch a guardar\n",
    "        path: Ruta donde guardar el modelo\n",
    "        epoch: N√∫mero de √©poca actual\n",
    "        val_loss: P√©rdida de validaci√≥n actual\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Crear directorio si no existe\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        # Guardar modelo con metadatos\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'val_loss': val_loss\n",
    "        }\n",
    "        \n",
    "        # Intentar diferentes m√©todos de guardado seg√∫n compatibilidad de versi√≥n\n",
    "        try:\n",
    "            torch.save(checkpoint, path, weights_only=True)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                torch.save(checkpoint, path, _use_new_zipfile_serialization=True)\n",
    "            except TypeError:\n",
    "                torch.save(checkpoint, path)\n",
    "        \n",
    "        print(f\"Modelo guardado en {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar el modelo en {path}: {str(e)}\")\n",
    "\n",
    "# Visualizaci√≥n mejorada con coordenadas geogr√°ficas\n",
    "def visualize_predictions_with_geospatial_coords():\n",
    "    \"\"\"\n",
    "    Visualiza las predicciones usando coordenadas geoespaciales reales\n",
    "    y muestra un mapa con m√°s detalle, incluyendo l√≠mites administrativos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import os\n",
    "        \n",
    "        # Determinar si tenemos archivos de predicciones\n",
    "        pred_file = PREDS_DIR / 'convbigru_predictions.npy'\n",
    "        if not os.path.exists(pred_file):\n",
    "            print(\"No se encontraron archivos de predicciones. Generando predicciones...\")\n",
    "            \n",
    "            # Generar predicciones si no existen archivos\n",
    "            model = convbigru_ae\n",
    "            model.eval()\n",
    "            \n",
    "            # Usar un solo batch del dataset de validaci√≥n\n",
    "            val_batch = next(iter(val_loader))\n",
    "            inputs, targets = val_batch\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            # Hacer predicci√≥n\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "            # Convertir a numpy\n",
    "            predictions = outputs.cpu().detach().numpy()\n",
    "            targets_np = targets.cpu().detach().numpy()\n",
    "            \n",
    "            # Guardar para uso futuro\n",
    "            np.save(PREDS_DIR / 'convbigru_predictions.npy', predictions)\n",
    "            np.save(PREDS_DIR / 'targets.npy', targets_np)\n",
    "            \n",
    "            print(f\"Predicciones generadas y guardadas con forma: {predictions.shape}\")\n",
    "            \n",
    "            # Usar las predicciones generadas\n",
    "            if len(predictions.shape) == 5:\n",
    "                # [B, seq, C, H, W]\n",
    "                sample_pred = predictions[0, 0, 0]  # Primer batch, primer elemento de secuencia, primer canal\n",
    "            elif len(predictions.shape) == 4:\n",
    "                # [B, seq, H, W]\n",
    "                sample_pred = predictions[0, 0]     # Primer batch, primer elemento de secuencia\n",
    "            else:\n",
    "                sample_pred = predictions\n",
    "                \n",
    "            # Mismo proceso para targets\n",
    "            if len(targets_np.shape) == 4:\n",
    "                sample_target = targets_np[0, 0]    # Primer batch, primer elemento de secuencia\n",
    "            elif len(targets_np.shape) == 3:\n",
    "                sample_target = targets_np[0]       # Primer batch\n",
    "            else:\n",
    "                sample_target = targets_np\n",
    "        else:\n",
    "            # Cargar predicciones existentes\n",
    "            predictions = np.load(PREDS_DIR / 'convbigru_predictions.npy')\n",
    "            targets = np.load(PREDS_DIR / 'targets.npy')\n",
    "            \n",
    "            # Extraer muestra para visualizaci√≥n\n",
    "            if len(predictions.shape) == 5:\n",
    "                sample_pred = predictions[0, 0, 0]  # [B, seq, C, H, W] -> [H, W]\n",
    "            elif len(predictions.shape) == 4:\n",
    "                sample_pred = predictions[0, 0]     # [B, seq, H, W] -> [H, W]\n",
    "            else:\n",
    "                sample_pred = predictions\n",
    "                \n",
    "            if len(targets.shape) == 4:\n",
    "                sample_target = targets[0, 0]       # [B, seq, H, W] -> [H, W]\n",
    "            elif len(targets.shape) == 3:\n",
    "                sample_target = targets[0]          # [B, H, W] -> [H, W]\n",
    "            else:\n",
    "                sample_target = targets\n",
    "        \n",
    "        # Extraer coordenadas lat/lon del dataset\n",
    "        latitudes = ds_full.latitude.values\n",
    "        longitudes = ds_full.longitude.values\n",
    "        \n",
    "        # Crear malla de coordenadas\n",
    "        lon_mesh, lat_mesh = np.meshgrid(longitudes, latitudes)\n",
    "        \n",
    "        # Crear figura con proyecci√≥n de mapa\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        \n",
    "        # Definir proyecci√≥n y l√≠mites del mapa para Colombia/regi√≥n de inter√©s\n",
    "        projection = ccrs.PlateCarree()\n",
    "        \n",
    "        # Crear tres subfiguras con la misma proyecci√≥n\n",
    "        ax1 = fig.add_subplot(131, projection=projection)\n",
    "        ax2 = fig.add_subplot(132, projection=projection)\n",
    "        ax3 = fig.add_subplot(133, projection=projection)\n",
    "        \n",
    "        # Configurar cada subplot\n",
    "        for ax, data, title in zip([ax1, ax2, ax3], \n",
    "                                   [sample_target, sample_pred, sample_pred - sample_target], \n",
    "                                   ['Valores Reales', 'Predicci√≥n ConvBiGRU-AE', 'Error (Predicci√≥n - Real)']):\n",
    "            # A√±adir caracter√≠sticas del mapa\n",
    "            ax.coastlines(resolution='10m', color='black', linewidth=1)\n",
    "            ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "            ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "            \n",
    "            # A√±adir el shapefile de Colombia\n",
    "            ax.add_geometries(gdf.geometry, crs=ccrs.PlateCarree(), edgecolor='black',\n",
    "                             facecolor='none', alpha=0.8, linewidth=0.5)\n",
    "            \n",
    "            # Crear mapa de contorno con coordenadas reales\n",
    "            im = ax.pcolormesh(lon_mesh, lat_mesh, data, cmap='viridis', \n",
    "                              transform=ccrs.PlateCarree())\n",
    "            \n",
    "            # A√±adir barra de color\n",
    "            cbar = fig.colorbar(im, ax=ax, orientation='horizontal', pad=0.05, fraction=0.05)\n",
    "            cbar.ax.tick_params(labelsize=8)\n",
    "            \n",
    "            # A√±adir t√≠tulo y cuadr√≠cula\n",
    "            ax.set_title(title, fontsize=14)\n",
    "            gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.5)\n",
    "            gl.top_labels = False\n",
    "            gl.right_labels = False\n",
    "            \n",
    "            # Establecer l√≠mites del mapa para la zona de Boyac√° en Colombia\n",
    "            ax.set_extent([-75.5, -71.5, 4.0, 7.5], crs=ccrs.PlateCarree())\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(MODELS_OUTPUT / 'geospatial_predictions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al visualizar predicciones geoespaciales: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 6) Preparaci√≥n de datos para modelos h√≠bridos - Versi√≥n simplificada\n",
    "\n",
    "# Funci√≥n para generar predicciones del modelo base en caso de que no existan\n",
    "def create_base_model_predictions(ds_full, idx_ref):\n",
    "    \"\"\"\n",
    "    Entrena un modelo ConvBiGRU b√°sico y genera predicciones para usar como entrada\n",
    "    del meta-modelo, para evitar el uso de datos sint√©ticos.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERACI√ìN DE PREDICCIONES CON MODELO BASE ConvBiGRU\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nüîÑ Iniciando generaci√≥n de predicciones del modelo base...\")\n",
    "    \n",
    "    # Verificar si ya existen las predicciones en el dataset\n",
    "    if 'convbigru_preds' in ds_full.data_vars:\n",
    "        print(\"‚úÖ Las predicciones 'convbigru_preds' ya existen en el dataset.\")\n",
    "        return ds_full\n",
    "    \n",
    "    # 1. Seleccionar datos para entrenamiento y validaci√≥n\n",
    "    print(\"1Ô∏è‚É£ Preparando datos para modelo base...\")\n",
    "    \n",
    "    # Usaremos precipitation como entrada y target\n",
    "    if 'total_precipitation' in ds_full:\n",
    "        precip_var = 'total_precipitation'\n",
    "    elif 'precip' in ds_full:\n",
    "        precip_var = 'precip'\n",
    "    else:\n",
    "        # Buscar cualquier variable que contenga 'precip' en el nombre\n",
    "        precip_vars = [var for var in ds_full.data_vars if 'precip' in var.lower()]\n",
    "        if precip_vars:\n",
    "            precip_var = precip_vars[0]\n",
    "        else:\n",
    "            print(\"‚ùå No se encontr√≥ ninguna variable de precipitaci√≥n\")\n",
    "            return ds_full\n",
    "    \n",
    "    print(f\"   - Usando variable '{precip_var}' como entrada y objetivo\")\n",
    "    \n",
    "    # Separar en train y validaci√≥n usando la fecha de referencia\n",
    "    ds_base_train = ds_full.sel(time=slice(None, ds_full.time.values[idx_ref-1]))\n",
    "    ds_base_val = ds_full.sel(time=slice(ds_full.time.values[idx_ref], None))\n",
    "    \n",
    "    # Convertir a arrays NumPy\n",
    "    X_base_train = ds_base_train[precip_var].values.astype(np.float32)\n",
    "    y_base_train = X_base_train.copy()  # Mismo input/output para el modelo base\n",
    "    \n",
    "    X_base_val = ds_base_val[precip_var].values.astype(np.float32)\n",
    "    y_base_val = X_base_val.copy()\n",
    "    \n",
    "    # A√±adir dimensi√≥n de canal si es necesario\n",
    "    if len(X_base_train.shape) == 3:  # [tiempo, lat, lon]\n",
    "        X_base_train = X_base_train.reshape(X_base_train.shape[0], 1, X_base_train.shape[1], X_base_train.shape[2])\n",
    "        X_base_val = X_base_val.reshape(X_base_val.shape[0], 1, X_base_val.shape[1], X_base_val.shape[2])\n",
    "        \n",
    "    print(f\"   - Forma de datos de entrenamiento: {X_base_train.shape}\")\n",
    "    print(f\"   - Forma de datos de validaci√≥n: {X_base_val.shape}\")\n",
    "    \n",
    "    # 2. Crear dataset de PyTorch\n",
    "    print(\"\\n2Ô∏è‚É£ Creando datasets y dataloaders...\")\n",
    "    \n",
    "    seq_length = min(12, X_base_train.shape[0] // 10)  # Secuencia m√°s corta para modelo base\n",
    "    \n",
    "    # Convertir a tensores PyTorch\n",
    "    X_base_train_tensor = torch.from_numpy(X_base_train).float()\n",
    "    y_base_train_tensor = torch.from_numpy(y_base_train).float()\n",
    "    \n",
    "    X_base_val_tensor = torch.from_numpy(X_base_val).float()\n",
    "    y_base_val_tensor = torch.from_numpy(y_base_val).float()\n",
    "    \n",
    "    # Crear datasets personalizados para secuencias\n",
    "    class SimpleSeqDataset(Dataset):\n",
    "        def __init__(self, features, targets, seq_length=12):\n",
    "            self.features = features\n",
    "            self.targets = targets\n",
    "            self.seq_length = seq_length\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.features) - self.seq_length + 1\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            # Input: secuencia de 'seq_length' elementos\n",
    "            x = self.features[idx:idx + self.seq_length]\n",
    "            # Target: siguiente elemento despu√©s de la secuencia\n",
    "            # Si queremos predecir m√∫ltiples pasos, podemos usar:\n",
    "            y = self.targets[idx + self.seq_length - 1:idx + self.seq_length]\n",
    "            return x, y\n",
    "    \n",
    "    # Crear datasets\n",
    "    train_base_dataset = SimpleSeqDataset(X_base_train_tensor, y_base_train_tensor, seq_length)\n",
    "    val_base_dataset = SimpleSeqDataset(X_base_val_tensor, y_base_val_tensor, seq_length)\n",
    "    \n",
    "    # Crear dataloaders\n",
    "    batch_size_base = min(batch_size, len(train_base_dataset) // 10)  # Batch m√°s peque√±o si hay pocos datos\n",
    "    batch_size_base = max(1, batch_size_base)  # Asegurar batch_size m√≠nimo 1\n",
    "    \n",
    "    train_base_loader = DataLoader(train_base_dataset, batch_size=batch_size_base, shuffle=True)\n",
    "    val_base_loader = DataLoader(val_base_dataset, batch_size=batch_size_base, shuffle=False)\n",
    "    \n",
    "    print(f\"   - Longitud de secuencia: {seq_length}\")\n",
    "    print(f\"   - Batch size: {batch_size_base}\")\n",
    "    print(f\"   - Batches por √©poca: {len(train_base_loader)}\")\n",
    "    \n",
    "    # 3. Crear modelo base simplificado\n",
    "    print(\"\\n3Ô∏è‚É£ Creando modelo base ConvBiGRU simplificado...\")\n",
    "    \n",
    "    input_channels_base = X_base_train.shape[1]\n",
    "    hidden_dim_base = 64  # M√°s peque√±o para modelo base\n",
    "    output_channels_base = 1\n",
    "    \n",
    "    # Modelo ConvBiGRU simplificado para generar predicciones base\n",
    "    class SimpleConvBiGRU(nn.Module):\n",
    "        def __init__(self, input_channels, hidden_dim, output_channels):\n",
    "            super(SimpleConvBiGRU, self).__init__()\n",
    "            self.conv = nn.Conv2d(input_channels, hidden_dim, kernel_size=3, padding=1)\n",
    "            self.bn = nn.BatchNorm2d(hidden_dim)\n",
    "            self.gru = nn.GRU(hidden_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "            self.output_conv = nn.Conv2d(hidden_dim*2, output_channels, kernel_size=3, padding=1)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # x tiene forma [batch, sequence, channels, height, width]\n",
    "            batch_size, seq_len, C, H, W = x.shape\n",
    "            \n",
    "            # Procesar cada paso de tiempo\n",
    "            outputs = []\n",
    "            for t in range(seq_len):\n",
    "                # Obtener el frame actual\n",
    "                x_t = x[:, t]  # [batch, channels, height, width]\n",
    "                \n",
    "                # Aplicar convoluci√≥n\n",
    "                x_t = F.relu(self.bn(self.conv(x_t)))\n",
    "                \n",
    "                # Extraer caracter√≠sticas para GRU (promedio espacial)\n",
    "                features = x_t.view(batch_size, -1, H*W)\n",
    "                features = features.mean(dim=2)  # [batch, hidden_dim]\n",
    "                \n",
    "                # A√±adir dimensi√≥n de secuencia para GRU\n",
    "                features = features.unsqueeze(1)  # [batch, 1, hidden_dim]\n",
    "                \n",
    "                # Si es el primer paso, inicializar salida GRU\n",
    "                if t == 0:\n",
    "                    gru_out, h = self.gru(features)\n",
    "                else:\n",
    "                    gru_out, h = self.gru(features, h)\n",
    "                \n",
    "                # Reformar para conv final\n",
    "                gru_features = gru_out.view(batch_size, -1, 1, 1)  # [batch, hidden_dim*2, 1, 1]\n",
    "                gru_features = F.interpolate(gru_features, size=(H, W), mode='bilinear', align_corners=False)\n",
    "                \n",
    "                # Generar salida\n",
    "                out = self.output_conv(gru_features)  # [batch, output_channels, height, width]\n",
    "                outputs.append(out)\n",
    "            \n",
    "            # Concatenar todos los outputs\n",
    "            return torch.stack(outputs, dim=1)  # [batch, sequence, output_channels, height, width]\n",
    "    \n",
    "    # Instanciar modelo\n",
    "    base_model = SimpleConvBiGRU(input_channels_base, hidden_dim_base, output_channels_base).to(DEVICE)\n",
    "    print(f\"   - Modelo creado con {input_channels_base} canales de entrada, {hidden_dim_base} dimensiones ocultas\")\n",
    "    \n",
    "    # 4. Entrenar modelo\n",
    "    print(\"\\n4Ô∏è‚É£ Entrenando modelo base...\")\n",
    "    \n",
    "    # Hiperpar√°metros\n",
    "    lr_base = 1e-3\n",
    "    epochs_base = 30\n",
    "    patience_base = 10\n",
    "    \n",
    "    # Optimizador y funci√≥n de p√©rdida\n",
    "    criterion_base = nn.MSELoss()\n",
    "    optimizer_base = optim.Adam(base_model.parameters(), lr=lr_base)\n",
    "    \n",
    "    # Entrenamiento\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(epochs_base):\n",
    "        # Train\n",
    "        base_model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_base_loader):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer_base.zero_grad()\n",
    "            output = base_model(data)\n",
    "            \n",
    "            # Ajustar dimensiones para calcular p√©rdida\n",
    "            if output.shape != target.shape:\n",
    "                if len(output.shape) == 5 and len(target.shape) == 4:\n",
    "                    output = output.squeeze(2)  # Eliminar dimensi√≥n de canal si es 1\n",
    "                elif len(output.shape) == 5 and len(target.shape) == 3:\n",
    "                    output = output[:, -1, 0]   # Tomar √∫ltimo elemento de secuencia y primer canal\n",
    "            \n",
    "            # Calcular p√©rdida\n",
    "            loss = criterion_base(output, target)\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer_base.step()\n",
    "        \n",
    "        # Validation\n",
    "        base_model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_base_loader:\n",
    "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                output = base_model(data)\n",
    "                \n",
    "                # Ajustar dimensiones\n",
    "                if output.shape != target.shape:\n",
    "                    if len(output.shape) == 5 and len(target.shape) == 4:\n",
    "                        output = output.squeeze(2)\n",
    "                    elif len(output.shape) == 5 and len(target.shape) == 3:\n",
    "                        output = output[:, -1, 0]\n",
    "                \n",
    "                # Calcular p√©rdida\n",
    "                loss = criterion_base(output, target)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        # Calculamos p√©rdidas promedio\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        \n",
    "        # Mostrar progreso\n",
    "        print(f\"   √âpoca {epoch+1}/{epochs_base}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Guardar mejor modelo\n",
    "            torch.save(base_model.state_dict(), MODELS_OUTPUT / 'simple_convbigru_base.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience_base:\n",
    "            print(\"   Early stopping - No mejora en validaci√≥n durante varias √©pocas\")\n",
    "            break\n",
    "    \n",
    "    # 5. Generar predicciones para todo el dataset\n",
    "    print(\"\\n5Ô∏è‚É£ Generando predicciones para todo el dataset...\")\n",
    "    \n",
    "    # Cargar mejor modelo\n",
    "    base_model.load_state_dict(torch.load(MODELS_OUTPUT / 'simple_convbigru_base.pth'))\n",
    "    base_model.eval()\n",
    "    \n",
    "    # Crear dataset para todas las fechas\n",
    "    all_data = torch.from_numpy(X_base_train).float()\n",
    "    if len(X_base_val) > 0:\n",
    "        all_data = torch.cat([all_data, torch.from_numpy(X_base_val).float()], dim=0)\n",
    "    \n",
    "    # Generar predicciones\n",
    "    all_predictions = []\n",
    "    seq_len = seq_length\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(all_data) - seq_len + 1):\n",
    "            # Obtener secuencia\n",
    "            seq = all_data[i:i+seq_len].unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
    "            \n",
    "            # Generar predicci√≥n\n",
    "            pred = base_model(seq)\n",
    "            \n",
    "            # Si estamos en el √∫ltimo paso, a√±adimos las √∫ltimas predicciones\n",
    "            if i == 0:\n",
    "                # A√±adir todas las predicciones de la primera secuencia\n",
    "                for j in range(seq_len):\n",
    "                    if len(pred.shape) == 5:  # [B, seq, C, H, W]\n",
    "                        single_pred = pred[0, j, 0].cpu().numpy()  # Primera batch, paso j, canal 0\n",
    "                    else:\n",
    "                        single_pred = pred[0, j].cpu().numpy()  # Primera batch, paso j\n",
    "                    all_predictions.append(single_pred)\n",
    "            else:\n",
    "                # Para el resto, a√±adir solo la √∫ltima predicci√≥n (paso a paso)\n",
    "                if len(pred.shape) == 5:  # [B, seq, C, H, W]\n",
    "                    single_pred = pred[0, -1, 0].cpu().numpy()  # Primera batch, √∫ltimo paso, canal 0\n",
    "                else:\n",
    "                    single_pred = pred[0, -1].cpu().numpy()  # Primera batch, √∫ltimo paso\n",
    "                all_predictions.append(single_pred)\n",
    "    \n",
    "    # Convertir lista de predicciones a array\n",
    "    predictions_array = np.array(all_predictions)\n",
    "    print(f\"   - Forma de predicciones generadas: {predictions_array.shape}\")\n",
    "    \n",
    "    # Verificar que tenemos predicciones para todas las fechas\n",
    "    if len(predictions_array) < len(ds_full.time):\n",
    "        print(f\"‚ö†Ô∏è No se generaron suficientes predicciones ({len(predictions_array)} vs {len(ds_full.time)} fechas)\")\n",
    "        missing = len(ds_full.time) - len(predictions_array)\n",
    "        # Rellenar con los √∫ltimos valores repetidos\n",
    "        last_pred = predictions_array[-1]\n",
    "        for _ in range(missing):\n",
    "            predictions_array = np.concatenate([predictions_array, last_pred[np.newaxis, ...]], axis=0)\n",
    "    \n",
    "    # 6. Guardar predicciones en el dataset\n",
    "    print(\"\\n6Ô∏è‚É£ Guardando predicciones en el dataset...\")\n",
    "    \n",
    "    # A√±adir variable al dataset\n",
    "    ds_updated = ds_full.copy()\n",
    "    ds_updated['convbigru_preds'] = (('time', 'latitude', 'longitude'), predictions_array)\n",
    "    \n",
    "    # Guardar dataset actualizado\n",
    "    output_file = DATA_OUTPUT / 'dataset_with_convbigru_preds.nc'\n",
    "    ds_updated.to_netcdf(output_file)\n",
    "    print(f\"‚úÖ Dataset con predicciones guardado en {output_file}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Proceso completo: modelo entrenado y predicciones generadas\")\n",
    "    return ds_updated\n",
    "\n",
    "def prepare_data_for_hybrid_models(ds_full, ds_ceemdan, ds_tvfemd, idx_ref, horizon=12):\n",
    "    \"\"\"\n",
    "    Prepara los datos para el entrenamiento de los modelos h√≠bridos ConvBiGRU-AE y ConvLSTM-AE.\n",
    "    Con verificaci√≥n de l√≠mites para evitar IndexError y c√°lculos robustos de fechas.\n",
    "    \n",
    "    Args:\n",
    "        ds_full: Dataset completo con todas las caracter√≠sticas.\n",
    "        ds_ceemdan: Dataset con caracter√≠sticas de CEEMDAN.\n",
    "        ds_tvfemd: Dataset con caracter√≠sticas de TVF-EMD.\n",
    "        idx_ref: √çndice de la fecha de referencia en el dataset.\n",
    "        horizon: Horizonte de predicci√≥n (n√∫mero de meses).\n",
    "        \n",
    "    Returns:\n",
    "        X_train, y_train, X_val, y_val: Conjuntos de datos preparados para entrenamiento y validaci√≥n.\n",
    "    \"\"\"\n",
    "    # Verificar l√≠mites y ajustar √≠ndices si es necesario\n",
    "    max_idx = len(ds_full.time.values) - 1\n",
    "    \n",
    "    # Verificar que hay suficientes datos para validaci√≥n\n",
    "    if idx_ref + horizon > max_idx:\n",
    "        available_horizon = max_idx - idx_ref\n",
    "        print(f\"‚ö†Ô∏è Advertencia: No hay suficientes datos futuros. Ajustando horizonte de {horizon} a {available_horizon}.\")\n",
    "        horizon = available_horizon\n",
    "    \n",
    "    # Para datos de entrenamiento, usar todo hasta referencia (excepto √∫ltimo mes para tener targets disponibles)\n",
    "    # Asegurarnos de que idx_ref > 0 para evitar errores\n",
    "    if idx_ref <= 0:\n",
    "        raise ValueError(f\"El √≠ndice de referencia ({idx_ref}) debe ser mayor que 0.\")\n",
    "    \n",
    "    # Obtener las fechas para los conjuntos de entrenamiento y validaci√≥n\n",
    "    train_end_date = ds_full.time.values[idx_ref]\n",
    "    val_end_idx = min(idx_ref + horizon, max_idx)\n",
    "    \n",
    "    print(f\"üìÖ Rango de fechas: entrenamiento hasta {train_end_date}, validaci√≥n hasta {ds_full.time.values[val_end_idx]}\")\n",
    "    print(f\"üìä Horizonte efectivo: {horizon} meses\")\n",
    "    \n",
    "    # Para entrenamiento, usar datos hist√≥ricos hasta la fecha de referencia (exclusive)\n",
    "    ds_train = ds_full.sel(time=slice(None, ds_full.time.values[idx_ref-1]))\n",
    "    \n",
    "    # Para validaci√≥n, usar datos desde la fecha de referencia hasta el horizonte disponible\n",
    "    ds_val = ds_full.sel(time=slice(ds_full.time.values[idx_ref], ds_full.time.values[val_end_idx]))\n",
    "    \n",
    "    print(f\"üìè Tama√±o conjunto de entrenamiento: {len(ds_train.time)}\")\n",
    "    print(f\"üìè Tama√±o conjunto de validaci√≥n: {len(ds_val.time)}\")\n",
    "    \n",
    "    # VALIDACI√ìN DE DATOS:\n",
    "    # 1. Verificar si tenemos suficientes datos\n",
    "    if len(ds_train.time) < 12:\n",
    "        print(\"‚ö†Ô∏è ADVERTENCIA: Muy pocos datos para entrenamiento (<12 meses)!\")\n",
    "    if len(ds_val.time) < 3:\n",
    "        print(\"‚ö†Ô∏è ADVERTENCIA: Muy pocos datos para validaci√≥n (<3 meses)!\")\n",
    "        \n",
    "    # 2. Verificar que los conjuntos no se superpongan\n",
    "    train_times = set(ds_train.time.values.astype('datetime64[M]').astype(str))\n",
    "    val_times = set(ds_val.time.values.astype('datetime64[M]').astype(str))\n",
    "    if train_times.intersection(val_times):\n",
    "        print(\"‚ö†Ô∏è ERROR: Superposici√≥n entre conjuntos de entrenamiento y validaci√≥n!\")\n",
    "        \n",
    "    # 3. Verificar variables disponibles\n",
    "    available_vars = list(ds_train.data_vars.keys())\n",
    "    print(f\"üìã Variables disponibles: {available_vars[:5]}... (total: {len(available_vars)})\")\n",
    "    \n",
    "    # Extraer caracter√≠sticas y etiquetas\n",
    "    # Primero verificar si tenemos predictores precomputados\n",
    "    use_synthetic_data = False\n",
    "    if 'convbigru_preds' in ds_train:\n",
    "        print(\"‚úÖ Usando predicciones precomputadas 'convbigru_preds'\")\n",
    "        X_train = ds_train['convbigru_preds'].values.astype(np.float32)\n",
    "        X_val = ds_val['convbigru_preds'].values.astype(np.float32)\n",
    "    else:\n",
    "        # No tenemos predicciones, usamos datos sint√©ticos\n",
    "        use_synthetic_data = True\n",
    "        print(\"‚ÑπÔ∏è Variable 'convbigru_preds' no encontrada. Preparando datos alternativos...\")\n",
    "        \n",
    "        # Intentar usar precipitaci√≥n directamente\n",
    "        if 'total_precipitation' in ds_train:\n",
    "            print(\"‚úÖ Usando 'total_precipitation' como caracter√≠stica principal\")\n",
    "            precipitation_var = 'total_precipitation'\n",
    "        elif 'precip' in ds_train:\n",
    "            print(\"‚úÖ Usando 'precip' como caracter√≠stica principal\")\n",
    "            precipitation_var = 'precip'\n",
    "        else:\n",
    "            # Intentar encontrar algo relacionado con precipitaci√≥n\n",
    "            precip_vars = [var for var in available_vars if 'precip' in var.lower()]\n",
    "            if precip_vars:\n",
    "                precipitation_var = precip_vars[0]\n",
    "                print(f\"‚úÖ Usando '{precipitation_var}' como caracter√≠stica principal\")\n",
    "            else:\n",
    "                # Usar la primera variable disponible\n",
    "                precipitation_var = available_vars[0]\n",
    "                print(f\"‚ö†Ô∏è No se encontraron variables de precipitaci√≥n. Usando '{precipitation_var}'\")\n",
    "        \n",
    "        # Preparar datos usando la variable seleccionada\n",
    "        X_train = ds_train[precipitation_var].values.astype(np.float32)\n",
    "        X_val = ds_val[precipitation_var].values.astype(np.float32)\n",
    "        \n",
    "        # A√±adir dimensi√≥n de canal si es necesario [tiempo, lat, lon] -> [tiempo, 1, lat, lon]\n",
    "        if len(X_train.shape) == 3:\n",
    "            X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
    "            X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Target es siempre precipitaci√≥n\n",
    "    if 'total_precipitation' in ds_train:\n",
    "        y_train = ds_train['total_precipitation'].values.astype(np.float32)\n",
    "        y_val = ds_val['total_precipitation'].values.astype(np.float32)\n",
    "    elif 'precip' in ds_train:\n",
    "        y_train = ds_train['precip'].values.astype(np.float32)\n",
    "        y_val = ds_val['precip'].values.astype(np.float32)\n",
    "    else:\n",
    "        # Si no encontramos precipitaci√≥n, usar la misma variable que para las caracter√≠sticas\n",
    "        y_train = X_train.copy()\n",
    "        y_val = X_val.copy()\n",
    "        # Si tiene dimensi√≥n de canal, la quitamos para el target\n",
    "        if len(y_train.shape) == 4:\n",
    "            y_train = y_train.squeeze(1)\n",
    "            y_val = y_val.squeeze(1)\n",
    "    \n",
    "    # Imprimir informaci√≥n sobre los datos\n",
    "    print(f\"\\nüìä Resumen de los datos:\")\n",
    "    print(f\"  - X_train: {X_train.shape}, Rango: [{X_train.min():.2f}, {X_train.max():.2f}]\")\n",
    "    print(f\"  - y_train: {y_train.shape}, Rango: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
    "    print(f\"  - X_val: {X_val.shape}, Rango: [{X_val.min():.2f}, {X_val.max():.2f}]\")\n",
    "    print(f\"  - y_val: {y_val.shape}, Rango: [{y_val.min():.2f}, {y_val.max():.2f}]\")\n",
    "    \n",
    "    # Recordatorio final\n",
    "    if use_synthetic_data:\n",
    "        print(\"\\n‚ö†Ô∏è NOTA: Se est√°n usando datos sint√©ticos porque no se encontr√≥ 'convbigru_preds'\")\n",
    "        print(\"   Si esto es inesperado, verifique que el dataset incluye las variables necesarias.\")\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "# Verificar si necesitamos generar predicciones base\n",
    "if 'convbigru_preds' not in ds_full.data_vars:\n",
    "    print(\"\\nüö© No se encontraron predicciones 'convbigru_preds'. Generando modelo base y predicciones...\")\n",
    "    ds_full = create_base_model_predictions(ds_full, idx_ref)\n",
    "    print(\"\\n‚úÖ Ahora puede continuar con el entrenamiento del modelo meta usando las predicciones generadas.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ El dataset ya contiene las predicciones 'convbigru_preds'. No es necesario generarlas.\")\n",
    "\n",
    "# Aplicar la funci√≥n mejorada\n",
    "print(\"\\nüîÑ Preparando datos con la funci√≥n mejorada...\")\n",
    "X_train, y_train, X_val, y_val = prepare_data_for_hybrid_models(ds_full, ds_ceemdan, ds_tvfemd, idx_ref, horizon=OUTPUT_HORIZON)\n",
    "\n",
    "# Verificar formas de los conjuntos de datos\n",
    "print(f\"\\n‚úÖ Formas finales de los conjuntos de datos:\")\n",
    "print(f\"  - Entrenamiento (X): {X_train.shape}\")\n",
    "print(f\"  - Entrenamiento (y): {y_train.shape}\")\n",
    "print(f\"  - Validaci√≥n (X): {X_val.shape}\")\n",
    "print(f\"  - Validaci√≥n (y): {y_val.shape}\")\n",
    "\n",
    "# Configuraci√≥n de hiperpar√°metros mejorados para entrenamiento\n",
    "print(\"\\n\\n----- CONFIGURACI√ìN DE HIPERPAR√ÅMETROS MEJORADOS -----\")\n",
    "\n",
    "# Par√°metros del modelo mejorados\n",
    "input_channels = X_train.shape[1]  # N√∫mero de caracter√≠sticas de entrada\n",
    "hidden_dim = 128  # Aumentado de 64 a 128 para mayor capacidad\n",
    "num_layers = 3    # Aumentado de 2 a 3 para mayor profundidad\n",
    "output_channels = 1\n",
    "seq_length = OUTPUT_HORIZON  # Definimos seq_length como igual al horizonte de predicci√≥n\n",
    "learning_rate = 0.0005  # Reducido para una convergencia m√°s estable\n",
    "num_epochs = 500  # Aumentado sustancialmente de 200 a 500\n",
    "patience = 50     # Aumentado para permitir m√°s intentos antes de early stopping\n",
    "batch_size = 16   # Mantenemos el mismo tama√±o de batch\n",
    "\n",
    "# Inicializar modelos con arquitectura m√°s potente\n",
    "print(f\"Inicializando modelos con input_channels={input_channels}, hidden_dim={hidden_dim}...\")\n",
    "convbigru_ae = ConvBiGRU_AE(input_channels, hidden_dim, num_layers, output_channels, seq_length).to(DEVICE)\n",
    "convbilstm_ae = ConvBiLSTM_AE(input_channels, hidden_dim, num_layers, output_channels, seq_length).to(DEVICE)\n",
    "\n",
    "# Optimizadores con decay para evitar sobreajuste\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_convbigru = optim.Adam(convbigru_ae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "optimizer_convbilstm = optim.Adam(convbilstm_ae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# A√±adir schedulers para reducir el learning rate cuando la p√©rdida se estanca\n",
    "scheduler_convbigru = ReduceLROnPlateau(optimizer_convbigru, mode='min', factor=0.5, patience=20, verbose=True)\n",
    "scheduler_convbilstm = ReduceLROnPlateau(optimizer_convbilstm, mode='min', factor=0.5, patience=20, verbose=True)\n",
    "\n",
    "# Create dataset objects with the prepared data\n",
    "train_dataset = PrecipitationDataset(X_train, y_train, seq_length)\n",
    "val_dataset = PrecipitationDataset(X_val, y_val, seq_length)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Configuraci√≥n actualizada: {num_epochs} √©pocas, LR={learning_rate}, paciencia={patience}\")\n",
    "\n",
    "# Verificar compatibilidad con forward pass\n",
    "# Probar un forward pass con un batch del DataLoader\n",
    "try:\n",
    "    sample_inputs, sample_targets = next(iter(train_loader))\n",
    "    sample_inputs = sample_inputs.to(DEVICE)\n",
    "    sample_targets = sample_targets.to(DEVICE)\n",
    "    \n",
    "    # Probar con ambos modelos\n",
    "    with torch.no_grad():\n",
    "        sample_outputs_bigru = convbigru_ae(sample_inputs)\n",
    "        sample_outputs_bilstm = convbilstm_ae(sample_inputs)\n",
    "    \n",
    "    print(\"Forward pass exitoso. Formas de salida:\")\n",
    "    print(f\"  - ConvBiGRU-AE: {sample_outputs_bigru.shape}\")\n",
    "    print(f\"  - ConvBiLSTM-AE: {sample_outputs_bilstm.shape}\")\n",
    "    models_ready = True\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el forward pass de prueba: {str(e)}\")\n",
    "    models_ready = False\n",
    "\n",
    "# Entrenamiento con las funciones mejoradas y mayor n√∫mero de √©pocas\n",
    "if models_ready:\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ENTRENAMIENTO DE CONVBIGRU_AE\")\n",
    "        print(\"=\"*50)\n",
    "        convbigru_ae, train_losses_bigru, val_losses_bigru = train_model(\n",
    "            convbigru_ae, train_loader, val_loader, criterion, \n",
    "            optimizer_convbigru, scheduler_convbigru,\n",
    "            num_epochs, patience\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ENTRENAMIENTO DE CONVBILSTM_AE\")\n",
    "        print(\"=\"*50)\n",
    "        convbilstm_ae, train_losses_bilstm, val_losses_bilstm = train_model(\n",
    "            convbilstm_ae, train_loader, val_loader, criterion, \n",
    "            optimizer_convbilstm, scheduler_convbilstm,\n",
    "            num_epochs, patience\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Entrenamiento completado correctamente\")\n",
    "        \n",
    "        # Guardar modelos entrenados\n",
    "        save_model_compatible(convbigru_ae, MODELS_OUTPUT / 'convbigru_ae_model.pth')\n",
    "        save_model_compatible(convbilstm_ae, MODELS_OUTPUT / 'convbilstm_ae_model.pth')\n",
    "        print(f\"Modelos guardados en {MODELS_OUTPUT}\")\n",
    "        \n",
    "        # Evaluaci√≥n y generaci√≥n de visualizaciones geoespaciales\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"VISUALIZACI√ìN DE PREDICCIONES CON COORDENADAS GEOESPACIALES\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Generar visualizaciones geoespaciales precisas\n",
    "        visualize_predictions_with_geospatial_coords()\n",
    "        \n",
    "        # Tambi√©n visualizar las curvas de aprendizaje detalladas\n",
    "        def plot_detailed_learning_curves():\n",
    "            \"\"\"\n",
    "            Crea visualizaciones detalladas de las curvas de aprendizaje para los modelos\n",
    "            ConvBiGRU-AE y ConvBiLSTM-AE, incluyendo an√°lisis de convergencia y comparativa.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                plt.figure(figsize=(20, 12))\n",
    "                \n",
    "                # 1. Gr√°fico de p√©rdida absoluta para ambos modelos\n",
    "                plt.subplot(2, 2, 1)\n",
    "                plt.plot(train_losses_bigru, label='ConvBiGRU - Train', color='blue', linestyle='-')\n",
    "                plt.plot(val_losses_bigru, label='ConvBiGRU - Val', color='blue', linestyle='--')\n",
    "                plt.plot(train_losses_bilstm, label='ConvBiLSTM - Train', color='red', linestyle='-')\n",
    "                plt.plot(val_losses_bilstm, label='ConvBiLSTM - Val', color='red', linestyle='--')\n",
    "                plt.title('Curvas de Aprendizaje - P√©rdida Absoluta', fontsize=14)\n",
    "                plt.xlabel('√âpoca', fontsize=12)\n",
    "                plt.ylabel('P√©rdida (MSE)', fontsize=12)\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.grid(alpha=0.3)\n",
    "                \n",
    "                # 2. Gr√°fico de p√©rdida relativa (normalizada al valor inicial)\n",
    "                plt.subplot(2, 2, 2)\n",
    "                rel_train_bigru = [l/train_losses_bigru[0] for l in train_losses_bigru]\n",
    "                rel_val_bigru = [l/val_losses_bigru[0] for l in val_losses_bigru]\n",
    "                rel_train_bilstm = [l/train_losses_bilstm[0] for l in train_losses_bilstm]\n",
    "                rel_val_bilstm = [l/val_losses_bilstm[0] for l in val_losses_bilstm]\n",
    "                \n",
    "                plt.plot(rel_train_bigru, label='ConvBiGRU - Train', color='blue', alpha=0.7)\n",
    "                plt.plot(rel_val_bigru, label='ConvBiGRU - Val', color='blue', linestyle='--', alpha=0.7)\n",
    "                plt.plot(rel_train_bilstm, label='ConvBiLSTM - Train', color='red', alpha=0.7)\n",
    "                plt.plot(rel_val_bilstm, label='ConvBiLSTM - Val', color='red', linestyle='--', alpha=0.7)\n",
    "                plt.title(f'Curvas de Aprendizaje - P√©rdida Relativa (% del valor inicial)', fontsize=14)\n",
    "                plt.xlabel('√âpoca', fontsize=12)\n",
    "                plt.ylabel('P√©rdida Relativa', fontsize=12)\n",
    "                plt.legend()\n",
    "                plt.grid(alpha=0.3)\n",
    "                \n",
    "                # 3. Comparaci√≥n de diferencia entre train y validation\n",
    "                plt.subplot(2, 2, 3)\n",
    "                diff_bigru = [t-v for t, v in zip(train_losses_bigru, val_losses_bigru)]\n",
    "                diff_bilstm = [t-v for t, v in zip(train_losses_bilstm, val_losses_bilstm)]\n",
    "                \n",
    "                plt.plot(diff_bigru, label='ConvBiGRU (Train-Val)', color='blue')\n",
    "                plt.plot(diff_bilstm, label='ConvBiLSTM (Train-Val)', color='red')\n",
    "                plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "                plt.title('Diferencia entre P√©rdidas de Train y Validaci√≥n', fontsize=14)\n",
    "                plt.xlabel('√âpoca', fontsize=12)\n",
    "                plt.ylabel('Train Loss - Val Loss', fontsize=12)\n",
    "                plt.legend()\n",
    "                plt.grid(alpha=0.3)\n",
    "                \n",
    "                # 4. Tasa de mejora (derivada de la p√©rdida)\n",
    "                plt.subplot(2, 2, 4)\n",
    "                # Calcular mejora por √©poca (primera derivada de la p√©rdida)\n",
    "                improve_rate_bigru = [train_losses_bigru[i-1] - train_losses_bigru[i] for i in range(1, len(train_losses_bigru))]\n",
    "                improve_rate_bilstm = [train_losses_bilstm[i-1] - train_losses_bilstm[i] for i in range(1, len(train_losses_bilstm))]\n",
    "                \n",
    "                plt.plot(improve_rate_bigru, label='ConvBiGRU', color='blue')\n",
    "                plt.plot(improve_rate_bilstm, label='ConvBiLSTM', color='red')\n",
    "                plt.title('Tasa de Mejora por √âpoca (Œî P√©rdida)', fontsize=14)\n",
    "                plt.xlabel('√âpoca', fontsize=12)\n",
    "                plt.ylabel('Mejora (Reducci√≥n de P√©rdida)', fontsize=12)\n",
    "                plt.grid(alpha=0.3)\n",
    "                plt.legend()\n",
    "                \n",
    "                # Ajustar dise√±o y guardar figura\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(MODELS_OUTPUT / 'detailed_learning_curves.png', dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                \n",
    "                # Resumen final de m√©tricas\n",
    "                best_val_bigru = min(val_losses_bigru)\n",
    "                best_val_bilstm = min(val_losses_bilstm)\n",
    "                best_epoch_bigru = val_losses_bigru.index(best_val_bigru)\n",
    "                best_epoch_bilstm = val_losses_bilstm.index(best_val_bilstm)\n",
    "                \n",
    "                print(\"\\n==== RESUMEN DE M√âTRICAS DE ENTRENAMIENTO ====\")\n",
    "                print(f\"ConvBiGRU-AE:\")\n",
    "                print(f\"  - Mejor p√©rdida de validaci√≥n: {best_val_bigru:.2f} (√âpoca {best_epoch_bigru})\")\n",
    "                print(f\"  - Reducci√≥n total de p√©rdida: {train_losses_bigru[0] - train_losses_bigru[-1]:.2f} ({(1 - train_losses_bigru[-1]/train_losses_bigru[0])*100:.1f}%)\")\n",
    "                \n",
    "                print(f\"\\nConvBiLSTM-AE:\")\n",
    "                print(f\"  - Mejor p√©rdida de validaci√≥n: {best_val_bilstm:.2f} (√âpoca {best_epoch_bilstm})\")\n",
    "                print(f\"  - Reducci√≥n total de p√©rdida: {train_losses_bilstm[0] - train_losses_bilstm[-1]:.2f} ({(1 - train_losses_bilstm[-1]/train_losses_bilstm[0])*100:.1f}%)\")\n",
    "                \n",
    "                if best_val_bigru < best_val_bilstm:\n",
    "                    print(f\"\\n‚úÖ ConvBiGRU-AE tiene mejor rendimiento con {(best_val_bilstm - best_val_bigru)/best_val_bilstm*100:.1f}% menor p√©rdida de validaci√≥n\")\n",
    "                else:\n",
    "                    print(f\"\\n‚úÖ ConvBiLSTM-AE tiene mejor rendimiento con {(best_val_bigru - best_val_bilstm)/best_val_bigru*100:.1f}% menor p√©rdida de validaci√≥n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error al visualizar curvas de aprendizaje: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                \n",
    "        plot_detailed_learning_curves()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error durante el entrenamiento: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\n‚ùå Entrenamiento cancelado debido a problemas con los modelos.\")\n",
    "\n",
    "# Implementaci√≥n de Modelos H√≠bridos Avanzados\n",
    "\n",
    "## TopoClus-CEEMDAN-TVF-AFC-ConvBiGRU‚ÄêAE y TopoClus-CEEMDAN-TVF-AFC-ConvLSTM‚ÄêAE\n",
    "\"\"\"\n",
    "Estos modelos representan una arquitectura avanzada que combina:\n",
    "\n",
    "1. **T√©cnicas de descomposici√≥n de se√±ales**:\n",
    "   - CEEMDAN (Complete Ensemble Empirical Mode Decomposition with Adaptive Noise)\n",
    "   - TVF-EMD (Time-Varying Filter Empirical Mode Decomposition)\n",
    "   \n",
    "2. **Caracter√≠sticas de autocorrelaci√≥n (AFC)** en diferentes lags temporales\n",
    "\n",
    "3. **Informaci√≥n topogr√°fica y orogr√°fica**:\n",
    "   - Clusters basados en elevaci√≥n (TopoClus)\n",
    "   - Embeddings espec√≠ficos por cluster\n",
    "\n",
    "4. **Arquitecturas neuronales avanzadas**:\n",
    "   - Encoder-Decoder Convolucional con BiGRU o BiLSTM\n",
    "   - Atenci√≥n topogr√°fica para modular caracter√≠sticas por tipo de terreno\n",
    "\n",
    "# Preparaci√≥n de datos y extracci√≥n de caracter√≠sticas para modelos h√≠bridos\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import math\n",
    "\n",
    "class FiLMLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature-wise Linear Modulation para adaptar las caracter√≠sticas seg√∫n el cluster orogr√°fico\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters, n_features):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_clusters, n_features * 2)\n",
    "        \n",
    "        # Inicializar con valores razonables (gamma cercano a 1, beta cercano a 0)\n",
    "        nn.init.normal_(self.embedding.weight[:, :n_features], 1.0, 0.1)\n",
    "        nn.init.zeros_(self.embedding.weight[:, n_features:])\n",
    "    \n",
    "    def forward(self, x, cluster_idx):\n",
    "        # x: [batch, channels, height, width]\n",
    "        # cluster_idx: [batch]\n",
    "        batch_size, channels = x.shape[0], x.shape[1]\n",
    "        \n",
    "        # Obtener par√°metros gamma y beta del embedding\n",
    "        params = self.embedding(cluster_idx)  # [batch, channels*2]\n",
    "        gamma, beta = params.chunk(2, dim=1)  # [batch, channels], [batch, channels]\n",
    "        \n",
    "        # Reshape para permitir broadcasting\n",
    "        gamma = gamma.view(batch_size, channels, 1, 1)\n",
    "        beta = beta.view(batch_size, channels, 1, 1)\n",
    "        \n",
    "        # Aplicar modulaci√≥n: Œ≥ ‚äó x + Œ≤\n",
    "        return gamma * x + beta\n",
    "\n",
    "class MultiResBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    Rama de procesamiento multi-resoluci√≥n con dilataciones variables\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dilations=(1, 2, 4)):\n",
    "        super().__init__()\n",
    "        self.branches = nn.ModuleList()\n",
    "        \n",
    "        # Crear una rama para cada dilataci√≥n\n",
    "        for dilation in dilations:\n",
    "            branch = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels, \n",
    "                    out_channels, \n",
    "                    kernel_size=3, \n",
    "                    padding=dilation, \n",
    "                    dilation=dilation\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            self.branches.append(branch)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for branch in self.branches:\n",
    "            outputs.append(branch(x))\n",
    "        \n",
    "        # Concatenar resultados de todas las ramas\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "class TopoClus_CEEMDAN_TVF_AFC_Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder compartido para ambos modelos que integra todas las fuentes de datos\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, hidden_dim, n_clusters, use_checkpoint=True):\n",
    "        super().__init__()\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        \n",
    "        # Reducci√≥n inicial de canales\n",
    "        self.channel_reduction = nn.Conv2d(input_channels, hidden_dim, kernel_size=1)\n",
    "        \n",
    "        # Procesamiento multi-resoluci√≥n\n",
    "        self.multi_res = MultiResBranch(hidden_dim, hidden_dim//2)\n",
    "        merged_channels = hidden_dim//2 * 3  # 3 ramas con dilaciones diferentes\n",
    "        \n",
    "        # Adaptaci√≥n por cluster (FiLM)\n",
    "        self.film = FiLMLayer(n_clusters, merged_channels)\n",
    "        \n",
    "        # Codificador principal (estructura tipo U-Net)\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(merged_channels, hidden_dim, 3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, hidden_dim*2, 3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim*2, hidden_dim*2, 3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim*2, hidden_dim*4, 3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim*4, hidden_dim*4, 3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, cluster_idx):\n",
    "        # x: [batch, channels, height, width]\n",
    "        \n",
    "        # Reducci√≥n de canales\n",
    "        x = self.channel_reduction(x)\n",
    "        \n",
    "        # Procesamiento multiresoluci√≥n\n",
    "        if self.use_checkpoint and self.training:\n",
    "            x = checkpoint(self.multi_res, x)\n",
    "        else:\n",
    "            x = self.multi_res(x)\n",
    "        \n",
    "        # Adaptaci√≥n por cluster\n",
    "        x = self.film(x, cluster_idx)\n",
    "        \n",
    "        # Codificador U-Net\n",
    "        # Guardar para conexiones skip\n",
    "        if self.use_checkpoint and self.training:\n",
    "            x1 = checkpoint(self.down1, x)\n",
    "        else:\n",
    "            x1 = self.down1(x)\n",
    "        \n",
    "        x = self.pool1(x1)\n",
    "        \n",
    "        if self.use_checkpoint and self.training:\n",
    "            x2 = checkpoint(self.down2, x)\n",
    "        else:\n",
    "            x2 = self.down2(x)\n",
    "        \n",
    "        x = self.pool2(x2)\n",
    "        \n",
    "        if self.use_checkpoint and self.training:\n",
    "            x = checkpoint(self.bottleneck, x)\n",
    "        else:\n",
    "            x = self.bottleneck(x)\n",
    "        \n",
    "        return x, x1, x2  # Retornar tambi√©n activaciones intermedias para skip connections\n",
    "\n",
    "class TopoClus_CEEMDAN_TVF_AFC_ConvBiGRU_AE(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo completo que integra codificador compartido con BiGRU para procesamiento temporal\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, hidden_dim, n_clusters, seq_length=12, output_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_horizon = seq_length  # Guardar el horizonte de salida\n",
    "        \n",
    "        self.encoder = TopoClus_CEEMDAN_TVF_AFC_Encoder(\n",
    "            input_channels, hidden_dim, n_clusters\n",
    "        )\n",
    "        \n",
    "        # BiGRU para procesamiento secuencial\n",
    "        self.bigru = nn.GRU(\n",
    "            hidden_dim*4*4*4,  # Tama√±o del bottleneck (asumiendo 2 maxpoolings)\n",
    "            hidden_dim*4,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(hidden_dim*8, hidden_dim*2, 2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim*4, hidden_dim*2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim*2, hidden_dim*2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "        self.up2 = nn.ConvTranspose2d(hidden_dim*2, hidden_dim, 2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim*2, hidden_dim, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False), \n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 7. Capa de salida multi-horizonte\n",
    "        out_channels = self.output_horizon * output_channels\n",
    "        self.output_conv = nn.Conv2d(hidden_dim, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x, cluster_idx, target_shape=None):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Codificar\n",
    "        bottleneck, skip1, skip2 = self.encoder(x, cluster_idx)\n",
    "        \n",
    "        # Aplanar el bottleneck para el GRU\n",
    "        flattened = bottleneck.view(batch_size, -1)\n",
    "        \n",
    "        # Reshape para BiGRU (a√±adir dim de secuencia)\n",
    "        gru_in = flattened.unsqueeze(1)\n",
    "        \n",
    "        # Procesar con BiGRU\n",
    "        gru_out, _ = self.bigru(gru_in)\n",
    "        \n",
    "        # Tomar salida y reshape para decodificador\n",
    "        gru_features = gru_out.view(batch_size, -1, 1, 1)\n",
    "        \n",
    "        # Redimensionar para que coincida con el bottleneck\n",
    "        h, w = bottleneck.shape[2], bottleneck.shape[3]\n",
    "        gru_features = F.interpolate(gru_features, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Decodificar\n",
    "        x = self.up1(gru_features)\n",
    "        \n",
    "        # Skip connection 1\n",
    "        x = torch.cat([x, skip2], dim=1)\n",
    "        x = self.dec1(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        \n",
    "        # Skip connection 2\n",
    "        x = torch.cat([x, skip1], dim=1)\n",
    "        x = self.dec2(x)\n",
    "        \n",
    "        # Salida multi-horizonte\n",
    "        x = self.output_conv(x)\n",
    "        \n",
    "        # Si no se proporciona target_shape, usar la forma de x\n",
    "        if target_shape is None:\n",
    "            height, width = x.shape[2], x.shape[3]  # Usar forma de la salida actual\n",
    "            # Ajustar para output_channels y output_horizon\n",
    "            target_height = height // self.output_horizon\n",
    "            target_width = width // output_channels\n",
    "            target_shape = (target_height, target_width)\n",
    "        \n",
    "        # Reorganizar para obtener [batch, seq, channels, height, width]\n",
    "        output = x.reshape(batch_size, self.output_horizon, output_channels, target_shape[0], target_shape[1])\n",
    "        \n",
    "        return output\n",
    "        \n",
    "class TopoClus_CEEMDAN_TVF_AFC_ConvBiLSTM_AE(nn.Module):\n",
    "    \"\"\"\n",
    "    Versi√≥n con BiLSTM en lugar de BiGRU\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, hidden_dim, n_clusters, seq_length=12, output_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_horizon = seq_length  # Guardar el horizonte de predicci√≥n\n",
    "        \n",
    "        self.encoder = TopoClus_CEEMDAN_TVF_AFC_Encoder(\n",
    "            input_channels, hidden_dim, n_clusters\n",
    "        )\n",
    "        \n",
    "        # BiLSTM para procesamiento secuencial\n",
    "        self.bilstm = nn.LSTM(\n",
    "            hidden_dim*4*4*4,  # Tama√±o del bottleneck (asumiendo 2 maxpoolings)\n",
    "            hidden_dim*4,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Decodificador (mismo que en versi√≥n BiGRU)\n",
    "        self.up1 = nn.ConvTranspose2d(hidden_dim*8, hidden_dim*2, 2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim*4, hidden_dim*2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim*2, hidden_dim*2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(hidden_dim*2, hidden_dim, 2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim*2, hidden_dim, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False), \n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 7. Capa de salida multi-horizonte\n",
    "        out_channels = self.output_horizon * output_channels\n",
    "        self.output_conv = nn.Conv2d(hidden_dim, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x, cluster_idx, target_shape=None):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Codificar\n",
    "        bottleneck, skip1, skip2 = self.encoder(x, cluster_idx)\n",
    "        \n",
    "        # Aplanar el bottleneck para el LSTM\n",
    "        flattened = bottleneck.view(batch_size, -1)\n",
    "        \n",
    "        # Reshape para BiLSTM (a√±adir dim de secuencia)\n",
    "        lstm_in = flattened.unsqueeze(1)\n",
    "        \n",
    "        # Procesar con BiLSTM\n",
    "        lstm_out, _ = self.bilstm(lstm_in)\n",
    "        \n",
    "        # Tomar salida y reshape para decodificador\n",
    "        lstm_features = lstm_out.view(batch_size, -1, 1, 1)\n",
    "        \n",
    "        # Redimensionar para que coincida con el bottleneck\n",
    "        h, w = bottleneck.shape[2], bottleneck.shape[3]\n",
    "        lstm_features = F.interpolate(lstm_features, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Decodificar (mismo proceso que BiGRU)\n",
    "        x = self.up1(lstm_features)\n",
    "        x = torch.cat([x, skip2], dim=1)\n",
    "        x = self.dec1(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, skip1], dim=1)\n",
    "        x = self.dec2(x)\n",
    "        \n",
    "        # Salida multi-horizonte\n",
    "        x = self.output_conv(x)\n",
    "        \n",
    "        # Si no se proporciona target_shape, usar la forma de x\n",
    "        if target_shape is None:\n",
    "            height, width = x.shape[2], x.shape[3]  # Usar forma de la salida actual\n",
    "            # Ajustar para output_channels y output_horizon\n",
    "            target_height = height // self.output_horizon\n",
    "            target_width = width // output_channels\n",
    "            target_shape = (target_height, target_width)\n",
    "            \n",
    "        # Reorganizar para obtener [batch, seq, channels, height, width]\n",
    "        output = x.reshape(batch_size, self.output_horizon, output_channels, target_shape[0], target_shape[1])\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Dataset para manejar las m√∫ltiples fuentes de caracter√≠sticas\n",
    "class MultiSourceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para manejar m√∫ltiples fuentes de caracter√≠sticas\n",
    "    \"\"\"\n",
    "    def __init__(self, X_list, y, seq_length=12):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_list: Lista de arrays de caracter√≠sticas\n",
    "            y: Array de targets\n",
    "            seq_length: Longitud de la secuencia\n",
    "        \"\"\"\n",
    "        self.X_list = [torch.FloatTensor(x) for x in X_list]\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Definir nombres de caracter√≠sticas basados en la estructura de X_list\n",
    "        feature_names = ['precipitation', 'temperature', 'elevation', 'clusters']\n",
    "        \n",
    "        # Determinar qu√© fuente contiene los clusters para FiLM\n",
    "        for i, x in enumerate(self.X_list):\n",
    "            if 'clusters' in x:\n",
    "                self.cluster_idx = i\n",
    "                break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_list[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Combinar todas las fuentes en un solo tensor\n",
    "        # Cada fuente: [batch_size, input_window, height, width]\n",
    "        batch_inputs = []\n",
    "        for x in self.X_list:\n",
    "            # Tomar ventana completa para esta caracter√≠stica\n",
    "            feature = x[idx]\n",
    "            batch_inputs.append(feature)\n",
    "        \n",
    "        # Concatenar en dimensi√≥n de canal: [input_window, num_sources, height, width]\n",
    "        combined_input = torch.cat([x.unsqueeze(1) for x in batch_inputs], dim=1)\n",
    "        \n",
    "        # Target: [output_horizon, height, width]\n",
    "        target = self.y[idx]\n",
    "        \n",
    "        # Extraer el √≠ndice de cluster si est√° disponible\n",
    "        if self.cluster_idx >= 0:\n",
    "            # Tomar el primer √≠ndice de tiempo y la moda de los clusters en el mapa\n",
    "            cluster_map = batch_inputs[self.cluster_idx][0]\n",
    "            cluster_idx = int(torch.mode(cluster_map.flatten())[0])\n",
    "        else:\n",
    "            # Si no hay datos de cluster, usar 0 como fallback\n",
    "            cluster_idx = 0\n",
    "        \n",
    "        return combined_input, target, cluster_idx\n",
    "\n",
    "print(\"Definiendo modelos y dataset...\")\n",
    "\n",
    "# Inicializar el dataset\n",
    "train_dataset = MultiSourceDataset(X_train, y_train, seq_length=OUTPUT_HORIZON)\n",
    "val_dataset = MultiSourceDataset(X_val, y_val, seq_length=OUTPUT_HORIZON)\n",
    "\n",
    "# Par√°metros de modelo\n",
    "if isinstance(X_train, list) and len(X_train) > 0:\n",
    "    combined_channels = sum(x.shape[1] for x in X_train[0])\n",
    "    print(f\"Canales de entrada combinados: {combined_channels}\")\n",
    "else:\n",
    "    combined_channels = X_train.shape[1]\n",
    "    print(f\"Canales de entrada: {combined_channels}\")\n",
    "\n",
    "# Definir cluster_ids basados en los datos disponibles\n",
    "# Si tenemos datos de clusters, obtener valores √∫nicos, sino usar un valor predeterminado\n",
    "cluster_ids = list(range(10))  # Default: suponemos 10 clusters\n",
    "# Definir target_shape basado en los datos\n",
    "if isinstance(X_train, list) and len(X_train) > 0:\n",
    "    # Si X_train es una lista, tomar las dimensiones del primer elemento\n",
    "    if len(X_train[0].shape) >= 3:\n",
    "        target_shape = X_train[0].shape[-2:]  # √öltimas dos dimensiones (altura, anchura)\n",
    "    else:\n",
    "        # Dimensiones por defecto si no podemos determinarlas\n",
    "        target_shape = (61, 65)\n",
    "else:\n",
    "    # Si X_train no es una lista, tomar sus dimensiones directamente\n",
    "    if len(X_train.shape) >= 3:\n",
    "        target_shape = X_train.shape[-2:]\n",
    "    else:\n",
    "        target_shape = (61, 65)\n",
    "\n",
    "print(f\"Target shape para la salida del modelo: {target_shape}\")\n",
    "\n",
    "# Instanciar modelos\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "convbigru_model = TopoClus_CEEMDAN_TVF_AFC_ConvBiGRU_AE(\n",
    "    combined_channels, hidden_dim, n_clusters, OUTPUT_HORIZON\n",
    ").to(device)\n",
    "\n",
    "convbilstm_model = TopoClus_CEEMDAN_TVF_AFC_ConvBiLSTM_AE(\n",
    "    combined_channels, hidden_dim, n_clusters, OUTPUT_HORIZON\n",
    ").to(device)\n",
    "\n",
    "def calculate_spatial_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas espaciales entre valores verdaderos y predichos.\n",
    "    \"\"\"\n",
    "    # Implementaci√≥n pendiente\n",
    "    pass\n",
    "\n",
    "# Funci√≥n para mostrar progreso sin borrar salidas previas\n",
    "def plot_progress_without_clearing(train_losses, val_losses, best_val_loss=None):\n",
    "    \"\"\"\n",
    "    Plotea el progreso del entrenamiento sin borrar la salida anterior.\n",
    "    Similar a plot_progress pero sin el clear_output().\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    if best_val_loss is not None:\n",
    "        plt.axhline(y=best_val_loss, color='r', linestyle='--', label=f'Best: {best_val_loss:.2f}')\n",
    "    plt.title(f'Loss vs. Epochs (Current: {val_losses[-1]:.2f})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    rel_loss = [l/train_losses[0] for l in train_losses]\n",
    "    rel_val_loss = [l/val_losses[0] for l in val_losses]\n",
    "    plt.plot(rel_loss, label='Train')\n",
    "    plt.plot(rel_val_loss, label='Val')\n",
    "    plt.title(f'Relative Loss (% of initial loss)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Relative Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_hybrid_model(name, model, train_loader, val_loader, epochs=100, patience=20):\n",
    "    \"\"\"\n",
    "    Entrena un modelo h√≠brido con optimizaciones avanzadas y realiza evaluaci√≥n\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*30}\")\n",
    "    print(f\"ENTRENAMIENTO DE {name}\")\n",
    "    print(f\"{'='*30}\")\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Importar tqdm si no est√° disponible\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "    except ImportError:\n",
    "        # Definir una versi√≥n simple si no est√° instalado\n",
    "        def tqdm(iterable, **kwargs):\n",
    "            print(kwargs.get('desc', ''))\n",
    "            return iterable\n",
    "    \n",
    "    # Optimizador y funci√≥n de p√©rdida\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=patience//2, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Mixed precision\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Tracking de m√©tricas\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    current_lr = optimizer.param_groups[0]['lr']  # Guardar LR inicial\n",
    "    \n",
    "    # Para guardar mejor modelo\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    start_time = time.time()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_name = name  # Asegurar que tenemos model_name definido\n",
    "    save_dir = MODELS_OUTPUT  # Usar el directorio de modelos definido globalmente\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_path = save_dir / f'{name}_{timestamp}.pt'\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # ===== ENTRENAMIENTO =====\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        batch_metrics = []\n",
    "        \n",
    "        # Barra de progreso para entrenamiento\n",
    "        train_progress = tqdm(train_loader, desc=f\"√âpoca {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "        \n",
    "        for inputs, targets in train_progress:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Asegurar compatibilidad de dimensiones\n",
    "            if len(outputs.shape) == 5 and len(targets.shape) <= 4:\n",
    "                outputs = outputs.squeeze(2)  # eliminar dim C si es 1\n",
    "                \n",
    "            if len(outputs.shape) != len(targets.shape):\n",
    "                if len(outputs.shape) == 5 and len(targets.shape) == 3:\n",
    "                    targets = targets.unsqueeze(1).unsqueeze(2).repeat(1, outputs.shape[1], 1, 1, 1)\n",
    "                elif len(outputs.shape) == 5 and len(targets.shape) == 4:\n",
    "                    targets = targets.unsqueeze(2)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping para estabilidad\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Actualizar barra de progreso con p√©rdida actual\n",
    "            train_progress.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            batch_metrics.append(loss.item())\n",
    "        \n",
    "        train_loss = np.mean(batch_metrics)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # ===== VALIDACI√ìN =====\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_outputs = []\n",
    "        val_targets = []\n",
    "        \n",
    "        # Barra de progreso para validaci√≥n\n",
    "        val_progress = tqdm(val_loader, desc=f\"√âpoca {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_progress:\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Ajustar dimensiones si es necesario\n",
    "                if len(outputs.shape) == 5 and len(targets.shape) <= 4:\n",
    "                    outputs = outputs.squeeze(2)\n",
    "                \n",
    "                if len(outputs.shape) != len(targets.shape):\n",
    "                    if len(outputs.shape) == 5 and len(targets.shape) == 3:\n",
    "                        targets = targets.unsqueeze(1).unsqueeze(2).repeat(1, outputs.shape[1], 1, 1, 1)\n",
    "                    elif len(outputs.shape) == 5 and len(targets.shape) == 4:\n",
    "                        targets = targets.unsqueeze(2)\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                val_progress.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Guardar para m√©tricas\n",
    "                val_outputs.append(outputs.cpu())\n",
    "                val_targets.append(targets.cpu())\n",
    "        \n",
    "        # Calcular p√©rdida promedio\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Calcular tiempo de la √©poca\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        # Actualizar scheduler\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Comprobar si el LR ha cambiado\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        lr_updated = new_lr != current_lr\n",
    "        old_lr = current_lr\n",
    "        current_lr = new_lr  # Actualizar para pr√≥xima comparaci√≥n\n",
    "        \n",
    "        # Mostrar gr√°fico de progreso cada 5 √©pocas o en la √∫ltima\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1 or epochs_no_improve == patience:\n",
    "            try:\n",
    "                plot_progress_without_clearing(train_losses, val_losses, best_val_loss)\n",
    "            except Exception as e:\n",
    "                print(f\"Error al mostrar gr√°fico: {str(e)}\")\n",
    "        \n",
    "        # Imprimir resumen de la √©poca\n",
    "        print(f\"\\nüìä √âpoca {epoch+1}/{epochs} completada en {epoch_time:.1f}s\")\n",
    "        print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | \" + \n",
    "              (f\"LR reducido: {old_lr:.6f} ‚Üí {new_lr:.6f}\" if lr_updated else f\"LR: {new_lr:.6f}\"))\n",
    "        \n",
    "        # Early stopping y guardado del mejor modelo\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            improvement = (best_val_loss - avg_val_loss) / best_val_loss * 100 if best_val_loss != float('inf') else 100\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            \n",
    "            # Guardar mejor modelo\n",
    "            save_model(model, model_path, epoch, avg_val_loss)\n",
    "            print(f\"   ‚≠ê ¬°Nuevo mejor modelo! Mejora: {improvement:.2f}%\")\n",
    "            \n",
    "            # Tambi√©n guardar checkpoint espec√≠fico de esta √©poca\n",
    "            epoch_path = save_dir / f'checkpoint_{model_name}_epoch_{epoch+1}.pth'\n",
    "            save_model(model, epoch_path, epoch, avg_val_loss)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"   ‚ùå Sin mejora durante {epochs_no_improve}/{patience} √©pocas. Mejor: {best_val_loss:.4f}\")\n",
    "            \n",
    "            # Guardar checkpoint regular cada 10 √©pocas\n",
    "            if epoch % 10 == 0:\n",
    "                checkpoint_path = save_dir / f'regular_checkpoint_{model_name}_epoch_{epoch+1}.pth'\n",
    "                save_model(model, checkpoint_path, epoch, avg_val_loss)\n",
    "                \n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"\\n‚ö†Ô∏è Early stopping activado despu√©s de {patience} √©pocas sin mejora\")\n",
    "                break\n",
    "    \n",
    "    # Tiempo total de entrenamiento\n",
    "    total_time = time.time() - start_time\n",
    "    hours, rem = divmod(total_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ENTRENAMIENTO FINALIZADO: {model_name}\")\n",
    "    print(f\"Tiempo total: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "    print(f\"Mejor p√©rdida de validaci√≥n: {best_val_loss:.4f}\")\n",
    "    print(f\"Modelos guardados en: {save_dir}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Cargar el mejor modelo\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"‚úÖ Mejor modelo cargado de la √©poca {checkpoint['epoch']+1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al cargar el mejor modelo: {str(e)}\")\n",
    "    \n",
    "    # Tiempo total de entrenamiento\n",
    "    total_time = time.time() - start_time\n",
    "    hours, rem = divmod(total_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ENTRENAMIENTO FINALIZADO: {model_name}\")\n",
    "    print(f\"Tiempo total: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "    print(f\"Mejor p√©rdida de validaci√≥n: {best_val_loss:.4f}\")\n",
    "    print(f\"Modelos guardados en: {save_dir}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Funci√≥n para verificar si un modelo existe y mostrar informaci√≥n sobre √©l\n",
    "def check_model_exists(model_path):\n",
    "    \"\"\"Verifica si un modelo existe y muestra informaci√≥n sobre √©l.\"\"\"\n",
    "    if os.path.exists(model_path):\n",
    "        try:\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "            print(f\"‚úÖ Modelo encontrado en: {model_path}\")\n",
    "            if isinstance(checkpoint, dict) and 'epoch' in checkpoint:\n",
    "                print(f\"   Guardado en √©poca: {checkpoint['epoch']+1}\")\n",
    "                print(f\"   P√©rdida validaci√≥n: {checkpoint['val_loss']:.4f}\")\n",
    "            else:\n",
    "                print(\"   (Formato antiguo - solo state_dict)\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error al cargar modelo {model_path}: {str(e)}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"‚ùå Modelo no encontrado: {model_path}\")\n",
    "        return False\n",
    "\n",
    "# Funci√≥n para visualizar m√©tricas de entrenamiento almacenadas\n",
    "def visualize_training_metrics(train_losses, val_losses, model_name=\"modelo\"):\n",
    "    \"\"\"\n",
    "    Visualiza las m√©tricas de entrenamiento con gr√°ficos detallados y estad√≠sticas.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # 1. Curva de p√©rdida b√°sica\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(train_losses, label='Entrenamiento', color='blue', linestyle='-', marker='.', alpha=0.7)\n",
    "    plt.plot(val_losses, label='Validaci√≥n', color='red', linestyle='-', marker='.', alpha=0.7)\n",
    "    \n",
    "    best_val_idx = np.argmin(val_losses)\n",
    "    best_val_loss = val_losses[best_val_idx]\n",
    "    plt.axvline(x=best_val_idx, color='green', linestyle='--', alpha=0.7, \n",
    "                label=f'Mejor √©poca: {best_val_idx+1}')\n",
    "    plt.axhline(y=best_val_loss, color='green', linestyle=':', alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Curva de Aprendizaje - {model_name}', fontsize=14)\n",
    "    plt.xlabel('√âpoca', fontsize=12)\n",
    "    plt.ylabel('P√©rdida', fontsize=12)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # 2. P√©rdida relativa (%)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    rel_train = [t/train_losses[0]*100 for t in train_losses]\n",
    "    rel_val = [v/val_losses[0]*100 for v in val_losses]\n",
    "    \n",
    "    plt.plot(rel_train, label='Entrenamiento', color='blue', alpha=0.7)\n",
    "    plt.plot(rel_val, label='Validaci√≥n', color='red', alpha=0.7)\n",
    "    plt.axvline(x=best_val_idx, color='green', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.title(f'P√©rdida Relativa (% del valor inicial)', fontsize=14)\n",
    "    plt.xlabel('√âpoca', fontsize=12)\n",
    "    plt.ylabel('Porcentaje (%)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Diferencia Train-Val (sobreajuste)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    diff = [t-v for t, v in zip(train_losses, val_losses)]\n",
    "    \n",
    "    plt.plot(diff, color='purple', alpha=0.7)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.axvline(x=best_val_idx, color='green', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Sombreado para zona de sobreajuste potencial\n",
    "    plt.fill_between(range(len(diff)), [0]*len(diff), diff, \n",
    "                     where=[d < 0 for d in diff], color='red', alpha=0.2,\n",
    "                     label='Posible subajuste')\n",
    "    plt.fill_between(range(len(diff)), [0]*len(diff), diff, \n",
    "                     where=[d > 0 for d in diff], color='orange', alpha=0.2,\n",
    "                     label='Posible sobreajuste')\n",
    "    \n",
    "    plt.title('Diferencia Train-Validaci√≥n (Indicador de Sobreajuste)', fontsize=14)\n",
    "    plt.xlabel('√âpoca', fontsize=12)\n",
    "    plt.ylabel('Train Loss - Val Loss', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Velocidad de convergencia (derivada de la p√©rdida)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    if len(val_losses) > 1:\n",
    "        val_improvement = [val_losses[i-1] - val_losses[i] for i in range(1, len(val_losses))]\n",
    "        improving = [i > 0 for i in val_improvement]\n",
    "        colors = ['green' if imp else 'red' for imp in improving]\n",
    "        \n",
    "        plt.bar(range(1, len(val_losses)), val_improvement, color=colors, alpha=0.7)\n",
    "        plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        if best_val_idx > 0:\n",
    "            plt.axvline(x=best_val_idx, color='green', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.title('Velocidad de Mejora por √âpoca', fontsize=14)\n",
    "        plt.xlabel('√âpoca', fontsize=12)\n",
    "        plt.ylabel('Mejora (reducci√≥n de p√©rdida)', fontsize=12)\n",
    "        plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Estad√≠sticas adicionales\n",
    "    print(f\"\\n{'='*40} ESTAD√çSTICAS DE ENTRENAMIENTO {'='*40}\")\n",
    "    print(f\"Modelo: {model_name}\")\n",
    "    print(f\"Total de √©pocas: {len(train_losses)}\")\n",
    "    print(f\"Mejor √©poca: {best_val_idx+1}\")\n",
    "    print(f\"Mejor p√©rdida validaci√≥n: {best_val_loss:.4f}\")\n",
    "    print(f\"P√©rdida inicial (val): {val_losses[0]:.4f}\")\n",
    "    print(f\"P√©rdida final (val): {val_losses[-1]:.4f}\")\n",
    "    print(f\"Mejora total: {(1 - best_val_loss/val_losses[0])*100:.2f}%\")\n",
    "    \n",
    "    # Calcular tendencias\n",
    "    last_epochs = min(10, len(val_losses))\n",
    "    if last_epochs > 1:\n",
    "        recent_trend = val_losses[-last_epochs:][0] - val_losses[-1]\n",
    "        print(f\"Tendencia √∫ltimas {last_epochs} √©pocas: {recent_trend:.4f} \" +\n",
    "              (\"üìâ mejorando\" if recent_trend > 0 else \"üìà empeorando\"))\n",
    "    \n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Reemplazar el c√≥digo de entrenamiento con la versi√≥n mejorada\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENTRENAMIENTO DE MODELOS CON VISUALIZACI√ìN MEJORADA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar que los modelos est√©n listos\n",
    "if models_ready:\n",
    "    try:\n",
    "        # Verificar directorios de salida y rutas de modelos\n",
    "        print(\"\\nüìÇ Configuraci√≥n de directorios y rutas:\")\n",
    "        print(f\"Directorio de salida: {MODELS_OUTPUT}\")\n",
    "        \n",
    "        # Comprobar si hay modelos guardados previamente\n",
    "        convbigru_output_path = MODELS_OUTPUT / 'convbigru_ae_model.pth'\n",
    "        convbilstm_output_path = MODELS_OUTPUT / 'convbilstm_ae_model.pth'\n",
    "        \n",
    "        print(\"\\nüîç Verificando modelos guardados previamente:\")\n",
    "        bigru_exists = check_model_exists(convbigru_output_path)\n",
    "        bilstm_exists = check_model_exists(convbilstm_output_path)\n",
    "        \n",
    "        # Iniciar entrenamiento con seguimiento detallado\n",
    "        if not bigru_exists or input(\"¬øVolver a entrenar ConvBiGRU-AE? (s/n): \").lower() == 's':\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"ENTRENAMIENTO DE CONVBIGRU_AE CON VISUALIZACI√ìN MEJORADA\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # Entrenar con versi√≥n mejorada\n",
    "            convbigru_ae, train_losses_bigru, val_losses_bigru = improved_train_model(\n",
    "                convbigru_ae, train_loader, val_loader, \n",
    "                criterion, optimizer_convbigru, scheduler_convbigru,\n",
    "                num_epochs, patience, model_name=\"ConvBiGRU-AE\"\n",
    "            )\n",
    "            \n",
    "            # Guardar modelo final con mensaje claro\n",
    "            torch.save({\n",
    "                'model_state_dict': convbigru_ae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer_convbigru.state_dict(),\n",
    "                'train_losses': train_losses_bigru,\n",
    "                'val_losses': val_losses_bigru\n",
    "            }, convbigru_output_path)\n",
    "            print(f\"\\n‚úÖ Modelo final ConvBiGRU-AE guardado en {convbigru_output_path}\")\n",
    "            \n",
    "            # Visualizar m√©tricas detalladas\n",
    "            print(\"\\nüìä Visualizaci√≥n detallada de m√©tricas de ConvBiGRU-AE:\")\n",
    "            visualize_training_metrics(train_losses_bigru, val_losses_bigru, \"ConvBiGRU-AE\")\n",
    "        \n",
    "        if not bilstm_exists or input(\"¬øVolver a entrenar ConvBiLSTM-AE? (s/n): \").lower() == 's':\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"ENTRENAMIENTO DE CONVBILSTM_AE CON VISUALIZACI√ìN MEJORADA\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            # Entrenar con versi√≥n mejorada\n",
    "            convbilstm_ae, train_losses_bilstm, val_losses_bilstm = improved_train_model(\n",
    "                convbilstm_ae, train_loader, val_loader, \n",
    "                criterion, optimizer_convbilstm, scheduler_convbilstm,\n",
    "                num_epochs, patience, model_name=\"ConvBiLSTM-AE\"\n",
    "            )\n",
    "            \n",
    "            # Guardar modelo final con mensaje claro\n",
    "            torch.save({\n",
    "                'model_state_dict': convbilstm_ae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer_convbilstm.state_dict(),\n",
    "                'train_losses': train_losses_bilstm,\n",
    "                'val_losses': val_losses_bilstm\n",
    "            }, convbilstm_output_path)\n",
    "            print(f\"\\n‚úÖ Modelo final ConvBiLSTM-AE guardado en {convbilstm_output_path}\")\n",
    "            \n",
    "            # Visualizar m√©tricas detalladas\n",
    "            print(\"\\nüìä Visualizaci√≥n detallada de m√©tricas de ConvBiLSTM-AE:\")\n",
    "            visualize_training_metrics(train_losses_bilstm, val_losses_bilstm, \"ConvBiLSTM-AE\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Proceso de entrenamiento mejorado completado\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error durante el entrenamiento mejorado: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\n‚ùå Los modelos no est√°n listos para entrenar. Verifica la configuraci√≥n.\")\n",
    "\n",
    "# Funci√≥n para analizar y visualizar modelos guardados\n",
    "def analyze_saved_models(model_dir=MODELS_OUTPUT, pattern=\"*_ae_model.pth\"):\n",
    "    \"\"\"\n",
    "    Analiza y visualiza informaci√≥n sobre los modelos guardados\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Encontrar todos los archivos que coinciden con el patr√≥n\n",
    "    model_files = list(Path(model_dir).glob(pattern))\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"‚ùå No se encontraron modelos con el patr√≥n '{pattern}' en {model_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìä Modelos encontrados: {len(model_files)}\")\n",
    "    for i, model_path in enumerate(model_files, 1):\n",
    "        print(f\"\\n{i}. {model_path.name}:\")\n",
    "        \n",
    "        # Obtener informaci√≥n del archivo\n",
    "        size_mb = os.path.getsize(model_path) / (1024*1024)\n",
    "        modified_time = datetime.fromtimestamp(os.path.getmtime(model_path))\n",
    "        \n",
    "        print(f\"   üìÅ Tama√±o: {size_mb:.2f} MB\")\n",
    "        print(f\"   üïí √öltima modificaci√≥n: {modified_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Intentar cargar el modelo para obtener m√°s informaci√≥n\n",
    "        try:\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "            if isinstance(checkpoint, dict):\n",
    "                print(\"   üìã Contenido del checkpoint:\")\n",
    "                for key, value in checkpoint.items():\n",
    "                    if key == 'model_state_dict':\n",
    "                        n_params = sum(p.numel() for p in value.values())\n",
    "                        print(f\"      - model_state_dict: {n_params:,} par√°metros\")\n",
    "                    elif key == 'optimizer_state_dict':\n",
    "                        print(f\"      - optimizer_state_dict: incluido\")\n",
    "                    elif isinstance(value, list):\n",
    "                        print(f\"      - {key}: lista de {len(value)} elementos\")\n",
    "                    elif isinstance(value, (int, float)):\n",
    "                        print(f\"      - {key}: {value}\")\n",
    "                    else:\n",
    "                        print(f\"      - {key}: {type(value).__name__}\")\n",
    "                        \n",
    "                # Si contiene historial de p√©rdidas, visualizarlo\n",
    "                if 'train_losses' in checkpoint and 'val_losses' in checkpoint:\n",
    "                    train_losses = checkpoint['train_losses']\n",
    "                    val_losses = checkpoint['val_losses']\n",
    "                    \n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    plt.plot(train_losses, label='Train')\n",
    "                    plt.plot(val_losses, label='Validation')\n",
    "                    plt.title(f'Historial de entrenamiento - {model_path.stem}')\n",
    "                    plt.xlabel('√âpoca')\n",
    "                    plt.ylabel('P√©rdida')\n",
    "                    plt.legend()\n",
    "                    plt.grid(alpha=0.3)\n",
    "                    plt.show()\n",
    "                    \n",
    "                    print(f\"      - √âpocas entrenadas: {len(train_losses)}\")\n",
    "                    print(f\"      - P√©rdida inicial: {train_losses[0]:.4f} (train), {val_losses[0]:.4f} (val)\")\n",
    "                    print(f\"      - P√©rdida final: {train_losses[-1]:.4f} (train), {val_losses[-1]:.4f} (val)\")\n",
    "                    print(f\"      - Mejor p√©rdida val: {min(val_losses):.4f} (√©poca {np.argmin(val_losses)+1})\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error al analizar el modelo: {str(e)}\")\n",
    "    \n",
    "    return model_files\n",
    "\n",
    "# Ejecutar an√°lisis de modelos guardados\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AN√ÅLISIS DE MODELOS GUARDADOS\")\n",
    "print(\"=\"*50)\n",
    "saved_models = analyze_saved_models()\n",
    "# Ejecutar an√°lisis de modelos guardados\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AN√ÅLISIS DE MODELOS GUARDADOS\")\n",
    "print(\"=\"*50)\n",
    "saved_models = analyze_saved_models()\n",
    "print(f\"\\nüìÇ Directorio de modelos guardados: {MODELS_OUTPUT}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
