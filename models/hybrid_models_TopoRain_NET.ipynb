{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd8a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 22:10:06,847 INFO Cargando datasets…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entorno configurado. Usando ruta base: ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 22:10:08,310 INFO Clusters codificados de texto a números: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-25 22:10:08,310 INFO Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-25 22:10:08,311 INFO Shapes: prec=(530, 61, 65), da_br=(530, 61, 65, 3), da_lags=(530, 61, 65, 7)\n",
      "2025-05-25 22:10:08,311 INFO Armando ventanas deslizantes con procesamiento por chunks…\n",
      "2025-05-25 22:10:08,311 INFO Procesando chunk de ventanas 0 a 49 de 468\n",
      "2025-05-25 22:10:08,310 INFO Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-25 22:10:08,311 INFO Shapes: prec=(530, 61, 65), da_br=(530, 61, 65, 3), da_lags=(530, 61, 65, 7)\n",
      "2025-05-25 22:10:08,311 INFO Armando ventanas deslizantes con procesamiento por chunks…\n",
      "2025-05-25 22:10:08,311 INFO Procesando chunk de ventanas 0 a 49 de 468\n",
      "2025-05-25 22:10:09,424 INFO Procesando chunk de ventanas 50 a 99 de 468\n",
      "2025-05-25 22:10:09,424 INFO Procesando chunk de ventanas 50 a 99 de 468\n",
      "2025-05-25 22:10:10,485 INFO Procesando chunk de ventanas 100 a 149 de 468\n",
      "2025-05-25 22:10:10,485 INFO Procesando chunk de ventanas 100 a 149 de 468\n",
      "2025-05-25 22:10:11,620 INFO Procesando chunk de ventanas 150 a 199 de 468\n",
      "2025-05-25 22:10:11,620 INFO Procesando chunk de ventanas 150 a 199 de 468\n",
      "2025-05-25 22:10:12,857 INFO Procesando chunk de ventanas 200 a 249 de 468\n",
      "2025-05-25 22:10:12,857 INFO Procesando chunk de ventanas 200 a 249 de 468\n",
      "2025-05-25 22:10:13,953 INFO Procesando chunk de ventanas 250 a 299 de 468\n",
      "2025-05-25 22:10:13,953 INFO Procesando chunk de ventanas 250 a 299 de 468\n",
      "2025-05-25 22:10:15,081 INFO Procesando chunk de ventanas 300 a 349 de 468\n",
      "2025-05-25 22:10:15,081 INFO Procesando chunk de ventanas 300 a 349 de 468\n",
      "2025-05-25 22:10:16,159 INFO Procesando chunk de ventanas 350 a 399 de 468\n",
      "2025-05-25 22:10:16,159 INFO Procesando chunk de ventanas 350 a 399 de 468\n",
      "2025-05-25 22:10:17,113 INFO Procesando chunk de ventanas 400 a 449 de 468\n",
      "2025-05-25 22:10:17,113 INFO Procesando chunk de ventanas 400 a 449 de 468\n",
      "2025-05-25 22:10:18,555 INFO Procesando chunk de ventanas 450 a 467 de 468\n",
      "2025-05-25 22:10:18,555 INFO Procesando chunk de ventanas 450 a 467 de 468\n",
      "2025-05-25 22:10:58,002 INFO Ventanas válidas totales: 432\n",
      "2025-05-25 22:10:58,002 INFO Ventanas válidas totales: 432\n",
      "2025-05-25 22:10:58,032 INFO Escalado de features y codificación de cluster…\n",
      "2025-05-25 22:10:58,032 INFO Escalado de features y codificación de cluster…\n",
      "2025-05-25 22:14:17,500 INFO Forma de matriz de topografía+cluster: (3965, 5)\n",
      "2025-05-25 22:14:17,500 INFO Forma de matriz de topografía+cluster: (3965, 5)\n",
      "2025-05-25 22:14:17,519 INFO Split train=302, val=130\n",
      "2025-05-25 22:14:17,531 INFO Entrenando LSTM…\n",
      "2025-05-25 22:14:17,519 INFO Split train=302, val=130\n",
      "2025-05-25 22:14:17,531 INFO Entrenando LSTM…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 22:29:08,792 INFO Entrenando GRU…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 22:41:34,828 INFO Entrenando MLP…\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TopoRain-Net: entrenamiento y evaluación de modelos base (LSTM, GRU, MLP, XGB)\n",
    "y meta-modelo MLP multisalida sobre features_fusion_branches + lags + topografía.\n",
    "Genera métricas, scatter, mapas y tablas (global, por elevación, por percentiles).\n",
    "\"\"\"\n",
    "\n",
    "import warnings, logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy            as np\n",
    "import pandas           as pd\n",
    "import xarray           as xr\n",
    "import geopandas        as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs      as ccrs\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics        import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost                import XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models    import Sequential\n",
    "from tensorflow.keras.layers    import Input, Dense, LSTM, GRU, Flatten, Reshape, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import ace_tools_open as tools\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuración y rutas\n",
    "# -----------------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuración del entorno (compatible con Colab y local)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Detectar si estamos en Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Si estamos en Colab, clonar el repositorio\n",
    "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
    "    %cd ml_precipitation_prediction\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn\n",
    "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
    "else:\n",
    "    # Si estamos en local, usar la ruta actual\n",
    "    if '/models' in os.getcwd():\n",
    "        BASE_PATH = Path('..')\n",
    "    else:\n",
    "        BASE_PATH = Path('.')\n",
    "\n",
    "BASE = Path(BASE_PATH)\n",
    "print(f\"Entorno configurado. Usando ruta base: {BASE}\")\n",
    "\n",
    "FULL_NC      = BASE/\"data\"/\"output\"/\"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
    "FUSION_NC    = BASE/\"models\"/\"output\"/\"features_fusion_branches.nc\"\n",
    "TRAINED_DIR  = BASE/\"models\"/\"output\"/\"trained_models\"\n",
    "TRAINED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INPUT_WINDOW   = 60\n",
    "OUTPUT_HORIZON = 3\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Carga de datos\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Cargando datasets…\")\n",
    "ds_full = xr.open_dataset(FULL_NC)\n",
    "ds_fuse = xr.open_dataset(FUSION_NC)\n",
    "\n",
    "# precipitacion y variables\n",
    "prec    = ds_full[\"total_precipitation\"].values            # (T, ny, nx)\n",
    "lags    = sorted([v for v in ds_full.data_vars if \"_lag\" in v])\n",
    "da_lags = np.stack([ds_full[lag].values for lag in lags], axis=-1)  # (T, ny, nx, n_lags)\n",
    "\n",
    "# ramas fusionadas\n",
    "branches = [\"FUSION_high\", \"FUSION_medium\", \"FUSION_low\"]\n",
    "# Asegurémonos de que da_br sea un ndarray correcto\n",
    "da_br = np.stack([ds_fuse[branch].values for branch in branches], axis=-1)  # (T, ny, nx, 3)\n",
    "\n",
    "# topografía y cluster\n",
    "elev    = ds_full[\"elevation\"].values.ravel()               # (cells,)\n",
    "slope   = ds_full[\"slope\"].values.ravel()\n",
    "\n",
    "# Manejar correctamente los valores de cluster (pueden ser texto)\n",
    "cluster_values = ds_full[\"cluster_elevation\"].values.ravel()\n",
    "# Verificar si los valores son strings o numéricos\n",
    "if isinstance(cluster_values[0], (str, np.str_)):\n",
    "    # Usar un LabelEncoder para convertir strings a enteros\n",
    "    le = LabelEncoder()\n",
    "    cluster = le.fit_transform(cluster_values)\n",
    "    logger.info(f\"Clusters codificados de texto a números: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    # Si ya son numéricos, convertir a enteros\n",
    "    cluster = cluster_values.astype(int)\n",
    "\n",
    "# dimensiones\n",
    "lat     = ds_full.latitude.values\n",
    "lon     = ds_full.longitude.values\n",
    "ny, nx  = len(lat), len(lon)\n",
    "cells   = ny*nx\n",
    "T       = prec.shape[0]\n",
    "\n",
    "logger.info(f\"Dimensiones: T={T}, ny={ny}, nx={nx}, cells={cells}\")\n",
    "logger.info(f\"Shapes: prec={prec.shape}, da_br={da_br.shape}, da_lags={da_lags.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Ventanas deslizantes (implementación escalable con chunks)\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Armando ventanas deslizantes con procesamiento por chunks…\")\n",
    "\n",
    "# Definir el tamaño de chunks para procesamiento por lotes\n",
    "CHUNK_SIZE = 50  # Ajustar según capacidad de memoria disponible\n",
    "\n",
    "# Número total de ventanas posibles\n",
    "n_windows = T - INPUT_WINDOW - OUTPUT_HORIZON + 1\n",
    "Xw, Yw = [], []\n",
    "\n",
    "# Procesar por chunks para evitar problemas de memoria\n",
    "for chunk_start in range(0, n_windows, CHUNK_SIZE):\n",
    "    chunk_end = min(chunk_start + CHUNK_SIZE, n_windows)\n",
    "    logger.info(f\"Procesando chunk de ventanas {chunk_start} a {chunk_end-1} de {n_windows}\")\n",
    "    \n",
    "    # Crear ventanas para este chunk\n",
    "    chunk_Xw, chunk_Yw = [], []\n",
    "    \n",
    "    for i in range(chunk_start, chunk_end):\n",
    "        # Stack de features en ventana\n",
    "        # Branches\n",
    "        bwin = da_br[i:i+INPUT_WINDOW].reshape(INPUT_WINDOW, cells, 3)\n",
    "        # Lags\n",
    "        lwin = da_lags[i:i+INPUT_WINDOW].reshape(INPUT_WINDOW, cells, len(lags))\n",
    "        # Concatenar\n",
    "        feat = np.concatenate([bwin, lwin], axis=-1)            # (W, cells, F)\n",
    "        chunk_Xw.append(feat.reshape(INPUT_WINDOW, cells*feat.shape[-1]))\n",
    "        # Targets\n",
    "        tw = [prec[i+INPUT_WINDOW+h].reshape(cells) for h in range(OUTPUT_HORIZON)]\n",
    "        chunk_Yw.append(np.stack(tw,axis=0))                    # (H, cells)\n",
    "    \n",
    "    # Convertir a arrays y aplicar filtro de NaNs dentro del chunk\n",
    "    chunk_X = np.stack(chunk_Xw)                              # (chunk_size, W, cells*F)\n",
    "    chunk_Y = np.stack(chunk_Yw)                              # (chunk_size, H, cells)\n",
    "    \n",
    "    # Filtrar NaNs en este chunk\n",
    "    chunk_mask = (~np.isnan(chunk_X).any(axis=(1,2))) & (~np.isnan(chunk_Y).any(axis=(1,2)))\n",
    "    valid_X = chunk_X[chunk_mask]\n",
    "    valid_Y = chunk_Y[chunk_mask]\n",
    "    \n",
    "    # Añadir los datos válidos de este chunk a las listas principales\n",
    "    if len(valid_X) > 0:\n",
    "        Xw.append(valid_X)\n",
    "        Yw.append(valid_Y)\n",
    "    \n",
    "    # Limpiar memoria explícitamente\n",
    "    del chunk_Xw, chunk_Yw, chunk_X, chunk_Y, valid_X, valid_Y\n",
    "    if 'gc' in sys.modules:\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "# Concatenar todos los chunks\n",
    "X = np.vstack(Xw) if Xw else np.array([])  # (N, W, cells*F)\n",
    "Y = np.vstack(Yw) if Yw else np.array([])  # (N, H, cells)\n",
    "N = len(X)\n",
    "\n",
    "logger.info(f\"Ventanas válidas totales: {N}\")\n",
    "\n",
    "# Opcional: Guardar en disco para futuros usos\n",
    "# np.save(BASE/\"models\"/\"output\"/\"ventanas_X.npy\", X)\n",
    "# np.save(BASE/\"models\"/\"output\"/\"ventanas_Y.npy\", Y)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Escalado + one-hot de cluster\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Escalado de features y codificación de cluster…\")\n",
    "# escalar X\n",
    "scX = StandardScaler()\n",
    "Xf  = scX.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
    "\n",
    "# preparar topografía+cluster (repite por celda)\n",
    "try:\n",
    "    # Para versiones más recientes de scikit-learn\n",
    "    ohe = OneHotEncoder(sparse_output=False)\n",
    "except TypeError:\n",
    "    # Para versiones anteriores de scikit-learn\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    \n",
    "c_ohe  = ohe.fit_transform(cluster.reshape(-1,1))         # (cells, n_clusters)\n",
    "topo   = np.hstack([elev.reshape(-1,1), slope.reshape(-1,1), c_ohe])  # (cells, 2+n_clusters)\n",
    "logger.info(f\"Forma de matriz de topografía+cluster: {topo.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Train/val split\n",
    "# -----------------------------------------------------------------------------\n",
    "split   = int(0.7*N)\n",
    "X_tr    = Xf[:split];    X_va = Xf[split:]\n",
    "Y_tr    = Y[:split];     Y_va = Y[split:]\n",
    "logger.info(f\"Split train={len(X_tr)}, val={len(X_va)}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Entrenamiento de modelos base\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_ts_model(kind):\n",
    "    m = Sequential([ Input(shape=(INPUT_WINDOW,X_tr.shape[-1])) ])\n",
    "    if kind==\"LSTM\": m.add(LSTM(64))\n",
    "    elif kind==\"GRU\": m.add(GRU(64))\n",
    "    elif kind==\"MLP\":\n",
    "        m.add(Flatten())\n",
    "        m.add(Dense(128,activation=\"relu\"))\n",
    "    m.add(Dense(OUTPUT_HORIZON*cells))\n",
    "    m.add(Reshape((OUTPUT_HORIZON,cells)))\n",
    "    m.compile(\"adam\",\"mse\")\n",
    "    return m\n",
    "\n",
    "BASES   = [\"LSTM\",\"GRU\",\"MLP\",\"XGB\"]\n",
    "pred_va = {}\n",
    "pred_fc = {}\n",
    "\n",
    "# TensorFlow models\n",
    "for b in [\"LSTM\",\"GRU\",\"MLP\"]:\n",
    "    logger.info(f\"Entrenando {b}…\")\n",
    "    m = build_ts_model(b)\n",
    "    m.fit(X_tr, Y_tr,\n",
    "          validation_data=(X_va,Y_va),\n",
    "          epochs=100, batch_size=16,\n",
    "          callbacks=[EarlyStopping(\"val_loss\",patience=10,restore_best_weights=True)],\n",
    "          verbose=0)\n",
    "    pred_va[b] = m.predict(X_va)             # (Nv,H,cells)\n",
    "    pred_fc[b] = m.predict(Xf[-1:])[0]       # (H,cells)\n",
    "\n",
    "# XGBoost por horizonte\n",
    "for h in range(OUTPUT_HORIZON):\n",
    "    logger.info(f\"Entrenando XGB H={h+1}…\")\n",
    "    xgb = XGBRegressor(n_estimators=100, max_depth=5, verbosity=0)\n",
    "    # entrenar\n",
    "    xgb.fit(X_tr.reshape(-1,X_tr.shape[-1]), Y_tr[:,h].ravel())\n",
    "    # preds\n",
    "    pv = xgb.predict(X_va.reshape(-1,X_va.shape[-1])).reshape(-1,cells)\n",
    "    fc = xgb.predict(Xf[-1:].reshape(-1,Xf.shape[-1])).ravel()\n",
    "    pred_va.setdefault(\"XGB\",[]).append(pv)\n",
    "    pred_fc.setdefault(\"XGB\",[]).append(fc)\n",
    "\n",
    "# apilar XGB preds → (Nv,H,cells)\n",
    "pred_va[\"XGB\"] = np.stack(pred_va[\"XGB\"],axis=1)\n",
    "pred_fc[\"XGB\"] = np.stack(pred_fc[\"XGB\"],axis=0)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Evaluación Base-models\n",
    "# -----------------------------------------------------------------------------\n",
    "rows=[]\n",
    "for b in BASES:\n",
    "    pv = pred_va[b]\n",
    "    for h in range(OUTPUT_HORIZON):\n",
    "        yt = Y_va[:,h,:].ravel()\n",
    "        yp = pv[:,h,:].ravel()\n",
    "        rows.append({\n",
    "            \"model\":b, \"horizon\":h+1,\n",
    "            \"RMSE\":np.sqrt(mean_squared_error(yt,yp)),\n",
    "            \"MAE\": mean_absolute_error(yt,yp),\n",
    "            \"MAPE\":np.mean(np.abs((yt-yp)/(yt+1e-5)))*100,\n",
    "            \"R2\":  r2_score(yt,yp)\n",
    "        })\n",
    "df_base = pd.DataFrame(rows)\n",
    "tools.display_dataframe_to_user(\"Base_models_metrics\", df_base)\n",
    "\n",
    "# scatter y mapas Base-models\n",
    "grid_lon,grid_lat = np.meshgrid(lon,lat)\n",
    "for b in BASES:\n",
    "    pv = pred_va[b]\n",
    "    for h in range(OUTPUT_HORIZON):\n",
    "        # scatter\n",
    "        yt,yp = Y_va[:,h,:].ravel(), pv[:,h,:].ravel()\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.scatter(yt,yp,s=2,alpha=0.3)\n",
    "        mn,mx = yt.min(),yp.max()\n",
    "        plt.plot([mn,mx],[mn,mx],'k--')\n",
    "        plt.title(f\"{b} True vs Pred H={h+1}\")\n",
    "        plt.show()\n",
    "        # mapas H\n",
    "        pm = yp.reshape(-1,cells).reshape(-1,ny,nx)[0]\n",
    "        tm = Y_va[:,h,:].reshape(-1,cells).reshape(-1,ny,nx)[0]\n",
    "        err= np.abs((tm-pm)/(tm+1e-5))*100\n",
    "        fig,ax = plt.subplots(1,2,figsize=(10,4),\n",
    "                              subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
    "        ax[0].pcolormesh(grid_lon,grid_lat,pm,transform=ccrs.PlateCarree(),cmap=\"Blues\")\n",
    "        ax[0].set_title(f\"{b} Pred H={h+1}\")\n",
    "        ax[1].pcolormesh(grid_lon,grid_lat,err,transform=ccrs.PlateCarree(),\n",
    "                         cmap=\"Reds\",vmin=0,vmax=100)\n",
    "        ax[1].set_title(f\"{b} MAPE% H={h+1}\")\n",
    "        plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Preparar datos para Meta-modelo\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Armando características para meta-modelo…\")\n",
    "X_meta_va = []\n",
    "for i in range(len(X_va)):\n",
    "    feats = []\n",
    "    for b in BASES:\n",
    "        feats.append(pred_va[b][i])                 # (H,cells)\n",
    "    stack = np.vstack(feats)                       # (B*H, cells)\n",
    "    vec   = stack.T.flatten()                      # (cells*B*H,)\n",
    "    topo_flat = topo.flatten()                     # (cells*topo_dim,)\n",
    "    X_meta_va.append(np.hstack([vec, topo_flat]))\n",
    "X_meta_va = np.stack(X_meta_va)                    # (Nv, ...)\n",
    "\n",
    "# forecast meta\n",
    "stack_fc = []\n",
    "for b in BASES:\n",
    "    stack_fc.append(pred_fc[b])                     # (H,cells)\n",
    "stack_fc = np.vstack(stack_fc)                      # (B*H, cells)\n",
    "Xm_fc   = np.hstack([stack_fc.T.flatten(), topo.flatten()])[None,:]  # (1,...)\n",
    "\n",
    "# target meta\n",
    "Y_meta_va = Y_va.reshape(len(X_va), -1)            # (Nv, H*cells)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8) Entrenamiento Meta-modelo MLP\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Entrenando meta-modelo…\")\n",
    "meta = Sequential([\n",
    "    Input(shape=(X_meta_va.shape[-1],)),\n",
    "    Dense(256,activation=\"relu\"), Dropout(0.4),\n",
    "    Dense(128,activation=\"relu\"),\n",
    "    Dense(Y_meta_va.shape[-1])\n",
    "])\n",
    "meta.compile(\"adam\",\"mse\")\n",
    "meta.fit(X_meta_va, Y_meta_va,\n",
    "         validation_split=0.2,\n",
    "         epochs=200, batch_size=32,\n",
    "         callbacks=[EarlyStopping(\"val_loss\",patience=20,restore_best_weights=True)],\n",
    "         verbose=0)\n",
    "\n",
    "# preds meta\n",
    "P_meta_va = meta.predict(X_meta_va)               # (Nv, H*cells)\n",
    "P_meta_fc = meta.predict(Xm_fc)                   # (1, H*cells)\n",
    "\n",
    "Y_meta_va = P_meta_va.reshape(-1,OUTPUT_HORIZON,cells)\n",
    "Y_meta_fc = P_meta_fc.reshape(OUTPUT_HORIZON,cells)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9) Evaluación Meta-modelo\n",
    "# -----------------------------------------------------------------------------\n",
    "rows=[]\n",
    "for h in range(OUTPUT_HORIZON):\n",
    "    yt = Y_va[:,h,:].ravel(); yp = Y_meta_va[:,h,:].ravel()\n",
    "    rows.append({\n",
    "        \"horizon\":h+1,\n",
    "        \"RMSE\":np.sqrt(mean_squared_error(yt,yp)),\n",
    "        \"MAE\": mean_absolute_error(yt,yp),\n",
    "        \"MAPE\":np.mean(np.abs((yt-yp)/(yt+1e-5)))*100,\n",
    "        \"R2\":  r2_score(yt,yp)\n",
    "    })\n",
    "df_meta = pd.DataFrame(rows)\n",
    "tools.display_dataframe_to_user(\"Meta_model_metrics\", df_meta)\n",
    "\n",
    "# scatter y mapas Meta\n",
    "for h in range(OUTPUT_HORIZON):\n",
    "    yt,yp = Y_va[:,h,:].ravel(), Y_meta_va[:,h,:].ravel()\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.scatter(yt,yp,s=2,alpha=0.3)\n",
    "    mn,mx = yt.min(),yp.max()\n",
    "    plt.plot([mn,mx],[mn,mx],'k--')\n",
    "    plt.title(f\"Meta True vs Pred H={h+1}\"); plt.show()\n",
    "\n",
    "    pm = yp.reshape(-1,cells).reshape(-1,ny,nx)[0]\n",
    "    tm = Y_va[:,h,:].reshape(-1,cells).reshape(-1,ny,nx)[0]\n",
    "    err= np.abs((tm-pm)/(tm+1e-5))*100\n",
    "    fig,ax = plt.subplots(1,2,figsize=(10,4),\n",
    "                          subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
    "    ax[0].pcolormesh(grid_lon,grid_lat,pm,transform=ccrs.PlateCarree(),cmap=\"Blues\")\n",
    "    ax[0].set_title(f\"Meta Pred H={h+1}\")\n",
    "    ax[1].pcolormesh(grid_lon,grid_lat,err,transform=ccrs.PlateCarree(),\n",
    "                     cmap=\"Reds\",vmin=0,vmax=100)\n",
    "    ax[1].set_title(f\"Meta MAPE% H={h+1}\")\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 10) Métricas desagregadas por elevación y percentiles\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Calculando métricas por elevación y percentiles…\")\n",
    "mask_low  = elev < 200\n",
    "mask_mid  = (elev>=200)&(elev<=1000)\n",
    "mask_high = elev>1000\n",
    "\n",
    "elev_rows = []\n",
    "pct_rows  = []\n",
    "for h in range(OUTPUT_HORIZON):\n",
    "    yt = Y_va[:,h,:].ravel(); yp = Y_meta_va[:,h,:].ravel()\n",
    "    # elevación\n",
    "    for name, m in zip([\"<200m\",\"200-1000m\",\">1000m\"], [mask_low,mask_mid,mask_high]):\n",
    "        yt_m,yp_m = yt[m], yp[m]\n",
    "        elev_rows.append({\n",
    "            \"horizon\":h+1, \"region\":name,\n",
    "            \"RMSE\":np.sqrt(mean_squared_error(yt_m,yp_m)),\n",
    "            \"MAE\": mean_absolute_error(yt_m,yp_m),\n",
    "            \"MAPE\":np.mean(np.abs((yt_m-yp_m)/(yt_m+1e-5)))*100,\n",
    "            \"R2\":  r2_score(yt_m,yp_m)\n",
    "        })\n",
    "    # percentiles\n",
    "    edges = [0,25,50,75,100]\n",
    "    pcts  = np.percentile(yt, edges)\n",
    "    for i in range(4):\n",
    "        lo,hi = pcts[i],pcts[i+1]\n",
    "        idx   = (yt>=lo)&(yt<hi)\n",
    "        yt_p, yp_p = yt[idx], yp[idx]\n",
    "        pct_rows.append({\n",
    "            \"horizon\":h+1,\n",
    "            \"pct_range\":f\"{edges[i]}-{edges[i+1]}%\",\n",
    "            \"RMSE\":np.sqrt(mean_squared_error(yt_p,yp_p)),\n",
    "            \"MAE\": mean_absolute_error(yt_p,yp_p),\n",
    "            \"MAPE\":np.mean(np.abs((yt_p-yp_p)/(yt_p+1e-5)))*100,\n",
    "            \"R2\":  r2_score(yt_p,yp_p)\n",
    "        })\n",
    "\n",
    "df_elev = pd.DataFrame(elev_rows)\n",
    "df_pct  = pd.DataFrame(pct_rows)\n",
    "tools.display_dataframe_to_user(\"Meta_by_elevation\", df_elev)\n",
    "tools.display_dataframe_to_user(\"Meta_by_percentile\", df_pct)\n",
    "\n",
    "logger.info(\"🎉 Proceso completado con éxito.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
