{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fbase-models/models/base_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "635dc005",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "635dc005",
        "outputId": "49c047f8-1576-48f8-a5fa-eb5a3df8c860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cloning into 'ml_precipitation_prediction'...\n",
            "remote: Enumerating objects: 904, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 904 (delta 26), reused 6 (delta 6), pack-reused 863 (from 1)\u001b[K\n",
            "Receiving objects: 100% (904/904), 99.82 MiB | 43.29 MiB/s, done.\n",
            "Resolving deltas: 100% (494/494), done.\n",
            "/content/ml_precipitation_prediction/ml_precipitation_prediction\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (1.7.2)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2025.3.1)\n",
            "Requirement already satisfied: rioxarray in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.19.0)\n",
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.4.3)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.0.1)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2024.12.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (4.67.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (2.1.4)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (4.5.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (2.18.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.2.3)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.11/dist-packages (from netCDF4->-r requirements.txt (line 5)) (1.6.4.post1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from netCDF4->-r requirements.txt (line 5)) (2025.4.26)\n",
            "Requirement already satisfied: pyproj>=3.3 in /usr/local/lib/python3.11/dist-packages (from rioxarray->-r requirements.txt (line 7)) (3.7.1)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.11/dist-packages (from rasterio->-r requirements.txt (line 8)) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio->-r requirements.txt (line 8)) (25.3.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio->-r requirements.txt (line 8)) (8.1.8)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.11/dist-packages (from rasterio->-r requirements.txt (line 8)) (0.7.2)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.11/dist-packages (from rasterio->-r requirements.txt (line 8)) (1.1.1)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from geopandas->-r requirements.txt (line 9)) (0.11.0)\n",
            "Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from geopandas->-r requirements.txt (line 9)) (2.1.0)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask->-r requirements.txt (line 10)) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask->-r requirements.txt (line 10)) (2025.3.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask->-r requirements.txt (line 10)) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask->-r requirements.txt (line 10)) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask->-r requirements.txt (line 10)) (0.12.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask->-r requirements.txt (line 10)) (8.7.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost->-r requirements.txt (line 13)) (2.21.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->-r requirements.txt (line 15)) (0.37.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->-r requirements.txt (line 16)) (1.0.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 15)) (0.45.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask->-r requirements.txt (line 10)) (3.21.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->-r requirements.txt (line 15)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->-r requirements.txt (line 15)) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->-r requirements.txt (line 15)) (0.15.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask->-r requirements.txt (line 10)) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 15)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 15)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 15)) (2.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 15)) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 15)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 15)) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->-r requirements.txt (line 15)) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->-r requirements.txt (line 15)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->-r requirements.txt (line 15)) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->-r requirements.txt (line 15)) (0.1.2)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (2025.3.1)\n",
            "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: ace_tools_open in /usr/local/lib/python3.11/dist-packages (0.1.0)\n",
            "Requirement already satisfied: cartopy in /usr/local/lib/python3.11/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from xarray) (24.2)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.2.2)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.11/dist-packages (from netCDF4) (1.6.4.post1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2025.4.26)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: itables in /usr/local/lib/python3.11/dist-packages (from ace_tools_open) (2.3.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (from ace_tools_open) (7.34.0)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.1.0)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.7.1)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython->ace_tools_open) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython->ace_tools_open) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython->ace_tools_open) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->ace_tools_open) (0.2.13)\n",
            "Entorno configurado. Usando ruta base: /content/drive/MyDrive/ml_precipitation_prediction\n"
          ]
        }
      ],
      "source": [
        "# Configuraci√≥n del entorno (compatible con Colab y local)\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import time\n",
        "import psutil\n",
        "\n",
        "# Regenerar el c√≥digo con las condiciones espec√≠ficas\n",
        "notebook_globals = {\n",
        "    \"USE_CROSS_VALIDATION\": False,\n",
        "    \"ENABLED_MODELS\": ['CNN', 'GRU'],\n",
        "    \"ENABLED_EXPERIMENTS\": ['time+cycles', 'all_features'],\n",
        "    \"ENABLED_HORIZONS\": [3],\n",
        "}\n",
        "\n",
        "# Detectar si estamos en Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Si estamos en Colab, clonar el repositorio\n",
        "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
        "    %cd ml_precipitation_prediction\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy\n",
        "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
        "else:\n",
        "    # Si estamos en local, usar la ruta actual\n",
        "    if '/models' in os.getcwd():\n",
        "        BASE_PATH = Path('..')\n",
        "    else:\n",
        "        BASE_PATH = Path('.')\n",
        "\n",
        "print(f\"Entorno configurado. Usando ruta base: {BASE_PATH}\")\n",
        "\n",
        "# Si BASE_PATH viene como string, lo convertimos\n",
        "BASE_PATH = Path(BASE_PATH)\n",
        "\n",
        "# Ahora puedes concatenar correctamente\n",
        "data_output_dir = BASE_PATH / 'data' / 'output'\n",
        "model_output_dir = BASE_PATH / 'models' / 'output'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8ccc04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f8ccc04",
        "outputId": "300862df-4d85-4ccf-b240-c5dcb6dc5756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Detectando dispositivo disponible...\n",
            "‚úÖ GPU detectada: /physical_device:GPU:0\n",
            "‚ÑπÔ∏è Entrenamiento acelerado con GPU activado.\n",
            "üìÇ Configurando directorios y cargando dataset...\n",
            "‚úîÔ∏è Modelos en: /content/drive/MyDrive/ml_precipitation_prediction/models/output/ST_HybridWaveStack\n",
            "‚úîÔ∏è Curvas en: /content/drive/MyDrive/ml_precipitation_prediction/models/output/ST_HybridWaveStack/learning_curves\n",
            "‚úîÔ∏è Dataset cargado desde: /content/drive/MyDrive/ml_precipitation_prediction/data/output/complete_dataset_with_features_with_clusters_elevation_with_windows.nc\n",
            "‚úÖ Variables requeridas presentes.\n",
            "\n",
            "üöÄ Experimento: time+cycles\n",
            "üìâ Secuencias orig.: 2101387, v√°lidas: 2101387, eliminadas: 0 (0.00%)\n",
            "üîß Modelo: GRU\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m41553/91936\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m4:11\u001b[0m 5ms/step - loss: 0.6829"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.layers import Input\n",
        "import traceback\n",
        "\n",
        "# ==== Variables de control ====\n",
        "USE_CROSS_VALIDATION = False\n",
        "ENABLED_MODELS = ['GRU'] # Primer ronda ENABLED_MODELS = ['CNN', 'GRU'], segunda ronda: ENABLED_MODELS = ['LSTM', 'BLSTM']\n",
        "ENABLED_EXPERIMENTS = ['time+cycles', 'time+cycles+elev', 'time+cycles+elev+cluster']\n",
        "ENABLED_HORIZONS = [3]\n",
        "INPUT_WINDOW = 60  # 5 a√±os (mensual)\n",
        "OUTPUT_HORIZON = 3\n",
        "\n",
        "# ==== Configuraci√≥n de entorno ====\n",
        "print(\"üîç Detectando dispositivo disponible...\")\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "USE_GPU = bool(gpu_devices)\n",
        "if USE_GPU:\n",
        "    print(\"‚úÖ GPU detectada:\", gpu_devices[0].name)\n",
        "    print(\"‚ÑπÔ∏è Entrenamiento acelerado con GPU activado.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se detect√≥ GPU. Usando CPU.\")\n",
        "\n",
        "# ==== Funciones auxiliares ====\n",
        "def build_model(model_type, input_shape, output_neurons):\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, LSTM, GRU, Bidirectional, Reshape\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape, dtype='float32'))\n",
        "    if model_type == 'LSTM':\n",
        "        model.add(LSTM(64))\n",
        "    elif model_type == 'GRU':\n",
        "        model.add(GRU(64))\n",
        "    elif model_type == 'BLSTM':\n",
        "        model.add(Bidirectional(LSTM(64)))\n",
        "    elif model_type == 'CNN':\n",
        "        model.add(Reshape((*input_shape, 1)))\n",
        "        model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "        model.add(Flatten())\n",
        "    model.add(Dense(output_neurons))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "def evaluate(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-5))) * 100\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, mape, r2\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size=16):\n",
        "        self.x, self.y = x_set.astype(np.float32), y_set.astype(np.float32)\n",
        "        self.batch_size = batch_size\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        return batch_x, batch_y\n",
        "\n",
        "def to_dataset(x, y):\n",
        "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
        "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "    return tf.data.Dataset.from_tensor_slices((x, y)).batch(16).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ==== Directorios y Dataset ====\n",
        "print(\"üìÇ Configurando directorios y cargando dataset...\")\n",
        "try:\n",
        "    model_output_dir_STH = model_output_dir / 'ST_HybridWaveStack'\n",
        "    curves_dir = model_output_dir_STH / \"learning_curves\"\n",
        "    if not model_output_dir_STH.exists():\n",
        "        model_output_dir_STH.mkdir(parents=True)\n",
        "    if not curves_dir.exists():\n",
        "        curves_dir.mkdir(parents=True)\n",
        "    print(f\"‚úîÔ∏è Modelos en: {model_output_dir_STH}\")\n",
        "    print(f\"‚úîÔ∏è Curvas en: {curves_dir}\")\n",
        "\n",
        "    file_path = data_output_dir / \"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
        "    ds = xr.open_dataset(file_path)\n",
        "    print(f\"‚úîÔ∏è Dataset cargado desde: {file_path}\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"‚ùå Error cargando dataset o creando carpetas: {e}\")\n",
        "\n",
        "# ==== Configuraci√≥n de experimentos ====\n",
        "experiment_settings = {\n",
        "    \"time+cycles\": ['year','month',\n",
        "                    'month_sin','month_cos','doy_sin','doy_cos'],\n",
        "    \"time+cycles+lag\":  ['year','month',\n",
        "                        'month_sin','month_cos','doy_sin','doy_cos',\n",
        "                        'total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag3',\n",
        "                        'total_precipitation_lag4','total_precipitation_lag12','total_precipitation_lag24','total_precipitation_lag36'],\n",
        "    \"time+cycles+lag+elev\": ['year','month',\n",
        "                             'month_sin','month_cos','doy_sin','doy_cos',\n",
        "                            'total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag3',\n",
        "                            'total_precipitation_lag4','total_precipitation_lag12','total_precipitation_lag24','total_precipitation_lag36',\n",
        "                            'elevation','slope','aspect'],\n",
        "    \"time+cycles+elev\":   ['year','month',\n",
        "                          'month_sin','month_cos','doy_sin','doy_cos',\n",
        "                          'elevation','slope','aspect'],\n",
        "    \"time+cycles+elev+cluster\":   ['year','month',\n",
        "                                  'month_sin','month_cos','doy_sin','doy_cos',\n",
        "                                  'elevation','slope','aspect',\n",
        "                                  'cluster_elevation'],\n",
        "    \"all_features\": ['year','month',\n",
        "                     'month_sin','month_cos','doy_sin','doy_cos',\n",
        "                    'total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag3',\n",
        "                    'total_precipitation_lag4','total_precipitation_lag12','total_precipitation_lag24','total_precipitation_lag36',\n",
        "                    'elevation','slope','aspect',\n",
        "                     'cluster_elevation']\n",
        "}\n",
        "\n",
        "# Validaci√≥n de variables\n",
        "ds_vars = set(ds.data_vars)\n",
        "for name, vars_list in experiment_settings.items():\n",
        "    missing = [v for v in vars_list if v not in ds_vars]\n",
        "    if missing:\n",
        "        raise ValueError(f\"‚ùå Faltan vars para '{name}': {missing}\")\n",
        "print(\"‚úÖ Variables requeridas presentes.\")\n",
        "\n",
        "results = []\n",
        "\n",
        "# ==== Entrenamiento modular ====\n",
        "for exp_name, variables in experiment_settings.items():\n",
        "    if exp_name not in ENABLED_EXPERIMENTS:\n",
        "        continue\n",
        "    print(f\"\\nüöÄ Experimento: {exp_name}\")\n",
        "    try:\n",
        "        # Preparar datos\n",
        "        cluster_idx = variables.index('cluster_elevation') if 'cluster_elevation' in variables else None\n",
        "        subset = ds[variables].to_array().transpose('time','latitude','longitude','variable').values\n",
        "        if cluster_idx is not None:\n",
        "            cd = subset[...,cluster_idx]\n",
        "            subset[...,cluster_idx] = LabelEncoder().fit_transform(cd.ravel()).reshape(cd.shape)\n",
        "        subset = subset.astype(np.float32)\n",
        "        target = ds['total_precipitation'].values\n",
        "        samples,lat,lon,feats = subset.shape\n",
        "        X = subset.reshape(samples, lat*lon, feats)\n",
        "        y = target.reshape(samples, lat*lon)\n",
        "        mask = ~np.isnan(y)\n",
        "        X, y = X[mask], y[mask]\n",
        "\n",
        "        # Generar secuencias\n",
        "        X_seq, Y_targets = [], []\n",
        "        for i in range(len(X) - INPUT_WINDOW - OUTPUT_HORIZON):\n",
        "            X_seq.append(X[i:i + INPUT_WINDOW])\n",
        "            Y_targets.append([y[i + INPUT_WINDOW + h] for h in range(OUTPUT_HORIZON)])\n",
        "        X_seq = np.array(X_seq)\n",
        "        Y_targets = np.array(Y_targets)\n",
        "\n",
        "        # --- FILTRAR Secuencias con NaNs y reportar ---\n",
        "        def filtrar_secuencias(Xs, ys):\n",
        "            total = len(Xs)\n",
        "            valid = (~np.isnan(Xs).any(axis=(1,2))) & (~np.isnan(ys).any(axis=1))\n",
        "            kept = valid.sum()\n",
        "            lost = total - kept\n",
        "            pct = 100 * lost/total\n",
        "            print(f\"üìâ Secuencias orig.: {total}, v√°lidas: {kept}, eliminadas: {lost} ({pct:.2f}%)\")\n",
        "            return Xs[valid], ys[valid]\n",
        "\n",
        "        X_seq, Y_targets = filtrar_secuencias(X_seq, Y_targets)\n",
        "\n",
        "        if len(X_seq)==0:\n",
        "            print(f\"‚ö†Ô∏è No quedan secuencias v√°lidas para '{exp_name}'. Saltando.\")\n",
        "            continue\n",
        "\n",
        "        input_shape = (X_seq.shape[1], X_seq.shape[2])\n",
        "\n",
        "        # Cross-validation\n",
        "        for model_name in ENABLED_MODELS:\n",
        "            print(f\"üîß Modelo: {model_name}\")\n",
        "            # Partici√≥n\n",
        "            if USE_CROSS_VALIDATION:\n",
        "                splitter = KFold(n_splits=3, shuffle=False).split(X_seq)\n",
        "            else:\n",
        "                split_idx = int(len(X_seq) * 0.7)\n",
        "                splitter = [(\n",
        "                    np.arange(split_idx),\n",
        "                    np.arange(split_idx, len(X_seq))\n",
        "                )]\n",
        "\n",
        "            for h in ENABLED_HORIZONS:\n",
        "                for split_id, (tr_idx, va_idx) in enumerate(splitter, start=1):\n",
        "                    X_tr, X_va = X_seq[tr_idx], X_seq[va_idx]\n",
        "                    y_tr, y_va = Y_targets[tr_idx], Y_targets[va_idx]\n",
        "\n",
        "                    # Escalado\n",
        "                    scalerX=StandardScaler()\n",
        "                    shpX = X_tr.shape\n",
        "                    X_tr = scalerX.fit_transform(X_tr.reshape(-1,shpX[-1])).reshape(shpX)\n",
        "                    shpX2 = X_va.shape\n",
        "                    X_va = scalerX.transform(X_va.reshape(-1,shpX[-1])).reshape(shpX2)\n",
        "                    scalery=StandardScaler()\n",
        "                    y_tr = scalery.fit_transform(y_tr.reshape(-1,1)).reshape(y_tr.shape)\n",
        "                    y_va = scalery.transform(y_va.reshape(-1,1)).reshape(y_va.shape)\n",
        "\n",
        "                    # Data generators\n",
        "                    train_gen = DataGenerator(X_tr, y_tr, batch_size=16)\n",
        "                    val_gen   = DataGenerator(X_va, y_va, batch_size=16)\n",
        "\n",
        "                    # Nombre de archivo\n",
        "                    postfix = f\"_H{h}\" if USE_CROSS_VALIDATION else \"_NoCV\"\n",
        "                    model_path = model_output_dir_STH / f\"{exp_name.replace('+','_')}_{model_name}{postfix}.h5\"\n",
        "                    if model_path.exists():\n",
        "                        print(\"‚è© Ya existe. Skip.\")\n",
        "                        continue\n",
        "\n",
        "                    model = build_model(model_name, input_shape, output_neurons = OUTPUT_HORIZON)\n",
        "                    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=9, restore_best_weights=True)\n",
        "                    history = model.fit(train_gen,\n",
        "                                        validation_data=val_gen,\n",
        "                                        epochs=50,\n",
        "                                        verbose=1,\n",
        "                                        callbacks=[es])\n",
        "\n",
        "                    # Evaluaci√≥n\n",
        "                    y_pred = model.predict(X_va)  # Mantiene la forma (samples, 3)\n",
        "                    y_true = y_va  # Tambi√©n (samples, 3)\n",
        "\n",
        "                    metrics = [evaluate(y_true[:, i], y_pred[:, i]) for i in range(OUTPUT_HORIZON)]\n",
        "\n",
        "                    results.append({\n",
        "                        'experiment': exp_name,\n",
        "                        'model': model_name,\n",
        "                        'horizon': h,\n",
        "                        'cv_enabled': USE_CROSS_VALIDATION,\n",
        "                        'RMSE': np.mean([m[0] for m in metrics]),\n",
        "                        'MAE': np.mean([m[1] for m in metrics]),\n",
        "                        'MAPE': np.mean([m[2] for m in metrics]),\n",
        "                        'R2': np.mean([m[3] for m in metrics]),\n",
        "                        'epochs': len(history.history['loss'])\n",
        "                    })\n",
        "\n",
        "                    # Curva\n",
        "                    plt.figure()\n",
        "                    plt.plot(history.history['loss'], label='Train')\n",
        "                    plt.plot(history.history['val_loss'], label='Val')\n",
        "                    plt.title(f'{exp_name} - {model_name} - {\"NoCV\" if not USE_CROSS_VALIDATION else f\"H{h}\"}')\n",
        "                    plt.xlabel('Epoch')\n",
        "                    plt.ylabel('Loss')\n",
        "                    plt.legend()\n",
        "                    fname = f\"{exp_name.replace('+','_')}_{model_name}{postfix}.png\"\n",
        "                    plt.savefig(curves_dir / fname)\n",
        "                    plt.close()\n",
        "\n",
        "                    model.save(model_path)\n",
        "                    print(f\"üíæ Modelo guardado: {model_path.name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en '{exp_name}': {e}\\n{traceback.format_exc()}\")\n",
        "\n",
        "# ==== Guardar resultados ====\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"resultados_modelos_cv_8anios_mvp.csv\", index=False)\n",
        "print(results_df.head())\n",
        "\n",
        "import ace_tools_open as tools\n",
        "tools.display_dataframe_to_user(name=\"Resultados CV MVP\", dataframe=results_df)\n",
        "print(\"‚úÖ Proceso finalizado con √©xito.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from pathlib import Path\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import traceback\n",
        "\n",
        "# ==== Directorios y Dataset ====\n",
        "print(\"üìÇ Configurando directorios y cargando dataset...\")\n",
        "try:\n",
        "    model_output_dir_STH = Path(\"/content/drive/MyDrive/ml_precipitation_prediction/models/output/ST_HybridWaveStack\")\n",
        "    data_output_dir = Path(\"/content/drive/MyDrive/ml_precipitation_prediction/data/output\")\n",
        "    model_files = sorted([f for f in model_output_dir_STH.glob(\"*.h5\")])\n",
        "    print(f\"‚úîÔ∏è Modelos encontrados: {len(model_files)}\")\n",
        "    print(f\"üß† Ejemplos: {[m.name for m in model_files[:3]]}\")\n",
        "\n",
        "    ds_path = data_output_dir / \"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
        "    ds = xr.open_dataset(ds_path)\n",
        "    print(f\"‚úîÔ∏è Dataset cargado desde: {ds_path}\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"‚ùå Error cargando modelos o dataset: {e}\")\n",
        "\n",
        "# ==== Configuraci√≥n general ====\n",
        "input_window = 96\n",
        "horizon = 3\n",
        "\n",
        "# Configuraci√≥n de features por experimento\n",
        "experiment_settings = {\n",
        "    \"all_features\": [\n",
        "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',\n",
        "        'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag3',\n",
        "        'total_precipitation_lag4', 'total_precipitation_lag12', 'total_precipitation_lag24', 'total_precipitation_lag36',\n",
        "        'elevation', 'slope', 'aspect', 'cluster_elevation'\n",
        "    ],\n",
        "    \"time+cycles\": [\n",
        "        'year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos'\n",
        "    ]\n",
        "}\n",
        "target_var = 'total_precipitation'\n",
        "\n",
        "# ==== Funci√≥n de evaluaci√≥n ====\n",
        "def evaluate(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-5))) * 100\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, mape, r2\n",
        "\n",
        "# ==== Evaluaci√≥n de modelos ====\n",
        "results = []\n",
        "pred_map = None\n",
        "true_map = None\n",
        "mape_map = None\n",
        "\n",
        "lat = ds.latitude.size\n",
        "lon = ds.longitude.size\n",
        "n_cells = lat * lon\n",
        "\n",
        "for model_path in model_files:\n",
        "    print(f\"üîç Evaluando modelo: {model_path.name}\")\n",
        "    try:\n",
        "        name_parts = model_path.stem.split(\"_\")\n",
        "        if \"all\" in name_parts and \"features\" in name_parts:\n",
        "            experiment_key = \"all_features\"\n",
        "        elif \"time\" in name_parts and \"cycles\" in name_parts:\n",
        "            experiment_key = \"time+cycles\"\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è No se pudo mapear correctamente el experimento para {model_path.name}\")\n",
        "            continue\n",
        "\n",
        "        if experiment_key not in experiment_settings:\n",
        "            print(f\"‚ö†Ô∏è No se encontraron variables para {experiment_key}\")\n",
        "            continue\n",
        "\n",
        "        variables = experiment_settings[experiment_key]\n",
        "\n",
        "        subset = ds[variables].to_array().transpose('time', 'latitude', 'longitude', 'variable').values\n",
        "        target = ds[target_var].values\n",
        "\n",
        "        # Codificar cluster si aplica\n",
        "        cluster_idx = variables.index('cluster_elevation') if 'cluster_elevation' in variables else None\n",
        "        if cluster_idx is not None:\n",
        "            encoded = LabelEncoder().fit_transform(subset[..., cluster_idx].ravel()).reshape(subset[..., cluster_idx].shape)\n",
        "            subset[..., cluster_idx] = encoded\n",
        "\n",
        "        # Preprocesamiento\n",
        "        subset = subset.astype(np.float32)\n",
        "        samples, lat, lon, feats = subset.shape\n",
        "        X_all = subset.reshape(samples, lat * lon, feats)\n",
        "        y_all = target.reshape(samples, lat * lon)\n",
        "\n",
        "        mask = ~np.isnan(y_all)\n",
        "        X_all = X_all[mask]\n",
        "        y_all = y_all[mask]\n",
        "\n",
        "        # Generar secuencias\n",
        "        X_seq, y_seq = [], []\n",
        "        for i in range(len(X_all) - input_window - horizon):\n",
        "            X_seq.append(X_all[i:i + input_window])\n",
        "            y_seq.append(y_all[i + input_window + horizon - 1])\n",
        "        X_seq = np.array(X_seq)\n",
        "        y_seq = np.array(y_seq)\n",
        "\n",
        "        # Divisi√≥n\n",
        "        split_idx = int(len(X_seq) * 0.7)\n",
        "        X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
        "        y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
        "\n",
        "        # Escalado\n",
        "        scaler_X = StandardScaler()\n",
        "        scaler_y = StandardScaler()\n",
        "\n",
        "        X_train_flat = X_train.reshape(-1, X_train.shape[-1])\n",
        "        X_test_flat = X_test.reshape(-1, X_test.shape[-1])\n",
        "        X_train_scaled = scaler_X.fit_transform(X_train_flat).reshape(X_train.shape)\n",
        "        X_test_scaled = scaler_X.transform(X_test_flat).reshape(X_test.shape)\n",
        "\n",
        "        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "        y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Cargar y evaluar modelo\n",
        "        model = tf.keras.models.load_model(model_path, compile=False)\n",
        "        y_pred_scaled = model.predict(X_test_scaled, verbose=0).flatten()\n",
        "        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "        y_true = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Ajustar longitud si no es m√∫ltiplo del grid\n",
        "        n_valid = (len(y_pred) // n_cells) * n_cells\n",
        "        y_pred = y_pred[:n_valid]\n",
        "        y_true = y_true[:n_valid]\n",
        "\n",
        "        rmse, mae, mape, r2 = evaluate(y_true, y_pred)\n",
        "        results.append({\n",
        "            'model': model_path.name,\n",
        "            'experiment': experiment_key,\n",
        "            'RMSE': rmse,\n",
        "            'MAE': mae,\n",
        "            'MAPE (%)': np.mean(np.abs((y_true - y_pred) / (y_true + 1e-5))) * 100,\n",
        "            'R2': r2\n",
        "        })\n",
        "\n",
        "        # Guardar para mapa si a√∫n no se ha hecho\n",
        "        if pred_map is None:\n",
        "          steps = n_valid // n_cells\n",
        "          pred_map = y_pred.reshape((steps, lat, lon)).mean(axis=0)\n",
        "          true_map = y_true.reshape((steps, lat, lon)).mean(axis=0)\n",
        "          mape_map = np.abs((true_map - pred_map) / (true_map + 1e-5)) * 100\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error evaluando {model_path.name}: {traceback.format_exc()}\")\n",
        "\n",
        "# === Guardar y mostrar resultados ===\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results.to_csv(model_output_dir_STH / \"metrics_modelos_test.csv\", index=False)\n",
        "\n",
        "# Crear NetCDF y CSV de mapas si se generaron\n",
        "if pred_map is not None:\n",
        "    ds_out = xr.Dataset(\n",
        "        {\n",
        "            \"predicted_mean\": ((\"lat\", \"lon\"), pred_map),\n",
        "            \"true_mean\": ((\"lat\", \"lon\"), true_map),\n",
        "            \"mape\": ((\"lat\", \"lon\"), mape_map),\n",
        "        },\n",
        "        coords={\"lat\": ds.latitude.values, \"lon\": ds.longitude.values},\n",
        "    )\n",
        "    ds_out.to_netcdf(model_output_dir_STH / \"predictions_and_mape.nc\")\n",
        "    mape_table = pd.DataFrame(mape_map, columns=ds.longitude.values, index=ds.latitude.values)\n",
        "    mape_table.to_csv(model_output_dir_STH / \"mape_per_cell.csv\")\n",
        "\n",
        "    def plot_map(data, title, cmap='viridis', vmin=None, vmax=None):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        ax = plt.axes(projection=ccrs.PlateCarree())\n",
        "        ax.set_title(title, fontsize=14)\n",
        "        mesh = plt.pcolormesh(ds_out.lon, ds_out.lat, data, cmap=cmap,\n",
        "                              shading='auto', vmin=vmin, vmax=vmax)\n",
        "        plt.colorbar(mesh, ax=ax, orientation='vertical', label=title)\n",
        "        ax.coastlines()\n",
        "        ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
        "        ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
        "        ax.add_feature(cfeature.LAKES, edgecolor='gray')\n",
        "        ax.add_feature(cfeature.RIVERS, edgecolor='blue')\n",
        "        ax.gridlines(draw_labels=True)\n",
        "        plt.show()\n",
        "\n",
        "    # üåßÔ∏è Mapa de predicci√≥n promedio 3 meses\n",
        "    plot_map(ds_out['predicted_mean'], 'Predicci√≥n promedio - 3 meses', cmap='Blues')\n",
        "\n",
        "    # üìâ Mapa de MAPE 3 meses\n",
        "    plot_map(ds_out['mape'], 'MAPE (%) - 3 meses', cmap='Reds', vmin=0, vmax=100)\n",
        "\n",
        "import ace_tools_open as tools\n",
        "tools.display_dataframe_to_user(name=\"Resultados Test Flexible\", dataframe=df_results)\n",
        "print(\"üìä Evaluaci√≥n finalizada.\")\n"
      ],
      "metadata": {
        "id": "PszSo0yXlmyd"
      },
      "id": "PszSo0yXlmyd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}