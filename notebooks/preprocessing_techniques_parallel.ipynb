{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fpreprocessing-techniques/notebooks/preprocessing_techniques_parallel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20481089",
      "metadata": {
        "id": "20481089"
      },
      "source": [
        "# Configuración para ejecución local y en Google Colab\n",
        "Este notebook está diseñado para ejecutarse tanto en un entorno local como en Google Colab. A continuación, se incluyen las configuraciones necesarias para ambos entornos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fb418c0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fb418c0d",
        "outputId": "cbbfd712-bbee-447e-98c0-2c82b1674f8d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-69e05db21346>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mIN_COLAB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Si estamos en Colab, clonar el repositorio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Configuración del entorno (compatible con Colab y local)\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# Detectar si estamos en Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Si estamos en Colab, clonar el repositorio\n",
        "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
        "    %cd ml_precipitation_prediction\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn tqdm PyEMD pyemd EMD-signal emd PyWavelets acstools\n",
        "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
        "else:\n",
        "    # Si estamos en local, usar la ruta actual\n",
        "    if '/notebooks' in os.getcwd():\n",
        "        BASE_PATH = Path('..')\n",
        "    else:\n",
        "        BASE_PATH = Path('.')\n",
        "\n",
        "print(f\"Entorno configurado. Usando ruta base: {BASE_PATH}\")\n",
        "\n",
        "# Si BASE_PATH viene como string, lo convertimos\n",
        "BASE_PATH = Path(BASE_PATH)\n",
        "\n",
        "# Ahora puedes concatenar correctamente\n",
        "model_output_dir = BASE_PATH / 'models' / 'output'\n",
        "model_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation.nc'\n",
        "print(f\"Buscando archivo en: {data_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca01a94",
      "metadata": {
        "id": "eca01a94"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PyEMD import EMD\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "\n",
        "# 📂 Cargar dataset\n",
        "ds = xr.open_dataset(data_file)\n",
        "precip = ds[\"total_precipitation\"]\n",
        "df = precip.to_dataframe().reset_index()\n",
        "df[\"date\"] = pd.to_datetime(df[\"time\"])\n",
        "df_monthly = df.groupby(\"date\")[\"total_precipitation\"].mean().interpolate()\n",
        "\n",
        "# 🧠 Descomposición EMD\n",
        "signal = df_monthly.values\n",
        "emd = EMD()\n",
        "imfs = emd.emd(signal)\n",
        "\n",
        "# 🔢 Obtener rango de años\n",
        "start_year = df_monthly.index.min().year\n",
        "end_year = df_monthly.index.max().year\n",
        "\n",
        "# 🎨 Graficar EMD (versión corregida de layout)\n",
        "fig, axes = plt.subplots(\n",
        "    imfs.shape[0] + 1, 1,\n",
        "    figsize=(16, 3 * (imfs.shape[0] + 1)),\n",
        "    sharex=True\n",
        ")\n",
        "\n",
        "# Ponemos el título general un poco más alto\n",
        "fig.suptitle(\n",
        "    f\"Descomposición EMD de la precipitación mensual en Boyacá ({start_year}–{end_year})\",\n",
        "    fontsize=18,\n",
        "    y=0.97      # subir el suptitle\n",
        ")\n",
        "\n",
        "# Ajustamos espacio vertical entre subplots y arriba\n",
        "fig.subplots_adjust(\n",
        "    top=0.90,  # reservamos el 10% superior para el suptitle\n",
        "    hspace=0.4  # aumentamos el espacio entre filas\n",
        ")\n",
        "\n",
        "# --- resto idéntico ---\n",
        "# Serie original\n",
        "axes[0].plot(df_monthly.index, signal, color='black', label=\"Serie original\")\n",
        "axes[0].set_ylabel(\"mm\", fontsize=12)\n",
        "axes[0].set_title(\"Serie original de precipitación mensual\", fontsize=14)\n",
        "axes[0].legend(loc=\"upper right\")\n",
        "axes[0].grid(True)\n",
        "\n",
        "# IMFs\n",
        "for i in range(imfs.shape[0]):\n",
        "    axes[i+1].plot(df_monthly.index, imfs[i], label=f\"IMF {i+1}\")\n",
        "    axes[i+1].set_title(f\"IMF {i+1} - Componente Intrínseca de Modo\", fontsize=12)\n",
        "    axes[i+1].set_ylabel(\"mm\", fontsize=10)\n",
        "    axes[i+1].grid(True)\n",
        "    axes[i+1].legend(loc=\"upper right\")\n",
        "\n",
        "# Eje de fechas\n",
        "axes[-1].xaxis.set_major_locator(mdates.YearLocator(base=5))\n",
        "axes[-1].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "plt.setp(axes[-1].xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
        "\n",
        "plt.xlabel(\"Año\", fontsize=13)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b45b75f7",
      "metadata": {
        "id": "b45b75f7"
      },
      "outputs": [],
      "source": [
        "# CELDA 1: Procesamiento paralelo usando joblib para CEEMDAN y TVF-EMD y guardar archivos independientes .nc\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PyEMD import CEEMDAN\n",
        "from emd import sift\n",
        "import pywt\n",
        "from tqdm import tqdm\n",
        "import contextlib\n",
        "import io\n",
        "from joblib import Parallel, delayed\n",
        "import seaborn as sns\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Cargar dataset base\n",
        "# ----------------------------\n",
        "DATASET_PATH = data_file\n",
        "ds = xr.open_dataset(DATASET_PATH)\n",
        "precip = ds[\"total_precipitation\"]\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Estadísticas mensuales\n",
        "# ----------------------------\n",
        "time_diffs = np.diff(precip.time.values) / np.timedelta64(1, 'D')\n",
        "monthly_sum = precip if np.median(time_diffs) > 2 else precip.resample(time='1M').sum(dim='time')\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Funciones auxiliares\n",
        "# ----------------------------\n",
        "latitudes = monthly_sum.latitude.values\n",
        "longitudes = monthly_sum.longitude.values\n",
        "ceemdan = CEEMDAN()\n",
        "\n",
        "def wavelet_energy(signal, wavelet='db4', level=4):\n",
        "    coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
        "    return [np.sum(np.square(c)) for c in coeffs]\n",
        "\n",
        "def compute_imfs_and_energy(ts, method='CEEMDAN'):\n",
        "    with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n",
        "        imfs = sift.sift(ts, max_imfs=8) if method == 'TVF-EMD' else ceemdan.ceemdan(ts)\n",
        "    energy_by_imf = [wavelet_energy(imf, level=4) for imf in imfs]\n",
        "    return imfs, energy_by_imf, len(imfs)\n",
        "\n",
        "def classify_frequency(row):\n",
        "    if row['L1'] > max(row['L2'], row['L3'], row['L4'], row['Approx']):\n",
        "        return 'Alta'\n",
        "    elif row['L2'] > row['L3'] and row['L2'] > row['Approx']:\n",
        "        return 'Media'\n",
        "    else:\n",
        "        return 'Baja'\n",
        "\n",
        "# Versión modificada de la función para evitar visualizaciones dentro del proceso paralelo\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Procesamiento por método y por CHUNKS (con resume)\n",
        "# ----------------------------\n",
        "def process_method_chunked(method, chunk_latitudes):\n",
        "    # path de salida basado en método y primer latitud del chunk\n",
        "    out_fname = f\"features_{method.lower()}_chunk_{chunk_latitudes[0]:.6f}.nc\"\n",
        "    out_path = os.path.join(model_output_dir, out_fname)\n",
        "\n",
        "    # Si ya existe, lo omitimos (resume)\n",
        "    if os.path.exists(out_path):\n",
        "        print(f\"👀 Ya existe {out_fname}, se omite chunk {chunk_latitudes[0]:.6f}-{chunk_latitudes[-1]:.6f}\")\n",
        "        return out_path\n",
        "\n",
        "    records = []\n",
        "    for lat in tqdm(chunk_latitudes, desc=f\"{method}\"):\n",
        "        for lon in longitudes:\n",
        "            ts = monthly_sum.sel(latitude=lat, longitude=lon).values\n",
        "            if np.any(np.isnan(ts)):\n",
        "                continue\n",
        "            imfs, energy, n_modes = compute_imfs_and_energy(ts, method=method)\n",
        "            for idx, e in enumerate(energy):\n",
        "                if len(e) < 5:\n",
        "                    continue\n",
        "                records.append({\n",
        "                    'lat': lat, 'lon': lon, 'imf': idx + 1,\n",
        "                    'L1': e[0], 'L2': e[1], 'L3': e[2], 'L4': e[3], 'Approx': e[4],\n",
        "                    'n_imfs': n_modes, 'method': method\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    df['frecuencia'] = df.apply(classify_frequency, axis=1)\n",
        "\n",
        "    df_original = ds.to_dataframe().reset_index()\n",
        "    df_merge = pd.merge(\n",
        "        df, df_original,\n",
        "        how='left',\n",
        "        left_on=['lat', 'lon'],\n",
        "        right_on=['latitude', 'longitude']\n",
        "    )\n",
        "\n",
        "    # Eliminar duplicados exactos\n",
        "    df_merge = df_merge.drop_duplicates(subset=[\"lat\", \"lon\", \"imf\"])\n",
        "    # Verificar índice único\n",
        "    if df_merge.duplicated(subset=[\"lat\", \"lon\", \"imf\"]).any():\n",
        "        raise ValueError(\"Hay duplicados en el índice (lat, lon, imf) incluso después de deduplicar.\")\n",
        "\n",
        "    df_merge[\"imf\"] = df_merge[\"imf\"].astype(int)\n",
        "    ds_out = df_merge.set_index([\"lat\", \"lon\", \"imf\"]).to_xarray()\n",
        "\n",
        "    ds_out.to_netcdf(out_path)\n",
        "    print(f\"✅ Chunk {chunk_latitudes[0]:.6f}-{chunk_latitudes[-1]:.6f} guardado en: {out_fname}\")\n",
        "    return out_path\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Ejecutar procesamiento secuencial por chunks\n",
        "# ----------------------------\n",
        "from numpy import array_split\n",
        "\n",
        "# Divide las latitudes en 8 partes (ajustable)\n",
        "chunks = array_split(latitudes, 8)\n",
        "\n",
        "# Ahora sí puedes lanzar cada chunk en un proceso hijo:\n",
        "from multiprocessing import Process\n",
        "from tqdm import tqdm\n",
        "\n",
        "def _run_chunk(method, chunk):\n",
        "    \"\"\"\n",
        "    Wrapper que ejecuta el chunk y captura excepciones normales.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        process_method_chunked(method, chunk)\n",
        "    except Exception as e:\n",
        "        start, end = chunk[0], chunk[-1]\n",
        "        print(f\"❌ Error en chunk {start:.6f}-{end:.6f} ({method}): {e}\")\n",
        "\n",
        "# Bucle principal que lanza cada chunk en un subproceso\n",
        "import gc\n",
        "from multiprocessing import Process\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 5.1. Ejecutar procesamiento secuencial por chunks, adaptado por método\n",
        "for method in ['CEEMDAN', 'TVF-EMD']:\n",
        "    print(f\"\\n▶️ Iniciando método {method}\")\n",
        "\n",
        "    # Si es TVF-EMD, hacemos chunks ultra-pequeños (1 lat cada uno)\n",
        "    if method == 'TVF-EMD':\n",
        "        method_chunks = [[lat] for lat in latitudes]\n",
        "    else:\n",
        "        method_chunks = array_split(latitudes, 8)  # tus 8 trozos originales\n",
        "\n",
        "    for chunk in method_chunks:\n",
        "        start, end = chunk[0], chunk[-1]\n",
        "        print(f\"   • Lanzando chunk {start:.6f}-{end:.6f}...\", end=\" \")\n",
        "\n",
        "        def _run_chunk(method, chunk):\n",
        "            try:\n",
        "                process_method_chunked(method, chunk)\n",
        "            except Exception as e:\n",
        "                print(f\"\\n❌ Error en chunk {chunk[0]:.6f}-{chunk[-1]:.6f} ({method}): {e}\")\n",
        "\n",
        "        p = Process(target=_run_chunk, args=(method, chunk))\n",
        "        p.start()\n",
        "        p.join()\n",
        "\n",
        "        if p.exitcode == 0:\n",
        "            print(\"✅ OK\")\n",
        "        else:\n",
        "            print(f\"⚠️ Falló con código {p.exitcode}, se omite\")\n",
        "\n",
        "        # Liberar memoria del padre\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c1a82a",
      "metadata": {
        "id": "97c1a82a"
      },
      "outputs": [],
      "source": [
        "# Celda para cargar, comparar y graficar los resultados desde los archivos .nc generados\n",
        "\n",
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "\n",
        "# Archivos generados previamente\n",
        "nc_ceemdan = os.path.join(model_output_dir, \"features_ceemdan.nc\")\n",
        "nc_tvfemd = os.path.join(model_output_dir, \"features_tvf-emd.nc\")\n",
        "\n",
        "# Cargar datasets\n",
        "ds_ceemdan = xr.open_dataset(nc_ceemdan)\n",
        "ds_tvfemd = xr.open_dataset(nc_tvfemd)\n",
        "\n",
        "# Convertir a DataFrame\n",
        "df_ceemdan = ds_ceemdan.to_dataframe().reset_index()\n",
        "df_tvfemd = ds_tvfemd.to_dataframe().reset_index()\n",
        "\n",
        "# Visualización comparativa\n",
        "sns.set(style='whitegrid')\n",
        "for df, method in zip([df_ceemdan, df_tvfemd], ['CEEMDAN', 'TVF-EMD']):\n",
        "    for tipo in ['Alta', 'Media', 'Baja']:\n",
        "        df_filtered = df[df['frecuencia'] == tipo]\n",
        "        df_map = df_filtered.groupby(['lat', 'lon']).size().reset_index(name='conteo')\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(df_map['lon'], df_map['lat'],\n",
        "                    c=df_map['conteo'], cmap='viridis', s=80, edgecolor='k')\n",
        "        plt.colorbar(label=f'N° de IMFs {tipo}')\n",
        "        plt.title(f\"{method}: Distribución espacial de IMFs {tipo}\")\n",
        "        plt.xlabel(\"Longitud\")\n",
        "        plt.ylabel(\"Latitud\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292794e0",
      "metadata": {
        "id": "292794e0"
      },
      "outputs": [],
      "source": [
        "# CELDA 2: Fusión final en un ensemble NetCDF con CEEMDAN + TVF-EMD\n",
        "\n",
        "import xarray as xr\n",
        "import os\n",
        "\n",
        "# Archivos generados previamente\n",
        "nc_ceemdan = os.path.join(model_output_dir, \"features_ceemdan.nc\")\n",
        "nc_tvfemd = os.path.join(model_output_dir, \"features_tvf-emd.nc\")\n",
        "\n",
        "# Cargar datasets independientes\n",
        "ds_ceemdan = xr.open_dataset(nc_ceemdan)\n",
        "ds_tvfemd = xr.open_dataset(nc_tvfemd)\n",
        "\n",
        "# Fusión en ensemble\n",
        "ds_ensemble = xr.merge([ds_ceemdan, ds_tvfemd])\n",
        "\n",
        "# Guardar ensemble final\n",
        "ensemble_path = os.path.join(model_output_dir, \"ensemble_ceemdan_tvfemd.nc\")\n",
        "ds_ensemble.to_netcdf(ensemble_path)\n",
        "\n",
        "ensemble_path"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "precipitation_prediction",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}