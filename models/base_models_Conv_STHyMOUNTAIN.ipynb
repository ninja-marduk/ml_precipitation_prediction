{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────── IMPORTS ─────────────────────────\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import sys, os, gc, warnings\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, ConvLSTM2D, SimpleRNN, Flatten, Dense, Reshape,\n",
    "    Lambda, Permute, Layer, TimeDistributed\n",
    ")\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Detectar si estamos en Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Instalar dependencias solo si estamos en Colab\n",
    "if IN_COLAB:\n",
    "    print(\"🔧 Detectado Google Colab. Instalando dependencias...\")\n",
    "    try:\n",
    "        # Instalar dependencias del sistema para cartopy\n",
    "        !apt-get -qq update\n",
    "        !apt-get -qq install libproj-dev proj-data proj-bin libgeos-dev\n",
    "        \n",
    "        # Instalar paquetes Python en orden correcto\n",
    "        !pip install -q --upgrade pip\n",
    "        !pip install -q numpy pandas xarray netCDF4\n",
    "        !pip install -q matplotlib seaborn\n",
    "        !pip install -q scikit-learn\n",
    "        !pip install -q geopandas\n",
    "        !pip install -q --no-binary cartopy cartopy\n",
    "        !pip install -q imageio\n",
    "        !pip install -q optuna lightgbm xgboost\n",
    "        \n",
    "        print(\"✅ Dependencias instaladas correctamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error instalando dependencias: {e}\")\n",
    "        print(\"Continuando sin algunas dependencias opcionales...\")\n",
    "\n",
    "# Importar cartopy después de la instalación\n",
    "try:\n",
    "    import cartopy.crs as ccrs\n",
    "    CARTOPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ Cartopy no disponible. Los mapas no se mostrarán.\")\n",
    "    CARTOPY_AVAILABLE = False\n",
    "    ccrs = None\n",
    "\n",
    "# ── ConvGRU2D: Implementación robusta ───────────────────────────\n",
    "class ConvGRU2DCell(Layer):\n",
    "    \"\"\"Celda ConvGRU2D robusta y completa\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, kernel_size, padding='same', activation='tanh', \n",
    "                 recurrent_activation='sigmoid', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.padding = padding\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.recurrent_activation = tf.keras.activations.get(recurrent_activation)\n",
    "        self.state_size = (filters,)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        \n",
    "        # Kernel para input (z, r, h)\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(*self.kernel_size, input_dim, self.filters * 3),\n",
    "            initializer='glorot_uniform',\n",
    "            name='kernel'\n",
    "        )\n",
    "        \n",
    "        # Kernel recurrente (z, r, h)\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(*self.kernel_size, self.filters, self.filters * 3),\n",
    "            initializer='orthogonal',\n",
    "            name='recurrent_kernel'\n",
    "        )\n",
    "        \n",
    "        # Bias\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.filters * 3,),\n",
    "            initializer='zeros',\n",
    "            name='bias'\n",
    "        )\n",
    "        \n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, states):\n",
    "        h_tm1 = states[0]  # Estado anterior\n",
    "        \n",
    "        # Convoluciones para input\n",
    "        x_conv = K.conv2d(inputs, self.kernel, padding=self.padding)\n",
    "        x_z, x_r, x_h = tf.split(x_conv, 3, axis=-1)\n",
    "        \n",
    "        # Convoluciones para estado recurrente\n",
    "        h_conv = K.conv2d(h_tm1, self.recurrent_kernel, padding=self.padding)\n",
    "        h_z, h_r, h_h = tf.split(h_conv, 3, axis=-1)\n",
    "        \n",
    "        # Bias\n",
    "        b_z, b_r, b_h = tf.split(self.bias, 3)\n",
    "        \n",
    "        # Gates\n",
    "        z = self.recurrent_activation(x_z + h_z + b_z)  # Update gate\n",
    "        r = self.recurrent_activation(x_r + h_r + b_r)  # Reset gate\n",
    "        \n",
    "        # Candidate hidden state\n",
    "        h_candidate = self.activation(x_h + r * h_h + b_h)\n",
    "        \n",
    "        # New hidden state\n",
    "        h = (1 - z) * h_tm1 + z * h_candidate\n",
    "        \n",
    "        return h, [h]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'padding': self.padding,\n",
    "            'activation': tf.keras.activations.serialize(self.activation),\n",
    "            'recurrent_activation': tf.keras.activations.serialize(self.recurrent_activation)\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class ConvGRU2D(Layer):\n",
    "    \"\"\"ConvGRU2D completo con soporte para return_sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, kernel_size, padding='same', activation='tanh',\n",
    "                 recurrent_activation='sigmoid', return_sequences=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.return_sequences = return_sequences\n",
    "        self.cell = ConvGRU2DCell(\n",
    "            filters, kernel_size, padding, activation, recurrent_activation\n",
    "        )\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.cell.build(input_shape[2:])  # Sin batch y time\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, time, height, width, channels)\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        time_steps = tf.shape(inputs)[1]\n",
    "        height = tf.shape(inputs)[2]\n",
    "        width = tf.shape(inputs)[3]\n",
    "        \n",
    "        # Estado inicial\n",
    "        initial_state = tf.zeros((batch_size, height, width, self.filters))\n",
    "        \n",
    "        # Procesar secuencia\n",
    "        outputs = []\n",
    "        state = initial_state\n",
    "        \n",
    "        for t in range(inputs.shape[1]):\n",
    "            output, [state] = self.cell(inputs[:, t], [state])\n",
    "            outputs.append(output)\n",
    "        \n",
    "        outputs = tf.stack(outputs, axis=1)\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return outputs\n",
    "        else:\n",
    "            return outputs[:, -1]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'padding': self.padding,\n",
    "            'activation': self.activation,\n",
    "            'recurrent_activation': self.recurrent_activation,\n",
    "            'return_sequences': self.return_sequences\n",
    "        })\n",
    "        return config\n",
    "\n",
    "print(\"✅ ConvGRU2D implementado de forma robusta\")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt, seaborn as sns, geopandas as gpd, imageio.v2 as imageio\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid'); sns.set_context('notebook')\n",
    "\n",
    "# ───────────────────────── ENTORNO / GPU ─────────────────────────\n",
    "## ╭─────────────────────────── Rutas ──────────────────────────╮\n",
    "# ▶️ Path configuration\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    # Instalar dependencias necesarias\n",
    "    %pip install -r requirements.txt\n",
    "    %pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
    "        if (p / '.git').exists():\n",
    "            BASE_PATH = p; break\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# limitar crecimiento de memoria‑GPU (evita OOM)\n",
    "for g in tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "# ───────────────────────── PATHS & CONST ─────────────────────────\n",
    "DATA_FILE = BASE_PATH/'data'/'output'/(\n",
    "    'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc')\n",
    "OUT_ROOT  = BASE_PATH/'models'/'output'/'Spatial_CONVRNN'\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SHAPE_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
    "DEPT_GDF   = gpd.read_file(SHAPE_DIR/'MGN_Departamento.shp')\n",
    "\n",
    "INPUT_WINDOW = 60\n",
    "HORIZON = 3 \n",
    "EPOCHS = 50\n",
    "BATCH = 4\n",
    "LR = 1e-3\n",
    "PATIENCE = 6\n",
    "\n",
    "# ───────────────────────── FEATURE SETS ─────────────────────────\n",
    "BASE_FEATS = ['year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
    "              'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
    "              'elevation','slope','aspect']\n",
    "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
    "KCE_FEATS = BASE_FEATS + ELEV_CLUSTER\n",
    "PAFC_FEATS= KCE_FEATS + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
    "EXPERIMENTS = {'BASIC':BASE_FEATS,'KCE':KCE_FEATS,'PAFC':PAFC_FEATS}\n",
    "\n",
    "# ───────────────────────── DATASET ─────────────────────────\n",
    "ds = xr.open_dataset(DATA_FILE)\n",
    "lat, lon = len(ds.latitude), len(ds.longitude)\n",
    "print(f\"Dataset → time={len(ds.time)}, lat={lat}, lon={lon}\")\n",
    "\n",
    "# ───────────────────────── HELPERS ─────────────────────────\n",
    "\n",
    "def windowed_arrays(X:np.ndarray, y:np.ndarray):\n",
    "    seq_X, seq_y = [], []\n",
    "    T = len(X)\n",
    "    for start in range(T-INPUT_WINDOW-HORIZON+1):\n",
    "        end_w = start+INPUT_WINDOW; end_y=end_w+HORIZON\n",
    "        Xw, yw = X[start:end_w], y[end_w:end_y]\n",
    "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
    "            continue\n",
    "        seq_X.append(Xw); seq_y.append(yw)\n",
    "    return np.asarray(seq_X,dtype=np.float32), np.asarray(seq_y,dtype=np.float32)\n",
    "\n",
    "def quick_plot(ax,data,cmap,title,vmin=None,vmax=None):\n",
    "    if CARTOPY_AVAILABLE and ccrs is not None:\n",
    "        # Versión con cartopy\n",
    "        mesh = ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',\n",
    "                             vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
    "        ax.coastlines()\n",
    "        try:\n",
    "            ax.add_geometries(DEPT_GDF.geometry,ccrs.PlateCarree(),\n",
    "                             edgecolor='black',facecolor='none',linewidth=1)\n",
    "        except:\n",
    "            pass\n",
    "        ax.gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
    "    else:\n",
    "        # Versión sin cartopy\n",
    "        mesh = ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',\n",
    "                             vmin=vmin,vmax=vmax)\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "    ax.set_title(title,fontsize=9)\n",
    "    return mesh\n",
    "\n",
    "# ───────────────────────── LIGHTWEIGHT HEAD ─────────────────────────\n",
    "\n",
    "def _spatial_head(x):\n",
    "    \"\"\"Proyección 1×1 → (B, H,lat,lon,1) con *shape hints* para que\n",
    "    Keras pueda reconstruir la capa `Lambda` al volver a cargar el modelo.\n",
    "    \"\"\"\n",
    "    #   1) Conv 1×1 que genera H mapas (uno por horizonte)\n",
    "    x = Conv2D(\n",
    "        HORIZON,\n",
    "        (1, 1),\n",
    "        padding=\"same\",\n",
    "        activation=\"linear\",\n",
    "        name=\"head_conv1x1\",\n",
    "    )(x)  # ==> (B, lat, lon, H)\n",
    "\n",
    "    #   2) Transponemos a (B, H, lat, lon)\n",
    "    x = Lambda(\n",
    "        lambda t: tf.transpose(t, [0, 3, 1, 2]),\n",
    "        output_shape=(HORIZON, lat, lon),\n",
    "        name=\"head_transpose\",\n",
    "    )(x)\n",
    "\n",
    "    #   3) Añadimos eje canales: (B, H, lat, lon, 1)\n",
    "    x = Lambda(\n",
    "        lambda t: tf.expand_dims(t, -1),\n",
    "        output_shape=(HORIZON, lat, lon, 1),\n",
    "        name=\"head_expand_dim\",\n",
    "    )(x)\n",
    "    return x\n",
    "\n",
    "# ───────────────────────── MODEL FACTORIES ─────────────────────────\n",
    "\n",
    "def build_conv_lstm(n_feats:int):\n",
    "    inp = Input(shape=(INPUT_WINDOW,lat,lon,n_feats))\n",
    "    x   = ConvLSTM2D(32,(3,3),padding='same',return_sequences=True)(inp)\n",
    "    x   = ConvLSTM2D(16,(3,3),padding='same',return_sequences=False)(x)\n",
    "    out = _spatial_head(x)\n",
    "    return Model(inp, out, name='ConvLSTM')\n",
    "\n",
    "def build_conv_gru(n_feats: int):\n",
    "    \"\"\"\n",
    "    Construye modelo ConvGRU usando nuestra implementación robusta\n",
    "    \"\"\"\n",
    "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
    "    \n",
    "    # Usar nuestra implementación de ConvGRU2D\n",
    "    x = ConvGRU2D(32, (3, 3), padding=\"same\", return_sequences=True)(inp)\n",
    "    x = ConvGRU2D(16, (3, 3), padding=\"same\", return_sequences=False)(x)\n",
    "    \n",
    "    out = _spatial_head(x)\n",
    "    return Model(inp, out, name=\"ConvGRU\")\n",
    "\n",
    "def build_conv_rnn(n_feats:int):\n",
    "    \"\"\"\n",
    "    Modelo ConvRNN corregido: procesa secuencias temporales de imágenes\n",
    "    \"\"\"\n",
    "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
    "    \n",
    "    # Opción 1: Usar TimeDistributed para procesar cada frame\n",
    "    # Aplicar convolución a cada timestep\n",
    "    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(inp)\n",
    "    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(x)\n",
    "    \n",
    "    # Aplanar cada frame para pasarlo por RNN\n",
    "    x = TimeDistributed(Flatten())(x)  # (batch, time, features)\n",
    "    \n",
    "    # RNN sobre la secuencia temporal\n",
    "    x = SimpleRNN(128, activation='tanh', return_sequences=False)(x)\n",
    "    \n",
    "    # Proyectar a la salida deseada\n",
    "    x = Dense(HORIZON * lat * lon)(x)\n",
    "    out = Reshape((HORIZON, lat, lon, 1))(x)\n",
    "    \n",
    "    return Model(inp, out, name='ConvRNN')\n",
    "\n",
    "MODELS = {'ConvLSTM': build_conv_lstm, 'ConvGRU': build_conv_gru, 'ConvRNN': build_conv_rnn}\n",
    "\n",
    "# ───────────────────────── TRAIN + EVAL LOOP ─────────────────────────\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, CSVLogger, Callback\n",
    "import json\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Callback personalizado para visualización en tiempo real\n",
    "class TrainingMonitor(Callback):\n",
    "    \"\"\"Callback para monitorear el entrenamiento en tiempo real\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, experiment_name):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.experiment_name = experiment_name\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.lrs = []\n",
    "        self.epochs = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Guardar métricas\n",
    "        self.epochs.append(epoch + 1)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "        # Obtener learning rate actual\n",
    "        if hasattr(self.model.optimizer, 'learning_rate'):\n",
    "            try:\n",
    "                lr = float(K.get_value(self.model.optimizer.learning_rate))\n",
    "            except:\n",
    "                lr = float(self.model.optimizer.learning_rate)\n",
    "        else:\n",
    "            lr = logs.get('lr', 0.001)  # Valor por defecto si no se puede obtener\n",
    "        \n",
    "        self.lrs.append(lr)\n",
    "        \n",
    "        # Limpiar output anterior\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Crear visualización\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot de losses\n",
    "        ax1.plot(self.epochs, self.losses, 'b-', label='Train Loss', linewidth=2)\n",
    "        ax1.plot(self.epochs, self.val_losses, 'r-', label='Val Loss', linewidth=2)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title(f'{self.model_name} - {self.experiment_name} - Training Progress')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot de tasa de mejora y convergencia\n",
    "        if len(self.val_losses) > 1:\n",
    "            # Calcular tasa de mejora epoch a epoch\n",
    "            improvements = []\n",
    "            for i in range(1, len(self.val_losses)):\n",
    "                prev_loss = self.val_losses[i-1]\n",
    "                curr_loss = self.val_losses[i]\n",
    "                improvement = ((prev_loss - curr_loss) / prev_loss) * 100\n",
    "                improvements.append(improvement)\n",
    "            \n",
    "            # Plot de tasa de mejora\n",
    "            ax2.plot(self.epochs[1:], improvements, 'g-', linewidth=2, alpha=0.7)\n",
    "            ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "            ax2.fill_between(self.epochs[1:], improvements, 0, \n",
    "                           where=[x > 0 for x in improvements], \n",
    "                           color='green', alpha=0.3, label='Mejora')\n",
    "            ax2.fill_between(self.epochs[1:], improvements, 0, \n",
    "                           where=[x <= 0 for x in improvements], \n",
    "                           color='red', alpha=0.3, label='Empeoramiento')\n",
    "            \n",
    "            # Línea de tendencia suavizada\n",
    "            if len(improvements) > 5:\n",
    "                window = min(5, len(improvements)//3)\n",
    "                smoothed = pd.Series(improvements).rolling(window=window, center=True).mean()\n",
    "                ax2.plot(self.epochs[1:], smoothed, 'b-', linewidth=2.5, \n",
    "                        label=f'Tendencia ({window} epochs)')\n",
    "            \n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Tasa de Mejora (%)')\n",
    "            ax2.set_title('Progreso del Entrenamiento')\n",
    "            ax2.legend(loc='best')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Añadir anotación de convergencia\n",
    "            if len(improvements) > 10:\n",
    "                recent_avg = np.mean(improvements[-5:])\n",
    "                if abs(recent_avg) < 0.5:\n",
    "                    ax2.text(0.95, 0.95, '⚠️ Posible convergencia', \n",
    "                            transform=ax2.transAxes, ha='right', va='top',\n",
    "                            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'Esperando más épocas...', \n",
    "                    transform=ax2.transAxes, ha='center', va='center',\n",
    "                    fontsize=12, color='gray')\n",
    "            ax2.set_title('Progreso del Entrenamiento')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "        plt.close()\n",
    "        \n",
    "        # Mostrar métricas actuales\n",
    "        print(f\"\\n📊 Época {epoch + 1}/{self.params['epochs']}\")\n",
    "        print(f\"   • Loss: {logs.get('loss'):.6f}\")\n",
    "        print(f\"   • Val Loss: {logs.get('val_loss'):.6f}\")\n",
    "        print(f\"   • MAE: {logs.get('mae'):.6f}\")\n",
    "        print(f\"   • Val MAE: {logs.get('val_mae'):.6f}\")\n",
    "        print(f\"   • Learning Rate: {self.lrs[-1]:.2e}\")\n",
    "        \n",
    "        # Mostrar mejora\n",
    "        if len(self.val_losses) > 1:\n",
    "            improvement = (self.val_losses[-2] - self.val_losses[-1]) / self.val_losses[-2] * 100\n",
    "            print(f\"   • Mejora: {improvement:.2f}%\")\n",
    "\n",
    "# Diccionario para almacenar historiales de entrenamiento\n",
    "all_histories = {}\n",
    "results = []\n",
    "\n",
    "# Función para guardar hiperparámetros\n",
    "def save_hyperparameters(exp_path, model_name, hyperparams):\n",
    "    \"\"\"Guarda los hiperparámetros en un archivo JSON\"\"\"\n",
    "    hp_file = exp_path / f\"{model_name}_hyperparameters.json\"\n",
    "    with open(hp_file, 'w') as f:\n",
    "        json.dump(hyperparams, f, indent=4)\n",
    "    print(f\"   💾 Hiperparámetros guardados en: {hp_file.name}\")\n",
    "\n",
    "# Función para plotear curvas de aprendizaje\n",
    "def plot_learning_curves(history, exp_path, model_name, show=True):\n",
    "    \"\"\"Genera y guarda las curvas de aprendizaje\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss (MSE)')\n",
    "    axes[0].set_title(f'{model_name} - Loss Evolution')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Análisis de Convergencia y Estabilidad\n",
    "    val_losses = history.history['val_loss']\n",
    "    train_losses = history.history['loss']\n",
    "    \n",
    "    if len(val_losses) > 1:\n",
    "        # Calcular métricas de convergencia\n",
    "        epochs = range(1, len(val_losses) + 1)\n",
    "        \n",
    "        # 1. Ratio de overfitting\n",
    "        overfit_ratio = [val_losses[i] / train_losses[i] for i in range(len(val_losses))]\n",
    "        \n",
    "        # 2. Estabilidad (desviación estándar móvil)\n",
    "        window = min(5, len(val_losses)//3)\n",
    "        val_std = pd.Series(val_losses).rolling(window=window).std()\n",
    "        \n",
    "        # Crear subplot con dos ejes Y\n",
    "        ax2_left = axes[1]\n",
    "        ax2_right = ax2_left.twinx()\n",
    "        \n",
    "        # Plot ratio de overfitting\n",
    "        line1 = ax2_left.plot(epochs, overfit_ratio, 'r-', linewidth=2, \n",
    "                             label='Ratio Val/Train', alpha=0.8)\n",
    "        ax2_left.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
    "        ax2_left.fill_between(epochs, 1.0, overfit_ratio, \n",
    "                            where=[x > 1.0 for x in overfit_ratio],\n",
    "                            color='red', alpha=0.2)\n",
    "        ax2_left.set_xlabel('Epoch')\n",
    "        ax2_left.set_ylabel('Ratio Val Loss / Train Loss', color='red')\n",
    "        ax2_left.tick_params(axis='y', labelcolor='red')\n",
    "        \n",
    "        # Plot estabilidad\n",
    "        line2 = ax2_right.plot(epochs[window-1:], val_std[window-1:], 'b-', \n",
    "                             linewidth=2, label='Estabilidad', alpha=0.8)\n",
    "        ax2_right.set_ylabel('Desviación Estándar (ventana móvil)', color='blue')\n",
    "        ax2_right.tick_params(axis='y', labelcolor='blue')\n",
    "        \n",
    "        # Título y leyenda combinada\n",
    "        ax2_left.set_title(f'{model_name} - Análisis de Convergencia')\n",
    "        \n",
    "        # Combinar leyendas\n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax2_left.legend(lines, labels, loc='upper left')\n",
    "        \n",
    "        ax2_left.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Añadir zonas de interpretación\n",
    "        if max(overfit_ratio) > 1.5:\n",
    "            ax2_left.text(0.02, 0.98, '⚠️ Alto overfitting detectado', \n",
    "                        transform=ax2_left.transAxes, va='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
    "        elif min(val_std[window-1:]) < 0.001:\n",
    "            ax2_left.text(0.02, 0.98, '✓ Entrenamiento estable', \n",
    "                        transform=ax2_left.transAxes, va='top',\n",
    "                        bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'Insufficient data for convergence analysis', \n",
    "                    transform=axes[1].transAxes, ha='center', va='center',\n",
    "                    fontsize=12, color='gray')\n",
    "        axes[1].set_title(f'{model_name} - Convergence Analysis')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    curves_path = exp_path / f\"{model_name}_learning_curves.png\"\n",
    "    plt.savefig(curves_path, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    return curves_path\n",
    "\n",
    "# Función para mostrar resumen de entrenamiento\n",
    "def print_training_summary(history, model_name, exp_name):\n",
    "    \"\"\"Imprime un resumen del entrenamiento\"\"\"\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    best_val_loss = min(history.history['val_loss'])\n",
    "    best_epoch = history.history['val_loss'].index(best_val_loss) + 1\n",
    "    \n",
    "    print(f\"\\n   📊 Resumen de entrenamiento {model_name} - {exp_name}:\")\n",
    "    print(f\"      • Épocas totales: {len(history.history['loss'])}\")\n",
    "    print(f\"      • Loss final (train): {final_loss:.6f}\")\n",
    "    print(f\"      • Loss final (val): {final_val_loss:.6f}\")\n",
    "    print(f\"      • Mejor loss (val): {best_val_loss:.6f} en época {best_epoch}\")\n",
    "    if 'lr' in history.history and len(history.history['lr']) > 0:\n",
    "        final_lr = history.history['lr'][-1]\n",
    "        print(f\"      • Learning rate final: {final_lr:.2e}\")\n",
    "    else:\n",
    "        print(f\"      • Learning rate final: No disponible\")\n",
    "\n",
    "for exp, feat_list in EXPERIMENTS.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🔬 EXPERIMENTO: {exp} ({len(feat_list)} features)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Preparar datos\n",
    "    Xarr = ds[feat_list].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
    "    yarr = ds['total_precipitation'].values.astype(np.float32)[...,None]\n",
    "    X, y = windowed_arrays(Xarr, yarr)\n",
    "    split = int(0.8*len(X))\n",
    "\n",
    "    sx = StandardScaler().fit(X[:split].reshape(-1,len(feat_list)))\n",
    "    sy = StandardScaler().fit(y[:split].reshape(-1,1))\n",
    "    X_sc = sx.transform(X.reshape(-1,len(feat_list))).reshape(X.shape)\n",
    "    y_sc = sy.transform(y.reshape(-1,1)).reshape(y.shape)\n",
    "    X_tr, X_va = X_sc[:split], X_sc[split:]\n",
    "    y_tr, y_va = y_sc[:split], y_sc[split:]\n",
    "\n",
    "    OUT_EXP = OUT_ROOT/exp\n",
    "    OUT_EXP.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Crear subdirectorio para métricas de entrenamiento\n",
    "    METRICS_DIR = OUT_EXP / 'training_metrics'\n",
    "    METRICS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    for mdl_name, builder in MODELS.items():\n",
    "        print(f\"\\n{'─'*50}\")\n",
    "        print(f\"🤖 Modelo: {mdl_name}\")\n",
    "        print(f\"{'─'*50}\")\n",
    "        \n",
    "        model_path = OUT_EXP/f\"{mdl_name.lower()}_best.keras\"\n",
    "        if model_path.exists():\n",
    "            model_path.unlink()\n",
    "        \n",
    "        try:\n",
    "            # Construir modelo\n",
    "            model = builder(n_feats=len(feat_list))\n",
    "            \n",
    "            # Definir optimizador con configuración explícita\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "            model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "            \n",
    "            # Hiperparámetros\n",
    "            hyperparams = {\n",
    "                'experiment': exp,\n",
    "                'model': mdl_name,\n",
    "                'features': feat_list,\n",
    "                'n_features': len(feat_list),\n",
    "                'input_window': INPUT_WINDOW,\n",
    "                'horizon': HORIZON,\n",
    "                'batch_size': BATCH,\n",
    "                'initial_lr': LR,\n",
    "                'epochs': EPOCHS,\n",
    "                'patience': PATIENCE,\n",
    "                'train_samples': len(X_tr),\n",
    "                'val_samples': len(X_va),\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'model_params': model.count_params()\n",
    "            }\n",
    "            \n",
    "            # Guardar hiperparámetros\n",
    "            save_hyperparameters(METRICS_DIR, mdl_name, hyperparams)\n",
    "            \n",
    "            # Callbacks mejorados\n",
    "            csv_logger = CSVLogger(\n",
    "                METRICS_DIR / f\"{mdl_name}_training_log.csv\",\n",
    "                separator=',',\n",
    "                append=False\n",
    "            )\n",
    "            \n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=PATIENCE//2,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            early_stop = EarlyStopping(\n",
    "                'val_loss',\n",
    "                patience=PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            checkpoint = ModelCheckpoint(\n",
    "                model_path,\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss',\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Agregar monitor de entrenamiento\n",
    "            training_monitor = TrainingMonitor(mdl_name, exp)\n",
    "            \n",
    "            callbacks = [early_stop, checkpoint, reduce_lr, csv_logger, training_monitor]\n",
    "            \n",
    "            # Entrenar con verbose=0 para usar nuestro monitor personalizado\n",
    "            print(f\"\\n🏃 Iniciando entrenamiento...\")\n",
    "            print(f\"   📊 Visualización en tiempo real activada\")\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_tr, y_tr,\n",
    "                validation_data=(X_va, y_va),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0  # Usar 0 para que solo se muestre nuestro monitor\n",
    "            )\n",
    "            \n",
    "            # Guardar historial\n",
    "            all_histories[f\"{exp}_{mdl_name}\"] = history\n",
    "            \n",
    "            # Mostrar resumen de entrenamiento\n",
    "            print_training_summary(history, mdl_name, exp)\n",
    "            \n",
    "            # Plotear y guardar curvas de aprendizaje\n",
    "            plot_learning_curves(history, METRICS_DIR, mdl_name, show=True)\n",
    "            \n",
    "            # Guardar historial como JSON\n",
    "            # Obtener learning rates del monitor de entrenamiento si no están en history\n",
    "            lr_values = history.history.get('lr', [])\n",
    "            if not lr_values and hasattr(training_monitor, 'lrs'):\n",
    "                lr_values = training_monitor.lrs\n",
    "            \n",
    "            history_dict = {\n",
    "                'loss': [float(x) for x in history.history['loss']],\n",
    "                'val_loss': [float(x) for x in history.history['val_loss']],\n",
    "                'mae': [float(x) for x in history.history.get('mae', [])],\n",
    "                'val_mae': [float(x) for x in history.history.get('val_mae', [])],\n",
    "                'lr': [float(x) for x in lr_values] if lr_values else []\n",
    "            }\n",
    "            \n",
    "            with open(METRICS_DIR / f\"{mdl_name}_history.json\", 'w') as f:\n",
    "                json.dump(history_dict, f, indent=4)\n",
    "\n",
    "            # ─ Predicciones y visualización ─\n",
    "            print(f\"\\n🎯 Generando predicciones...\")\n",
    "            y_hat_sc = model.predict(X_va[-1:], verbose=0)\n",
    "            y_hat = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
    "            y_true = sy.inverse_transform(y_va[-1:].reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
    "\n",
    "            # ─ Mapas & GIF ─\n",
    "            vmin, vmax = 0, max(y_true.max(), y_hat.max())\n",
    "            frames = []\n",
    "            dates = pd.date_range(ds.time.values[-HORIZON], periods=HORIZON, freq='MS')\n",
    "            \n",
    "            for h in range(HORIZON):\n",
    "                err = np.clip(np.abs((y_true[h]-y_hat[h])/(y_true[h]+1e-5))*100, 0, 100)\n",
    "                fig, axs = plt.subplots(1, 3, figsize=(12, 4), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "                quick_plot(axs[0], y_true[h], 'Blues', f\"Real h={h+1}\", vmin, vmax)\n",
    "                quick_plot(axs[1], y_hat[h], 'Blues', f\"{mdl_name} h={h+1}\", vmin, vmax)\n",
    "                quick_plot(axs[2], err, 'Reds', f\"MAPE% h={h+1}\", 0, 100)\n",
    "                fig.suptitle(f\"{mdl_name} – {exp} – {dates[h].strftime('%Y-%m')}\")\n",
    "                png = OUT_EXP/f\"{mdl_name}_{h+1}.png\"\n",
    "                fig.savefig(png, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "                frames.append(imageio.imread(png))\n",
    "            \n",
    "            imageio.mimsave(OUT_EXP/f\"{mdl_name}.gif\", frames, fps=0.5)\n",
    "\n",
    "            # ─ Métricas de evaluación ─\n",
    "            for h in range(HORIZON):\n",
    "                rmse = np.sqrt(mean_squared_error(y_true[h].ravel(), y_hat[h].ravel()))\n",
    "                mae = mean_absolute_error(y_true[h].ravel(), y_hat[h].ravel())\n",
    "                r2 = r2_score(y_true[h].ravel(), y_hat[h].ravel())\n",
    "                \n",
    "                results.append({\n",
    "                    'Experiment': exp,\n",
    "                    'Model': mdl_name,\n",
    "                    'H': h+1,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAE': mae,\n",
    "                    'R2': r2\n",
    "                })\n",
    "                \n",
    "                print(f\"   📈 H={h+1}: RMSE={rmse:.4f}, MAE={mae:.4f}, R²={r2:.4f}\")\n",
    "            \n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Error en {mdl_name}: {str(e)}\")\n",
    "            print(f\"  → Saltando {mdl_name} para {exp}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "# ───────────────────────── CSV FINAL ─────────────────────────\n",
    "res_df=pd.DataFrame(results)\n",
    "res_df.to_csv(OUT_ROOT/'metrics_spatial.csv',index=False)\n",
    "print(\"\\n📑 Metrics saved →\", OUT_ROOT/'metrics_spatial.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────── VISUALIZACIÓN COMPARATIVA ─────────────────────────\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 GENERANDO VISUALIZACIONES COMPARATIVAS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear directorio para comparaciones\n",
    "COMP_DIR = OUT_ROOT / 'comparisons'\n",
    "COMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. Comparación de métricas entre modelos\n",
    "if res_df is not None and len(res_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # RMSE por modelo y experimento\n",
    "    pivot_rmse = res_df.pivot_table(values='RMSE', index='Model', columns='Experiment', aggfunc='mean')\n",
    "    pivot_rmse.plot(kind='bar', ax=axes[0,0])\n",
    "    axes[0,0].set_title('RMSE Promedio por Modelo y Experimento', fontsize=14, pad=10)\n",
    "    axes[0,0].set_ylabel('RMSE')\n",
    "    axes[0,0].set_xlabel('Modelo')\n",
    "    axes[0,0].legend(title='Experimento', loc='upper left')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # MAE por modelo y experimento\n",
    "    pivot_mae = res_df.pivot_table(values='MAE', index='Model', columns='Experiment', aggfunc='mean')\n",
    "    pivot_mae.plot(kind='bar', ax=axes[0,1])\n",
    "    axes[0,1].set_title('MAE Promedio por Modelo y Experimento', fontsize=14, pad=10)\n",
    "    axes[0,1].set_ylabel('MAE')\n",
    "    axes[0,1].set_xlabel('Modelo')\n",
    "    axes[0,1].legend(title='Experimento', loc='upper left')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # R² por modelo y experimento\n",
    "    pivot_r2 = res_df.pivot_table(values='R2', index='Model', columns='Experiment', aggfunc='mean')\n",
    "    pivot_r2.plot(kind='bar', ax=axes[1,0])\n",
    "    axes[1,0].set_title('R² Promedio por Modelo y Experimento', fontsize=14, pad=10)\n",
    "    axes[1,0].set_ylabel('R²')\n",
    "    axes[1,0].set_xlabel('Modelo')\n",
    "    axes[1,0].legend(title='Experimento', loc='lower right')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Métricas por horizonte - Crear subplot separado para mejor visualización\n",
    "    ax_horizon = axes[1,1]\n",
    "    \n",
    "    # Limpiar el eje antes de plotear\n",
    "    ax_horizon.clear()\n",
    "    \n",
    "    # Colores distintos para cada métrica\n",
    "    colors_metrics = {'RMSE': 'tab:blue', 'MAE': 'tab:orange', 'R2': 'tab:green'}\n",
    "    \n",
    "    # Plotear cada métrica por separado con su propia escala\n",
    "    for i, metric in enumerate(['RMSE', 'MAE', 'R2']):\n",
    "        if i == 0:\n",
    "            ax = ax_horizon\n",
    "        else:\n",
    "            ax = ax_horizon.twinx()\n",
    "            if i == 2:\n",
    "                # Mover el segundo eje Y a la derecha\n",
    "                ax.spines['right'].set_position(('outward', 60))\n",
    "        \n",
    "        data = res_df.groupby(['H', 'Model'])[metric].mean().unstack()\n",
    "        \n",
    "        for model in data.columns:\n",
    "            line = ax.plot(data.index, data[model], \n",
    "                          marker='o', \n",
    "                          label=f'{model} ({metric})',\n",
    "                          color=colors_metrics[metric],\n",
    "                          alpha=0.7 + 0.1*list(data.columns).index(model),\n",
    "                          linewidth=2)\n",
    "        \n",
    "        ax.set_ylabel(metric, color=colors_metrics[metric])\n",
    "        ax.tick_params(axis='y', labelcolor=colors_metrics[metric])\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.set_xlabel('Horizonte (meses)')\n",
    "            ax.set_title('Evolución de Métricas por Horizonte', fontsize=14, pad=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Crear leyenda personalizada fuera del gráfico\n",
    "    handles, labels = [], []\n",
    "    for model in res_df['Model'].unique(): \n",
    "        handles.append(plt.Line2D([0], [0], marker='o', color='gray', \n",
    "                                 label=model, markersize=8, linewidth=2))\n",
    "    \n",
    "    ax_horizon.legend(handles=handles, title='Modelo', \n",
    "                     bbox_to_anchor=(1.15, 0.5), loc='center left')\n",
    "    \n",
    "    # Ajustar espaciado\n",
    "    plt.subplots_adjust(hspace=0.3, wspace=0.4, right=0.85)\n",
    "    plt.savefig(COMP_DIR / 'metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 2. Tabla resumen de mejores modelos\n",
    "print(\"\\n📋 TABLA RESUMEN - MEJORES MODELOS POR EXPERIMENTO:\")\n",
    "print(\"─\" * 60)\n",
    "\n",
    "best_models = res_df.groupby('Experiment').apply(\n",
    "    lambda x: x.loc[x['RMSE'].idxmin()]\n",
    ")[['Model', 'RMSE', 'MAE', 'R2']]\n",
    "\n",
    "print(best_models.to_string())\n",
    "\n",
    "# 3. Comparación de curvas de aprendizaje\n",
    "if all_histories:\n",
    "    n_experiments = len(all_histories)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_experiments + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 6 * n_rows))\n",
    "    \n",
    "    # Asegurar que axes sea siempre un array 2D\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    elif n_cols == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    # Flatten para iterar más fácilmente\n",
    "    axes_flat = axes.flatten()\n",
    "    \n",
    "    for idx, (key, history) in enumerate(all_histories.items()):\n",
    "        if idx < len(axes_flat):\n",
    "            ax = axes_flat[idx]\n",
    "            \n",
    "            # Plot con estilo mejorado\n",
    "            epochs = range(1, len(history.history['loss']) + 1)\n",
    "            \n",
    "            ax.plot(epochs, history.history['loss'], \n",
    "                   'b-', label='Train Loss', linewidth=2.5, alpha=0.8)\n",
    "            ax.plot(epochs, history.history['val_loss'], \n",
    "                   'r-', label='Val Loss', linewidth=2.5, alpha=0.8)\n",
    "            \n",
    "            # Marcar el mejor punto\n",
    "            best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "            best_val_loss = min(history.history['val_loss'])\n",
    "            ax.plot(best_epoch, best_val_loss, 'r*', markersize=15, \n",
    "                   label=f'Best: {best_val_loss:.4f}')\n",
    "            \n",
    "            ax.set_xlabel('Epoch', fontsize=11)\n",
    "            ax.set_ylabel('Loss', fontsize=11)\n",
    "            ax.set_title(f'{key}', fontsize=13, fontweight='bold', pad=10)\n",
    "            ax.legend(loc='upper right', frameon=True, fancybox=True, shadow=True)\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            \n",
    "            # Añadir anotación con información adicional\n",
    "            final_train = history.history['loss'][-1]\n",
    "            final_val = history.history['val_loss'][-1]\n",
    "            ax.text(0.02, 0.98, f'Final: {final_train:.4f}/{final_val:.4f}',\n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n",
    "                   fontsize=9)\n",
    "    \n",
    "    # Ocultar ejes no utilizados\n",
    "    for idx in range(len(all_histories), len(axes_flat)):\n",
    "        axes_flat[idx].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Curvas de Aprendizaje - Todos los Experimentos', \n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(COMP_DIR / 'all_learning_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 4. Resumen de hiperparámetros y tiempos de entrenamiento\n",
    "print(\"\\n⏱️ RESUMEN DE ENTRENAMIENTO:\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "for exp in EXPERIMENTS.keys():\n",
    "    metrics_dir = OUT_ROOT / exp / 'training_metrics'\n",
    "    if metrics_dir.exists():\n",
    "        print(f\"\\n🔬 Experimento: {exp}\")\n",
    "        for model in MODELS.keys():\n",
    "            hp_file = metrics_dir / f\"{model}_hyperparameters.json\"\n",
    "            history_file = metrics_dir / f\"{model}_history.json\"\n",
    "            \n",
    "            if hp_file.exists() and history_file.exists():\n",
    "                with open(hp_file, 'r') as f:\n",
    "                    hp = json.load(f)\n",
    "                with open(history_file, 'r') as f:\n",
    "                    hist = json.load(f)\n",
    "                \n",
    "                print(f\"\\n   • {model}:\")\n",
    "                print(f\"     - Parámetros del modelo: {hp['model_params']:,}\")\n",
    "                print(f\"     - Épocas entrenadas: {len(hist['loss'])}\")\n",
    "                print(f\"     - Mejor loss validación: {min(hist['val_loss']):.6f}\")\n",
    "                # Verificar que 'lr' existe y tiene valores antes de acceder\n",
    "                if 'lr' in hist and len(hist['lr']) > 0:\n",
    "                    print(f\"     - Learning rate final: {hist['lr'][-1]}\")\n",
    "                else:\n",
    "                    print(f\"     - Learning rate final: N/A\")\n",
    "\n",
    "# 5. Generar GIF comparativo de predicciones\n",
    "print(\"\\n🎬 Generando GIF comparativo...\")\n",
    "\n",
    "for exp in EXPERIMENTS.keys():\n",
    "    exp_dir = OUT_ROOT / exp\n",
    "    if exp_dir.exists():\n",
    "        # Buscar todos los GIFs generados\n",
    "        gif_files = list(exp_dir.glob(\"*.gif\"))\n",
    "        if gif_files:\n",
    "            print(f\"\\n   📁 {exp}: {len(gif_files)} GIFs encontrados\")\n",
    "            for gif in gif_files:\n",
    "                print(f\"      • {gif.name}\")\n",
    "\n",
    "print(\"\\n✅ Visualizaciones comparativas completadas!\")\n",
    "print(f\"📂 Resultados guardados en: {COMP_DIR}\")\n",
    "\n",
    "# 6. Mostrar imágenes de predicción más recientes\n",
    "print(\"\\n🖼️ PREDICCIONES MÁS RECIENTES:\")\n",
    "for exp in EXPERIMENTS.keys():\n",
    "    exp_dir = OUT_ROOT / exp\n",
    "    if exp_dir.exists():\n",
    "        print(f\"\\n{exp}:\")\n",
    "        # Mostrar primera imagen de cada modelo\n",
    "        for model in MODELS.keys():\n",
    "            img_path = exp_dir / f\"{model}_1.png\"\n",
    "            if img_path.exists():\n",
    "                from IPython.display import Image, display\n",
    "                print(f\"  {model}:\")\n",
    "                display(Image(str(img_path), width=800))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016dc7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────── GRÁFICOS MEJORADOS DE EVOLUCIÓN POR HORIZONTE ─────────────────────────\n",
    "print(\"\\n📊 Generando gráficos mejorados de evolución por horizonte...\")\n",
    "\n",
    "if res_df is not None and len(res_df) > 0:\n",
    "    # Crear figura con subplots separados para cada métrica\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    metrics = ['RMSE', 'MAE', 'R2']\n",
    "    titles = ['RMSE por Horizonte', 'MAE por Horizonte', 'R² por Horizonte']\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(res_df['Model'].unique())))\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Obtener datos pivoteados\n",
    "        data = res_df.groupby(['H', 'Model'])[metric].mean().unstack()\n",
    "        \n",
    "        # Plotear cada modelo\n",
    "        for i, model in enumerate(data.columns):\n",
    "            ax.plot(data.index, data[model], \n",
    "                   marker='o', \n",
    "                   label=model,\n",
    "                   color=colors[i],\n",
    "                   linewidth=2.5,\n",
    "                   markersize=8,\n",
    "                   markeredgewidth=2,\n",
    "                   markeredgecolor='white')\n",
    "        \n",
    "        ax.set_xlabel('Horizonte (meses)', fontsize=12)\n",
    "        ax.set_ylabel(metric, fontsize=12)\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold', pad=10)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.set_xticks(data.index)\n",
    "        \n",
    "        # Leyenda solo en el primer gráfico\n",
    "        if idx == 0:\n",
    "            ax.legend(title='Modelo', loc='best', frameon=True, fancybox=True, shadow=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(COMP_DIR / 'metrics_evolution_by_horizon.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Crear gráfico adicional con todas las métricas normalizadas\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Normalizar métricas para comparación\n",
    "    for metric in ['RMSE', 'MAE', 'R2']:\n",
    "        data = res_df.groupby(['H', 'Model'])[metric].mean().unstack()\n",
    "        \n",
    "        # Normalizar cada métrica (min-max scaling)\n",
    "        if metric == 'R2':\n",
    "            # Para R2, mayor es mejor, así que invertimos\n",
    "            data_norm = 1 - (data - data.min().min()) / (data.max().max() - data.min().min())\n",
    "        else:\n",
    "            # Para RMSE y MAE, menor es mejor\n",
    "            data_norm = (data - data.min().min()) / (data.max().max() - data.min().min())\n",
    "        \n",
    "        for i, model in enumerate(data_norm.columns):\n",
    "            linestyle = '-' if metric == 'RMSE' else '--' if metric == 'MAE' else ':'\n",
    "            ax.plot(data_norm.index, data_norm[model], \n",
    "                   marker='o' if metric == 'RMSE' else 's' if metric == 'MAE' else '^',\n",
    "                   label=f'{model} - {metric}',\n",
    "                   linewidth=2,\n",
    "                   markersize=7,\n",
    "                   linestyle=linestyle,\n",
    "                   alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Horizonte (meses)', fontsize=12)\n",
    "    ax.set_ylabel('Métrica Normalizada (0=mejor, 1=peor)', fontsize=12)\n",
    "    ax.set_title('Comparación Normalizada de Todas las Métricas', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', ncol=1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(COMP_DIR / 'normalized_metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"✅ Gráficos mejorados guardados en:\", COMP_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343bbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────── TABLA VISUAL DE MÉTRICAS ─────────────────────────\n",
    "print(\"\\n📊 Generando tabla visual de métricas...\")\n",
    "\n",
    "if res_df is not None and len(res_df) > 0:\n",
    "    # Crear tabla resumen con colores\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Preparar datos para la tabla\n",
    "    summary_data = []\n",
    "    experiments = res_df['Experiment'].unique()\n",
    "    models = res_df['Model'].unique()\n",
    "    \n",
    "    # Headers\n",
    "    headers = ['Experimento', 'Modelo', 'RMSE↓', 'MAE↓', 'R²↑', 'Mejor H']\n",
    "    \n",
    "    for exp in experiments:\n",
    "        for model in models:\n",
    "            exp_model_data = res_df[(res_df['Experiment'] == exp) & (res_df['Model'] == model)]\n",
    "            if not exp_model_data.empty:\n",
    "                avg_rmse = exp_model_data['RMSE'].mean()\n",
    "                avg_mae = exp_model_data['MAE'].mean()\n",
    "                avg_r2 = exp_model_data['R2'].mean()\n",
    "                best_h = exp_model_data.loc[exp_model_data['RMSE'].idxmin(), 'H']\n",
    "                \n",
    "                summary_data.append([\n",
    "                    exp, model, \n",
    "                    f'{avg_rmse:.4f}', \n",
    "                    f'{avg_mae:.4f}', \n",
    "                    f'{avg_r2:.4f}',\n",
    "                    f'H={best_h}'\n",
    "                ])\n",
    "    \n",
    "    # Crear tabla\n",
    "    table = ax.table(cellText=summary_data, colLabels=headers, \n",
    "                    cellLoc='center', loc='center')\n",
    "    \n",
    "    # Estilizar tabla\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 2)\n",
    "    \n",
    "    # Colorear celdas según rendimiento\n",
    "    for i in range(len(summary_data)):\n",
    "        # Obtener valores para comparación\n",
    "        rmse_val = float(summary_data[i][2])\n",
    "        mae_val = float(summary_data[i][3])\n",
    "        r2_val = float(summary_data[i][4])\n",
    "        \n",
    "        # Encontrar min/max para normalización\n",
    "        all_rmse = [float(row[2]) for row in summary_data]\n",
    "        all_mae = [float(row[3]) for row in summary_data]\n",
    "        all_r2 = [float(row[4]) for row in summary_data]\n",
    "        \n",
    "        # Normalizar y colorear RMSE (menor es mejor)\n",
    "        rmse_norm = (rmse_val - min(all_rmse)) / (max(all_rmse) - min(all_rmse))\n",
    "        rmse_color = plt.cm.RdYlGn(1 - rmse_norm)\n",
    "        table[(i+1, 2)].set_facecolor(rmse_color)\n",
    "        \n",
    "        # Normalizar y colorear MAE (menor es mejor)\n",
    "        mae_norm = (mae_val - min(all_mae)) / (max(all_mae) - min(all_mae))\n",
    "        mae_color = plt.cm.RdYlGn(1 - mae_norm)\n",
    "        table[(i+1, 3)].set_facecolor(mae_color)\n",
    "        \n",
    "        # Normalizar y colorear R² (mayor es mejor)\n",
    "        r2_norm = (r2_val - min(all_r2)) / (max(all_r2) - min(all_r2))\n",
    "        r2_color = plt.cm.RdYlGn(r2_norm)\n",
    "        table[(i+1, 4)].set_facecolor(r2_color)\n",
    "        \n",
    "        # Colorear experimento\n",
    "        exp_colors = {'BASIC': '#e8f4f8', 'KCE': '#f0e8f8', 'PAFC': '#f8e8f0'}\n",
    "        table[(i+1, 0)].set_facecolor(exp_colors.get(summary_data[i][0], 'white'))\n",
    "    \n",
    "    # Colorear headers\n",
    "    for j in range(len(headers)):\n",
    "        table[(0, j)].set_facecolor('#4a86e8')\n",
    "        table[(0, j)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    plt.title('Resumen de Métricas por Modelo y Experimento\\n(Verde=Mejor, Rojo=Peor)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Añadir leyenda\n",
    "    plt.text(0.5, -0.05, '↓ = Menor es mejor, ↑ = Mayor es mejor', \n",
    "            transform=ax.transAxes, ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    plt.savefig(COMP_DIR / 'metrics_summary_table.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Identificar el mejor modelo global\n",
    "    print(\"\\n🏆 MEJOR MODELO GLOBAL:\")\n",
    "    print(\"─\" * 50)\n",
    "    \n",
    "    # Calcular score compuesto (normalizado)\n",
    "    res_df['score'] = (\n",
    "        (1 - (res_df['RMSE'] - res_df['RMSE'].min()) / (res_df['RMSE'].max() - res_df['RMSE'].min())) +\n",
    "        (1 - (res_df['MAE'] - res_df['MAE'].min()) / (res_df['MAE'].max() - res_df['MAE'].min())) +\n",
    "        ((res_df['R2'] - res_df['R2'].min()) / (res_df['R2'].max() - res_df['R2'].min()))\n",
    "    ) / 3\n",
    "    \n",
    "    best_overall = res_df.loc[res_df['score'].idxmax()]\n",
    "    print(f\"Modelo: {best_overall['Model']}\")\n",
    "    print(f\"Experimento: {best_overall['Experiment']}\")\n",
    "    print(f\"Horizonte: {best_overall['H']}\")\n",
    "    print(f\"RMSE: {best_overall['RMSE']:.4f}\")\n",
    "    print(f\"MAE: {best_overall['MAE']:.4f}\")\n",
    "    print(f\"R²: {best_overall['R2']:.4f}\")\n",
    "    print(f\"Score compuesto: {best_overall['score']:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Todas las visualizaciones han sido generadas y guardadas!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
