{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f4e7925",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/main/models/base_models_STHyMOUNTAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af3744f4",
      "metadata": {
        "id": "af3744f4"
      },
      "source": [
        "# üìò Entrenamiento de Modelos Baseline para Predicci√≥n Espaciotemporal de Precipitaci√≥n Mensual STHyMOUNTAIN\n",
        "\n",
        "Este notebook implementa modelos baseline para la predicci√≥n de precipitaciones usando datos espaciotemporales."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a994be36",
      "metadata": {},
      "source": [
        "## üîç Implementaci√≥n de Modelos Avanzados y T√©cnicas de Validaci√≥n\n",
        "\n",
        "Adem√°s de los modelos tabulares baseline, implementaremos:\n",
        "\n",
        "1. **Optimizaci√≥n avanzada con Optuna** para los modelos tabulares XGBoost y LightGBM\n",
        "2. **Validaci√≥n robusta** mediante:\n",
        "   - Hold-Out Validation (ya implementada)\n",
        "   - Cross-Validation (k=5)\n",
        "   - Bootstrapping (100 muestras)\n",
        "3. **Modelos de Deep Learning** para capturar patrones espaciales y temporales:\n",
        "   - Redes CNN para patrones espaciales\n",
        "   - Redes ConvLSTM para patrones espaciotemporales\n",
        "\n",
        "El objetivo es proporcionar una evaluaci√≥n completa de diferentes enfoques de modelado para la predicci√≥n de precipitaci√≥n en regiones monta√±osas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "06416284",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06416284",
        "outputId": "a8e4c864-34e9-41b2-d5c3-e6ccaaf3699e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entorno configurado. Usando ruta base: ..\n",
            "Directorio para salida de modelos creado: ../models/output\n"
          ]
        }
      ],
      "source": [
        "# Configuraci√≥n del entorno (compatible con Colab y local)\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detectar si estamos en Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')   \n",
        "    # Si estamos en Colab, clonar el repositorio\n",
        "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
        "    %cd ml_precipitation_prediction\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn\n",
        "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
        "else:\n",
        "    # Si estamos en local, usar la ruta actual\n",
        "    if '/models' in os.getcwd():\n",
        "        BASE_PATH = Path('..')\n",
        "    else:\n",
        "        BASE_PATH = Path('.')\n",
        "\n",
        "print(f\"Entorno configurado. Usando ruta base: {BASE_PATH}\")\n",
        "\n",
        "# Si BASE_PATH viene como string, lo convertimos\n",
        "BASE_PATH = Path(BASE_PATH)\n",
        "\n",
        "# Ahora puedes concatenar correctamente\n",
        "model_output_dir = BASE_PATH / 'models' / 'output'\n",
        "model_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Directorio para salida de modelos creado: {model_output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "e47fb555",
      "metadata": {
        "id": "e47fb555"
      },
      "outputs": [],
      "source": [
        "# 1. Importaciones necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import optuna\n",
        "import pickle\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importaciones para barras de progreso y mejora de visualizaci√≥n\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import time\n",
        "\n",
        "# Configurar visualizaci√≥n m√°s atractiva\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"notebook\", font_scale=1.2)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "313434be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaciones adicionales para Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, save_model, load_model\n",
        "from tensorflow.keras.layers import (Dense, Dropout, Conv2D, Conv3D, ConvLSTM2D, BatchNormalization, \n",
        "                                   MaxPooling2D, Flatten, Input, concatenate, Reshape, TimeDistributed, UpSampling2D)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(\"TensorFlow versi√≥n:\", tf.__version__)\n",
        "\n",
        "# Configurar GPU si est√° disponible\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    print(f\"GPU disponible: {physical_devices}\")\n",
        "    # Permitir crecimiento de memoria seg√∫n sea necesario\n",
        "    for device in physical_devices:\n",
        "        tf.config.experimental.set_memory_growth(device, True)\n",
        "else:\n",
        "    print(\"No se detect√≥ GPU. Usando CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26215d90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "26215d90",
        "outputId": "ccb06926-e151-453f-a306-999f15566bd9"
      },
      "outputs": [],
      "source": [
        "# 2. Cargar el dataset NetCDF\n",
        "def load_dataset(file_path):\n",
        "    \"\"\"Carga un archivo NetCDF y lo convierte a pandas DataFrame\"\"\"\n",
        "    try:\n",
        "        # Cargar el archivo NetCDF con xarray\n",
        "        print(f\"Intentando cargar el archivo: {file_path}\")\n",
        "        ds = xr.open_dataset(file_path)\n",
        "        print(\"Archivo cargado exitosamente con xarray\")\n",
        "\n",
        "        # Mostrar informaci√≥n del dataset cargado\n",
        "        print(\"\\nInformaci√≥n del dataset:\")\n",
        "        print(ds.info())\n",
        "        print(\"\\nVariables disponibles:\")\n",
        "        for var_name in ds.data_vars:\n",
        "            print(f\"- {var_name}: {ds[var_name].shape}\")\n",
        "\n",
        "        # Convertir a DataFrame\n",
        "        df = ds.to_dataframe().reset_index()\n",
        "        return df, ds\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el archivo NetCDF: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Ruta al dataset\n",
        "data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features.nc'\n",
        "print(f\"Buscando archivo en: {data_file}\")\n",
        "\n",
        "# Cargar el dataset\n",
        "df, ds_original = load_dataset(data_file)\n",
        "\n",
        "# Verificar si se carg√≥ correctamente\n",
        "if df is not None:\n",
        "    print(f\"Dataset cargado con √©xito. Dimensiones: {df.shape}\")\n",
        "    print(\"\\nPrimeras filas del DataFrame:\")\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"No se pudo cargar el dataset. Verificar la ruta y el formato del archivo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f0aebbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f0aebbc",
        "outputId": "48e82987-ff47-41e8-9f40-57e53cf90cf7"
      },
      "outputs": [],
      "source": [
        "# 3. Preparaci√≥n de los datos\n",
        "if df is not None:\n",
        "    # Identificar la columna objetivo (precipitaci√≥n)\n",
        "    target_column = 'total_precipitation'  # Ajustar si tiene otro nombre en tu dataset\n",
        "\n",
        "    # Ver si existe 'precip_target' o usar 'total_precipitation'\n",
        "    if 'total_precipitation' in df.columns:\n",
        "        target_column = 'total_precipitation'\n",
        "\n",
        "    print(f\"Columna objetivo identificada: {target_column}\")\n",
        "\n",
        "    # Separar variables predictoras y variable objetivo\n",
        "    feature_cols = [col for col in df.columns if col != target_column and not pd.isna(df[col]).all()]\n",
        "\n",
        "    # Eliminar columnas no num√©ricas para los modelos (como fechas o coordenadas si no se usan como features)\n",
        "    non_feature_cols = ['time', 'spatial_ref']\n",
        "    feature_cols = [col for col in feature_cols if col not in non_feature_cols]\n",
        "\n",
        "    # Eliminar filas con valores NaN\n",
        "    print(f\"Filas antes de eliminar NaN: {df.shape[0]}\")\n",
        "    df_clean = df.dropna(subset=[target_column] + feature_cols)\n",
        "    print(f\"Filas despu√©s de eliminar NaN: {df_clean.shape[0]}\")\n",
        "\n",
        "    # Separar features y target\n",
        "    X = df_clean[feature_cols]\n",
        "    y = df_clean[target_column]\n",
        "\n",
        "    print(f\"\\nFeatures seleccionadas ({len(feature_cols)}):\\n{feature_cols}\")\n",
        "    print(f\"\\nVariable objetivo: {target_column}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da222af5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da222af5",
        "outputId": "579fbd57-a790-42dd-807a-e61fc1daacbf"
      },
      "outputs": [],
      "source": [
        "# 4. Divisi√≥n del conjunto de datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Dimensiones del conjunto de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Dimensiones del conjunto de prueba: {X_test.shape}\")\n",
        "\n",
        "# 5. Estandarizaci√≥n de variables predictoras\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Guardar el scaler para uso futuro\n",
        "with open(model_output_dir / 'scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(\"Escalador guardado en models/output/scaler.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "c5ba053b",
      "metadata": {
        "id": "c5ba053b"
      },
      "outputs": [],
      "source": [
        "# 6. Funciones de evaluaci√≥n y entrenamiento\n",
        "def evaluar_modelo(y_true, y_pred):\n",
        "    \"\"\"Eval√∫a el rendimiento de un modelo usando m√∫ltiples m√©tricas\"\"\"\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, r2\n",
        "\n",
        "def entrenar_y_evaluar_modelo(modelo, nombre, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Entrena un modelo y eval√∫a su rendimiento con visualizaci√≥n del progreso\"\"\"\n",
        "    # Crear widget para mostrar informaci√≥n del proceso\n",
        "    display(HTML(f'<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\">' +\n",
        "                 f'<h3>üîÑ Entrenando modelo: {nombre}</h3>' +\n",
        "                 f'<div id=\"status_{nombre}\">Estado: Iniciando entrenamiento...</div>' +\n",
        "                 f'</div>'))\n",
        "    \n",
        "    # Tiempo de inicio\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Entrenar el modelo con seguimiento visual seg√∫n el tipo\n",
        "    if hasattr(modelo, 'fit_generator') or nombre in ['XGBoost', 'XGBoost_Optuna', 'LightGBM', 'LightGBM_Optuna']:\n",
        "        # Para modelos que soportan entrenamiento por lotes como XGBoost, LightGBM\n",
        "        print(f\"Entrenando {nombre} con visualizaci√≥n de progreso...\")\n",
        "        if hasattr(modelo, 'n_estimators'):\n",
        "            n_estimators = modelo.n_estimators\n",
        "            for i in tqdm(range(n_estimators), desc=f\"Entrenando {nombre}\"):\n",
        "                if i == 0:\n",
        "                    # Primera iteraci√≥n, ajuste inicial\n",
        "                    if nombre.startswith('LightGBM'):\n",
        "                        # LightGBM tiene par√°metro verbose\n",
        "                        temp_modelo = type(modelo)(n_estimators=1, **{k:v for k,v in modelo.get_params().items() \n",
        "                                                                 if k != 'n_estimators' and k != 'verbose'}, verbose=-1)\n",
        "                    else:\n",
        "                        temp_modelo = type(modelo)(n_estimators=1, **{k:v for k,v in modelo.get_params().items() \n",
        "                                                                if k != 'n_estimators'})\n",
        "                    temp_modelo.fit(X_train, y_train)\n",
        "                elif i == n_estimators - 1:\n",
        "                    # √öltima iteraci√≥n, ajuste completo\n",
        "                    modelo.fit(X_train, y_train)\n",
        "                \n",
        "                # Actualizar progreso visual\n",
        "                if i % max(1, n_estimators // 10) == 0:\n",
        "                    clear_output(wait=True)\n",
        "                    display(HTML(f'<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\">' +\n",
        "                                f'<h3>üîÑ Entrenando modelo: {nombre}</h3>' +\n",
        "                                f'<div id=\"status_{nombre}\">Estado: Progreso {i+1}/{n_estimators} estimadores ({((i+1)/n_estimators*100):.1f}%)</div>' +\n",
        "                                f'</div>'))\n",
        "                    time.sleep(0.1)  # Peque√±a pausa para actualizaci√≥n visual\n",
        "        else:\n",
        "            # Si no tiene n_estimators, entrenamiento directo\n",
        "            modelo.fit(X_train, y_train)\n",
        "    else:\n",
        "        # Para modelos est√°ndar como RandomForest\n",
        "        modelo.fit(X_train, y_train)\n",
        "    \n",
        "    # Tiempo de entrenamiento\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Visualizar tiempo de entrenamiento\n",
        "    display(HTML(f'<div style=\"background-color:#e6ffe6; padding:10px; border-radius:5px;\">' +\n",
        "                f'<h3>‚úÖ Entrenamiento completado: {nombre}</h3>' +\n",
        "                f'<div>Tiempo de entrenamiento: {training_time:.2f} segundos</div>' +\n",
        "                f'</div>'))\n",
        "    \n",
        "    print(f\"Evaluando rendimiento de {nombre}...\")\n",
        "    predicciones = modelo.predict(X_test)\n",
        "    rmse, mae, r2 = evaluar_modelo(y_test, predicciones)\n",
        "    \n",
        "    # Visualizar m√©tricas con estilo\n",
        "    display(HTML(f'<div style=\"background-color:#f5f5dc; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
        "                f'<h3>üìä M√©tricas para {nombre}</h3>' +\n",
        "                f'<table style=\"width:100%; text-align:left;\">' +\n",
        "                f'<tr><th>M√©trica</th><th>Valor</th></tr>' +\n",
        "                f'<tr><td>RMSE</td><td>{rmse:.4f}</td></tr>' +\n",
        "                f'<tr><td>MAE</td><td>{mae:.4f}</td></tr>' +\n",
        "                f'<tr><td>R¬≤</td><td>{r2:.4f}</td></tr>' +\n",
        "                f'</table></div>'))\n",
        "    \n",
        "    return modelo, (rmse, mae, r2)\n",
        "\n",
        "def guardar_modelo(modelo, nombre):\n",
        "    \"\"\"Guarda un modelo entrenado en disco\"\"\"\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"{nombre}_{timestamp}.pkl\"\n",
        "    with open(model_output_dir / filename, 'wb') as f:\n",
        "        pickle.dump(modelo, f)\n",
        "    \n",
        "    # Visualizar confirmaci√≥n de guardado\n",
        "    display(HTML(f'<div style=\"background-color:#e6ffee; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
        "                f'<h3>üíæ Modelo guardado</h3>' +\n",
        "                f'<div>Modelo <b>{nombre}</b> guardado como: {filename}</div>' +\n",
        "                f'</div>'))\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e59f9865",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Entrenamiento de modelos base sin optimizaci√≥n\n",
        "\n",
        "# Inicializar diccionarios para almacenar resultados y modelos\n",
        "resultados_base = {}  # Para almacenar m√©tricas (RMSE, MAE, R2)\n",
        "modelos_base = {}     # Para almacenar instancias de modelos\n",
        "modelos_guardados = {} # Para almacenar nombres de archivos guardados\n",
        "\n",
        "print(\"\\nüîç Entrenando modelos baseline sin optimizaci√≥n de hiperpar√°metros...\")\n",
        "\n",
        "# 1. Modelo RandomForest b√°sico\n",
        "print(\"\\nEntrenando RandomForest base...\")\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model, rf_metrics = entrenar_y_evaluar_modelo(\n",
        "    rf_model, 'RandomForest', X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "resultados_base['RandomForest'] = rf_metrics\n",
        "modelos_base['RandomForest'] = rf_model\n",
        "modelo_file = guardar_modelo(rf_model, 'RandomForest')\n",
        "modelos_guardados['RandomForest'] = modelo_file\n",
        "\n",
        "# Visualizar importancia de caracter√≠sticas para RandomForest\n",
        "plt.figure(figsize=(12, 6))\n",
        "feat_importances = rf_model.feature_importances_\n",
        "indices = np.argsort(feat_importances)[::-1]\n",
        "plt.bar(range(len(indices)), feat_importances[indices], color='skyblue')\n",
        "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
        "plt.title('Importancia de Caracter√≠sticas - RandomForest')\n",
        "plt.tight_layout()\n",
        "plt.savefig(model_output_dir / 'randomforest_feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# 2. Modelo XGBoost b√°sico\n",
        "print(\"\\nEntrenando XGBoost base...\")\n",
        "xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
        "xgb_model, xgb_metrics = entrenar_y_evaluar_modelo(\n",
        "    xgb_model, 'XGBoost', X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "resultados_base['XGBoost'] = xgb_metrics\n",
        "modelos_base['XGBoost'] = xgb_model\n",
        "modelo_file = guardar_modelo(xgb_model, 'XGBoost')\n",
        "modelos_guardados['XGBoost'] = modelo_file\n",
        "\n",
        "# Visualizar importancia de caracter√≠sticas para XGBoost\n",
        "plt.figure(figsize=(12, 6))\n",
        "feat_importances = xgb_model.feature_importances_\n",
        "indices = np.argsort(feat_importances)[::-1]\n",
        "plt.bar(range(len(indices)), feat_importances[indices], color='coral')\n",
        "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
        "plt.title('Importancia de Caracter√≠sticas - XGBoost')\n",
        "plt.tight_layout()\n",
        "plt.savefig(model_output_dir / 'xgboost_feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# 3. Modelo LightGBM b√°sico\n",
        "print(\"\\nEntrenando LightGBM base...\")\n",
        "lgbm_model = LGBMRegressor(n_estimators=100, random_state=42)\n",
        "lgbm_model, lgbm_metrics = entrenar_y_evaluar_modelo(\n",
        "    lgbm_model, 'LightGBM', X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "resultados_base['LightGBM'] = lgbm_metrics\n",
        "modelos_base['LightGBM'] = lgbm_model\n",
        "modelo_file = guardar_modelo(lgbm_model, 'LightGBM')\n",
        "modelos_guardados['LightGBM'] = modelo_file\n",
        "\n",
        "# Visualizar importancia de caracter√≠sticas para LightGBM\n",
        "plt.figure(figsize=(12, 6))\n",
        "feat_importances = lgbm_model.feature_importances_\n",
        "indices = np.argsort(feat_importances)[::-1]\n",
        "plt.bar(range(len(indices)), feat_importances[indices], color='lightgreen')\n",
        "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
        "plt.title('Importancia de Caracter√≠sticas - LightGBM')\n",
        "plt.tight_layout()\n",
        "plt.savefig(model_output_dir / 'lightgbm_feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# Comparar resultados de modelos base\n",
        "print(\"\\nüîç Comparaci√≥n de modelos base sin optimizaci√≥n:\")\n",
        "temp_df = pd.DataFrame(resultados_base, index=['RMSE', 'MAE', 'R2']).T\n",
        "print(\"\\nOrdenados por RMSE (menor es mejor):\")\n",
        "display(temp_df.sort_values('RMSE'))\n",
        "\n",
        "# Visualizar comparaci√≥n de RMSE\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=temp_df.index, y=temp_df['RMSE'])\n",
        "plt.title('Comparaci√≥n de RMSE - Modelos Base')\n",
        "plt.ylabel('RMSE (menor es mejor)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(model_output_dir / 'baseline_rmse_comparison.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b730861a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementaci√≥n de funci√≥n para optimizaci√≥n adaptativa de memoria\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "import warnings\n",
        "import optuna\n",
        "\n",
        "def run_memory_efficient_optimization(model_type, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Ejecuta optimizaci√≥n adaptativa de hiperpar√°metros considerando uso de memoria.\n",
        "    \n",
        "    Args:\n",
        "        model_type (str): Tipo de modelo a optimizar ('RandomForest', 'XGBoost', 'LightGBM')\n",
        "        X_train, y_train: Datos de entrenamiento\n",
        "        X_test, y_test: Datos de prueba\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (mejores_par√°metros, modelo_optimizado, m√©tricas)\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìä Iniciando optimizaci√≥n adaptativa para {model_type}...\")\n",
        "    \n",
        "    # Verificar memoria disponible\n",
        "    available_memory_gb = psutil.virtual_memory().available / (1024**3)\n",
        "    print(f\"Memoria RAM disponible: {available_memory_gb:.2f} GB\")\n",
        "    \n",
        "    # Conjunto de referencias de modelos\n",
        "    model_types = {\n",
        "        'RandomForest': RandomForestRegressor,\n",
        "        'XGBoost': XGBRegressor,\n",
        "        'LightGBM': LGBMRegressor\n",
        "    }\n",
        "    \n",
        "    if model_type not in model_types:\n",
        "        raise ValueError(f\"Tipo de modelo no soportado: {model_type}. Debe ser uno de: {list(model_types.keys())}\")\n",
        "    \n",
        "    # Ajustar par√°metros seg√∫n memoria disponible\n",
        "    if available_memory_gb < 2.0:\n",
        "        print(\"‚ö†Ô∏è Memoria baja detectada. Ajustando optimizaci√≥n para uso m√≠nimo de memoria.\")\n",
        "        n_trials = 10\n",
        "        max_estimators = 50\n",
        "        max_depth = 6\n",
        "        subsample = 0.5\n",
        "    elif available_memory_gb < 8.0:\n",
        "        print(\"‚ÑπÔ∏è Memoria moderada detectada. Usando configuraci√≥n balanceada.\")\n",
        "        n_trials = 30\n",
        "        max_estimators = 300\n",
        "        max_depth = 12\n",
        "        subsample = 0.7\n",
        "    else:\n",
        "        print(\"‚úÖ Memoria amplia detectada. Usando configuraci√≥n completa para optimizaci√≥n.\")\n",
        "        n_trials = 50\n",
        "        max_estimators = 500\n",
        "        max_depth = 20\n",
        "        subsample = 0.9\n",
        "        \n",
        "    # Funci√≥n objetivo para Optuna seg√∫n tipo de modelo\n",
        "    def objective(trial):\n",
        "        # Par√°metros comunes\n",
        "        common_params = {\n",
        "            'random_state': 42\n",
        "        }\n",
        "        \n",
        "        # Par√°metros espec√≠ficos por tipo de modelo\n",
        "        if model_type == 'RandomForest':\n",
        "            params = {\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 50, max_estimators),\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, max_depth),\n",
        "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
        "                'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
        "                'n_jobs': -1 if available_memory_gb > 4.0 else 1\n",
        "            }\n",
        "        elif model_type == 'XGBoost':\n",
        "            params = {\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 50, max_estimators),\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, max_depth),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "                'subsample': trial.suggest_float('subsample', 0.5, subsample),\n",
        "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "                'gamma': trial.suggest_float('gamma', 0, 5),\n",
        "                'tree_method': 'hist',  # M√°s eficiente en memoria\n",
        "                'use_label_encoder': False,  # Evitar warning\n",
        "                'n_jobs': -1 if available_memory_gb > 4.0 else 1\n",
        "            }\n",
        "        elif model_type == 'LightGBM':\n",
        "            params = {\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 50, max_estimators),\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, max_depth),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "                'subsample': trial.suggest_float('subsample', 0.5, subsample),\n",
        "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
        "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
        "                'boosting_type': 'gbdt',  # Tipo m√°s eficiente\n",
        "                'n_jobs': -1 if available_memory_gb > 4.0 else 1,\n",
        "                'verbose': -1\n",
        "            }\n",
        "        \n",
        "        # Combinar par√°metros comunes y espec√≠ficos\n",
        "        params.update(common_params)\n",
        "        \n",
        "        # Asegurar liberaci√≥n de memoria antes de crear nuevo modelo\n",
        "        gc.collect()\n",
        "        \n",
        "        # Crear y entrenar modelo\n",
        "        try:\n",
        "            model_class = model_types[model_type]\n",
        "            model = model_class(**params)\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "            return rmse\n",
        "        except Exception as e:\n",
        "            print(f\"Error al entrenar modelo con par√°metros: {params}\")\n",
        "            print(f\"Error: {e}\")\n",
        "            # Retornar un valor alto para que Optuna evite estos par√°metros\n",
        "            return float('inf')\n",
        "    \n",
        "    # Configuraci√≥n de estudio Optuna\n",
        "    pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
        "    sampler = optuna.samplers.TPESampler(seed=42)\n",
        "    \n",
        "    study_name = f\"{model_type}_memory_optimized\"\n",
        "    storage_name = \"sqlite:///{}.db\".format(model_output_dir / study_name)\n",
        "    \n",
        "    # Crear y ejecutar estudio\n",
        "    try:\n",
        "        study = optuna.create_study(\n",
        "            study_name=study_name,\n",
        "            storage=storage_name,\n",
        "            direction='minimize',\n",
        "            sampler=sampler,\n",
        "            pruner=pruner,\n",
        "            load_if_exists=True\n",
        "        )\n",
        "        \n",
        "        print(f\"Ejecutando {n_trials} pruebas para {model_type}...\")\n",
        "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
        "        \n",
        "        # Obtener mejores par√°metros\n",
        "        best_params = study.best_params\n",
        "        best_params['random_state'] = 42\n",
        "        \n",
        "        # Si es XGBoost, agregar par√°metros adicionales\n",
        "        if model_type == 'XGBoost':\n",
        "            best_params['tree_method'] = 'hist'\n",
        "            best_params['use_label_encoder'] = False\n",
        "        \n",
        "        # Entrenar modelo final con mejores par√°metros\n",
        "        print(f\"\\n‚úÖ Entrenando modelo final {model_type} con mejores par√°metros...\")\n",
        "        model_class = model_types[model_type]\n",
        "        best_model = model_class(**best_params)\n",
        "        \n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter(\"ignore\")\n",
        "            best_model.fit(X_train, y_train)\n",
        "        \n",
        "        # Evaluar modelo final\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        \n",
        "        print(f\"\\nüìä M√©tricas de {model_type} optimizado:\")\n",
        "        print(f\"RMSE: {rmse:.4f}\")\n",
        "        print(f\"MAE: {mae:.4f}\")\n",
        "        print(f\"R¬≤: {r2:.4f}\")\n",
        "        \n",
        "        # A√±adir al diccionario de resultados\n",
        "        resultados_base[f\"{model_type}_Optuna\"] = (rmse, mae, r2)\n",
        "        \n",
        "        # Guardar modelo\n",
        "        filename = f\"{model_type}_optimized.pkl\"\n",
        "        with open(model_output_dir / filename, 'wb') as f:\n",
        "            pickle.dump(best_model, f)\n",
        "        print(f\"Modelo guardado como: {filename}\")\n",
        "        \n",
        "        # Devolver resultados\n",
        "        return best_params, best_model, (rmse, mae, r2)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error durante la optimizaci√≥n: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {}, None, (float('inf'), float('inf'), -float('inf'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b0f81dd",
      "metadata": {},
      "source": [
        "## üß† Implementaci√≥n de Modelos de Deep Learning\n",
        "\n",
        "A continuaci√≥n implementaremos modelos basados en redes neuronales profundas para capturar patrones espaciales y temporales en los datos de precipitaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e290e874",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementaci√≥n de modelo CNN para predicci√≥n espacial\n",
        "\n",
        "# Necesitamos reformatear los datos para CNN\n",
        "# Primero verificamos si hay columnas de coordenadas 'x' y 'y' en nuestros datos\n",
        "print(\"\\nüîç Preparando datos para modelos CNN...\")\n",
        "\n",
        "# Verificar si tenemos columnas de coordenadas en nuestros datos\n",
        "coord_cols = [col for col in feature_cols if col in ['x', 'y', 'latitude', 'longitude', 'lat', 'lon']]\n",
        "\n",
        "if len(coord_cols) >= 2:\n",
        "    print(f\"Columnas de coordenadas encontradas: {coord_cols}\")\n",
        "    \n",
        "    # Mapeo de nombres de columnas comunes\n",
        "    lat_names = ['latitude', 'lat', 'y']\n",
        "    lon_names = ['longitude', 'lon', 'x']\n",
        "    \n",
        "    # Identificar columnas de latitud y longitud\n",
        "    lat_col = next((col for col in coord_cols if col in lat_names), None)\n",
        "    lon_col = next((col for col in coord_cols if col in lon_names), None)\n",
        "    \n",
        "    if lat_col and lon_col:\n",
        "        print(f\"Usando {lat_col} y {lon_col} como coordenadas para CNN\")\n",
        "        \n",
        "        # Convertir datos a formato espacial para CNN\n",
        "        def prepare_spatial_data(X_data, y_data, lat_col, lon_col):\n",
        "            \"\"\"Prepara datos espaciales para CNN\"\"\"\n",
        "            # Extraer coordenadas √∫nicas en orden\n",
        "            lats = sorted(X_data[lat_col].unique())\n",
        "            lons = sorted(X_data[lon_col].unique())\n",
        "            \n",
        "            # Crear diccionarios de mapeo para √≠ndices\n",
        "            lat_to_idx = {lat: idx for idx, lat in enumerate(lats)}\n",
        "            lon_to_idx = {lon: idx for idx, lon in enumerate(lons)}\n",
        "            \n",
        "            # Dimensiones de la grilla\n",
        "            grid_height = len(lats)\n",
        "            grid_width = len(lons)\n",
        "            n_features = X_data.shape[1] - 2  # Restar las dos columnas de coordenadas\n",
        "            \n",
        "            # Inicializar arrays\n",
        "            # Las dimensiones son: [muestras, altura, ancho, canales]\n",
        "            X_grid = np.zeros((len(X_data), grid_height, grid_width, n_features))\n",
        "            y_grid = np.zeros((len(y_data), grid_height, grid_width, 1))\n",
        "            \n",
        "            # Recorrer todos los datos y ubicarlos en la grilla\n",
        "            non_coord_cols = [col for col in X_data.columns if col != lat_col and col != lon_col]\n",
        "            \n",
        "            for idx in range(len(X_data)):\n",
        "                lat = X_data.iloc[idx][lat_col]\n",
        "                lon = X_data.iloc[idx][lon_col]\n",
        "                \n",
        "                lat_idx = lat_to_idx[lat]\n",
        "                lon_idx = lon_to_idx[lon]\n",
        "                \n",
        "                # Colocar caracter√≠sticas en la grilla\n",
        "                for i, col in enumerate(non_coord_cols):\n",
        "                    X_grid[idx, lat_idx, lon_idx, i] = X_data.iloc[idx][col]\n",
        "                \n",
        "                # Colocar valor objetivo\n",
        "                y_grid[idx, lat_idx, lon_idx, 0] = y_data.iloc[idx]\n",
        "            \n",
        "            return X_grid, y_grid\n",
        "        \n",
        "        # Convertir datos de entrenamiento a formato espacial\n",
        "        print(\"Convirtiendo datos a formato espacial...\")\n",
        "        try:\n",
        "            # Reconstruir dataframes a partir de arrays escalados\n",
        "            X_train_df = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
        "            X_test_df = pd.DataFrame(X_test_scaled, columns=feature_cols)\n",
        "            \n",
        "            # Preparar datos espaciales\n",
        "            X_train_spatial, y_train_spatial = prepare_spatial_data(X_train_df, y_train, lat_col, lon_col)\n",
        "            X_test_spatial, y_test_spatial = prepare_spatial_data(X_test_df, y_test, lat_col, lon_col)\n",
        "            \n",
        "            print(f\"Datos espaciales preparados:\")\n",
        "            print(f\"X_train_spatial: {X_train_spatial.shape}\")\n",
        "            print(f\"y_train_spatial: {y_train_spatial.shape}\")\n",
        "            print(f\"X_test_spatial: {X_test_spatial.shape}\")\n",
        "            print(f\"y_test_spatial: {y_test_spatial.shape}\")\n",
        "            \n",
        "            # Si la conversi√≥n fue exitosa, procedemos con el modelo CNN\n",
        "            # Modelo CNN para predicci√≥n de precipitaci√≥n\n",
        "            def create_cnn_model(input_shape):\n",
        "                \"\"\"Crea un modelo CNN para predicci√≥n espacial\"\"\"\n",
        "                # Usar Input como primera capa para evitar el warning\n",
        "                inputs = Input(shape=input_shape)\n",
        "                \n",
        "                # Primera capa convolucional\n",
        "                x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
        "                x = BatchNormalization()(x)\n",
        "                \n",
        "                # Segunda capa convolucional\n",
        "                x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "                x = BatchNormalization()(x)\n",
        "                x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "                x = Dropout(0.25)(x)\n",
        "                \n",
        "                # Tercera capa convolucional\n",
        "                x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "                x = BatchNormalization()(x)\n",
        "                x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "                x = Dropout(0.25)(x)\n",
        "                \n",
        "                # Cuarta capa convolucional\n",
        "                x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "                x = BatchNormalization()(x)\n",
        "                \n",
        "                # Capa de salida\n",
        "                outputs = Conv2D(1, kernel_size=(1, 1), activation='linear', padding='same')(x)\n",
        "                \n",
        "                # Crear modelo usando API funcional\n",
        "                model = Model(inputs=inputs, outputs=outputs)\n",
        "                \n",
        "                # Compilar modelo\n",
        "                model.compile(\n",
        "                    loss='mse',\n",
        "                    optimizer=Adam(learning_rate=0.001),\n",
        "                    metrics=['mae']\n",
        "                )\n",
        "                \n",
        "                return model\n",
        "            \n",
        "            # Crear y entrenar modelo CNN\n",
        "            print(\"\\nüß† Creando y entrenando modelo CNN...\")\n",
        "            input_shape = X_train_spatial.shape[1:]  # (altura, ancho, canales)\n",
        "            cnn_model = create_cnn_model(input_shape)\n",
        "            \n",
        "            # Mostrar resumen del modelo\n",
        "            cnn_model.summary()\n",
        "            \n",
        "            # Callbacks para entrenamiento\n",
        "            callbacks = [\n",
        "                EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),\n",
        "                ModelCheckpoint(filepath=model_output_dir / 'cnn_model_best.h5',\n",
        "                              save_best_only=True, monitor='val_loss')\n",
        "            ]\n",
        "            \n",
        "            # Entrenar modelo\n",
        "            history = cnn_model.fit(\n",
        "                X_train_spatial, y_train_spatial,\n",
        "                validation_split=0.2,\n",
        "                epochs=100,\n",
        "                batch_size=32,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "            \n",
        "            # Evaluar modelo\n",
        "            print(\"\\nüìä Evaluando modelo CNN...\")\n",
        "            cnn_metrics = cnn_model.evaluate(X_test_spatial, y_test_spatial)\n",
        "            print(f\"Loss (MSE): {cnn_metrics[0]:.4f}\")\n",
        "            print(f\"MAE: {cnn_metrics[1]:.4f}\")\n",
        "            \n",
        "            # Predecir con el modelo\n",
        "            y_pred_cnn = cnn_model.predict(X_test_spatial)\n",
        "            \n",
        "            # Aplanar las predicciones para calcular m√©tricas\n",
        "            y_test_flat = y_test_spatial.flatten()\n",
        "            y_pred_flat = y_pred_cnn.flatten()\n",
        "            \n",
        "            # Filtrar valores donde y_test_flat > 0 (presumiblemente donde hay datos)\n",
        "            valid_indices = y_test_flat > 0\n",
        "            y_test_valid = y_test_flat[valid_indices]\n",
        "            y_pred_valid = y_pred_flat[valid_indices]\n",
        "            \n",
        "            # Calcular m√©tricas\n",
        "            cnn_rmse = np.sqrt(mean_squared_error(y_test_valid, y_pred_valid))\n",
        "            cnn_mae = mean_absolute_error(y_test_valid, y_pred_valid)\n",
        "            cnn_r2 = r2_score(y_test_valid, y_pred_valid)\n",
        "            \n",
        "            print(f\"RMSE: {cnn_rmse:.4f}\")\n",
        "            print(f\"MAE: {cnn_mae:.4f}\")\n",
        "            print(f\"R¬≤: {cnn_r2:.4f}\")\n",
        "            \n",
        "            # Guardar modelo\n",
        "            cnn_model.save(model_output_dir / 'cnn_model_final.h5')\n",
        "            print(\"Modelo CNN guardado como 'cnn_model_final.h5'\")\n",
        "            \n",
        "            # Visualizar la historia del entrenamiento\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(history.history['loss'])\n",
        "            plt.plot(history.history['val_loss'])\n",
        "            plt.title('P√©rdida del modelo')\n",
        "            plt.ylabel('P√©rdida')\n",
        "            plt.xlabel('√âpoca')\n",
        "            plt.legend(['Entrenamiento', 'Validaci√≥n'], loc='upper right')\n",
        "            \n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(history.history['mae'])\n",
        "            plt.plot(history.history['val_mae'])\n",
        "            plt.title('Error absoluto medio')\n",
        "            plt.ylabel('MAE')\n",
        "            plt.xlabel('√âpoca')\n",
        "            plt.legend(['Entrenamiento', 'Validaci√≥n'], loc='upper right')\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(model_output_dir / 'cnn_training_history.png')\n",
        "            plt.show()\n",
        "            \n",
        "            # A√±adir resultados a nuestro diccionario de resultados\n",
        "            resultados_base['CNN'] = (cnn_rmse, cnn_mae, cnn_r2)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error preparando datos espaciales para CNN: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"No se pudieron identificar columnas de latitud y longitud.\")\n",
        "else:\n",
        "    print(\"No se encontraron suficientes columnas de coordenadas para implementar CNN.\")\n",
        "    print(\"El modelo CNN requiere al menos columnas de latitud y longitud.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d853bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementaci√≥n de modelo ConvLSTM para predicci√≥n espaciotemporal\n",
        "print(\"\\nüîç Preparando datos para modelo ConvLSTM...\")\n",
        "\n",
        "# Verificar si tenemos el DataFrame disponible\n",
        "if 'df' not in locals() or df is None:\n",
        "    print(\"DataFrame no disponible, intentando recargarlo...\")\n",
        "    try:\n",
        "        # Recargar el dataset si no est√° disponible\n",
        "        data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features.nc'\n",
        "        print(f\"Recargando archivo desde: {data_file}\")\n",
        "        df, ds_original = load_dataset(data_file)\n",
        "        \n",
        "        if df is None:\n",
        "            print(\"Error: No se pudo recargar el DataFrame. Verificar la ruta del archivo.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error al recargar el DataFrame: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Verificar si tenemos las variables necesarias\n",
        "if 'feature_cols' not in locals() or 'target_column' not in locals():\n",
        "    print(\"Variables necesarias no definidas, intentando redefinirlas...\")\n",
        "    if df is not None:\n",
        "        # Identificar la columna objetivo (precipitaci√≥n)\n",
        "        target_column = 'total_precipitation'  # Ajustar si tiene otro nombre en tu dataset\n",
        "        \n",
        "        # Ver si existe 'precip_target' o usar 'total_precipitation'\n",
        "        if 'total_precipitation' in df.columns:\n",
        "            target_column = 'total_precipitation'\n",
        "        \n",
        "        # Separar variables predictoras y variable objetivo\n",
        "        feature_cols = [col for col in df.columns if col != target_column and not pd.isna(df[col]).all()]\n",
        "        \n",
        "        # Eliminar columnas no num√©ricas para los modelos\n",
        "        non_feature_cols = ['time', 'spatial_ref']\n",
        "        feature_cols = [col for col in feature_cols if col not in non_feature_cols]\n",
        "\n",
        "# Si el DataFrame est√° disponible, continuar con la preparaci√≥n de datos\n",
        "if df is not None:\n",
        "    # Para ConvLSTM necesitamos datos con dimensi√≥n temporal\n",
        "    time_cols = [col for col in df.columns if col in ['time', 'date', 'month', 'year', 'day']]\n",
        "\n",
        "    if len(time_cols) > 0 and len(coord_cols) >= 2:\n",
        "        print(f\"Columnas temporales encontradas: {time_cols}\")\n",
        "        time_col = time_cols[0]\n",
        "        \n",
        "        # Funci√≥n para preparar datos espaciotemporales\n",
        "        def prepare_spatiotemporal_data(df, feature_cols, target_column, lat_col, lon_col, time_col, \n",
        "                                        sequence_length=3):\n",
        "            \"\"\"Prepara datos para ConvLSTM con dimensi√≥n espaciotemporal\"\"\"\n",
        "            print(\"Preparando datos espaciotemporales para ConvLSTM...\")\n",
        "            try:\n",
        "                # Asegurarnos que la columna temporal est√° ordenada\n",
        "                # Verificar el tipo de la columna temporal\n",
        "                time_dtype = df[time_col].dtype\n",
        "                print(f\"Tipo de dato de columna temporal: {time_dtype}\")\n",
        "                \n",
        "                if pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
        "                    # Ya es datetime, ordenamos\n",
        "                    df_sorted = df.sort_values(by=time_col)\n",
        "                else:\n",
        "                    # Intentar convertir a datetime\n",
        "                    try:\n",
        "                        df[time_col] = pd.to_datetime(df[time_col])\n",
        "                        df_sorted = df.sort_values(by=time_col)\n",
        "                    except Exception as e:\n",
        "                        print(f\"No se pudo convertir columna temporal a datetime: {e}\")\n",
        "                        # Si no podemos convertir, asumimos que ya est√° ordenado\n",
        "                        df_sorted = df\n",
        "                \n",
        "                # Extraer coordenadas √∫nicas\n",
        "                lats = sorted(df_sorted[lat_col].unique())\n",
        "                lons = sorted(df_sorted[lon_col].unique())\n",
        "                time_steps = sorted(df_sorted[time_col].unique())\n",
        "                \n",
        "                print(f\"Dimensiones espaciotemporales:\")\n",
        "                print(f\"- Latitudes (filas): {len(lats)}\")\n",
        "                print(f\"- Longitudes (columnas): {len(lons)}\")\n",
        "                print(f\"- Pasos temporales: {len(time_steps)}\")\n",
        "                \n",
        "                # Crear mapeos para √≠ndices\n",
        "                lat_to_idx = {lat: idx for idx, lat in enumerate(lats)}\n",
        "                lon_to_idx = {lon: idx for idx, lon in enumerate(lons)}\n",
        "                time_to_idx = {time: idx for idx, time in enumerate(time_steps)}\n",
        "                \n",
        "                # Filtrar columnas feature eliminando coordenadas y tiempo\n",
        "                feature_cols_filtered = [col for col in feature_cols if col != lat_col and col != lon_col and col != time_col]\n",
        "                n_features = len(feature_cols_filtered)\n",
        "                \n",
        "                # Dimensiones de la grilla espaciotemporal\n",
        "                grid_height = len(lats)\n",
        "                grid_width = len(lons)\n",
        "                n_timesteps = len(time_steps)\n",
        "                \n",
        "                print(f\"Caracter√≠sticas a usar: {n_features}\")\n",
        "                \n",
        "                # Crear un DataFrame indexado para acceso r√°pido\n",
        "                df_indexed = df_sorted.set_index([time_col, lat_col, lon_col])\n",
        "                \n",
        "                # Crear matrices 3D para cada paso temporal\n",
        "                # Las dimensiones son: [tiempo, altura, ancho, features]\n",
        "                X_spatiotemporal = np.zeros((n_timesteps, grid_height, grid_width, n_features))\n",
        "                y_spatiotemporal = np.zeros((n_timesteps, grid_height, grid_width, 1))\n",
        "                \n",
        "                # Llenar matrices con datos disponibles\n",
        "                for t_idx, t in enumerate(time_steps):\n",
        "                    for lat_idx, lat in enumerate(lats):\n",
        "                        for lon_idx, lon in enumerate(lons):\n",
        "                            try:\n",
        "                                # Obtener datos para esta coordenada y tiempo\n",
        "                                data = df_indexed.loc[(t, lat, lon)]\n",
        "                                \n",
        "                                # Llenar caracter√≠sticas\n",
        "                                for f_idx, feat in enumerate(feature_cols_filtered):\n",
        "                                    X_spatiotemporal[t_idx, lat_idx, lon_idx, f_idx] = data[feat]\n",
        "                                \n",
        "                                # Llenar target\n",
        "                                y_spatiotemporal[t_idx, lat_idx, lon_idx, 0] = data[target_column]\n",
        "                            except KeyError:\n",
        "                                # Este punto espaciotemporal no existe en los datos\n",
        "                                pass\n",
        "                \n",
        "                # Crear secuencias para ConvLSTM\n",
        "                # Para cada paso temporal t, usaremos t-sequence_length hasta t-1 para predecir t\n",
        "                n_sequences = n_timesteps - sequence_length\n",
        "                \n",
        "                if n_sequences <= 0:\n",
        "                    print(\"No hay suficientes pasos temporales para crear secuencias. Ajustando sequence_length.\")\n",
        "                    sequence_length = max(1, n_timesteps // 2)\n",
        "                    n_sequences = n_timesteps - sequence_length\n",
        "                    print(f\"Nuevo sequence_length: {sequence_length}, n_sequences: {n_sequences}\")\n",
        "                \n",
        "                # Crear arrays para secuencias\n",
        "                X_sequences = np.zeros((n_sequences, sequence_length, grid_height, grid_width, n_features))\n",
        "                y_sequences = np.zeros((n_sequences, grid_height, grid_width, 1))\n",
        "                \n",
        "                for i in range(n_sequences):\n",
        "                    X_sequences[i] = X_spatiotemporal[i:i+sequence_length]\n",
        "                    y_sequences[i] = y_spatiotemporal[i+sequence_length]\n",
        "                \n",
        "                print(f\"Secuencias creadas:\")\n",
        "                print(f\"X_sequences: {X_sequences.shape}\")\n",
        "                print(f\"y_sequences: {y_sequences.shape}\")\n",
        "                \n",
        "                # Dividir en train/test\n",
        "                train_size = int(0.8 * n_sequences)\n",
        "                X_train = X_sequences[:train_size]\n",
        "                y_train = y_sequences[:train_size]\n",
        "                X_test = X_sequences[train_size:]\n",
        "                y_test = y_sequences[train_size:]\n",
        "                \n",
        "                return X_train, y_train, X_test, y_test\n",
        "            except Exception as e:\n",
        "                print(f\"Error preparando datos espaciotemporales: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                return None, None, None, None\n",
        "        \n",
        "        # Intentar preparar datos espaciotemporales\n",
        "        try:\n",
        "            X_train_convlstm, y_train_convlstm, X_test_convlstm, y_test_convlstm = prepare_spatiotemporal_data(\n",
        "                df, feature_cols, target_column, lat_col, lon_col, time_col, sequence_length=3\n",
        "            )\n",
        "            \n",
        "            # Si los datos se preparan correctamente, crear y entrenar modelo ConvLSTM\n",
        "            if X_train_convlstm is not None:\n",
        "                print(\"\\nüß† Creando y entrenando modelo ConvLSTM...\")\n",
        "                \n",
        "                def create_convlstm_model(input_shape):\n",
        "                    \"\"\"Crea un modelo ConvLSTM para predicci√≥n espaciotemporal\"\"\"\n",
        "                    model = Sequential([\n",
        "                        # Capa ConvLSTM\n",
        "                        ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same',\n",
        "                                  return_sequences=True, activation='tanh',\n",
        "                                  input_shape=input_shape),\n",
        "                        BatchNormalization(),\n",
        "                        \n",
        "                        # Segunda capa ConvLSTM\n",
        "                        ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same',\n",
        "                                   return_sequences=False, activation='tanh'),\n",
        "                        BatchNormalization(),\n",
        "                        \n",
        "                        # Capa convolucional para reducir mapas de caracter√≠sticas\n",
        "                        Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "                        BatchNormalization(),\n",
        "                        MaxPooling2D(pool_size=(2, 2)),\n",
        "                        \n",
        "                        # Capas finales\n",
        "                        Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "                        UpSampling2D(size=(2, 2)),  # Restaurar dimensi√≥n original\n",
        "                        Conv2D(filters=1, kernel_size=(3, 3), activation='linear', padding='same')\n",
        "                    ])\n",
        "                    \n",
        "                    # Compilar modelo\n",
        "                    model.compile(\n",
        "                        loss='mse',\n",
        "                        optimizer=Adam(learning_rate=0.001),\n",
        "                        metrics=['mae']\n",
        "                    )\n",
        "                    \n",
        "                    return model\n",
        "                \n",
        "                # Crear modelo ConvLSTM\n",
        "                input_shape = X_train_convlstm.shape[1:]  # (sequence_length, height, width, features)\n",
        "                convlstm_model = create_convlstm_model(input_shape)\n",
        "                \n",
        "                # Mostrar resumen del modelo\n",
        "                convlstm_model.summary()\n",
        "                \n",
        "                # Callbacks para entrenamiento\n",
        "                callbacks = [\n",
        "                    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "                    ModelCheckpoint(filepath=model_output_dir / 'convlstm_model_best.h5',\n",
        "                                  save_best_only=True, monitor='val_loss')\n",
        "                ]\n",
        "                \n",
        "                # Entrenar modelo\n",
        "                history = convlstm_model.fit(\n",
        "                    X_train_convlstm, y_train_convlstm,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=100,\n",
        "                    batch_size=16,\n",
        "                    callbacks=callbacks,\n",
        "                    verbose=1\n",
        "                )\n",
        "                \n",
        "                # Evaluar modelo\n",
        "                print(\"\\nüìä Evaluando modelo ConvLSTM...\")\n",
        "                convlstm_metrics = convlstm_model.evaluate(X_test_convlstm, y_test_convlstm)\n",
        "                print(f\"Loss (MSE): {convlstm_metrics[0]:.4f}\")\n",
        "                print(f\"MAE: {convlstm_metrics[1]:.4f}\")\n",
        "                \n",
        "                # Predecir con el modelo\n",
        "                y_pred_convlstm = convlstm_model.predict(X_test_convlstm)\n",
        "                \n",
        "                # Aplanar las predicciones para calcular m√©tricas\n",
        "                y_test_flat = y_test_convlstm.flatten()\n",
        "                y_pred_flat = y_pred_convlstm.flatten()\n",
        "                \n",
        "                # Filtrar valores donde y_test_flat > 0 (presumiblemente donde hay datos)\n",
        "                valid_indices = y_test_flat > 0\n",
        "                y_test_valid = y_test_flat[valid_indices]\n",
        "                y_pred_valid = y_pred_flat[valid_indices]\n",
        "                \n",
        "                # Calcular m√©tricas\n",
        "                convlstm_rmse = np.sqrt(mean_squared_error(y_test_valid, y_pred_valid))\n",
        "                convlstm_mae = mean_absolute_error(y_test_valid, y_pred_valid)\n",
        "                convlstm_r2 = r2_score(y_test_valid, y_pred_valid)\n",
        "                \n",
        "                print(f\"RMSE: {convlstm_rmse:.4f}\")\n",
        "                print(f\"MAE: {convlstm_mae:.4f}\")\n",
        "                print(f\"R¬≤: {convlstm_r2:.4f}\")\n",
        "                \n",
        "                # Guardar modelo\n",
        "                convlstm_model.save(model_output_dir / 'convlstm_model_final.h5')\n",
        "                print(\"Modelo ConvLSTM guardado como 'convlstm_model_final.h5'\")\n",
        "                \n",
        "                # Visualizar la historia del entrenamiento\n",
        "                plt.figure(figsize=(12, 5))\n",
        "                plt.subplot(1, 2, 1)\n",
        "                plt.plot(history.history['loss'])\n",
        "                plt.plot(history.history['val_loss'])\n",
        "                plt.title('P√©rdida del modelo ConvLSTM')\n",
        "                plt.ylabel('P√©rdida')\n",
        "                plt.xlabel('√âpoca')\n",
        "                plt.legend(['Entrenamiento', 'Validaci√≥n'], loc='upper right')\n",
        "                \n",
        "                plt.subplot(1, 2, 2)\n",
        "                plt.plot(history.history['mae'])\n",
        "                plt.plot(history.history['val_mae'])\n",
        "                plt.title('Error absoluto medio ConvLSTM')\n",
        "                plt.ylabel('MAE')\n",
        "                plt.xlabel('√âpoca')\n",
        "                plt.legend(['Entrenamiento', 'Validaci√≥n'], loc='upper right')\n",
        "                \n",
        "                plt.tight_layout()\n",
        "                plt.savefig(model_output_dir / 'convlstm_training_history.png')\n",
        "                plt.show()\n",
        "                \n",
        "                # A√±adir resultados a nuestro diccionario de comparaci√≥n\n",
        "                resultados_base['ConvLSTM'] = (convlstm_rmse, convlstm_mae, convlstm_r2)\n",
        "            else:\n",
        "                print(\"No se pudieron preparar datos para ConvLSTM.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al ejecutar preparaci√≥n de datos para ConvLSTM: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "    else:\n",
        "        print(\"No se encontraron columnas temporales o espaciales suficientes para implementar ConvLSTM.\")\n",
        "        print(\"El modelo ConvLSTM requiere al menos una columna temporal y dos columnas espaciales.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87873507",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejecutar optimizaci√≥n adaptativa de memoria RAM para modelos base\n",
        "print(\"\\nüîç Ejecutando optimizaci√≥n adaptativa de memoria RAM para Random Forest...\")\n",
        "rf_params, rf_model_opt, rf_metrics_opt = run_memory_efficient_optimization('RandomForest', X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "\n",
        "print(\"\\nüîç Ejecutando optimizaci√≥n adaptativa de memoria RAM para XGBoost...\")\n",
        "xgb_params, xgb_model_opt, xgb_metrics_opt = run_memory_efficient_optimization('XGBoost', X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "\n",
        "print(\"\\nüîç Ejecutando optimizaci√≥n adaptativa de memoria RAM para LightGBM...\")\n",
        "lgbm_params, lgbm_model_opt, lgbm_metrics_opt = run_memory_efficient_optimization('LightGBM', X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "\n",
        "# Resumen de par√°metros √≥ptimos encontrados\n",
        "print(\"\\nüìä Mejores par√°metros encontrados para cada modelo:\")\n",
        "print(f\"\\nRandom Forest: {rf_params}\")\n",
        "print(f\"\\nXGBoost: {xgb_params}\")\n",
        "print(f\"\\nLightGBM: {lgbm_params}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "precipitation_prediction",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
