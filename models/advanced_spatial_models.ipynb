{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🚀 Mejoras Propuestas para Modelos Espaciales de Precipitación\n",
        "\n",
        "## 📊 Análisis de Resultados Actuales\n",
        "\n",
        "### Problemas Identificados:\n",
        "\n",
        "1. **R² Negativos**: Varios modelos muestran R² < 0, especialmente en H=2\n",
        "   - ConvLSTM en KCE/PAFC: R² = -0.042 y -0.161\n",
        "   - ConvRNN en KCE: R² = -0.340\n",
        "   - ConvGRU en PAFC: R² = -0.052\n",
        "\n",
        "2. **Degradación en H=3**: RMSE > 100 en muchos casos\n",
        "   - ConvLSTM: RMSE hasta 111.3\n",
        "   - ConvGRU: RMSE hasta 106.9\n",
        "   - ConvRNN: RMSE hasta 87.3\n",
        "\n",
        "3. **Inestabilidad**: Batch size = 4 es muy pequeño\n",
        "\n",
        "### Mejores Resultados Actuales:\n",
        "- **H=1**: ConvRNN BASIC (RMSE=43.49, R²=0.767)\n",
        "- **H=2**: ConvGRU BASIC (RMSE=32.40, R²=0.401)\n",
        "- **H=3**: ConvRNN KCE (RMSE=71.47, R²=0.611)\n",
        "\n",
        "## 🔧 Mejoras Implementadas\n",
        "\n",
        "### 1. Optimización de Hiperparámetros\n",
        "\n",
        "| Parámetro | Valor Original | Valor Mejorado | Justificación |\n",
        "|-----------|----------------|----------------|---------------|\n",
        "| Batch Size | 4 | **16** | Mayor estabilidad en gradientes |\n",
        "| Learning Rate | 1e-3 | **5e-4** | Convergencia más suave |\n",
        "| Epochs | 50 | **100** | Más tiempo con early stopping |\n",
        "| Patience | 6 | **10** | Evitar detención prematura |\n",
        "| Dropout | 0 | **0.2** | Regularización |\n",
        "| L2 Reg | 0 | **1e-5** | Prevenir overfitting |\n",
        "\n",
        "### 2. Arquitecturas Mejoradas\n",
        "\n",
        "#### ConvLSTM con Atención (ConvLSTM_Att)\n",
        "```python\n",
        "- 3 capas ConvLSTM (64→32→16 filtros)\n",
        "- CBAM (Channel + Spatial Attention)\n",
        "- BatchNorm + Dropout en cada capa\n",
        "- Cabeza multi-escala (1×1, 3×3, 5×5)\n",
        "```\n",
        "\n",
        "#### ConvGRU Residual (ConvGRU_Res)\n",
        "```python\n",
        "- Skip connections desde input\n",
        "- BatchNorm mejorado\n",
        "- 2 bloques ConvGRU (64→32 filtros)\n",
        "- Conexión residual final\n",
        "```\n",
        "\n",
        "#### Transformer Híbrido (Hybrid_Trans)\n",
        "```python\n",
        "- Encoder CNN temporal\n",
        "- Multi-head attention (4 heads)\n",
        "- LSTM para agregación temporal\n",
        "- Decoder espacial\n",
        "```\n",
        "\n",
        "### 3. Técnicas Avanzadas\n",
        "\n",
        "#### Learning Rate Scheduling\n",
        "- **Warmup**: 5 épocas iniciales\n",
        "- **Cosine Decay**: Reducción suave después del warmup\n",
        "- **ReduceLROnPlateau**: Reducción adicional si se estanca\n",
        "\n",
        "#### Data Augmentation\n",
        "- Ruido gaussiano (σ=0.005)\n",
        "- Preserva coherencia espacial y temporal\n",
        "\n",
        "#### Regularización\n",
        "- Dropout espacial (0.2)\n",
        "- L2 en todos los pesos\n",
        "- Batch Normalization\n",
        "\n",
        "## 📈 Mejoras Esperadas\n",
        "\n",
        "### Por Horizonte:\n",
        "- **H=1**: RMSE < 40 (mejora ~8%)\n",
        "- **H=2**: RMSE < 30, R² > 0.5 (mejora significativa)\n",
        "- **H=3**: RMSE < 65, R² > 0.65 (mejora ~10%)\n",
        "\n",
        "### Por Modelo:\n",
        "1. **ConvLSTM_Att**: Mejor captura de patrones espaciales relevantes\n",
        "2. **ConvGRU_Res**: Mayor estabilidad y menos degradación temporal\n",
        "3. **Hybrid_Trans**: Mejor modelado de dependencias largas\n",
        "\n",
        "## 🚀 Próximos Pasos\n",
        "\n",
        "### Corto Plazo:\n",
        "1. Entrenar modelos con configuración mejorada\n",
        "2. Validar mejoras en métricas\n",
        "3. Análisis de errores por región\n",
        "\n",
        "### Medio Plazo:\n",
        "1. **Ensemble Methods**: Combinar mejores modelos\n",
        "2. **Multi-Task Learning**: Predecir múltiples variables\n",
        "3. **Physics-Informed Loss**: Incorporar restricciones físicas\n",
        "\n",
        "### Largo Plazo:\n",
        "1. **Modelos 3D**: ConvLSTM3D para capturar altura\n",
        "2. **Graph Neural Networks**: Para relaciones espaciales irregulares\n",
        "3. **Uncertainty Quantification**: Intervalos de confianza\n",
        "\n",
        "## 💻 Uso del Script\n",
        "\n",
        "```bash\n",
        "# Entrenar modelos avanzados\n",
        "python models/train_advanced_models.py\n",
        "\n",
        "# Con GPU específica\n",
        "CUDA_VISIBLE_DEVICES=0 python models/train_advanced_models.py\n",
        "```\n",
        "\n",
        "## 📊 Monitoreo\n",
        "\n",
        "Los resultados se guardan en:\n",
        "- `models/output/Advanced_Spatial/advanced_results.csv`\n",
        "- Historiales de entrenamiento por experimento\n",
        "- Modelos guardados en formato .keras\n",
        "\n",
        "## 🔍 Comparación con Baseline\n",
        "\n",
        "El script genera automáticamente comparaciones con los modelos originales, mostrando:\n",
        "- % de mejora en RMSE\n",
        "- Evolución de R² por horizonte\n",
        "- Tabla resumen de mejores modelos \n",
        "\n",
        "## 📊 Análisis de Resultados y Mejoras Propuestas\n",
        "\n",
        "### Problemas Identificados en los Modelos Originales:\n",
        "1. **R² negativos** en varios casos (especialmente H=2)\n",
        "2. **Degradación severa** en H=3 (RMSE >100)\n",
        "3. **Batch size muy pequeño** (4) causando inestabilidad\n",
        "4. **Arquitecturas muy simples** (solo 2 capas)\n",
        "\n",
        "### Mejoras Implementadas:\n",
        "\n",
        "#### 1. **Hiperparámetros Optimizados**\n",
        "- Batch size: 4 → 16 (mejor estabilidad)\n",
        "- Learning rate: 1e-3 → 5e-4 (más conservador)\n",
        "- Epochs: 50 → 100 (con early stopping)\n",
        "- Regularización: Dropout (0.2) + L2 (1e-5)\n",
        "\n",
        "#### 2. **Arquitecturas Mejoradas**\n",
        "- **ConvLSTM con Atención**: CBAM (Channel + Spatial Attention)\n",
        "- **ConvGRU con Skip Connections**: Conexiones residuales\n",
        "- **PredRNN++**: Estado del arte para predicción espacio-temporal\n",
        "- **ConvTransformer**: Híbrido CNN + Transformer\n",
        "\n",
        "#### 3. **Técnicas Avanzadas**\n",
        "- Learning rate scheduling (cosine decay con warmup)\n",
        "- Data augmentation (ruido gaussiano)\n",
        "- Multi-scale processing en la cabeza de salida\n",
        "- Batch normalization en todas las capas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ───────────────────────── IMPORTS MEJORADOS ─────────────────────────\n",
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import sys, os, gc, warnings\n",
        "import numpy as np, pandas as pd, xarray as xr\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv2D, ConvLSTM2D, SimpleRNN, LSTM, GRU, Flatten, Dense, Reshape,\n",
        "    Lambda, Permute, Layer, TimeDistributed, BatchNormalization, Dropout,\n",
        "    Add, Multiply, Concatenate, GlobalAveragePooling2D, Activation,\n",
        "    LayerNormalization, MultiHeadAttention, Conv3D, MaxPooling2D\n",
        ")\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, \n",
        "    CSVLogger, Callback, LearningRateScheduler\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam, AdamW\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt, seaborn as sns, geopandas as gpd, imageio.v2 as imageio\n",
        "import cartopy.crs as ccrs\n",
        "from IPython.display import clear_output, display\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context('notebook')\n",
        "\n",
        "# GPU config\n",
        "for g in tf.config.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(g, True)\n",
        "\n",
        "print(\"✅ Imports completados\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ───────────────────────── CONFIGURACIÓN MEJORADA ─────────────────────────\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p / '.git').exists():\n",
        "            BASE_PATH = p; break\n",
        "\n",
        "# Paths\n",
        "DATA_FILE = BASE_PATH/'data'/'output'/(\n",
        "    'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc')\n",
        "OUT_ROOT = BASE_PATH/'models'/'output'/'Advanced_Spatial'\n",
        "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "SHAPE_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
        "DEPT_GDF = gpd.read_file(SHAPE_DIR/'MGN_Departamento.shp')\n",
        "\n",
        "# ⚡ HIPERPARÁMETROS OPTIMIZADOS\n",
        "INPUT_WINDOW = 60\n",
        "HORIZON = 3\n",
        "EPOCHS = 100  # Más épocas con early stopping\n",
        "BATCH = 16    # Batch size aumentado para estabilidad\n",
        "LR = 5e-4     # Learning rate más conservador\n",
        "PATIENCE = 10 # Más paciencia\n",
        "DROPOUT = 0.2 # Regularización\n",
        "L2_REG = 1e-5 # Regularización L2\n",
        "\n",
        "# Feature sets\n",
        "BASE_FEATS = ['year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
        "              'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
        "              'elevation','slope','aspect']\n",
        "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
        "KCE_FEATS = BASE_FEATS + ELEV_CLUSTER\n",
        "PAFC_FEATS = KCE_FEATS + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
        "EXPERIMENTS = {'BASIC':BASE_FEATS, 'KCE':KCE_FEATS, 'PAFC':PAFC_FEATS}\n",
        "\n",
        "# Cargar dataset\n",
        "ds = xr.open_dataset(DATA_FILE)\n",
        "lat, lon = len(ds.latitude), len(ds.longitude)\n",
        "print(f\"Dataset → time={len(ds.time)}, lat={lat}, lon={lon}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ───────────────────────── CAPAS DE ATENCIÓN ─────────────────────────\n",
        "\n",
        "class SpatialAttention(Layer):\n",
        "    \"\"\"Atención espacial para resaltar regiones importantes\"\"\"\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.conv = Conv2D(1, (7, 7), padding='same', activation='sigmoid')\n",
        "        super().build(input_shape)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # Calcular estadísticas del canal\n",
        "        avg_pool = K.mean(inputs, axis=-1, keepdims=True)\n",
        "        max_pool = K.max(inputs, axis=-1, keepdims=True)\n",
        "        concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
        "        \n",
        "        # Generar mapa de atención\n",
        "        attention = self.conv(concat)\n",
        "        \n",
        "        return Multiply()([inputs, attention])\n",
        "\n",
        "\n",
        "class ChannelAttention(Layer):\n",
        "    \"\"\"Atención de canal para ponderar features importantes\"\"\"\n",
        "    \n",
        "    def __init__(self, reduction_ratio=8, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "        self.fc1 = Dense(channels // self.reduction_ratio, activation='relu')\n",
        "        self.fc2 = Dense(channels, activation='sigmoid')\n",
        "        super().build(input_shape)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # Global pooling\n",
        "        avg_pool = GlobalAveragePooling2D()(inputs)\n",
        "        max_pool = K.max(inputs, axis=[1, 2])\n",
        "        \n",
        "        # Shared MLP\n",
        "        avg_out = self.fc2(self.fc1(avg_pool))\n",
        "        max_out = self.fc2(self.fc1(max_pool))\n",
        "        \n",
        "        # Combinar\n",
        "        attention = avg_out + max_out\n",
        "        attention = K.expand_dims(K.expand_dims(attention, 1), 1)\n",
        "        \n",
        "        return Multiply()([inputs, attention])\n",
        "\n",
        "\n",
        "class CBAM(Layer):\n",
        "    \"\"\"Convolutional Block Attention Module\"\"\"\n",
        "    \n",
        "    def __init__(self, reduction_ratio=8, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.channel_attention = ChannelAttention(reduction_ratio)\n",
        "        self.spatial_attention = SpatialAttention()\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.channel_attention(inputs)\n",
        "        x = self.spatial_attention(x)\n",
        "        return x\n",
        "\n",
        "print(\"✅ Capas de atención implementadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ───────────────────────── CAPAS AVANZADAS ─────────────────────────\n",
        "\n",
        "class ConvGRU2DCell(Layer):\n",
        "    \"\"\"Celda ConvGRU2D mejorada con BatchNorm\"\"\"\n",
        "    \n",
        "    def __init__(self, filters, kernel_size, padding='same', activation='tanh',\n",
        "                 recurrent_activation='sigmoid', use_batch_norm=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
        "        self.padding = padding\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.recurrent_activation = tf.keras.activations.get(recurrent_activation)\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.state_size = (filters,)\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "        \n",
        "        # Kernels\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(*self.kernel_size, input_dim, self.filters * 3),\n",
        "            initializer='glorot_uniform',\n",
        "            regularizer=l1_l2(l1=0, l2=L2_REG),\n",
        "            name='kernel'\n",
        "        )\n",
        "        \n",
        "        self.recurrent_kernel = self.add_weight(\n",
        "            shape=(*self.kernel_size, self.filters, self.filters * 3),\n",
        "            initializer='orthogonal',\n",
        "            regularizer=l1_l2(l1=0, l2=L2_REG),\n",
        "            name='recurrent_kernel'\n",
        "        )\n",
        "        \n",
        "        self.bias = self.add_weight(\n",
        "            shape=(self.filters * 3,),\n",
        "            initializer='zeros',\n",
        "            name='bias'\n",
        "        )\n",
        "        \n",
        "        if self.use_batch_norm:\n",
        "            self.bn_x = BatchNormalization()\n",
        "            self.bn_h = BatchNormalization()\n",
        "        \n",
        "        super().build(input_shape)\n",
        "    \n",
        "    def call(self, inputs, states, training=None):\n",
        "        h_tm1 = states[0]\n",
        "        \n",
        "        # Convoluciones\n",
        "        x_conv = K.conv2d(inputs, self.kernel, padding=self.padding)\n",
        "        h_conv = K.conv2d(h_tm1, self.recurrent_kernel, padding=self.padding)\n",
        "        \n",
        "        if self.use_batch_norm:\n",
        "            x_conv = self.bn_x(x_conv, training=training)\n",
        "            h_conv = self.bn_h(h_conv, training=training)\n",
        "        \n",
        "        x_z, x_r, x_h = tf.split(x_conv, 3, axis=-1)\n",
        "        h_z, h_r, h_h = tf.split(h_conv, 3, axis=-1)\n",
        "        b_z, b_r, b_h = tf.split(self.bias, 3)\n",
        "        \n",
        "        # Gates\n",
        "        z = self.recurrent_activation(x_z + h_z + b_z)\n",
        "        r = self.recurrent_activation(x_r + h_r + b_r)\n",
        "        \n",
        "        # Hidden state\n",
        "        h_candidate = self.activation(x_h + r * h_h + b_h)\n",
        "        h = (1 - z) * h_tm1 + z * h_candidate\n",
        "        \n",
        "        return h, [h]\n",
        "\n",
        "\n",
        "class ConvGRU2D(Layer):\n",
        "    \"\"\"ConvGRU2D mejorado con soporte para BatchNorm y Dropout\"\"\"\n",
        "    \n",
        "    def __init__(self, filters, kernel_size, padding='same', activation='tanh',\n",
        "                 recurrent_activation='sigmoid', return_sequences=False,\n",
        "                 use_batch_norm=True, dropout=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.activation = activation\n",
        "        self.recurrent_activation = recurrent_activation\n",
        "        self.return_sequences = return_sequences\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.cell = ConvGRU2DCell(\n",
        "            filters, kernel_size, padding, activation, \n",
        "            recurrent_activation, use_batch_norm\n",
        "        )\n",
        "        \n",
        "        if dropout > 0:\n",
        "            self.dropout_layer = Dropout(dropout)\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.cell.build(input_shape[2:])\n",
        "        super().build(input_shape)\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        time_steps = tf.shape(inputs)[1]\n",
        "        height = tf.shape(inputs)[2]\n",
        "        width = tf.shape(inputs)[3]\n",
        "        \n",
        "        # Estado inicial\n",
        "        initial_state = tf.zeros((batch_size, height, width, self.filters))\n",
        "        \n",
        "        # Procesar secuencia\n",
        "        outputs = []\n",
        "        state = initial_state\n",
        "        \n",
        "        for t in range(inputs.shape[1]):\n",
        "            output, [state] = self.cell(inputs[:, t], [state], training=training)\n",
        "            \n",
        "            if self.dropout > 0:\n",
        "                output = self.dropout_layer(output, training=training)\n",
        "                \n",
        "            outputs.append(output)\n",
        "        \n",
        "        outputs = tf.stack(outputs, axis=1)\n",
        "        \n",
        "        if self.return_sequences:\n",
        "            return outputs\n",
        "        else:\n",
        "            return outputs[:, -1]\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'filters': self.filters,\n",
        "            'kernel_size': self.kernel_size,\n",
        "            'padding': self.padding,\n",
        "            'activation': self.activation,\n",
        "            'recurrent_activation': self.recurrent_activation,\n",
        "            'return_sequences': self.return_sequences,\n",
        "            'use_batch_norm': self.use_batch_norm,\n",
        "            'dropout': self.dropout\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"✅ Capas ConvGRU mejoradas implementadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ───────────────────────── MODEL BUILDERS AVANZADOS ─────────────────────────\n",
        "\n",
        "def _advanced_spatial_head(x, use_attention=True):\n",
        "    \"\"\"Cabeza de proyección mejorada con atención opcional\"\"\"\n",
        "    \n",
        "    if use_attention:\n",
        "        x = CBAM()(x)\n",
        "    \n",
        "    # Multi-scale processing\n",
        "    conv1 = Conv2D(HORIZON, (1, 1), padding='same')(x)\n",
        "    conv3 = Conv2D(HORIZON, (3, 3), padding='same')(x)\n",
        "    conv5 = Conv2D(HORIZON, (5, 5), padding='same')(x)\n",
        "    \n",
        "    # Combine multi-scale features\n",
        "    x = Add()([conv1, conv3, conv5])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('linear')(x)\n",
        "    \n",
        "    # Reshape to output format\n",
        "    x = Lambda(lambda t: tf.transpose(t, [0, 3, 1, 2]),\n",
        "               output_shape=(HORIZON, lat, lon))(x)\n",
        "    x = Lambda(lambda t: tf.expand_dims(t, -1),\n",
        "               output_shape=(HORIZON, lat, lon, 1))(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "def build_convlstm_attention(n_feats: int):\n",
        "    \"\"\"ConvLSTM con mecanismo de atención\"\"\"\n",
        "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
        "    \n",
        "    # Primera capa con más filtros\n",
        "    x = ConvLSTM2D(64, (3, 3), padding='same', return_sequences=True,\n",
        "                   kernel_regularizer=l1_l2(l1=0, l2=L2_REG))(inp)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(DROPOUT)(x)\n",
        "    \n",
        "    # Segunda capa con atención\n",
        "    x = ConvLSTM2D(32, (3, 3), padding='same', return_sequences=True,\n",
        "                   kernel_regularizer=l1_l2(l1=0, l2=L2_REG))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    # Aplicar atención temporal\n",
        "    x = TimeDistributed(CBAM())(x)\n",
        "    \n",
        "    # Capa final\n",
        "    x = ConvLSTM2D(16, (3, 3), padding='same', return_sequences=False,\n",
        "                   kernel_regularizer=l1_l2(l1=0, l2=L2_REG))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    out = _advanced_spatial_head(x)\n",
        "    return Model(inp, out, name='ConvLSTM_Attention')\n",
        "\n",
        "\n",
        "def build_convgru_residual(n_feats: int):\n",
        "    \"\"\"ConvGRU con skip connections\"\"\"\n",
        "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
        "    \n",
        "    # Encoder path\n",
        "    enc1 = ConvGRU2D(64, (3, 3), return_sequences=True, \n",
        "                     use_batch_norm=True, dropout=DROPOUT)(inp)\n",
        "    \n",
        "    enc2 = ConvGRU2D(32, (3, 3), return_sequences=True,\n",
        "                     use_batch_norm=True, dropout=DROPOUT)(enc1)\n",
        "    \n",
        "    # Bottleneck\n",
        "    bottleneck = ConvGRU2D(16, (3, 3), return_sequences=False,\n",
        "                           use_batch_norm=True)(enc2)\n",
        "    \n",
        "    # Skip connection from input\n",
        "    skip = TimeDistributed(Conv2D(16, (1, 1), padding='same'))(inp)\n",
        "    skip = Lambda(lambda x: x[:, -1])(skip)  # Take last timestep\n",
        "    \n",
        "    # Combine\n",
        "    x = Add()([bottleneck, skip])\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    out = _advanced_spatial_head(x)\n",
        "    return Model(inp, out, name='ConvGRU_Residual')\n",
        "\n",
        "\n",
        "def build_hybrid_transformer(n_feats: int):\n",
        "    \"\"\"Modelo híbrido CNN + Transformer\"\"\"\n",
        "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
        "    \n",
        "    # Encoder convolucional\n",
        "    x = TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu'))(inp)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    \n",
        "    # Reducir dimensionalidad espacial\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2), padding='same'))(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "    \n",
        "    # Self-attention temporal\n",
        "    x = MultiHeadAttention(num_heads=4, key_dim=32, dropout=DROPOUT)(x, x)\n",
        "    x = LayerNormalization()(x)\n",
        "    \n",
        "    # Agregación temporal con LSTM\n",
        "    x = LSTM(128, return_sequences=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(DROPOUT)(x)\n",
        "    \n",
        "    # Decodificador espacial\n",
        "    x = Dense(lat * lon * 16)(x)\n",
        "    x = Reshape((lat, lon, 16))(x)\n",
        "    \n",
        "    out = _advanced_spatial_head(x)\n",
        "    return Model(inp, out, name='Hybrid_Transformer')\n",
        "\n",
        "\n",
        "# Diccionario de modelos\n",
        "ADVANCED_MODELS = {\n",
        "    'ConvLSTM_Att': build_convlstm_attention,\n",
        "    'ConvGRU_Res': build_convgru_residual,\n",
        "    'Hybrid_Trans': build_hybrid_transformer\n",
        "}\n",
        "\n",
        "print(\"✅ Model builders avanzados creados\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ───────────────────────── UTILIDADES DE ENTRENAMIENTO ─────────────────────────\n",
        "\n",
        "def cosine_decay_with_warmup(epoch, lr_base=LR, total_epochs=EPOCHS, warmup_epochs=5):\n",
        "    \"\"\"Cosine decay learning rate con warmup\"\"\"\n",
        "    if epoch < warmup_epochs:\n",
        "        return lr_base * (epoch + 1) / warmup_epochs\n",
        "    else:\n",
        "        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
        "        return lr_base * 0.5 * (1 + np.cos(np.pi * progress))\n",
        "\n",
        "\n",
        "class AdvancedTrainingMonitor(Callback):\n",
        "    \"\"\"Monitor avanzado con métricas adicionales\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, experiment_name, validation_data=None):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.experiment_name = experiment_name\n",
        "        self.validation_data = validation_data\n",
        "        self.history = {\n",
        "            'loss': [], 'val_loss': [], 'mae': [], 'val_mae': [],\n",
        "            'lr': [], 'epoch': []\n",
        "        }\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Guardar métricas\n",
        "        self.history['epoch'].append(epoch + 1)\n",
        "        self.history['loss'].append(logs.get('loss'))\n",
        "        self.history['val_loss'].append(logs.get('val_loss'))\n",
        "        self.history['mae'].append(logs.get('mae'))\n",
        "        self.history['val_mae'].append(logs.get('val_mae'))\n",
        "        \n",
        "        # Learning rate\n",
        "        lr = float(K.get_value(self.model.optimizer.learning_rate))\n",
        "        self.history['lr'].append(lr)\n",
        "        \n",
        "        # Clear output\n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        # Crear visualización mejorada\n",
        "        fig = plt.figure(figsize=(20, 5))\n",
        "        \n",
        "        # Loss curves\n",
        "        ax1 = plt.subplot(141)\n",
        "        ax1.plot(self.history['epoch'], self.history['loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "        ax1.plot(self.history['epoch'], self.history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title(f'{self.model_name} - Loss Evolution')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # MAE curves\n",
        "        ax2 = plt.subplot(142)\n",
        "        ax2.plot(self.history['epoch'], self.history['mae'], 'g-', label='Train MAE', linewidth=2)\n",
        "        ax2.plot(self.history['epoch'], self.history['val_mae'], 'm-', label='Val MAE', linewidth=2)\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('MAE')\n",
        "        ax2.set_title('MAE Evolution')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Tasa de mejora y convergencia\n",
        "        ax3 = plt.subplot(143)\n",
        "        if len(self.history['val_loss']) > 1:\n",
        "            # Calcular tasa de mejora epoch a epoch\n",
        "            improvements = []\n",
        "            for i in range(1, len(self.history['val_loss'])):\n",
        "                prev_loss = self.history['val_loss'][i-1]\n",
        "                curr_loss = self.history['val_loss'][i]\n",
        "                improvement = ((prev_loss - curr_loss) / prev_loss) * 100\n",
        "                improvements.append(improvement)\n",
        "            \n",
        "            # Plot de tasa de mejora\n",
        "            ax3.plot(self.history['epoch'][1:], improvements, 'g-', linewidth=2, alpha=0.7)\n",
        "            ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "            ax3.fill_between(self.history['epoch'][1:], improvements, 0, \n",
        "                           where=[x > 0 for x in improvements], \n",
        "                           color='green', alpha=0.3, label='Mejora')\n",
        "            ax3.fill_between(self.history['epoch'][1:], improvements, 0, \n",
        "                           where=[x <= 0 for x in improvements], \n",
        "                           color='red', alpha=0.3, label='Empeoramiento')\n",
        "            \n",
        "            # Línea de tendencia suavizada\n",
        "            if len(improvements) > 5:\n",
        "                window = min(5, len(improvements)//3)\n",
        "                smoothed = pd.Series(improvements).rolling(window=window, center=True).mean()\n",
        "                ax3.plot(self.history['epoch'][1:], smoothed, 'b-', linewidth=2.5, \n",
        "                        label=f'Tendencia ({window} epochs)')\n",
        "            \n",
        "            ax3.set_xlabel('Epoch')\n",
        "            ax3.set_ylabel('Tasa de Mejora (%)')\n",
        "            ax3.set_title('Progreso del Entrenamiento')\n",
        "            ax3.legend(loc='best')\n",
        "            ax3.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Añadir anotación de convergencia\n",
        "            if len(improvements) > 10:\n",
        "                recent_avg = np.mean(improvements[-5:])\n",
        "                if abs(recent_avg) < 0.5:\n",
        "                    ax3.text(0.95, 0.95, '⚠️ Posible convergencia', \n",
        "                            transform=ax3.transAxes, ha='right', va='top',\n",
        "                            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
        "        \n",
        "        # Loss distribution (últimas 10 épocas)\n",
        "        if len(self.history['loss']) > 10:\n",
        "            ax4 = plt.subplot(144)\n",
        "            recent_losses = self.history['val_loss'][-10:]\n",
        "            ax4.hist(recent_losses, bins=10, alpha=0.7, color='purple')\n",
        "            ax4.axvline(np.mean(recent_losses), color='red', linestyle='--', \n",
        "                       label=f'Mean: {np.mean(recent_losses):.4f}')\n",
        "            ax4.set_xlabel('Val Loss')\n",
        "            ax4.set_ylabel('Frequency')\n",
        "            ax4.set_title('Recent Val Loss Distribution')\n",
        "            ax4.legend()\n",
        "        \n",
        "        plt.suptitle(f'{self.model_name} - {self.experiment_name} - Epoch {epoch + 1}', \n",
        "                    fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        display(fig)\n",
        "        plt.close()\n",
        "        \n",
        "        # Mostrar métricas\n",
        "        print(f\"\\n📊 Época {epoch + 1}/{self.params['epochs']}\")\n",
        "        print(f\"   • Loss: {logs.get('loss'):.6f} | Val Loss: {logs.get('val_loss'):.6f}\")\n",
        "        print(f\"   • MAE: {logs.get('mae'):.6f} | Val MAE: {logs.get('val_mae'):.6f}\")\n",
        "        print(f\"   • Learning Rate: {lr:.2e}\")\n",
        "        \n",
        "        # Calcular mejora\n",
        "        if len(self.history['val_loss']) > 1:\n",
        "            improvement = (self.history['val_loss'][-2] - self.history['val_loss'][-1]) \n",
        "            improvement_pct = improvement / self.history['val_loss'][-2] * 100\n",
        "            print(f\"   • Mejora: {improvement:.6f} ({improvement_pct:.2f}%)\")\n",
        "            \n",
        "            # Detectar overfitting\n",
        "            overfit_ratio = self.history['val_loss'][-1] / self.history['loss'][-1]\n",
        "            if overfit_ratio > 1.5:\n",
        "                print(f\"   ⚠️  Posible overfitting detectado (ratio: {overfit_ratio:.2f})\")\n",
        "\n",
        "\n",
        "def create_callbacks(model_name, exp_name, model_path):\n",
        "    \"\"\"Crear callbacks optimizados\"\"\"\n",
        "    \n",
        "    # Crear directorio para métricas si no existe\n",
        "    metrics_dir = model_path.parent / 'training_metrics'\n",
        "    metrics_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    callbacks = [\n",
        "        # Early stopping mejorado\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=PATIENCE,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1,\n",
        "            min_delta=1e-4\n",
        "        ),\n",
        "        \n",
        "        # Model checkpoint\n",
        "        ModelCheckpoint(\n",
        "            model_path,\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        \n",
        "        # Reduce LR on plateau\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=PATIENCE//2,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        \n",
        "        # Learning rate scheduler (cosine decay)\n",
        "        LearningRateScheduler(cosine_decay_with_warmup, verbose=0),\n",
        "        \n",
        "        # CSV logger\n",
        "        CSVLogger(\n",
        "            metrics_dir / f\"{model_name}_training_log.csv\",\n",
        "            separator=',',\n",
        "            append=False\n",
        "        ),\n",
        "        \n",
        "        # Advanced monitor\n",
        "        AdvancedTrainingMonitor(model_name, exp_name)\n",
        "    ]\n",
        "    \n",
        "    return callbacks\n",
        "\n",
        "print(\"✅ Utilidades de entrenamiento creadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ───────────────────────── HELPERS ─────────────────────────\n",
        "\n",
        "def windowed_arrays(X:np.ndarray, y:np.ndarray):\n",
        "    \"\"\"Crear ventanas deslizantes para series temporales\"\"\"\n",
        "    seq_X, seq_y = [], []\n",
        "    T = len(X)\n",
        "    for start in range(T-INPUT_WINDOW-HORIZON+1):\n",
        "        end_w = start+INPUT_WINDOW\n",
        "        end_y = end_w+HORIZON\n",
        "        Xw, yw = X[start:end_w], y[end_w:end_y]\n",
        "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
        "            continue\n",
        "        seq_X.append(Xw)\n",
        "        seq_y.append(yw)\n",
        "    return np.asarray(seq_X,dtype=np.float32), np.asarray(seq_y,dtype=np.float32)\n",
        "\n",
        "\n",
        "def quick_plot(ax,data,cmap,title,vmin=None,vmax=None):\n",
        "    \"\"\"Plotear mapa geográfico\"\"\"\n",
        "    mesh = ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',\n",
        "                         vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
        "    ax.coastlines()\n",
        "    ax.add_geometries(DEPT_GDF.geometry,ccrs.PlateCarree(),\n",
        "                      edgecolor='black',facecolor='none',linewidth=1)\n",
        "    ax.gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
        "    ax.set_title(title,fontsize=9)\n",
        "    return mesh\n",
        "\n",
        "\n",
        "def save_hyperparameters(exp_path, model_name, hyperparams):\n",
        "    \"\"\"Guarda los hiperparámetros en un archivo JSON\"\"\"\n",
        "    hp_file = exp_path / f\"{model_name}_hyperparameters.json\"\n",
        "    with open(hp_file, 'w') as f:\n",
        "        json.dump(hyperparams, f, indent=4)\n",
        "    print(f\"   💾 Hiperparámetros guardados en: {hp_file.name}\")\n",
        "\n",
        "\n",
        "def plot_learning_curves(history, exp_path, model_name, show=True):\n",
        "    \"\"\"Genera y guarda las curvas de aprendizaje\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Loss\n",
        "    axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "    axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss (MSE)')\n",
        "    axes[0].set_title(f'{model_name} - Loss Evolution')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Análisis de Convergencia y Estabilidad\n",
        "    val_losses = history.history['val_loss']\n",
        "    train_losses = history.history['loss']\n",
        "    \n",
        "    if len(val_losses) > 1:\n",
        "        # Calcular métricas de convergencia\n",
        "        epochs = range(1, len(val_losses) + 1)\n",
        "        \n",
        "        # 1. Ratio de overfitting\n",
        "        overfit_ratio = [val_losses[i] / train_losses[i] for i in range(len(val_losses))]\n",
        "        \n",
        "        # 2. Estabilidad (desviación estándar móvil)\n",
        "        window = min(5, len(val_losses)//3)\n",
        "        val_std = pd.Series(val_losses).rolling(window=window).std()\n",
        "        \n",
        "        # Crear subplot con dos ejes Y\n",
        "        ax2_left = axes[1]\n",
        "        ax2_right = ax2_left.twinx()\n",
        "        \n",
        "        # Plot ratio de overfitting\n",
        "        line1 = ax2_left.plot(epochs, overfit_ratio, 'r-', linewidth=2, \n",
        "                             label='Ratio Val/Train', alpha=0.8)\n",
        "        ax2_left.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
        "        ax2_left.fill_between(epochs, 1.0, overfit_ratio, \n",
        "                            where=[x > 1.0 for x in overfit_ratio],\n",
        "                            color='red', alpha=0.2)\n",
        "        ax2_left.set_xlabel('Epoch')\n",
        "        ax2_left.set_ylabel('Ratio Val Loss / Train Loss', color='red')\n",
        "        ax2_left.tick_params(axis='y', labelcolor='red')\n",
        "        \n",
        "        # Plot estabilidad\n",
        "        line2 = ax2_right.plot(epochs[window-1:], val_std[window-1:], 'b-', \n",
        "                             linewidth=2, label='Estabilidad', alpha=0.8)\n",
        "        ax2_right.set_ylabel('Desviación Estándar (ventana móvil)', color='blue')\n",
        "        ax2_right.tick_params(axis='y', labelcolor='blue')\n",
        "        \n",
        "        # Título y leyenda combinada\n",
        "        ax2_left.set_title(f'{model_name} - Análisis de Convergencia')\n",
        "        \n",
        "        # Combinar leyendas\n",
        "        lines = line1 + line2\n",
        "        labels = [l.get_label() for l in lines]\n",
        "        ax2_left.legend(lines, labels, loc='upper left')\n",
        "        \n",
        "        ax2_left.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Añadir zonas de interpretación\n",
        "        if max(overfit_ratio) > 1.5:\n",
        "            ax2_left.text(0.02, 0.98, '⚠️ Alto overfitting detectado', \n",
        "                        transform=ax2_left.transAxes, va='top',\n",
        "                        bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
        "        elif min(val_std[window-1:]) < 0.001:\n",
        "            ax2_left.text(0.02, 0.98, '✓ Entrenamiento estable', \n",
        "                        transform=ax2_left.transAxes, va='top',\n",
        "                        bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n",
        "    else:\n",
        "        axes[1].text(0.5, 0.5, 'Insufficient data for convergence analysis', \n",
        "                    transform=axes[1].transAxes, ha='center', va='center',\n",
        "                    fontsize=12, color='gray')\n",
        "        axes[1].set_title(f'{model_name} - Convergence Analysis')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Guardar\n",
        "    curves_path = exp_path / f\"{model_name}_learning_curves.png\"\n",
        "    plt.savefig(curves_path, dpi=150, bbox_inches='tight')\n",
        "    \n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "    \n",
        "    return curves_path\n",
        "\n",
        "\n",
        "def print_training_summary(history, model_name, exp_name):\n",
        "    \"\"\"Imprime un resumen del entrenamiento\"\"\"\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    final_val_loss = history.history['val_loss'][-1]\n",
        "    best_val_loss = min(history.history['val_loss'])\n",
        "    best_epoch = history.history['val_loss'].index(best_val_loss) + 1\n",
        "    \n",
        "    print(f\"\\n   📊 Resumen de entrenamiento {model_name} - {exp_name}:\")\n",
        "    print(f\"      • Épocas totales: {len(history.history['loss'])}\")\n",
        "    print(f\"      • Loss final (train): {final_loss:.6f}\")\n",
        "    print(f\"      • Loss final (val): {final_val_loss:.6f}\")\n",
        "    print(f\"      • Mejor loss (val): {best_val_loss:.6f} en época {best_epoch}\")\n",
        "    if 'lr' in history.history and len(history.history['lr']) > 0:\n",
        "        final_lr = history.history['lr'][-1]\n",
        "        print(f\"      • Learning rate final: {final_lr:.2e}\")\n",
        "    else:\n",
        "        print(f\"      • Learning rate final: No disponible\")\n",
        "\n",
        "print(\"✅ Funciones helper creadas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ───────────────────────── TRAIN + EVAL LOOP ─────────────────────────\n",
        "\n",
        "# Diccionario para almacenar historiales de entrenamiento\n",
        "all_histories = {}\n",
        "results = []\n",
        "\n",
        "for exp, feat_list in EXPERIMENTS.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"🔬 EXPERIMENTO: {exp} ({len(feat_list)} features)\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Preparar datos\n",
        "    Xarr = ds[feat_list].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
        "    yarr = ds['total_precipitation'].values.astype(np.float32)[...,None]\n",
        "    X, y = windowed_arrays(Xarr, yarr)\n",
        "    split = int(0.8*len(X))\n",
        "    val_split = int(0.9*len(X))\n",
        "\n",
        "    # Normalización\n",
        "    sx = StandardScaler().fit(X[:split].reshape(-1,len(feat_list)))\n",
        "    sy = StandardScaler().fit(y[:split].reshape(-1,1))\n",
        "    X_sc = sx.transform(X.reshape(-1,len(feat_list))).reshape(X.shape)\n",
        "    y_sc = sy.transform(y.reshape(-1,1)).reshape(y.shape)\n",
        "    \n",
        "    # Splits\n",
        "    X_tr, X_va, X_te = X_sc[:split], X_sc[split:val_split], X_sc[val_split:]\n",
        "    y_tr, y_va, y_te = y_sc[:split], y_sc[split:val_split], y_sc[val_split:]\n",
        "    \n",
        "    print(f\"   Datos: Train={len(X_tr)}, Val={len(X_va)}, Test={len(X_te)}\")\n",
        "\n",
        "    OUT_EXP = OUT_ROOT/exp\n",
        "    OUT_EXP.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Crear subdirectorio para métricas de entrenamiento\n",
        "    METRICS_DIR = OUT_EXP / 'training_metrics'\n",
        "    METRICS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "    for mdl_name, builder in ADVANCED_MODELS.items():\n",
        "        print(f\"\\n{'─'*50}\")\n",
        "        print(f\"🤖 Modelo: {mdl_name}\")\n",
        "        print(f\"{'─'*50}\")\n",
        "        \n",
        "        model_path = OUT_EXP/f\"{mdl_name.lower()}_best.keras\"\n",
        "        if model_path.exists():\n",
        "            model_path.unlink()\n",
        "        \n",
        "        try:\n",
        "            # Construir modelo\n",
        "            model = builder(n_feats=len(feat_list))\n",
        "            \n",
        "            # Definir optimizador con configuración explícita\n",
        "            optimizer = AdamW(learning_rate=LR, weight_decay=L2_REG)\n",
        "            model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "            \n",
        "            # Hiperparámetros\n",
        "            hyperparams = {\n",
        "                'experiment': exp,\n",
        "                'model': mdl_name,\n",
        "                'features': feat_list,\n",
        "                'n_features': len(feat_list),\n",
        "                'input_window': INPUT_WINDOW,\n",
        "                'horizon': HORIZON,\n",
        "                'batch_size': BATCH,\n",
        "                'initial_lr': LR,\n",
        "                'epochs': EPOCHS,\n",
        "                'patience': PATIENCE,\n",
        "                'dropout': DROPOUT,\n",
        "                'l2_reg': L2_REG,\n",
        "                'train_samples': len(X_tr),\n",
        "                'val_samples': len(X_va),\n",
        "                'test_samples': len(X_te),\n",
        "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'model_params': model.count_params()\n",
        "            }\n",
        "            \n",
        "            # Guardar hiperparámetros\n",
        "            save_hyperparameters(METRICS_DIR, mdl_name, hyperparams)\n",
        "            \n",
        "            # Callbacks\n",
        "            callbacks = create_callbacks(mdl_name, exp, model_path)\n",
        "            \n",
        "            # Entrenar con verbose=0 para usar nuestro monitor personalizado\n",
        "            print(f\"\\n🏃 Iniciando entrenamiento...\")\n",
        "            print(f\"   📊 Visualización en tiempo real activada\")\n",
        "            print(f\"   📈 Parámetros del modelo: {model.count_params():,}\")\n",
        "            \n",
        "            history = model.fit(\n",
        "                X_tr, y_tr,\n",
        "                validation_data=(X_va, y_va),\n",
        "                epochs=EPOCHS,\n",
        "                batch_size=BATCH,\n",
        "                callbacks=callbacks,\n",
        "                verbose=0  # Usar 0 para que solo se muestre nuestro monitor\n",
        "            )\n",
        "            \n",
        "            # Guardar historial\n",
        "            all_histories[f\"{exp}_{mdl_name}\"] = history\n",
        "            \n",
        "            # Mostrar resumen de entrenamiento\n",
        "            print_training_summary(history, mdl_name, exp)\n",
        "            \n",
        "            # Plotear y guardar curvas de aprendizaje\n",
        "            plot_learning_curves(history, METRICS_DIR, mdl_name, show=True)\n",
        "            \n",
        "            # Guardar historial como JSON\n",
        "            # Obtener learning rates del monitor de entrenamiento si no están en history\n",
        "            training_monitor = [cb for cb in callbacks if isinstance(cb, AdvancedTrainingMonitor)][0]\n",
        "            lr_values = history.history.get('lr', [])\n",
        "            if not lr_values and hasattr(training_monitor, 'history'):\n",
        "                lr_values = training_monitor.history['lr']\n",
        "            \n",
        "            history_dict = {\n",
        "                'loss': [float(x) for x in history.history['loss']],\n",
        "                'val_loss': [float(x) for x in history.history['val_loss']],\n",
        "                'mae': [float(x) for x in history.history.get('mae', [])],\n",
        "                'val_mae': [float(x) for x in history.history.get('val_mae', [])],\n",
        "                'lr': [float(x) for x in lr_values] if lr_values else []\n",
        "            }\n",
        "            \n",
        "            with open(METRICS_DIR / f\"{mdl_name}_history.json\", 'w') as f:\n",
        "                json.dump(history_dict, f, indent=4)\n",
        "\n",
        "            # ─ Evaluación en Test Set ─\n",
        "            print(f\"\\n📊 Evaluando en test set...\")\n",
        "            test_loss, test_mae = model.evaluate(X_te, y_te, verbose=0)\n",
        "            print(f\"   Test Loss: {test_loss:.6f}, Test MAE: {test_mae:.6f}\")\n",
        "\n",
        "            # ─ Predicciones y visualización ─\n",
        "            print(f\"\\n🎯 Generando predicciones...\")\n",
        "            # Usar las primeras 5 muestras del test set\n",
        "            sample_indices = min(5, len(X_te))\n",
        "            y_hat_sc = model.predict(X_te[:sample_indices], verbose=0)\n",
        "            y_hat = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(-1,HORIZON,lat,lon)\n",
        "            y_true = sy.inverse_transform(y_te[:sample_indices].reshape(-1,1)).reshape(-1,HORIZON,lat,lon)\n",
        "\n",
        "            # ─ Métricas de evaluación por horizonte ─\n",
        "            for h in range(HORIZON):\n",
        "                rmse = np.sqrt(mean_squared_error(y_true[:,h].ravel(), y_hat[:,h].ravel()))\n",
        "                mae = mean_absolute_error(y_true[:,h].ravel(), y_hat[:,h].ravel())\n",
        "                r2 = r2_score(y_true[:,h].ravel(), y_hat[:,h].ravel())\n",
        "                \n",
        "                results.append({\n",
        "                    'Experiment': exp,\n",
        "                    'Model': mdl_name,\n",
        "                    'H': h+1,\n",
        "                    'RMSE': rmse,\n",
        "                    'MAE': mae,\n",
        "                    'R2': r2,\n",
        "                    'Test_Loss': test_loss,\n",
        "                    'Parameters': model.count_params()\n",
        "                })\n",
        "                \n",
        "                print(f\"   📈 H={h+1}: RMSE={rmse:.4f}, MAE={mae:.4f}, R²={r2:.4f}\")\n",
        "\n",
        "            # ─ Mapas & GIF ─\n",
        "            print(f\"\\n🎨 Generando visualizaciones...\")\n",
        "            # Usar la primera muestra para visualización\n",
        "            sample_idx = 0\n",
        "            vmin, vmax = 0, max(y_true[sample_idx].max(), y_hat[sample_idx].max())\n",
        "            frames = []\n",
        "            dates = pd.date_range(ds.time.values[-HORIZON], periods=HORIZON, freq='MS')\n",
        "            \n",
        "            for h in range(HORIZON):\n",
        "                err = np.clip(np.abs((y_true[sample_idx,h]-y_hat[sample_idx,h])/(y_true[sample_idx,h]+1e-5))*100, 0, 100)\n",
        "                fig, axs = plt.subplots(1, 3, figsize=(12, 4), subplot_kw={'projection': ccrs.PlateCarree()})\n",
        "                quick_plot(axs[0], y_true[sample_idx,h], 'Blues', f\"Real h={h+1}\", vmin, vmax)\n",
        "                quick_plot(axs[1], y_hat[sample_idx,h], 'Blues', f\"{mdl_name} h={h+1}\", vmin, vmax)\n",
        "                quick_plot(axs[2], err, 'Reds', f\"MAPE% h={h+1}\", 0, 100)\n",
        "                fig.suptitle(f\"{mdl_name} – {exp} – {dates[h].strftime('%Y-%m')}\")\n",
        "                png = OUT_EXP/f\"{mdl_name}_{h+1}.png\"\n",
        "                fig.savefig(png, bbox_inches='tight')\n",
        "                plt.close(fig)\n",
        "                frames.append(imageio.imread(png))\n",
        "            \n",
        "            imageio.mimsave(OUT_EXP/f\"{mdl_name}.gif\", frames, fps=0.5)\n",
        "            print(f\"   ✅ GIF guardado: {OUT_EXP/f'{mdl_name}.gif'}\")\n",
        "            \n",
        "            tf.keras.backend.clear_session()\n",
        "            gc.collect()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️ Error en {mdl_name}: {str(e)}\")\n",
        "            print(f\"  → Saltando {mdl_name} para {exp}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "# ───────────────────────── CSV FINAL ─────────────────────────\n",
        "res_df = pd.DataFrame(results)\n",
        "res_df.to_csv(OUT_ROOT/'metrics_advanced.csv', index=False)\n",
        "print(\"\\n📑 Metrics saved →\", OUT_ROOT/'metrics_advanced.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ───────────────────────── VISUALIZACIÓN COMPARATIVA ─────────────────────────\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📊 GENERANDO VISUALIZACIONES COMPARATIVAS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Crear directorio para comparaciones\n",
        "COMP_DIR = OUT_ROOT / 'comparisons'\n",
        "COMP_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# 1. Comparación de métricas entre modelos\n",
        "if res_df is not None and len(res_df) > 0:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "    \n",
        "    # RMSE por modelo y experimento\n",
        "    pivot_rmse = res_df.pivot_table(values='RMSE', index='Model', columns='Experiment', aggfunc='mean')\n",
        "    pivot_rmse.plot(kind='bar', ax=axes[0,0])\n",
        "    axes[0,0].set_title('RMSE Promedio por Modelo y Experimento', fontsize=14, pad=10)\n",
        "    axes[0,0].set_ylabel('RMSE')\n",
        "    axes[0,0].set_xlabel('Modelo')\n",
        "    axes[0,0].legend(title='Experimento', loc='upper left')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    axes[0,0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # MAE por modelo y experimento\n",
        "    pivot_mae = res_df.pivot_table(values='MAE', index='Model', columns='Experiment', aggfunc='mean')\n",
        "    pivot_mae.plot(kind='bar', ax=axes[0,1])\n",
        "    axes[0,1].set_title('MAE Promedio por Modelo y Experimento', fontsize=14, pad=10)\n",
        "    axes[0,1].set_ylabel('MAE')\n",
        "    axes[0,1].set_xlabel('Modelo')\n",
        "    axes[0,1].legend(title='Experimento', loc='upper left')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # R² por modelo y experimento\n",
        "    pivot_r2 = res_df.pivot_table(values='R2', index='Model', columns='Experiment', aggfunc='mean')\n",
        "    pivot_r2.plot(kind='bar', ax=axes[1,0])\n",
        "    axes[1,0].set_title('R² Promedio por Modelo y Experimento', fontsize=14, pad=10)\n",
        "    axes[1,0].set_ylabel('R²')\n",
        "    axes[1,0].set_xlabel('Modelo')\n",
        "    axes[1,0].legend(title='Experimento', loc='lower right')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    axes[1,0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Evolución de métricas por horizonte\n",
        "    ax_horizon = axes[1,1]\n",
        "    \n",
        "    for model in res_df['Model'].unique():\n",
        "        model_data = res_df[res_df['Model'] == model]\n",
        "        horizon_means = model_data.groupby('H')['RMSE'].mean()\n",
        "        ax_horizon.plot(horizon_means.index, horizon_means.values, \n",
        "                       marker='o', label=model, linewidth=2.5, markersize=8)\n",
        "    \n",
        "    ax_horizon.set_xlabel('Horizonte (meses)')\n",
        "    ax_horizon.set_ylabel('RMSE')\n",
        "    ax_horizon.set_title('Evolución de RMSE por Horizonte', fontsize=14, pad=10)\n",
        "    ax_horizon.legend(title='Modelo', loc='best')\n",
        "    ax_horizon.grid(True, alpha=0.3)\n",
        "    ax_horizon.set_xticks([1, 2, 3])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(COMP_DIR / 'metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# 2. Tabla resumen de mejores modelos\n",
        "print(\"\\n📋 TABLA RESUMEN - MEJORES MODELOS POR EXPERIMENTO:\")\n",
        "print(\"─\" * 60)\n",
        "\n",
        "best_models = res_df.groupby('Experiment').apply(\n",
        "    lambda x: x.loc[x['RMSE'].idxmin()]\n",
        ")[['Model', 'RMSE', 'MAE', 'R2']]\n",
        "\n",
        "print(best_models.to_string())\n",
        "\n",
        "# 3. Comparación con modelos originales si existen\n",
        "old_metrics_path = BASE_PATH / 'models' / 'output' / 'Spatial_CONVRNN' / 'metrics_spatial.csv'\n",
        "if old_metrics_path.exists():\n",
        "    print(\"\\n📊 COMPARACIÓN CON MODELOS ORIGINALES:\")\n",
        "    print(\"─\" * 60)\n",
        "    \n",
        "    old_df = pd.read_csv(old_metrics_path)\n",
        "    \n",
        "    # Calcular mejoras promedio\n",
        "    for exp in EXPERIMENTS.keys():\n",
        "        print(f\"\\n{exp}:\")\n",
        "        \n",
        "        # Mejores modelos nuevos\n",
        "        new_best = res_df[res_df['Experiment'] == exp].groupby('Model')['RMSE'].mean().idxmin()\n",
        "        new_rmse = res_df[(res_df['Experiment'] == exp) & (res_df['Model'] == new_best)]['RMSE'].mean()\n",
        "        \n",
        "        # Mejor modelo original\n",
        "        old_best_rmse = old_df[old_df['Experiment'] == exp]['RMSE'].min()\n",
        "        \n",
        "        improvement = (old_best_rmse - new_rmse) / old_best_rmse * 100\n",
        "        \n",
        "        print(f\"  • Mejor modelo nuevo: {new_best} (RMSE: {new_rmse:.4f})\")\n",
        "        print(f\"  • Mejor RMSE original: {old_best_rmse:.4f}\")\n",
        "        print(f\"  • Mejora: {improvement:.2f}%\")\n",
        "\n",
        "print(\"\\n✅ Visualizaciones comparativas completadas!\")\n",
        "print(f\"📂 Resultados guardados en: {COMP_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ───────────────────────── ANÁLISIS DETALLADO DE RESULTADOS ─────────────────────────\n",
        "\n",
        "if res_df is not None and len(res_df) > 0:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"📊 ANÁLISIS DETALLADO DE RESULTADOS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # 1. Métricas por horizonte de predicción\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "    \n",
        "    metrics = ['RMSE', 'MAE', 'R2']\n",
        "    titles = ['RMSE por Horizonte', 'MAE por Horizonte', 'R² por Horizonte']\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(res_df['Model'].unique())))\n",
        "    \n",
        "    for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Obtener datos pivoteados\n",
        "        data = res_df.groupby(['H', 'Model'])[metric].mean().unstack()\n",
        "        \n",
        "        # Plotear cada modelo\n",
        "        for i, model in enumerate(data.columns):\n",
        "            ax.plot(data.index, data[model], \n",
        "                   marker='o', \n",
        "                   label=model,\n",
        "                   color=colors[i],\n",
        "                   linewidth=2.5,\n",
        "                   markersize=8,\n",
        "                   markeredgewidth=2,\n",
        "                   markeredgecolor='white')\n",
        "        \n",
        "        ax.set_xlabel('Horizonte (meses)', fontsize=12)\n",
        "        ax.set_ylabel(metric, fontsize=12)\n",
        "        ax.set_title(title, fontsize=14, fontweight='bold', pad=10)\n",
        "        ax.grid(True, alpha=0.3, linestyle='--')\n",
        "        ax.set_xticks(data.index)\n",
        "        \n",
        "        # Leyenda solo en el primer gráfico\n",
        "        if idx == 0:\n",
        "            ax.legend(title='Modelo', loc='best', frameon=True, fancybox=True, shadow=True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(COMP_DIR / 'metrics_evolution_by_horizon.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # 2. Tabla visual de métricas\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Preparar datos para la tabla\n",
        "    summary_data = []\n",
        "    experiments = res_df['Experiment'].unique()\n",
        "    models = res_df['Model'].unique()\n",
        "    \n",
        "    # Headers\n",
        "    headers = ['Experimento', 'Modelo', 'RMSE↓', 'MAE↓', 'R²↑', 'Mejor H', 'Parámetros']\n",
        "    \n",
        "    for exp in experiments:\n",
        "        for model in models:\n",
        "            exp_model_data = res_df[(res_df['Experiment'] == exp) & (res_df['Model'] == model)]\n",
        "            if not exp_model_data.empty:\n",
        "                avg_rmse = exp_model_data['RMSE'].mean()\n",
        "                avg_mae = exp_model_data['MAE'].mean()\n",
        "                avg_r2 = exp_model_data['R2'].mean()\n",
        "                best_h = exp_model_data.loc[exp_model_data['RMSE'].idxmin(), 'H']\n",
        "                params = exp_model_data['Parameters'].iloc[0]\n",
        "                \n",
        "                summary_data.append([\n",
        "                    exp, model, \n",
        "                    f'{avg_rmse:.4f}', \n",
        "                    f'{avg_mae:.4f}', \n",
        "                    f'{avg_r2:.4f}',\n",
        "                    f'H={best_h}',\n",
        "                    f'{params:,}'\n",
        "                ])\n",
        "    \n",
        "    # Crear tabla\n",
        "    table = ax.table(cellText=summary_data, colLabels=headers, \n",
        "                    cellLoc='center', loc='center')\n",
        "    \n",
        "    # Estilizar tabla\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1.2, 2)\n",
        "    \n",
        "    # Colorear celdas según rendimiento\n",
        "    for i in range(len(summary_data)):\n",
        "        # Obtener valores para comparación\n",
        "        rmse_val = float(summary_data[i][2])\n",
        "        mae_val = float(summary_data[i][3])\n",
        "        r2_val = float(summary_data[i][4])\n",
        "        \n",
        "        # Encontrar min/max para normalización\n",
        "        all_rmse = [float(row[2]) for row in summary_data]\n",
        "        all_mae = [float(row[3]) for row in summary_data]\n",
        "        all_r2 = [float(row[4]) for row in summary_data]\n",
        "        \n",
        "        # Normalizar y colorear RMSE (menor es mejor)\n",
        "        rmse_norm = (rmse_val - min(all_rmse)) / (max(all_rmse) - min(all_rmse))\n",
        "        rmse_color = plt.cm.RdYlGn(1 - rmse_norm)\n",
        "        table[(i+1, 2)].set_facecolor(rmse_color)\n",
        "        \n",
        "        # Normalizar y colorear MAE (menor es mejor)\n",
        "        mae_norm = (mae_val - min(all_mae)) / (max(all_mae) - min(all_mae))\n",
        "        mae_color = plt.cm.RdYlGn(1 - mae_norm)\n",
        "        table[(i+1, 3)].set_facecolor(mae_color)\n",
        "        \n",
        "        # Normalizar y colorear R² (mayor es mejor)\n",
        "        r2_norm = (r2_val - min(all_r2)) / (max(all_r2) - min(all_r2))\n",
        "        r2_color = plt.cm.RdYlGn(r2_norm)\n",
        "        table[(i+1, 4)].set_facecolor(r2_color)\n",
        "        \n",
        "        # Colorear experimento\n",
        "        exp_colors = {'BASIC': '#e8f4f8', 'KCE': '#f0e8f8', 'PAFC': '#f8e8f0'}\n",
        "        table[(i+1, 0)].set_facecolor(exp_colors.get(summary_data[i][0], 'white'))\n",
        "    \n",
        "    # Colorear headers\n",
        "    for j in range(len(headers)):\n",
        "        table[(0, j)].set_facecolor('#4a86e8')\n",
        "        table[(0, j)].set_text_props(weight='bold', color='white')\n",
        "    \n",
        "    plt.title('Resumen de Métricas por Modelo y Experimento\\n(Verde=Mejor, Rojo=Peor)', \n",
        "             fontsize=16, fontweight='bold', pad=20)\n",
        "    \n",
        "    # Añadir leyenda\n",
        "    plt.text(0.5, -0.05, '↓ = Menor es mejor, ↑ = Mayor es mejor', \n",
        "            transform=ax.transAxes, ha='center', fontsize=10, style='italic')\n",
        "    \n",
        "    plt.savefig(COMP_DIR / 'metrics_summary_table.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # 3. Identificar el mejor modelo global\n",
        "    print(\"\\n🏆 MEJOR MODELO GLOBAL:\")\n",
        "    print(\"─\" * 50)\n",
        "    \n",
        "    # Calcular score compuesto (normalizado)\n",
        "    res_df['score'] = (\n",
        "        (1 - (res_df['RMSE'] - res_df['RMSE'].min()) / (res_df['RMSE'].max() - res_df['RMSE'].min())) +\n",
        "        (1 - (res_df['MAE'] - res_df['MAE'].min()) / (res_df['MAE'].max() - res_df['MAE'].min())) +\n",
        "        ((res_df['R2'] - res_df['R2'].min()) / (res_df['R2'].max() - res_df['R2'].min()))\n",
        "    ) / 3\n",
        "    \n",
        "    best_overall = res_df.loc[res_df['score'].idxmax()]\n",
        "    print(f\"Modelo: {best_overall['Model']}\")\n",
        "    print(f\"Experimento: {best_overall['Experiment']}\")\n",
        "    print(f\"Horizonte: {best_overall['H']}\")\n",
        "    print(f\"RMSE: {best_overall['RMSE']:.4f}\")\n",
        "    print(f\"MAE: {best_overall['MAE']:.4f}\")\n",
        "    print(f\"R²: {best_overall['R2']:.4f}\")\n",
        "    print(f\"Score compuesto: {best_overall['score']:.4f}\")\n",
        "    \n",
        "    # 4. Análisis de mejora por horizonte\n",
        "    print(\"\\n📈 ANÁLISIS DE MEJORA POR HORIZONTE:\")\n",
        "    print(\"─\" * 50)\n",
        "    \n",
        "    for h in [1, 2, 3]:\n",
        "        h_data = res_df[res_df['H'] == h]\n",
        "        best_h = h_data.loc[h_data['RMSE'].idxmin()]\n",
        "        \n",
        "        print(f\"\\nHorizonte {h}:\")\n",
        "        print(f\"  • Mejor modelo: {best_h['Model']} - {best_h['Experiment']}\")\n",
        "        print(f\"  • RMSE: {best_h['RMSE']:.4f}\")\n",
        "        print(f\"  • R²: {best_h['R2']:.4f}\")\n",
        "\n",
        "print(\"\\n✅ Análisis detallado completado!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ───────────────────────── MOSTRAR PREDICCIONES MÁS RECIENTES ─────────────────────────\n",
        "print(\"\\n🖼️ PREDICCIONES MÁS RECIENTES:\")\n",
        "for exp in EXPERIMENTS.keys():\n",
        "    exp_dir = OUT_ROOT / exp\n",
        "    if exp_dir.exists():\n",
        "        print(f\"\\n{exp}:\")\n",
        "        # Mostrar primera imagen de cada modelo\n",
        "        for model in ADVANCED_MODELS.keys():\n",
        "            img_path = exp_dir / f\"{model}_1.png\"\n",
        "            gif_path = exp_dir / f\"{model}.gif\"\n",
        "            \n",
        "            if img_path.exists():\n",
        "                from IPython.display import Image, display\n",
        "                print(f\"  {model} - Primera predicción (H=1):\")\n",
        "                display(Image(str(img_path), width=800))\n",
        "                \n",
        "            if gif_path.exists():\n",
        "                print(f\"  📹 GIF animado disponible: {gif_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🎉 NOTEBOOK COMPLETADO!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n📊 Resultados guardados en: {OUT_ROOT}\")\n",
        "print(f\"📈 Métricas en: {OUT_ROOT/'metrics_advanced.csv'}\")\n",
        "print(f\"🖼️ Visualizaciones en: {COMP_DIR}\")\n",
        "print(\"\\n💡 Próximos pasos:\")\n",
        "print(\"   1. Revisar las métricas y seleccionar el mejor modelo\")\n",
        "print(\"   2. Hacer fine-tuning de hiperparámetros si es necesario\")\n",
        "print(\"   3. Entrenar un ensemble con los mejores modelos\")\n",
        "print(\"   4. Evaluar en datos más recientes o diferentes regiones\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
