{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f64a97",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/hybrid_models_TopoRain_NET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "itarOiHGzTAU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "itarOiHGzTAU",
    "outputId": "92741f25-a5ad-49cd-ed9e-e851ec87059e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-29 19:28:55,478 [INFO] Configuración de threading de TensorFlow aplicada\n",
      "Entorno configurado. Usando ruta base: ..\n",
      "Entorno configurado. Usando ruta base: ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/dask/dataframe/_pyarrow_compat.py:15: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 13.0.0. Please consider upgrading.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-29 19:28:57,939 [INFO] Cargando datasets y separando características CEEMDAN y TFV-EMD...\n",
      "2025-05-29 19:28:58,929 [INFO] Clusters codificados de texto a números: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 19:28:58,929 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 19:28:58,930 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 19:28:58,930 [INFO] Definiendo máscaras para los niveles de elevación...\n",
      "2025-05-29 19:28:58,931 [INFO] Distribución de celdas por nivel de elevación:\n",
      "2025-05-29 19:28:58,931 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 19:28:58,932 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 19:28:58,932 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 19:28:58,929 [INFO] Clusters codificados de texto a números: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 19:28:58,929 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 19:28:58,930 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 19:28:58,930 [INFO] Definiendo máscaras para los niveles de elevación...\n",
      "2025-05-29 19:28:58,931 [INFO] Distribución de celdas por nivel de elevación:\n",
      "2025-05-29 19:28:58,931 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 19:28:58,932 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 19:28:58,932 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 19:28:58,940 [INFO] Cargando datasets y separando características CEEMDAN y TFV-EMD...\n",
      "2025-05-29 19:28:58,940 [INFO] Cargando datasets y separando características CEEMDAN y TFV-EMD...\n",
      "2025-05-29 19:28:59,058 [INFO] Clusters codificados de texto a números: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 19:28:59,059 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 19:28:59,060 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 19:28:59,060 [INFO] Definiendo máscaras para los niveles de elevación...\n",
      "2025-05-29 19:28:59,061 [INFO] Distribución de celdas por nivel de elevación:\n",
      "2025-05-29 19:28:59,062 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 19:28:59,062 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 19:28:59,063 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 19:28:59,065 [INFO] Cargando datasets y separando características CEEMDAN y TFV-EMD...\n",
      "2025-05-29 19:28:59,058 [INFO] Clusters codificados de texto a números: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 19:28:59,059 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 19:28:59,060 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 19:28:59,060 [INFO] Definiendo máscaras para los niveles de elevación...\n",
      "2025-05-29 19:28:59,061 [INFO] Distribución de celdas por nivel de elevación:\n",
      "2025-05-29 19:28:59,062 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 19:28:59,062 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 19:28:59,063 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 19:28:59,065 [INFO] Cargando datasets y separando características CEEMDAN y TFV-EMD...\n",
      "2025-05-29 19:28:59,183 [INFO] Clusters codificados de texto a números: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 19:28:59,184 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 19:28:59,184 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 19:28:59,185 [INFO] Definiendo máscaras para los niveles de elevación...\n",
      "2025-05-29 19:28:59,186 [INFO] Distribución de celdas por nivel de elevación:\n",
      "2025-05-29 19:28:59,186 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 19:28:59,187 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 19:28:59,187 [INFO]   Nivel 3 (>2264m): 996 celdas\n",
      "2025-05-29 19:28:59,183 [INFO] Clusters codificados de texto a números: {'high': 0, 'low': 1, 'medium': 2}\n",
      "2025-05-29 19:28:59,184 [INFO] Dimensiones: T=530, ny=61, nx=65, cells=3965\n",
      "2025-05-29 19:28:59,184 [INFO] Shapes: prec=(530, 61, 65), da_ceemdan=(530, 61, 65, 3), da_tvfemd=(530, 61, 65, 3)\n",
      "2025-05-29 19:28:59,185 [INFO] Definiendo máscaras para los niveles de elevación...\n",
      "2025-05-29 19:28:59,186 [INFO] Distribución de celdas por nivel de elevación:\n",
      "2025-05-29 19:28:59,186 [INFO]   Nivel 1 (<957m): 2048 celdas\n",
      "2025-05-29 19:28:59,187 [INFO]   Nivel 2 (957-2264m): 921 celdas\n",
      "2025-05-29 19:28:59,187 [INFO]   Nivel 3 (>2264m): 996 celdas\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "TopoRain-Net: entrenamiento y evaluación de modelos específicos por nivel de elevación.\n",
    "Implementa modelos BiGRU autoencoder-decoder para cada nivel de elevación,\n",
    "con fusión optimizada de características CEEMDAN y TFV-EMD usando XGBoost.\n",
    "Un meta-modelo integra las predicciones de los tres modelos de elevación.\n",
    "Genera métricas, scatter, mapas y tablas (global, por elevación, por percentiles).\n",
    "\"\"\"\n",
    "\n",
    "import warnings, logging\n",
    "from pathlib import Path\n",
    "# Configuración del entorno (compatible con Colab y local)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuración de logging y trazabilidad mejorada\n",
    "# -----------------------------------------------------------------------------\n",
    "# Crear directorio para logs\n",
    "LOG_DIR = Path(\"logs\")\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configurar formato de timestamp\n",
    "timestamp_format = \"%Y-%m-%d_%H-%M-%S\"\n",
    "run_timestamp = datetime.datetime.now().strftime(timestamp_format)\n",
    "log_filename = f\"toporain_net_run_{run_timestamp}.log\"\n",
    "\n",
    "# Configurar logging con formato detallado y salida a archivo\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_DIR / log_filename),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Clase para trazabilidad del proceso\n",
    "class ProcessTracker:\n",
    "    def __init__(self, name=\"TopoRain-NET\"):\n",
    "        self.name = name\n",
    "        self.start_time = time.time()\n",
    "        self.section_times = {}\n",
    "        self.current_section = None\n",
    "        self.section_start = None\n",
    "        self.metrics = defaultdict(dict)\n",
    "        self.resources = []\n",
    "        self.checkpoints = []\n",
    "        \n",
    "    def start_section(self, section_name):\n",
    "        \"\"\"Inicia el cronómetro para una sección del proceso\"\"\"\n",
    "        if self.current_section:\n",
    "            self.end_section()\n",
    "            \n",
    "        self.current_section = section_name\n",
    "        self.section_start = time.time()\n",
    "        logger.info(f\"▶️ INICIANDO: {section_name}\")\n",
    "        # Registrar recursos al inicio\n",
    "        self._log_resources()\n",
    "        \n",
    "    def end_section(self):\n",
    "        \"\"\"Finaliza la sección actual y registra el tiempo transcurrido\"\"\"\n",
    "        if not self.current_section:\n",
    "            return\n",
    "            \n",
    "        elapsed = time.time() - self.section_start\n",
    "        self.section_times[self.current_section] = elapsed\n",
    "        logger.info(f\"✓ COMPLETADO: {self.current_section} en {elapsed:.2f} segundos\")\n",
    "        # Registrar recursos al final\n",
    "        self._log_resources()\n",
    "        self.current_section = None\n",
    "        \n",
    "    def log_metric(self, section, metric_name, value):\n",
    "        \"\"\"Registra una métrica\"\"\"\n",
    "        self.metrics[section][metric_name] = value\n",
    "        logger.info(f\"📊 MÉTRICA: {section} - {metric_name}: {value}\")\n",
    "        \n",
    "    def add_checkpoint(self, description, data=None):\n",
    "        \"\"\"Añade un punto de control con datos opcionales\"\"\"\n",
    "        checkpoint = {\n",
    "            'timestamp': time.time(),\n",
    "            'description': description,\n",
    "            'elapsed_total': time.time() - self.start_time,\n",
    "            'data': data\n",
    "        }\n",
    "        self.checkpoints.append(checkpoint)\n",
    "        logger.info(f\"🔖 CHECKPOINT: {description}\")\n",
    "        \n",
    "    def _log_resources(self):\n",
    "        \"\"\"Registra el uso de recursos actual\"\"\"\n",
    "        mem_info = get_memory_info()\n",
    "        cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "        \n",
    "        # Obtener información de GPU si está disponible\n",
    "        gpu_info = get_gpu_memory_info()\n",
    "        gpu_usage = None\n",
    "        if gpu_info and gpu_info[0]['memory_used_mb'] > 0:\n",
    "            gpu_usage = {\n",
    "                'used_mb': gpu_info[0]['memory_used_mb'],\n",
    "                'total_mb': gpu_info[0]['memory_total_mb'],\n",
    "                'percent': gpu_info[0]['memory_used_percent']\n",
    "            }\n",
    "        \n",
    "        resources = {\n",
    "            'timestamp': time.time(),\n",
    "            'memory_used_gb': mem_info['total_gb'] - mem_info['free_gb'],\n",
    "            'memory_total_gb': mem_info['total_gb'],\n",
    "            'memory_percent': mem_info['used_percent'],\n",
    "            'cpu_percent': cpu_percent,\n",
    "            'gpu': gpu_usage\n",
    "        }\n",
    "        self.resources.append(resources)\n",
    "        \n",
    "    def _convert_numpy_types(self, obj):\n",
    "        \"\"\"\n",
    "        Convierte recursivamente tipos de numpy a tipos nativos de Python\n",
    "        para hacer el objeto JSON serializable\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (np.bool_)):\n",
    "            return bool(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self._convert_numpy_types(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_numpy_types(item) for item in obj]\n",
    "        elif isinstance(obj, tuple):\n",
    "            return tuple(self._convert_numpy_types(item) for item in obj)\n",
    "        else:\n",
    "            return obj\n",
    "        \n",
    "    def summary(self):\n",
    "        \"\"\"Genera un resumen del proceso\"\"\"\n",
    "        total_time = time.time() - self.start_time\n",
    "        \n",
    "        # Calcular estadísticas de recursos\n",
    "        if self.resources:\n",
    "            avg_mem = sum(r['memory_percent'] for r in self.resources) / len(self.resources)\n",
    "            max_mem = max(r['memory_percent'] for r in self.resources)\n",
    "            avg_cpu = sum(r['cpu_percent'] for r in self.resources) / len(self.resources)\n",
    "            max_cpu = max(r['cpu_percent'] for r in self.resources)\n",
    "        else:\n",
    "            avg_mem = max_mem = avg_cpu = max_cpu = 0\n",
    "        \n",
    "        summary_dict = {\n",
    "            'name': self.name,\n",
    "            'total_time': total_time,\n",
    "            'start_time': self.start_time,\n",
    "            'end_time': time.time(),\n",
    "            'section_times': self.section_times,\n",
    "            'metrics': dict(self.metrics),\n",
    "            'resources': {\n",
    "                'avg_memory_percent': avg_mem,\n",
    "                'max_memory_percent': max_mem,\n",
    "                'avg_cpu_percent': avg_cpu,\n",
    "                'max_cpu_percent': max_cpu\n",
    "            },\n",
    "            'num_checkpoints': len(self.checkpoints)\n",
    "        }\n",
    "        \n",
    "        # Convertir tipos numpy a tipos nativos de Python para JSON\n",
    "        summary_dict = self._convert_numpy_types(summary_dict)\n",
    "        \n",
    "        # Guardar resumen en formato JSON\n",
    "        summary_path = LOG_DIR / f\"summary_{run_timestamp}.json\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary_dict, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"📑 RESUMEN DEL PROCESO GUARDADO: {summary_path}\")\n",
    "        \n",
    "        # Imprimir resumen\n",
    "        logger.info(f\"📋 RESUMEN DE EJECUCIÓN - {self.name}\")\n",
    "        logger.info(f\"  Tiempo total: {total_time:.2f} segundos\")\n",
    "        logger.info(f\"  Secciones completadas: {len(self.section_times)}\")\n",
    "        for section, time_taken in sorted(self.section_times.items(), key=lambda x: x[1], reverse=True):\n",
    "            logger.info(f\"    - {section}: {time_taken:.2f} segundos\")\n",
    "        logger.info(f\"  Checkpoints registrados: {len(self.checkpoints)}\")\n",
    "        logger.info(f\"  Uso de recursos:\")\n",
    "        logger.info(f\"    - Memoria promedio: {avg_mem:.1f}%\")\n",
    "        logger.info(f\"    - Memoria máxima: {max_mem:.1f}%\")\n",
    "        logger.info(f\"    - CPU promedio: {avg_cpu:.1f}%\")\n",
    "        logger.info(f\"    - CPU máxima: {max_cpu:.1f}%\")\n",
    "        \n",
    "        return summary_dict\n",
    "\n",
    "# Inicializar el rastreador de procesos\n",
    "tracker = ProcessTracker()\n",
    "\n",
    "# Función para decorar funciones con trazabilidad\n",
    "def trace(section_name=None):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            func_name = section_name or func.__name__\n",
    "            tracker.start_section(func_name)\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                tracker.end_section()\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ ERROR en {func_name}: {str(e)}\")\n",
    "                tracker.end_section()\n",
    "                raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Intentar configurar el paralelismo antes de cualquier operación que inicialice el contexto\n",
    "try:\n",
    "    # Configurar threading para TensorFlow\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "    logger.info(\"Configuración de threading de TensorFlow aplicada\")\n",
    "except RuntimeError as e:\n",
    "    # Si ya se inicializó el contexto, informar pero seguir adelante\n",
    "    logger.warning(f\"No se pudo configurar threading de TensorFlow: {str(e)}. Continuando con valores por defecto.\")\n",
    "\n",
    "# Detectar si estamos en Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Si estamos en Colab, clonar el repositorio\n",
    "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
    "    %cd ml_precipitation_prediction\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy\n",
    "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
    "else:\n",
    "    # Si estamos en local, usar la ruta actual\n",
    "    if '/models' in os.getcwd():\n",
    "        BASE_PATH = Path('..')\n",
    "    else:\n",
    "        BASE_PATH = Path('.')\n",
    "\n",
    "BASE = Path(BASE_PATH)\n",
    "print(f\"Entorno configurado. Usando ruta base: {BASE}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FULL_NC      = BASE/\"data\"/\"output\"/\"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
    "FUSION_NC    = BASE/\"models\"/\"output\"/\"features_fusion_branches.nc\"\n",
    "TRAINED_DIR  = BASE/\"models\"/\"output\"/\"trained_models\"\n",
    "TRAINED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PRED_DIR = BASE/\"models\"/\"output\"/\"predictions\"\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HISTORY_DIR = BASE/\"models\"/\"output\"/\"histories\"\n",
    "HISTORY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INPUT_WINDOW   = 60\n",
    "OUTPUT_HORIZON = 3\n",
    "\n",
    "import numpy            as np\n",
    "import pandas           as pd\n",
    "import xarray           as xr\n",
    "import geopandas        as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs      as ccrs\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics        import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from xgboost                import XGBRegressor\n",
    "# Añadir importación de LightGBM y reducción de dimensionalidad\n",
    "from lightgbm               import LGBMRegressor\n",
    "from sklearn.decomposition  import PCA\n",
    "from sklearn.pipeline       import Pipeline\n",
    "\n",
    "from tensorflow.keras.models    import Sequential, Model\n",
    "from tensorflow.keras.layers    import Input, Dense, LSTM, GRU, Flatten, Reshape, Dropout, Concatenate, BatchNormalization, TimeDistributed, RepeatVector, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Importar TensorFlow aquí y configurarlo antes de cualquier operación\n",
    "\n",
    "# Actualizar importación de mixed_precision para compatibilidad con versiones recientes de TF\n",
    "try:\n",
    "    # Para TensorFlow 2.4+\n",
    "    from tensorflow.keras import mixed_precision\n",
    "except ImportError:\n",
    "    # Fallback para versiones más antiguas de TF\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "\n",
    "import ace_tools_open as tools\n",
    "\n",
    "# Configurar crecimiento de memoria GPU dinámico para evitar ResourceExhaustedError\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logger.info(f\"GPU configurada para crecimiento dinámico de memoria: {len(gpus)} GPUs disponibles\")\n",
    "    except RuntimeError as e:\n",
    "        logger.error(f\"Error configurando GPU: {str(e)}\")\n",
    "\n",
    "# También limitar la memoria de TensorFlow para operaciones CPU\n",
    "tf.config.threading.set_inter_op_parallelism_threads(4)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(4)\n",
    "\n",
    "# Funciones auxiliares para gestión eficiente de memoria\n",
    "def get_memory_info():\n",
    "    \"\"\"Obtiene información de memoria del sistema\"\"\"\n",
    "    mem_info = psutil.virtual_memory()\n",
    "    return {\n",
    "        'total_gb': mem_info.total / (1024**3),\n",
    "        'available_gb': mem_info.available / (1024**3),\n",
    "        'used_percent': mem_info.percent,\n",
    "        'free_gb': mem_info.free / (1024**3)\n",
    "    }\n",
    "\n",
    "# Funciones auxiliares para persistencia de modelos\n",
    "def get_model_path(model_type, level_name, component_idx=None):\n",
    "    \"\"\"\n",
    "    Genera la ruta para guardar o cargar un modelo específico\n",
    "    \n",
    "    Args:\n",
    "        model_type: Tipo de modelo ('fusion', 'bigru', 'meta')\n",
    "        level_name: Nombre del nivel de elevación\n",
    "        component_idx: Índice de componente (para modelos de fusión)\n",
    "        \n",
    "    Returns:\n",
    "        Path: Ruta completa del archivo del modelo\n",
    "    \"\"\"\n",
    "    if model_type == 'fusion':\n",
    "        return TRAINED_DIR / f\"fusion_xgb_{level_name}_comp{component_idx}.pkl\"\n",
    "    elif model_type == 'bigru':\n",
    "        return TRAINED_DIR / f\"BiGRU_{level_name}_model.keras\"\n",
    "    elif model_type == 'meta':\n",
    "        return TRAINED_DIR / \"meta_fusion_model.pkl\"\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo de modelo no reconocido: {model_type}\")\n",
    "\n",
    "def save_model(model, model_type, level_name, component_idx=None, extra_info=None):\n",
    "    \"\"\"\n",
    "    Guarda un modelo con su información asociada\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a guardar\n",
    "        model_type: Tipo de modelo ('fusion', 'bigru', 'meta')\n",
    "        level_name: Nombre del nivel de elevación\n",
    "        component_idx: Índice de componente (para modelos de fusión)\n",
    "        extra_info: Información adicional a guardar (pesos, métricas, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True si se guardó correctamente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_path = get_model_path(model_type, level_name, component_idx)\n",
    "        \n",
    "        # Para modelos XGBoost y otros que requieren pickle\n",
    "        if model_type in ['fusion', 'meta']:\n",
    "            with open(model_path, 'wb') as f:\n",
    "                import pickle\n",
    "                data_to_save = {'model': model}\n",
    "                if extra_info:\n",
    "                    data_to_save['info'] = extra_info\n",
    "                pickle.dump(data_to_save, f)\n",
    "        \n",
    "        # Para modelos Keras\n",
    "        elif model_type == 'bigru':\n",
    "            model.save(model_path)\n",
    "            \n",
    "            # Si hay info adicional, guardarla por separado\n",
    "            if extra_info:\n",
    "                info_path = model_path.parent / f\"{model_path.stem}_info.pkl\"\n",
    "                with open(info_path, 'wb') as f:\n",
    "                    import pickle\n",
    "                    pickle.dump(extra_info, f)\n",
    "        \n",
    "        logger.info(f\"Modelo {model_type} para {level_name} guardado en: {model_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al guardar modelo {model_type} para {level_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def load_model(model_type, level_name, component_idx=None):\n",
    "    \"\"\"\n",
    "    Carga un modelo previamente guardado\n",
    "    \n",
    "    Args:\n",
    "        model_type: Tipo de modelo ('fusion', 'bigru', 'meta')\n",
    "        level_name: Nombre del nivel de elevación\n",
    "        component_idx: Índice de componente (para modelos de fusión)\n",
    "        \n",
    "    Returns:\n",
    "        model: Modelo cargado o None si no existe\n",
    "        extra_info: Información adicional o None si no existe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_path = get_model_path(model_type, level_name, component_idx)\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            return None, None\n",
    "            \n",
    "        # Para modelos XGBoost y otros almacenados con pickle\n",
    "        if model_type in ['fusion', 'meta']:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                import pickle\n",
    "                data = pickle.load(f)\n",
    "                if isinstance(data, dict) and 'model' in data:\n",
    "                    model = data['model']\n",
    "                    extra_info = data.get('info')\n",
    "                else:\n",
    "                    # Compatibilidad con formato antiguo\n",
    "                    model = data\n",
    "                    extra_info = None\n",
    "        \n",
    "        # Para modelos Keras\n",
    "        elif model_type == 'bigru':\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "            \n",
    "            # Intentar cargar info adicional si existe\n",
    "            extra_info = None\n",
    "            info_path = model_path.parent / f\"{model_path.stem}_info.pkl\"\n",
    "            if info_path.exists():\n",
    "                with open(info_path, 'rb') as f:\n",
    "                    import pickle\n",
    "                    extra_info = pickle.load(f)\n",
    "        \n",
    "        logger.info(f\"Modelo {model_type} para {level_name} cargado desde: {model_path}\")\n",
    "        return model, extra_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al cargar modelo {model_type} para {level_name}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def model_exists(model_type, level_name, component_idx=None):\n",
    "    \"\"\"\n",
    "    Verifica si existe un modelo previamente guardado\n",
    "    \n",
    "    Args:\n",
    "        model_type: Tipo de modelo ('fusion', 'bigru', 'meta')\n",
    "        level_name: Nombre del nivel de elevación\n",
    "        component_idx: Índice de componente (para modelos de fusión)\n",
    "        \n",
    "    Returns:\n",
    "        bool: True si el modelo existe\n",
    "    \"\"\"\n",
    "    model_path = get_model_path(model_type, level_name, component_idx)\n",
    "    return model_path.exists()\n",
    "\n",
    "# Funciones auxiliares para monitorear la memoria de la GPU\n",
    "def get_gpu_memory_info():\n",
    "    \"\"\"Obtiene la información de memoria de la GPU disponible\"\"\"\n",
    "    if not gpus:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Intentar usar NVIDIA-SMI a través de subprocess if está disponible\n",
    "        import subprocess\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used,memory.free,memory.total', '--format=csv,noheader,nounits'],\n",
    "            encoding='utf-8')\n",
    "        gpu_info = []\n",
    "        for line in result.strip().split('\\n'):\n",
    "            values = [float(x) for x in line.split(',')]\n",
    "            gpu_info.append({\n",
    "                'memory_used_mb': values[0],\n",
    "                'memory_free_mb': values[1],\n",
    "                'memory_total_mb': values[2],\n",
    "                'memory_used_percent': values[0] / values[2] * 100\n",
    "            })\n",
    "        return gpu_info\n",
    "    except (ImportError, subprocess.SubprocessError, FileNotFoundError):\n",
    "        # Si nvidia-smi no está disponible, usar tensorflow para obtener información limitada\n",
    "        try:\n",
    "            memory_info = []\n",
    "            for i, gpu in enumerate(gpus):\n",
    "                # En versiones nuevas de TF podemos obtener información de memoria usando experimental.VirtualDeviceConfiguration\n",
    "                try:\n",
    "                    mem_info = tf.config.experimental.get_memory_info(f'GPU:{i}')\n",
    "                    total_memory = mem_info['current'] + mem_info['peak']  # Aproximación\n",
    "                    memory_info.append({\n",
    "                        'memory_used_mb': mem_info['current'] / (1024 * 1024),\n",
    "                        'memory_free_mb': (total_memory - mem_info['current']) / (1024 * 1024),\n",
    "                        'memory_total_mb': total_memory / (1024 * 1024),\n",
    "                        'memory_used_percent': mem_info['current'] / total_memory * 100 if total_memory else 0\n",
    "                    })\n",
    "                except (KeyError, AttributeError, ValueError):\n",
    "                    # Si no podemos obtener información específica, proveer una estimación\n",
    "                    memory_info.append({\n",
    "                        'memory_used_mb': -1,  # No conocido\n",
    "                        'memory_free_mb': -1,  # No conocido\n",
    "                        'memory_total_mb': -1,  # No conocido\n",
    "                        'memory_used_percent': -1  # No conocido\n",
    "                    })\n",
    "            return memory_info\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Función mejorada para limpiar la memoria\n",
    "def clear_memory(force_garbage_collection=True):\n",
    "    \"\"\"\n",
    "    Limpia la memoria de manera más agresiva, liberando recursos de TensorFlow y Python\n",
    "\n",
    "    Args:\n",
    "        force_garbage_collection: Si es True, fuerza la recolección de basura\n",
    "    \"\"\"\n",
    "    # 1. Limpiar sesión de TensorFlow para liberar variables y tensores\n",
    "    try:\n",
    "        tf.keras.backend.clear_session()\n",
    "        logger.debug(\"Sesión de Keras limpiada\")\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Error al limpiar sesión de Keras: {str(e)}\")\n",
    "\n",
    "    # 2. Reiniciar gráfico de operaciones de TF si está disponible\n",
    "    try:\n",
    "        # Para versiones antiguas de TF que tienen reset_default_graph\n",
    "        if hasattr(tf, 'reset_default_graph'):\n",
    "            tf.reset_default_graph()\n",
    "            logger.debug(\"Gráfico de TF reiniciado\")\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Error al reiniciar gráfico de TF: {str(e)}\")\n",
    "\n",
    "    # 3. Forzar recolección de basura de Python\n",
    "    if force_garbage_collection:\n",
    "        import gc\n",
    "        # Realizar múltiples pasadas para asegurar la limpieza completa\n",
    "        collected = gc.collect()\n",
    "        logger.debug(f\"GC recolectó {collected} objetos\")\n",
    "\n",
    "        # Segunda pasada para objetos que posiblemente se liberaron en la primera\n",
    "        collected = gc.collect()\n",
    "        logger.debug(f\"GC recolectó {collected} objetos adicionales\")\n",
    "\n",
    "    # 4. Intentar liberar memoria al sistema operativo\n",
    "    if 'psutil' in sys.modules:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        try:\n",
    "            # En Linux\n",
    "            if hasattr(process, 'memory_full_info'):\n",
    "                mi = process.memory_full_info()\n",
    "                logger.debug(f\"Memoria usada: RSS={mi.rss/1e6:.1f}MB, VMS={mi.vms/1e6:.1f}MB\")\n",
    "\n",
    "            # En sistemas POSIX, sincronizar filesystem para liberar buffers\n",
    "            if hasattr(os, 'sync'):\n",
    "                os.sync()\n",
    "                logger.debug(\"Sincronizado el sistema de archivos\")\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error en operaciones de memoria del proceso: {str(e)}\")\n",
    "\n",
    "    # 5. Intentar liberar la memoria GPU específicamente\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Ejecutar operaciones vacías para forzar sincronización de GPU\n",
    "            dummy = tf.random.normal([1, 1])\n",
    "            _ = dummy.numpy()  # Forzar ejecución\n",
    "            logger.debug(\"Operaciones GPU sincronizadas\")\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error al sincronizar GPU: {str(e)}\")\n",
    "\n",
    "# Función para crear un conjunto de datos de TensorFlow con mejor manejo de errores de memoria\n",
    "def create_tf_dataset(X, Y, batch_size=32, force_cpu=False, max_retries=3):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow Dataset from numpy arrays with batching.\n",
    "    Includes robust error handling with automatic CPU fallback and batch size adjustment.\n",
    "\n",
    "    Args:\n",
    "        X: Input features array\n",
    "        Y: Target labels array\n",
    "        batch_size: Size of batches for training\n",
    "        force_cpu: If True, forces operations to run on CPU\n",
    "        max_retries: Maximum number of retry attempts with smaller batch size\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset object configured for training\n",
    "    \"\"\"\n",
    "    # Verificar la memoria disponible y ajustar parámetros automáticamente\n",
    "    gpu_info = get_gpu_memory_info()\n",
    "    mem_info = get_memory_info()\n",
    "\n",
    "    # Estimar si la GPU está cerca del límite (>80% usada) para decidir si forzar CPU\n",
    "    auto_force_cpu = False\n",
    "    if gpu_info and not force_cpu:\n",
    "        for gpu in gpu_info:\n",
    "            if gpu['memory_used_percent'] > 80:\n",
    "                logger.warning(f\"GPU usage high ({gpu['memory_used_percent']:.1f}%), forcing CPU execution\")\n",
    "                auto_force_cpu = True\n",
    "                break\n",
    "\n",
    "    # Si hay más de un intento, reducir el batch size\n",
    "    actual_force_cpu = force_cpu or auto_force_cpu\n",
    "    actual_batch_size = batch_size\n",
    "\n",
    "    # Bucle de reintento con tamaños de batch más pequeños\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Verificar si hay NaNs antes de convertir a tensores\n",
    "            if np.isnan(X).any() or np.isnan(Y).any():\n",
    "                logger.warning(\"Se detectaron NaNs en los datos. Reemplazando con ceros.\")\n",
    "                X = np.nan_to_num(X, nan=0.0)\n",
    "                Y = np.nan_to_num(Y, nan=0.0)\n",
    "\n",
    "            # Estrategia específica para CPU o GPU\n",
    "            if actual_force_cpu:\n",
    "                with tf.device('/CPU:0'):\n",
    "                    # Convertir a tensores explícitamente para mejor control\n",
    "                    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "                    Y_tensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "\n",
    "                    # Crear dataset usando los tensores convertidos\n",
    "                    dataset = tf.data.Dataset.from_tensor_slices((X_tensor, Y_tensor))\n",
    "                    logger.info(f\"Dataset creado en CPU con batch_size={actual_batch_size}\")\n",
    "            else:\n",
    "                # Intentar crear el dataset con GPU\n",
    "                dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "                logger.info(f\"Dataset creado en GPU con batch_size={actual_batch_size}\")\n",
    "\n",
    "            # Configurar el dataset para entrenamiento con un buffer size adaptativo\n",
    "            # Usar buffer size más pequeño para reducir uso de memoria\n",
    "            samples = len(X)\n",
    "            buffer_size = min(samples, 1000)  # Máximo 1000 elementos en memoria\n",
    "\n",
    "            # Ajustar buffer size si la memoria está baja\n",
    "            if mem_info['available_gb'] < 2.0:  # Menos de 2GB disponibles\n",
    "                buffer_size = min(buffer_size, 100)  # Reducir a máximo 100 elementos\n",
    "                logger.warning(f\"Memoria disponible baja ({mem_info['available_gb']:.1f}GB), buffer reducido a {buffer_size}\")\n",
    "\n",
    "            dataset = dataset.shuffle(buffer_size=buffer_size, seed=42)\n",
    "            dataset = dataset.batch(actual_batch_size)\n",
    "            dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "            # Probar que el dataset funciona extrayendo un batch\n",
    "            try:\n",
    "                for _ in dataset.take(1):\n",
    "                    pass  # Solo verificar que podemos iterar\n",
    "                logger.info(\"Dataset verificado correctamente\")\n",
    "            except tf.errors.ResourceExhaustedError as e:\n",
    "                raise e  # Relanzar para manejar en el bloque catch\n",
    "\n",
    "            return dataset\n",
    "\n",
    "        except (tf.errors.ResourceExhaustedError, tf.errors.InternalError, tf.errors.FailedPreconditionError,\n",
    "                tf.errors.AbortedError, tf.errors.OOM) as e:\n",
    "            # Si estamos en el último intento, reducir drásticamente\n",
    "            if attempt == max_retries - 1:\n",
    "                logger.error(f\"Error crítico al crear dataset: {str(e)}\")\n",
    "\n",
    "                # Último intento desesperado: mínimo batch size y forzar CPU\n",
    "                logger.warning(\"Intento final con configuración mínima (batch=1, CPU)\")\n",
    "                with tf.device('/CPU:0'):\n",
    "                    logger.info(\"Creando dataset final con configuración mínima\")\n",
    "                    # Crear con el menor batch posible\n",
    "                    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "                    Y_tensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
    "                    dataset = tf.data.Dataset.from_tensor_slices((X_tensor, Y_tensor))\n",
    "                    dataset = dataset.batch(1)  # Mínimo batch size\n",
    "                    return dataset\n",
    "            else:\n",
    "                # Reducir batch size y forzar CPU en próximo intento\n",
    "                prev_batch = actual_batch_size\n",
    "                actual_batch_size = max(1, actual_batch_size // 2)\n",
    "                actual_force_cpu = True\n",
    "\n",
    "                logger.warning(f\"Intento {attempt+1}/{max_retries}: Reduciendo batch size de {prev_batch} a {actual_batch_size} y forzando CPU\")\n",
    "\n",
    "                # Limpiar memoria antes del próximo intento\n",
    "                clear_memory()\n",
    "                time.sleep(1)  # Pequeña pausa para permitir que el sistema se estabilice\n",
    "\n",
    "# Función genérica para predecir en lotes\n",
    "def predict_in_batches(model, X, batch_size=32, verbose=0):\n",
    "    \"\"\"\n",
    "    Genera predicciones de cualquier modelo en lotes para evitar problemas de memoria\n",
    "\n",
    "    Args:\n",
    "        model: Modelo entrenado (Keras, TensorFlow, etc.)\n",
    "        X: Datos de entrada (numpy array)\n",
    "        batch_size: Tamaño del lote para procesamiento\n",
    "        verbose: Nivel de verbosidad para las predicciones\n",
    "\n",
    "    Returns:\n",
    "        Array con predicciones\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "\n",
    "    # Si X es muy pequeño, predecir directamente\n",
    "    if n_samples <= batch_size:\n",
    "        return model.predict(X, verbose=verbose)\n",
    "\n",
    "    # Ajustar batch_size según memoria disponible\n",
    "    try:\n",
    "        mem_info = get_memory_info()\n",
    "        adaptive_batch = min(batch_size, max(8, int(mem_info['available_gb'] * 10)))\n",
    "        logger.info(f\"Generando predicciones en lotes de {adaptive_batch} muestras\")\n",
    "        batch_size = adaptive_batch\n",
    "    except:\n",
    "        # Si falla la adaptación, usar el batch_size proporcionado\n",
    "        logger.info(f\"Generando predicciones en lotes de {batch_size} muestras\")\n",
    "\n",
    "    # Inferir la forma de salida del modelo haciendo una predicción en un único ejemplo\n",
    "    try:\n",
    "        sample_pred = model.predict(X[:1], verbose=0)\n",
    "        output_shape = sample_pred.shape[1:]  # Excluye la dimensión del batch\n",
    "    except:\n",
    "        # Si falla, asumir forma desconocida y manejarla después\n",
    "        output_shape = None\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # Procesar por lotes\n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        batch_X = X[start_idx:end_idx]\n",
    "\n",
    "        # Para mayor seguridad, comprobar si hay NaNs\n",
    "        has_nans = np.isnan(batch_X).any()\n",
    "        if has_nans:\n",
    "            logger.warning(f\"Detectados NaN en lote {start_idx}-{end_idx}, realizando imputación\")\n",
    "            # Reemplazar NaNs con 0 para evitar errores\n",
    "            batch_X = np.nan_to_num(batch_X, nan=0.0)\n",
    "\n",
    "        # Predecir lote\n",
    "        try:\n",
    "            batch_preds = model.predict(batch_X, verbose=0 if start_idx > 0 else verbose)\n",
    "            predictions.append(batch_preds)\n",
    "\n",
    "            # Liberar memoria cada 5 lotes\n",
    "            if (start_idx // batch_size) % 5 == 0 and start_idx > 0:\n",
    "                # Liberar memoria explícitamente\n",
    "                if 'gc' in sys.modules:\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error al predecir lote {start_idx}-{end_idx}: {str(e)}\")\n",
    "            # Intentar con un batch más pequeño como último recurso\n",
    "            try:\n",
    "                smaller_batch = max(1, batch_size // 4)\n",
    "                logger.warning(f\"Reintentando con batch más pequeño: {smaller_batch}\")\n",
    "                mini_batch_preds = []\n",
    "                for mini_start in range(start_idx, end_idx, smaller_batch):\n",
    "                    mini_end = min(mini_start + smaller_batch, end_idx)\n",
    "                    mini_X = X[mini_start:mini_end]\n",
    "                    mini_pred = model.predict(mini_X, verbose=0)\n",
    "                    mini_batch_preds.append(mini_pred)\n",
    "                batch_preds = np.vstack(mini_batch_preds)\n",
    "                predictions.append(batch_preds)\n",
    "            except Exception as e2:\n",
    "                logger.error(f\"Error en segundo intento de lote: {str(e2)}\")\n",
    "                # Si también falla, rellenar con ceros\n",
    "                if output_shape:\n",
    "                    batch_size_curr = end_idx - start_idx\n",
    "                    zeros_shape = (batch_size_curr,) + output_shape\n",
    "                    logger.warning(f\"Rellenando con ceros de forma {zeros_shape}\")\n",
    "                    predictions.append(np.zeros(zeros_shape))\n",
    "                else:\n",
    "                    raise e2\n",
    "\n",
    "    # Concatenar resultados\n",
    "    try:\n",
    "        return np.vstack(predictions)\n",
    "    except:\n",
    "        # Si vstack falla (por ejemplo, formas inconsistentes), devolver una lista\n",
    "        logger.warning(\"No se pudo concatenar predicciones, devolviendo lista de arrays\")\n",
    "        return predictions\n",
    "\n",
    "# Funciones específicas para XGBoost con optimización de memoria\n",
    "def predict_xgb_in_batches(model, X, batch_size=100):\n",
    "    \"\"\"\n",
    "    Genera predicciones XGBoost en lotes para evitar problemas de memoria\n",
    "\n",
    "    Args:\n",
    "        model: Modelo XGBoost entrenado\n",
    "        X: Datos de entrada (numpy array)\n",
    "        batch_size: Tamaño del lote para procesamiento\n",
    "\n",
    "    Returns:\n",
    "        Array con predicciones\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    predictions = np.zeros(n_samples)\n",
    "\n",
    "    # Ajustar tamaño de lote según memoria disponible\n",
    "    mem_info = get_memory_info()\n",
    "    adaptive_batch = min(batch_size, max(10, int(mem_info['available_gb'] * 10)))\n",
    "\n",
    "    logger.info(f\"Generando predicciones XGBoost en lotes de {adaptive_batch} muestras\")\n",
    "\n",
    "    # Procesar por lotes\n",
    "    for start_idx in range(0, n_samples, adaptive_batch):\n",
    "        end_idx = min(start_idx + adaptive_batch, n_samples)\n",
    "        batch_X = X[start_idx:end_idx]\n",
    "\n",
    "        # Para mayor seguridad, comprobar si hay NaNs\n",
    "        has_nans = np.isnan(batch_X).any()\n",
    "        if has_nans:\n",
    "            logger.warning(f\"Detectados NaN en lote {start_idx}-{end_idx}, realizando imputación\")\n",
    "            # Reemplazar NaNs con 0 para evitar errores\n",
    "            batch_X = np.nan_to_num(batch_X, nan=0.0)\n",
    "\n",
    "        # Predecir lote\n",
    "        batch_preds = model.predict(batch_X)\n",
    "        predictions[start_idx:end_idx] = batch_preds\n",
    "\n",
    "        # Liberar memoria cada 5 lotes\n",
    "        if (start_idx // adaptive_batch) % 5 == 0 and start_idx > 0:\n",
    "            if 'gc' in sys.modules:\n",
    "                import gc\n",
    "                gc.collect()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def train_xgb_with_memory_optimization(X_train, y_train, X_val=None, y_val=None, params=None):\n",
    "    \"\"\"\n",
    "    Entrena un modelo XGBoost con optimizaciones de memoria y velocidad\n",
    "\n",
    "    Args:\n",
    "        X_train: Datos de entrenamiento\n",
    "        y_train: Etiquetas de entrenamiento\n",
    "        X_val: Datos de validación (opcional)\n",
    "        y_val: Etiquetas de validación (opcional)\n",
    "        params: Parámetros de XGBoost (diccionario)\n",
    "\n",
    "    Returns:\n",
    "        Modelo XGBoost entrenado\n",
    "    \"\"\"\n",
    "    # Parámetros por defecto optimizados para velocidad y memoria\n",
    "    default_params = {\n",
    "        'n_estimators': 60,  # Reducido para mayor velocidad\n",
    "        'max_depth': 4,      # Reducido para mayor velocidad\n",
    "        'learning_rate': 0.2, # Aumentado para convergencia más rápida\n",
    "        'subsample': 0.7,     # Reducido para mayor velocidad\n",
    "        'colsample_bytree': 0.7, # Reducido para mayor velocidad\n",
    "        'tree_method': 'hist',  # Método más eficiente en memoria\n",
    "        'predictor': 'cpu_predictor',  # Evitar problemas de GPU\n",
    "        'n_jobs': 1  # Un hilo por modelo para permitir paralelismo entre modelos\n",
    "    }\n",
    "\n",
    "    # Actualizar con parámetros personalizados si se proporcionan\n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "\n",
    "    # Crear y entrenar modelo con early stopping si hay datos de validación\n",
    "    if X_val is not None and y_val is not None:\n",
    "        model = XGBRegressor(**default_params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False\n",
    "        )\n",
    "    else:\n",
    "        # Sin early stopping si no hay datos de validación\n",
    "        model = XGBRegressor(**default_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_xgb_horizon_predictions(meta_models, base_model_preds, cells, horizons=3):\n",
    "    \"\"\"\n",
    "    Genera predicciones por horizonte usando modelos XGBoost en metamodelado\n",
    "\n",
    "    Args:\n",
    "        meta_models: Lista de modelos XGBoost (uno por horizonte)\n",
    "        base_model_preds: Diccionario de predicciones de modelos base {modelo: predicciones}\n",
    "        cells: Número de celdas espaciales\n",
    "        horizons: Número de horizontes temporales\n",
    "\n",
    "    Returns:\n",
    "        Array de predicciones (muestras, horizontes, celdas)\n",
    "    \"\"\"\n",
    "    # Determinar número de muestras del primer modelo base\n",
    "    first_model = list(base_model_preds.keys())[0]\n",
    "    n_samples = base_model_preds[first_model].shape[0]\n",
    "\n",
    "    # Inicializar array para predicciones\n",
    "    Y_pred = np.zeros((n_samples, horizons, cells))\n",
    "\n",
    "    # Procesar cada horizonte\n",
    "    for h in range(horizons):\n",
    "        logger.info(f\"Generando predicciones para horizonte {h+1}/{horizontes}\")\n",
    "\n",
    "        # Si no hay modelo para este horizonte, continuar al siguiente\n",
    "        if h >= len(meta_models):\n",
    "            logger.warning(f\"No hay modelo meta-XGB para horizonte {h+1}\")\n",
    "            continue\n",
    "\n",
    "        # Preparar características para este horizonte\n",
    "        X_meta_batches = []\n",
    "        batch_size = 100\n",
    "\n",
    "        # Procesar por lotes para evitar problemas de memoria\n",
    "        for start_idx in range(0, n_samples, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "\n",
    "            # Preparar entradas para metamodelo\n",
    "            X_meta_batch_parts = []\n",
    "            for model_name in base_model_preds:\n",
    "                if h < base_model_preds[model_name].shape[1]:\n",
    "                    # Extraer predicciones del modelo base para este horizonte\n",
    "                    model_preds = base_model_preds[model_name][start_idx:end_idx, h, :]\n",
    "                    X_meta_batch_parts.append(model_preds.reshape(end_idx - start_idx, -1))\n",
    "\n",
    "            # Concatenar características de todos los modelos base\n",
    "            if X_meta_batch_parts:\n",
    "                X_meta_batch = np.hstack(X_meta_batch_parts)\n",
    "\n",
    "                # Predecir con el meta-modelo XGB para este lote\n",
    "                Y_pred[start_idx:end_idx, h, :] = meta_models[h].predict(X_meta_batch).reshape(-1, cells)\n",
    "\n",
    "            # Liberar memoria cada 5 lotes\n",
    "            if (start_idx // batch_size) % 5 == 0 and start_idx > 0:\n",
    "                # Liberar memoria explícitamente\n",
    "                if 'gc' in sys.modules:\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "\n",
    "    return Y_pred\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Carga de datos con separación explícita de características CEEMDAN y TFV-EMD\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Cargando datasets y separando características CEEMDAN y TFV-EMD...\")\n",
    "ds_full = xr.open_dataset(FULL_NC)\n",
    "ds_fuse = xr.open_dataset(FUSION_NC)\n",
    "\n",
    "# precipitacion y variables\n",
    "prec = ds_full[\"total_precipitation\"].values  # (T, ny, nx)\n",
    "lags = sorted([v for v in ds_full.data_vars if \"_lag\" in v])\n",
    "da_lags = np.stack([ds_full[lag].values for lag in lags], axis=-1)  # (T, ny, nx, n_lags)\n",
    "\n",
    "# Separar características CEEMDAN y TFV-EMD para optimizar su fusión\n",
    "ceemdan_branches = [\"CEEMDAN_high\", \"CEEMDAN_medium\", \"CEEMDAN_low\"]\n",
    "tvfemd_branches = [\"TVFEMD_high\", \"TVFEMD_medium\", \"TVFEMD_low\"]\n",
    "fusion_branches = [\"FUSION_high\", \"FUSION_medium\", \"FUSION_low\"]\n",
    "\n",
    "# Cargar datos CEEMDAN\n",
    "da_ceemdan = np.stack([ds_fuse[branch].values for branch in ceemdan_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar datos TFV-EMD\n",
    "da_tvfemd = np.stack([ds_fuse[branch].values for branch in tvfemd_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar fusión predefinida (para referencia)\n",
    "da_fusion = np.stack([ds_fuse[branch].values for branch in fusion_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "\n",
    "# topografía y cluster\n",
    "elev = ds_full[\"elevation\"].values.ravel()  # (cells,)\n",
    "slope = ds_full[\"slope\"].values.ravel()\n",
    "\n",
    "# Manejar correctamente los valores de cluster (pueden ser texto)\n",
    "cluster_values = ds_full[\"cluster_elevation\"].values.ravel()\n",
    "# Verificar si los valores son strings o numéricos\n",
    "if isinstance(cluster_values[0], (str, np.str_)):\n",
    "    # Usar un LabelEncoder para convertir strings a enteros\n",
    "    le = LabelEncoder()\n",
    "    cluster = le.fit_transform(cluster_values)\n",
    "    logger.info(f\"Clusters codificados de texto a números: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    # Si ya son numéricos, convertir a enteros\n",
    "    cluster = cluster_values.astype(int)\n",
    "\n",
    "# dimensiones\n",
    "lat = ds_full.latitude.values\n",
    "lon = ds_full.longitude.values\n",
    "ny, nx = len(lat), len(lon)\n",
    "cells = ny*nx\n",
    "T = prec.shape[0]\n",
    "\n",
    "logger.info(f\"Dimensiones: T={T}, ny={ny}, nx={nx}, cells={cells}\")\n",
    "logger.info(f\"Shapes: prec={prec.shape}, da_ceemdan={da_ceemdan.shape}, da_tvfemd={da_tvfemd.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Definir máscaras para los niveles de elevación\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Definiendo máscaras para los niveles de elevación...\")\n",
    "mask_nivel1 = elev < 957  # nivel_1: 58-956m\n",
    "mask_nivel2 = (elev >= 957) & (elev <= 2264)  # nivel_2: 957-2264m\n",
    "mask_nivel3 = elev > 2264  # nivel_3: 2264-4728m\n",
    "\n",
    "logger.info(f\"Distribución de celdas por nivel de elevación:\")\n",
    "logger.info(f\"  Nivel 1 (<957m): {np.sum(mask_nivel1)} celdas\")\n",
    "logger.info(f\"  Nivel 2 (957-2264m): {np.sum(mask_nivel2)} celdas\")\n",
    "logger.info(f\"  Nivel 3 (>2264m): {np.sum(mask_nivel3)} celdas\")\n",
    "\n",
    "# Crear diccionario de máscaras para facilitar el procesamiento\n",
    "elevation_masks = {\n",
    "    \"nivel_1\": mask_nivel1,\n",
    "    \"nivel_2\": mask_nivel2,\n",
    "    \"nivel_3\": mask_nivel3\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Implementar función para optimizar fusión de CEEMDAN y TFV-EMD con XGBoost\n",
    "# -----------------------------------------------------------------------------\n",
    "import concurrent.futures\n",
    "import tqdm\n",
    "from functools import partial\n",
    "\n",
    "@trace(\"Optimización de fusión\")\n",
    "def optimize_fusion_with_xgboost(ceemdan_data, tvfemd_data, target_data, masks, test_size=0.2, force_retrain=False):\n",
    "    \"\"\"\n",
    "    Optimiza la fusión de CEEMDAN y TFV-EMD usando XGBoost para cada nivel de elevación.\n",
    "    Implementa paralelismo adaptativo basado en CPU/GPU y memoria disponible.\n",
    "    \n",
    "    Args:\n",
    "        ceemdan_data: Array de características CEEMDAN (T, ny, nx, 3)\n",
    "        tvfemd_data: Array de características TFV-EMD (T, ny, nx, 3)\n",
    "        target_data: Array de valores objetivo (precipitación) (T, ny, nx)\n",
    "        masks: Diccionario de máscaras por nivel de elevación\n",
    "        test_size: Proporción del conjunto de prueba\n",
    "        force_retrain: Si es True, fuerza el reentrenamiento aunque existan modelos guardados\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary con modelos XGBoost para fusión por nivel y componente\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    # Comprobar si todos los modelos ya existen\n",
    "    all_models_exist = True\n",
    "    if not force_retrain:\n",
    "        for level_name in masks:\n",
    "            for component_idx in range(3):\n",
    "                if not model_exists('fusion', level_name, component_idx):\n",
    "                    all_models_exist = False\n",
    "                    break\n",
    "            if not all_models_exist:\n",
    "                break\n",
    "                \n",
    "        if all_models_exist:\n",
    "            logger.info(\"Todos los modelos de fusión existen. Cargando...\")\n",
    "            return load_all_fusion_models(masks)\n",
    "    \n",
    "    # Determinar recursos computacionales disponibles\n",
    "    mem_info = get_memory_info()\n",
    "    cpu_count = os.cpu_count()\n",
    "    \n",
    "    print(f\"\\n🖥️  Recursos detectados: {cpu_count} CPUs, {mem_info['total_gb']:.1f}GB RAM ({mem_info['available_gb']:.1f}GB disponible)\")\n",
    "    \n",
    "    # SOLUCIÓN: Aumentar agresivamente el número de trabajadores para forzar paralelismo\n",
    "    # y aprovechar mejor los recursos subutilizados\n",
    "    optimal_workers = max(3, min(cpu_count - 1, 8))  # Mínimo 3 workers, máximo CPU-1 o 8\n",
    "    \n",
    "    # Verificar disponibilidad de GPU para tree_method\n",
    "    gpu_available = len(gpus) > 0\n",
    "    tree_method = 'gpu_hist' if gpu_available else 'hist'\n",
    "    \n",
    "    print(f\"🔧 Configuración optimizada: {optimal_workers} workers en paralelo FORZADOS, tree_method={tree_method}\")\n",
    "    print(f\"🧠 Memoria disponible: {mem_info['available_gb']:.2f}GB ({mem_info['used_percent']:.1f}% usado)\")\n",
    "    \n",
    "    # Total de componentes a procesar\n",
    "    total_levels = len(masks)\n",
    "    total_components = total_levels * 3  # 3 componentes por nivel\n",
    "    \n",
    "    # Inicializar estructuras de datos para resultados\n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]  # Placeholder para los 3 componentes\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "    \n",
    "    # Barra de progreso global\n",
    "    print(f\"\\n📊 Iniciando entrenamiento acelerado de {total_components} componentes ({total_levels} niveles × 3 componentes)\")\n",
    "    \n",
    "    # Función para procesar un componente específico\n",
    "    def process_component(level_name, mask, component_idx):\n",
    "        # Verificar si el modelo ya existe (a menos que se fuerce reentrenamiento)\n",
    "        if not force_retrain and model_exists('fusion', level_name, component_idx):\n",
    "            print(f\"🔄 Nivel {level_name}, componente {component_idx}: Cargando modelo existente...\")\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            if model and info:\n",
    "                weights = info.get('weights')\n",
    "                rmse = info.get('rmse', 0.0)\n",
    "                fit_time = info.get('fit_time', 0.0)\n",
    "                total_time = 0.1  # Tiempo mínimo para evitar divisiones por cero\n",
    "                print(f\"✅ {level_name}, comp{component_idx} (cargado): RMSE={rmse:.4f}, pesos=[CEEMDAN={weights[0]:.2f}, TFV-EMD={weights[1]:.2f}]\")\n",
    "                return {\n",
    "                    'level': level_name,\n",
    "                    'component': component_idx,\n",
    "                    'model': model,\n",
    "                    'weights': weights,\n",
    "                    'rmse': rmse,\n",
    "                    'fit_time': fit_time,\n",
    "                    'total_time': total_time,\n",
    "                    'loaded': True\n",
    "                }\n",
    "        \n",
    "        # Si llegamos aquí, necesitamos entrenar el modelo\n",
    "        print(f\"▶️  Nivel {level_name}, componente {component_idx}: Iniciando entrenamiento rápido...\")\n",
    "        comp_start = time.time()\n",
    "        cells_in_level = np.sum(mask)\n",
    "        \n",
    "        # Reformatear los datos para el entrenamiento\n",
    "        X_ceemdan = ceemdan_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        X_tvfemd = tvfemd_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        y_target = target_data.reshape(T, -1)[:, mask]\n",
    "        \n",
    "        print(f\"   Datos: {X_ceemdan.shape[0]} muestras, {cells_in_level} celdas\")\n",
    "        \n",
    "        # Concatenar características\n",
    "        X_combined = np.column_stack([X_ceemdan, X_tvfemd])\n",
    "        \n",
    "        # División simple para mayor velocidad (sin estratificación que consume tiempo)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_target, test_size=test_size, random_state=42\n",
    "        )\n",
    "        split_method = \"simple (optimizado para velocidad)\"\n",
    "        \n",
    "        print(f\"   Split: {X_train.shape[0]} train, {X_test.shape[0]} test ({split_method})\")\n",
    "        \n",
    "        # SOLUCIÓN: Optimizar hiperparámetros para mayor velocidad\n",
    "        n_samples, n_features = X_train.shape\n",
    "        # Reducir profundidad y número de árboles para entrenamientos más rápidos\n",
    "        max_depth = min(4, max(3, int(np.log2(n_features/2))))  # Profundidad reducida\n",
    "        n_estimators = min(60, max(30, int(30 + 5 * np.log(n_samples))))  # Menos árboles\n",
    "        learning_rate = min(0.3, max(0.08, 0.2))  # Learning rate más alto para convergencia rápida\n",
    "        subsample = 0.7  # Usar menos datos por árbol\n",
    "        colsample = 0.7  # Usar menos columnas por árbol\n",
    "        \n",
    "        # Configurar modelo XGBoost con paralelismo más eficiente\n",
    "        model = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample,\n",
    "            tree_method=tree_method,\n",
    "            n_jobs=1,  # Un hilo por modelo para maximizar paralelismo entre modelos\n",
    "            enable_categorical=False,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        # Entrenar modelo con mensaje de progreso\n",
    "        print(f\"   Entrenamiento ultra-rápido: {n_estimators} estimators, depth={max_depth}, lr={learning_rate:.3f}\")\n",
    "        fit_start = time.time()\n",
    "        \n",
    "        # Entrenamiento simplificado para mayor velocidad\n",
    "        model.fit(\n",
    "            X_train, y_train.ravel(),\n",
    "            eval_set=[(X_test, y_test.ravel())],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        fit_time = time.time() - fit_start\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test.ravel(), y_pred))\n",
    "        \n",
    "        # Extraer pesos de importancia para CEEMDAN vs TFV-EMD\n",
    "        importance = model.feature_importances_\n",
    "        cells_per_feature = cells_in_level\n",
    "        \n",
    "        # Promedio de importancia para cada fuente\n",
    "        ceemdan_importance = np.mean(importance[:cells_per_feature])\n",
    "        tvfemd_importance = np.mean(importance[cells_per_feature:])\n",
    "        \n",
    "        # Normalizar para que sumen 1\n",
    "        total_importance = ceemdan_importance + tvfemd_importance\n",
    "        ceemdan_weight = ceemdan_importance / total_importance\n",
    "        tvfemd_weight = tvfemd_importance / total_importance\n",
    "        \n",
    "        comp_time = time.time() - comp_start\n",
    "        \n",
    "        print(f\"✅ {level_name}, comp{component_idx}: RMSE={rmse:.4f}, tiempo={comp_time:.1f}s, \"\n",
    "              f\"pesos=[CEEMDAN={ceemdan_weight:.2f}, TFV-EMD={tvfemd_weight:.2f}]\")\n",
    "        \n",
    "        weights = (ceemdan_weight, tvfemd_weight)\n",
    "        \n",
    "        # Guardar modelo para uso futuro con información adicional\n",
    "        info = {\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'training_date': datetime.datetime.now().strftime(timestamp_format),\n",
    "            'hyper_params': {\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'learning_rate': learning_rate,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        save_model(model, 'fusion', level_name, component_idx, info)\n",
    "        \n",
    "        # Devolver resultados\n",
    "        return {\n",
    "            'level': level_name,\n",
    "            'component': component_idx,\n",
    "            'model': model,\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'loaded': False\n",
    "        }\n",
    "    \n",
    "    # Procesar niveles y componentes usando paralelismo adaptativo\n",
    "    all_tasks = []\n",
    "    for level_name, mask in masks.items():\n",
    "        # Crear tareas para todos los componentes\n",
    "        for component_idx in range(3):\n",
    "            all_tasks.append((level_name, mask, component_idx))\n",
    "    \n",
    "    # SOLUCIÓN: FORZAR paralelismo siempre\n",
    "    all_results = []\n",
    "    \n",
    "    # Mostrar mensaje claro sobre el modo paralelo\n",
    "    print(f\"\\n⚡ Activando procesamiento paralelo forzado con {optimal_workers} workers para acelerar el entrenamiento\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=optimal_workers) as executor:\n",
    "        # Crear lista de futuros\n",
    "        futures = []\n",
    "        for level_name, mask, component_idx in all_tasks:\n",
    "            futures.append(executor.submit(\n",
    "                process_component, level_name, mask, component_idx\n",
    "            ))\n",
    "        \n",
    "        # Mostrar progreso mientras se completan las tareas\n",
    "        completed = 0\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            completed += 1\n",
    "            progress = completed / len(futures)\n",
    "            print(f\"⏳ Progreso global: {completed}/{len(futures)} componentes ({progress:.1%})\")\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en componente: {str(e)}\")\n",
    "    \n",
    "    # Organizar resultados por nivel y componente\n",
    "    for result in all_results:\n",
    "        level = result['level']\n",
    "        component = result['component']\n",
    "        \n",
    "        # Guardar modelo y pesos\n",
    "        fusion_models[level][component] = result['model']\n",
    "        fusion_weights[level][component] = result['weights']\n",
    "        \n",
    "        # Registrar métricas para trazabilidad\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"rmse\", result['rmse'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"ceemdan_weight\", result['weights'][0])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"tvfemd_weight\", result['weights'][1])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"train_time\", result['fit_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"total_time\", result['total_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"loaded\", result.get('loaded', False))\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\n🏁 Optimización de fusión completada:\")\n",
    "    for level_name, components in fusion_models.items():\n",
    "        valid_components = sum(1 for model in components if model is not None)\n",
    "        print(f\"  - {level_name}: {valid_components}/3 componentes entrenados\")\n",
    "    \n",
    "    tracker.add_checkpoint(\"Optimización de fusión completada\", \n",
    "                          {\"num_models\": sum(len(models) for models in fusion_models.values())})\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "def load_all_fusion_models(masks):\n",
    "    \"\"\"\n",
    "    Carga todos los modelos de fusión existentes\n",
    "    \n",
    "    Args:\n",
    "        masks: Diccionario de máscaras por nivel de elevación\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fusion_models, fusion_weights)\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "        \n",
    "        # Cargar los tres modelos de componentes\n",
    "        for component_idx in range(3):\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            \n",
    "            if model is not None and info is not None:\n",
    "                fusion_models[level_name][component_idx] = model\n",
    "                fusion_weights[level_name][component_idx] = info['weights']\n",
    "                logger.info(f\"Modelo fusión {level_name}, componente {component_idx} cargado\")\n",
    "                \n",
    "                # Registrar métricas para trazabilidad\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"rmse\", info.get('rmse', 0))\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"ceemdan_weight\", info['weights'][0])\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"tvfemd_weight\", info['weights'][1])\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"loaded\", True)\n",
    "            else:\n",
    "                logger.warning(f\"No se pudo cargar el modelo fusión {level_name}, componente {component_idx}\")\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Carga de datos con separación explícita de características CEEMDAN y TFV-EMD\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Cargando datasets y separando características CEEMDAN y TFV-EMD...\")\n",
    "ds_full = xr.open_dataset(FULL_NC)\n",
    "ds_fuse = xr.open_dataset(FUSION_NC)\n",
    "\n",
    "# precipitacion y variables\n",
    "prec = ds_full[\"total_precipitation\"].values  # (T, ny, nx)\n",
    "lags = sorted([v for v in ds_full.data_vars if \"_lag\" in v])\n",
    "da_lags = np.stack([ds_full[lag].values for lag in lags], axis=-1)  # (T, ny, nx, n_lags)\n",
    "\n",
    "# Separar características CEEMDAN y TFV-EMD para optimizar su fusión\n",
    "ceemdan_branches = [\"CEEMDAN_high\", \"CEEMDAN_medium\", \"CEEMDAN_low\"]\n",
    "tvfemd_branches = [\"TVFEMD_high\", \"TVFEMD_medium\", \"TVFEMD_low\"]\n",
    "fusion_branches = [\"FUSION_high\", \"FUSION_medium\", \"FUSION_low\"]\n",
    "\n",
    "# Cargar datos CEEMDAN\n",
    "da_ceemdan = np.stack([ds_fuse[branch].values for branch in ceemdan_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar datos TFV-EMD\n",
    "da_tvfemd = np.stack([ds_fuse[branch].values for branch in tvfemd_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar fusión predefinida (para referencia)\n",
    "da_fusion = np.stack([ds_fuse[branch].values for branch in fusion_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "\n",
    "# topografía y cluster\n",
    "elev = ds_full[\"elevation\"].values.ravel()  # (cells,)\n",
    "slope = ds_full[\"slope\"].values.ravel()\n",
    "\n",
    "# Manejar correctamente los valores de cluster (pueden ser texto)\n",
    "cluster_values = ds_full[\"cluster_elevation\"].values.ravel()\n",
    "# Verificar si los valores son strings o numéricos\n",
    "if isinstance(cluster_values[0], (str, np.str_)):\n",
    "    # Usar un LabelEncoder para convertir strings a enteros\n",
    "    le = LabelEncoder()\n",
    "    cluster = le.fit_transform(cluster_values)\n",
    "    logger.info(f\"Clusters codificados de texto a números: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    # Si ya son numéricos, convertir a enteros\n",
    "    cluster = cluster_values.astype(int)\n",
    "\n",
    "# dimensiones\n",
    "lat = ds_full.latitude.values\n",
    "lon = ds_full.longitude.values\n",
    "ny, nx = len(lat), len(lon)\n",
    "cells = ny*nx\n",
    "T = prec.shape[0]\n",
    "\n",
    "logger.info(f\"Dimensiones: T={T}, ny={ny}, nx={nx}, cells={cells}\")\n",
    "logger.info(f\"Shapes: prec={prec.shape}, da_ceemdan={da_ceemdan.shape}, da_tvfemd={da_tvfemd.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Definir máscaras para los niveles de elevación\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Definiendo máscaras para los niveles de elevación...\")\n",
    "mask_nivel1 = elev < 957  # nivel_1: 58-956m\n",
    "mask_nivel2 = (elev >= 957) & (elev <= 2264)  # nivel_2: 957-2264m\n",
    "mask_nivel3 = elev > 2264  # nivel_3: 2264-4728m\n",
    "\n",
    "logger.info(f\"Distribución de celdas por nivel de elevación:\")\n",
    "logger.info(f\"  Nivel 1 (<957m): {np.sum(mask_nivel1)} celdas\")\n",
    "logger.info(f\"  Nivel 2 (957-2264m): {np.sum(mask_nivel2)} celdas\")\n",
    "logger.info(f\"  Nivel 3 (>2264m): {np.sum(mask_nivel3)} celdas\")\n",
    "\n",
    "# Crear diccionario de máscaras para facilitar el procesamiento\n",
    "elevation_masks = {\n",
    "    \"nivel_1\": mask_nivel1,\n",
    "    \"nivel_2\": mask_nivel2,\n",
    "    \"nivel_3\": mask_nivel3\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Implementar función para optimizar fusión de CEEMDAN y TFV-EMD con XGBoost\n",
    "# -----------------------------------------------------------------------------\n",
    "import concurrent.futures\n",
    "import tqdm\n",
    "from functools import partial\n",
    "\n",
    "@trace(\"Optimización de fusión\")\n",
    "def optimize_fusion_with_xgboost(ceemdan_data, tvfemd_data, target_data, masks, test_size=0.2, force_retrain=False):\n",
    "    \"\"\"\n",
    "    Optimiza la fusión de CEEMDAN y TFV-EMD usando XGBoost para cada nivel de elevación.\n",
    "    Implementa paralelismo adaptativo basado en CPU/GPU y memoria disponible.\n",
    "    \n",
    "    Args:\n",
    "        ceemdan_data: Array de características CEEMDAN (T, ny, nx, 3)\n",
    "        tvfemd_data: Array de características TFV-EMD (T, ny, nx, 3)\n",
    "        target_data: Array de valores objetivo (precipitación) (T, ny, nx)\n",
    "        masks: Diccionario de máscaras por nivel de elevación\n",
    "        test_size: Proporción del conjunto de prueba\n",
    "        force_retrain: Si es True, fuerza el reentrenamiento aunque existan modelos guardados\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary con modelos XGBoost para fusión por nivel y componente\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    # Comprobar si todos los modelos ya existen\n",
    "    all_models_exist = True\n",
    "    if not force_retrain:\n",
    "        for level_name in masks:\n",
    "            for component_idx in range(3):\n",
    "                if not model_exists('fusion', level_name, component_idx):\n",
    "                    all_models_exist = False\n",
    "                    break\n",
    "            if not all_models_exist:\n",
    "                break\n",
    "                \n",
    "        if all_models_exist:\n",
    "            logger.info(\"Todos los modelos de fusión existen. Cargando...\")\n",
    "            return load_all_fusion_models(masks)\n",
    "    \n",
    "    # Determinar recursos computacionales disponibles\n",
    "    mem_info = get_memory_info()\n",
    "    cpu_count = os.cpu_count()\n",
    "    \n",
    "    print(f\"\\n🖥️  Recursos detectados: {cpu_count} CPUs, {mem_info['total_gb']:.1f}GB RAM ({mem_info['available_gb']:.1f}GB disponible)\")\n",
    "    \n",
    "    # SOLUCIÓN: Aumentar agresivamente el número de trabajadores para forzar paralelismo\n",
    "    # y aprovechar mejor los recursos subutilizados\n",
    "    optimal_workers = max(3, min(cpu_count - 1, 8))  # Mínimo 3 workers, máximo CPU-1 o 8\n",
    "    \n",
    "    # Verificar disponibilidad de GPU para tree_method\n",
    "    gpu_available = len(gpus) > 0\n",
    "    tree_method = 'gpu_hist' if gpu_available else 'hist'\n",
    "    \n",
    "    print(f\"🔧 Configuración optimizada: {optimal_workers} workers en paralelo FORZADOS, tree_method={tree_method}\")\n",
    "    print(f\"🧠 Memoria disponible: {mem_info['available_gb']:.2f}GB ({mem_info['used_percent']:.1f}% usado)\")\n",
    "    \n",
    "    # Total de componentes a procesar\n",
    "    total_levels = len(masks)\n",
    "    total_components = total_levels * 3  # 3 componentes por nivel\n",
    "    \n",
    "    # Inicializar estructuras de datos para resultados\n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]  # Placeholder para los 3 componentes\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "    \n",
    "    # Barra de progreso global\n",
    "    print(f\"\\n📊 Iniciando entrenamiento acelerado de {total_components} componentes ({total_levels} niveles × 3 componentes)\")\n",
    "    \n",
    "    # Función para procesar un componente específico\n",
    "    def process_component(level_name, mask, component_idx):\n",
    "        # Verificar si el modelo ya existe (a menos que se fuerce reentrenamiento)\n",
    "        if not force_retrain and model_exists('fusion', level_name, component_idx):\n",
    "            print(f\"🔄 Nivel {level_name}, componente {component_idx}: Cargando modelo existente...\")\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            if model and info:\n",
    "                weights = info.get('weights')\n",
    "                rmse = info.get('rmse', 0.0)\n",
    "                fit_time = info.get('fit_time', 0.0)\n",
    "                total_time = 0.1  # Tiempo mínimo para evitar divisiones por cero\n",
    "                print(f\"✅ {level_name}, comp{component_idx} (cargado): RMSE={rmse:.4f}, pesos=[CEEMDAN={weights[0]:.2f}, TFV-EMD={weights[1]:.2f}]\")\n",
    "                return {\n",
    "                    'level': level_name,\n",
    "                    'component': component_idx,\n",
    "                    'model': model,\n",
    "                    'weights': weights,\n",
    "                    'rmse': rmse,\n",
    "                    'fit_time': fit_time,\n",
    "                    'total_time': total_time,\n",
    "                    'loaded': True\n",
    "                }\n",
    "        \n",
    "        # Si llegamos aquí, necesitamos entrenar el modelo\n",
    "        print(f\"▶️  Nivel {level_name}, componente {component_idx}: Iniciando entrenamiento rápido...\")\n",
    "        comp_start = time.time()\n",
    "        cells_in_level = np.sum(mask)\n",
    "        \n",
    "        # Reformatear los datos para el entrenamiento\n",
    "        X_ceemdan = ceemdan_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        X_tvfemd = tvfemd_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        y_target = target_data.reshape(T, -1)[:, mask]\n",
    "        \n",
    "        print(f\"   Datos: {X_ceemdan.shape[0]} muestras, {cells_in_level} celdas\")\n",
    "        \n",
    "        # Concatenar características\n",
    "        X_combined = np.column_stack([X_ceemdan, X_tvfemd])\n",
    "        \n",
    "        # División simple para mayor velocidad (sin estratificación que consume tiempo)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_target, test_size=test_size, random_state=42\n",
    "        )\n",
    "        split_method = \"simple (optimizado para velocidad)\"\n",
    "        \n",
    "        print(f\"   Split: {X_train.shape[0]} train, {X_test.shape[0]} test ({split_method})\")\n",
    "        \n",
    "        # SOLUCIÓN: Optimizar hiperparámetros para mayor velocidad\n",
    "        n_samples, n_features = X_train.shape\n",
    "        # Reducir profundidad y número de árboles para entrenamientos más rápidos\n",
    "        max_depth = min(4, max(3, int(np.log2(n_features/2))))  # Profundidad reducida\n",
    "        n_estimators = min(60, max(30, int(30 + 5 * np.log(n_samples))))  # Menos árboles\n",
    "        learning_rate = min(0.3, max(0.08, 0.2))  # Learning rate más alto para convergencia rápida\n",
    "        subsample = 0.7  # Usar menos datos por árbol\n",
    "        colsample = 0.7  # Usar menos columnas por árbol\n",
    "        \n",
    "        # Configurar modelo XGBoost con paralelismo más eficiente\n",
    "        model = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample,\n",
    "            tree_method=tree_method,\n",
    "            n_jobs=1,  # Un hilo por modelo para maximizar paralelismo entre modelos\n",
    "            enable_categorical=False,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        # Entrenar modelo con mensaje de progreso\n",
    "        print(f\"   Entrenamiento ultra-rápido: {n_estimators} estimators, depth={max_depth}, lr={learning_rate:.3f}\")\n",
    "        fit_start = time.time()\n",
    "        \n",
    "        # Entrenamiento simplificado para mayor velocidad\n",
    "        model.fit(\n",
    "            X_train, y_train.ravel(),\n",
    "            eval_set=[(X_test, y_test.ravel())],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        fit_time = time.time() - fit_start\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test.ravel(), y_pred))\n",
    "        \n",
    "        # Extraer pesos de importancia para CEEMDAN vs TFV-EMD\n",
    "        importance = model.feature_importances_\n",
    "        cells_per_feature = cells_in_level\n",
    "        \n",
    "        # Promedio de importancia para cada fuente\n",
    "        ceemdan_importance = np.mean(importance[:cells_per_feature])\n",
    "        tvfemd_importance = np.mean(importance[cells_per_feature:])\n",
    "        \n",
    "        # Normalizar para que sumen 1\n",
    "        total_importance = ceemdan_importance + tvfemd_importance\n",
    "        ceemdan_weight = ceemdan_importance / total_importance\n",
    "        tvfemd_weight = tvfemd_importance / total_importance\n",
    "        \n",
    "        comp_time = time.time() - comp_start\n",
    "        \n",
    "        print(f\"✅ {level_name}, comp{component_idx}: RMSE={rmse:.4f}, tiempo={comp_time:.1f}s, \"\n",
    "              f\"pesos=[CEEMDAN={ceemdan_weight:.2f}, TFV-EMD={tvfemd_weight:.2f}]\")\n",
    "        \n",
    "        weights = (ceemdan_weight, tvfemd_weight)\n",
    "        \n",
    "        # Guardar modelo para uso futuro con información adicional\n",
    "        info = {\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'training_date': datetime.datetime.now().strftime(timestamp_format),\n",
    "            'hyper_params': {\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'learning_rate': learning_rate,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        save_model(model, 'fusion', level_name, component_idx, info)\n",
    "        \n",
    "        # Devolver resultados\n",
    "        return {\n",
    "            'level': level_name,\n",
    "            'component': component_idx,\n",
    "            'model': model,\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'loaded': False\n",
    "        }\n",
    "    \n",
    "    # Procesar niveles y componentes usando paralelismo adaptativo\n",
    "    all_tasks = []\n",
    "    for level_name, mask in masks.items():\n",
    "        # Crear tareas para todos los componentes\n",
    "        for component_idx in range(3):\n",
    "            all_tasks.append((level_name, mask, component_idx))\n",
    "    \n",
    "    # SOLUCIÓN: FORZAR paralelismo siempre\n",
    "    all_results = []\n",
    "    \n",
    "    # Mostrar mensaje claro sobre el modo paralelo\n",
    "    print(f\"\\n⚡ Activando procesamiento paralelo forzado con {optimal_workers} workers para acelerar el entrenamiento\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=optimal_workers) as executor:\n",
    "        # Crear lista de futuros\n",
    "        futures = []\n",
    "        for level_name, mask, component_idx in all_tasks:\n",
    "            futures.append(executor.submit(\n",
    "                process_component, level_name, mask, component_idx\n",
    "            ))\n",
    "        \n",
    "        # Mostrar progreso mientras se completan las tareas\n",
    "        completed = 0\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            completed += 1\n",
    "            progress = completed / len(futures)\n",
    "            print(f\"⏳ Progreso global: {completed}/{len(futures)} componentes ({progress:.1%})\")\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en componente: {str(e)}\")\n",
    "    \n",
    "    # Organizar resultados por nivel y componente\n",
    "    for result in all_results:\n",
    "        level = result['level']\n",
    "        component = result['component']\n",
    "        \n",
    "        # Guardar modelo y pesos\n",
    "        fusion_models[level][component] = result['model']\n",
    "        fusion_weights[level][component] = result['weights']\n",
    "        \n",
    "        # Registrar métricas para trazabilidad\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"rmse\", result['rmse'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"ceemdan_weight\", result['weights'][0])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"tvfemd_weight\", result['weights'][1])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"train_time\", result['fit_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"total_time\", result['total_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"loaded\", result.get('loaded', False))\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\n🏁 Optimización de fusión completada:\")\n",
    "    for level_name, components in fusion_models.items():\n",
    "        valid_components = sum(1 for model in components if model is not None)\n",
    "        print(f\"  - {level_name}: {valid_components}/3 componentes entrenados\")\n",
    "    \n",
    "    tracker.add_checkpoint(\"Optimización de fusión completada\", \n",
    "                          {\"num_models\": sum(len(models) for models in fusion_models.values())})\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "def load_all_fusion_models(masks):\n",
    "    \"\"\"\n",
    "    Carga todos los modelos de fusión existentes\n",
    "    \n",
    "    Args:\n",
    "        masks: Diccionario de máscaras por nivel de elevación\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fusion_models, fusion_weights)\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "        \n",
    "        # Cargar los tres modelos de componentes\n",
    "        for component_idx in range(3):\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            \n",
    "            if model is not None and info is not None:\n",
    "                fusion_models[level_name][component_idx] = model\n",
    "                fusion_weights[level_name][component_idx] = info['weights']\n",
    "                logger.info(f\"Modelo fusión {level_name}, componente {component_idx} cargado\")\n",
    "                \n",
    "                # Registrar métricas para trazabilidad\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"rmse\", info.get('rmse', 0))\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"ceemdan_weight\", info['weights'][0])\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"tvfemd_weight\", info['weights'][1])\n",
    "                tracker.log_metric(f\"{level_name}_comp{component_idx}\", \"loaded\", True)\n",
    "            else:\n",
    "                logger.warning(f\"No se pudo cargar el modelo fusión {level_name}, componente {component_idx}\")\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Carga de datos con separación explícita de características CEEMDAN y TFV-EMD\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Cargando datasets y separando características CEEMDAN y TFV-EMD...\")\n",
    "ds_full = xr.open_dataset(FULL_NC)\n",
    "ds_fuse = xr.open_dataset(FUSION_NC)\n",
    "\n",
    "# precipitacion y variables\n",
    "prec = ds_full[\"total_precipitation\"].values  # (T, ny, nx)\n",
    "lags = sorted([v for v in ds_full.data_vars if \"_lag\" in v])\n",
    "da_lags = np.stack([ds_full[lag].values for lag in lags], axis=-1)  # (T, ny, nx, n_lags)\n",
    "\n",
    "# Separar características CEEMDAN y TFV-EMD para optimizar su fusión\n",
    "ceemdan_branches = [\"CEEMDAN_high\", \"CEEMDAN_medium\", \"CEEMDAN_low\"]\n",
    "tvfemd_branches = [\"TVFEMD_high\", \"TVFEMD_medium\", \"TVFEMD_low\"]\n",
    "fusion_branches = [\"FUSION_high\", \"FUSION_medium\", \"FUSION_low\"]\n",
    "\n",
    "# Cargar datos CEEMDAN\n",
    "da_ceemdan = np.stack([ds_fuse[branch].values for branch in ceemdan_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar datos TFV-EMD\n",
    "da_tvfemd = np.stack([ds_fuse[branch].values for branch in tvfemd_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "# Cargar fusión predefinida (para referencia)\n",
    "da_fusion = np.stack([ds_fuse[branch].values for branch in fusion_branches], axis=-1)  # (T, ny, nx, 3)\n",
    "\n",
    "# topografía y cluster\n",
    "elev = ds_full[\"elevation\"].values.ravel()  # (cells,)\n",
    "slope = ds_full[\"slope\"].values.ravel()\n",
    "\n",
    "# Manejar correctamente los valores de cluster (pueden ser texto)\n",
    "cluster_values = ds_full[\"cluster_elevation\"].values.ravel()\n",
    "# Verificar si los valores son strings o numéricos\n",
    "if isinstance(cluster_values[0], (str, np.str_)):\n",
    "    # Usar un LabelEncoder para convertir strings a enteros\n",
    "    le = LabelEncoder()\n",
    "    cluster = le.fit_transform(cluster_values)\n",
    "    logger.info(f\"Clusters codificados de texto a números: {dict(zip(le.classes_, range(len(le.classes_))))}\")\n",
    "else:\n",
    "    # Si ya son numéricos, convertir a enteros\n",
    "    cluster = cluster_values.astype(int)\n",
    "\n",
    "# dimensiones\n",
    "lat = ds_full.latitude.values\n",
    "lon = ds_full.longitude.values\n",
    "ny, nx = len(lat), len(lon)\n",
    "cells = ny*nx\n",
    "T = prec.shape[0]\n",
    "\n",
    "logger.info(f\"Dimensiones: T={T}, ny={ny}, nx={nx}, cells={cells}\")\n",
    "logger.info(f\"Shapes: prec={prec.shape}, da_ceemdan={da_ceemdan.shape}, da_tvfemd={da_tvfemd.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Definir máscaras para los niveles de elevación\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Definiendo máscaras para los niveles de elevación...\")\n",
    "mask_nivel1 = elev < 957  # nivel_1: 58-956m\n",
    "mask_nivel2 = (elev >= 957) & (elev <= 2264)  # nivel_2: 957-2264m\n",
    "mask_nivel3 = elev > 2264  # nivel_3: 2264-4728m\n",
    "\n",
    "logger.info(f\"Distribución de celdas por nivel de elevación:\")\n",
    "logger.info(f\"  Nivel 1 (<957m): {np.sum(mask_nivel1)} celdas\")\n",
    "logger.info(f\"  Nivel 2 (957-2264m): {np.sum(mask_nivel2)} celdas\")\n",
    "logger.info(f\"  Nivel 3 (>2264m): {np.sum(mask_nivel3)} celdas\")\n",
    "\n",
    "# Crear diccionario de máscaras para facilitar el procesamiento\n",
    "elevation_masks = {\n",
    "    \"nivel_1\": mask_nivel1,\n",
    "    \"nivel_2\": mask_nivel2,\n",
    "    \"nivel_3\": mask_nivel3\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Implementar función para optimizar fusión de CEEMDAN y TFV-EMD con XGBoost\n",
    "# -----------------------------------------------------------------------------\n",
    "import concurrent.futures\n",
    "import tqdm\n",
    "from functools import partial\n",
    "\n",
    "@trace(\"Optimización de fusión\")\n",
    "def optimize_fusion_with_xgboost(ceemdan_data, tvfemd_data, target_data, masks, test_size=0.2, force_retrain=False):\n",
    "    \"\"\"\n",
    "    Optimiza la fusión de CEEMDAN y TFV-EMD usando XGBoost para cada nivel de elevación.\n",
    "    Implementa paralelismo adaptativo basado en CPU/GPU y memoria disponible.\n",
    "    \n",
    "    Args:\n",
    "        ceemdan_data: Array de características CEEMDAN (T, ny, nx, 3)\n",
    "        tvfemd_data: Array de características TFV-EMD (T, ny, nx, 3)\n",
    "        target_data: Array de valores objetivo (precipitación) (T, ny, nx)\n",
    "        masks: Diccionario de máscaras por nivel de elevación\n",
    "        test_size: Proporción del conjunto de prueba\n",
    "        force_retrain: Si es True, fuerza el reentrenamiento aunque existan modelos guardados\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary con modelos XGBoost para fusión por nivel y componente\n",
    "    \"\"\"\n",
    "    fusion_models = {}\n",
    "    fusion_weights = {}\n",
    "    \n",
    "    # Comprobar si todos los modelos ya existen\n",
    "    all_models_exist = True\n",
    "    if not force_retrain:\n",
    "        for level_name in masks:\n",
    "            for component_idx in range(3):\n",
    "                if not model_exists('fusion', level_name, component_idx):\n",
    "                    all_models_exist = False\n",
    "                    break\n",
    "            if not all_models_exist:\n",
    "                break\n",
    "                \n",
    "        if all_models_exist:\n",
    "            logger.info(\"Todos los modelos de fusión existen. Cargando...\")\n",
    "            return load_all_fusion_models(masks)\n",
    "    \n",
    "    # Determinar recursos computacionales disponibles\n",
    "    mem_info = get_memory_info()\n",
    "    cpu_count = os.cpu_count()\n",
    "    \n",
    "    print(f\"\\n🖥️  Recursos detectados: {cpu_count} CPUs, {mem_info['total_gb']:.1f}GB RAM ({mem_info['available_gb']:.1f}GB disponible)\")\n",
    "    \n",
    "    # SOLUCIÓN: Aumentar agresivamente el número de trabajadores para forzar paralelismo\n",
    "    # y aprovechar mejor los recursos subutilizados\n",
    "    optimal_workers = max(3, min(cpu_count - 1, 8))  # Mínimo 3 workers, máximo CPU-1 o 8\n",
    "    \n",
    "    # Verificar disponibilidad de GPU para tree_method\n",
    "    gpu_available = len(gpus) > 0\n",
    "    tree_method = 'gpu_hist' if gpu_available else 'hist'\n",
    "    \n",
    "    print(f\"🔧 Configuración optimizada: {optimal_workers} workers en paralelo FORZADOS, tree_method={tree_method}\")\n",
    "    print(f\"🧠 Memoria disponible: {mem_info['available_gb']:.2f}GB ({mem_info['used_percent']:.1f}% usado)\")\n",
    "    \n",
    "    # Total de componentes a procesar\n",
    "    total_levels = len(masks)\n",
    "    total_components = total_levels * 3  # 3 componentes por nivel\n",
    "    \n",
    "    # Inicializar estructuras de datos para resultados\n",
    "    for level_name in masks.keys():\n",
    "        fusion_models[level_name] = [None, None, None]  # Placeholder para los 3 componentes\n",
    "        fusion_weights[level_name] = [None, None, None]\n",
    "    \n",
    "    # Barra de progreso global\n",
    "    print(f\"\\n📊 Iniciando entrenamiento acelerado de {total_components} componentes ({total_levels} niveles × 3 componentes)\")\n",
    "    \n",
    "    # Función para procesar un componente específico\n",
    "    def process_component(level_name, mask, component_idx):\n",
    "        # Verificar si el modelo ya existe (a menos que se fuerce reentrenamiento)\n",
    "        if not force_retrain and model_exists('fusion', level_name, component_idx):\n",
    "            print(f\"🔄 Nivel {level_name}, componente {component_idx}: Cargando modelo existente...\")\n",
    "            model, info = load_model('fusion', level_name, component_idx)\n",
    "            if model and info:\n",
    "                weights = info.get('weights')\n",
    "                rmse = info.get('rmse', 0.0)\n",
    "                fit_time = info.get('fit_time', 0.0)\n",
    "                total_time = 0.1  # Tiempo mínimo para evitar divisiones por cero\n",
    "                print(f\"✅ {level_name}, comp{component_idx} (cargado): RMSE={rmse:.4f}, pesos=[CEEMDAN={weights[0]:.2f}, TFV-EMD={weights[1]:.2f}]\")\n",
    "                return {\n",
    "                    'level': level_name,\n",
    "                    'component': component_idx,\n",
    "                    'model': model,\n",
    "                    'weights': weights,\n",
    "                    'rmse': rmse,\n",
    "                    'fit_time': fit_time,\n",
    "                    'total_time': total_time,\n",
    "                    'loaded': True\n",
    "                }\n",
    "        \n",
    "        # Si llegamos aquí, necesitamos entrenar el modelo\n",
    "        print(f\"▶️  Nivel {level_name}, componente {component_idx}: Iniciando entrenamiento rápido...\")\n",
    "        comp_start = time.time()\n",
    "        cells_in_level = np.sum(mask)\n",
    "        \n",
    "        # Reformatear los datos para el entrenamiento\n",
    "        X_ceemdan = ceemdan_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        X_tvfemd = tvfemd_data[:, :, :, component_idx].reshape(T, -1)[:, mask]\n",
    "        y_target = target_data.reshape(T, -1)[:, mask]\n",
    "        \n",
    "        print(f\"   Datos: {X_ceemdan.shape[0]} muestras, {cells_in_level} celdas\")\n",
    "        \n",
    "        # Concatenar características\n",
    "        X_combined = np.column_stack([X_ceemdan, X_tvfemd])\n",
    "        \n",
    "        # División simple para mayor velocidad (sin estratificación que consume tiempo)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_target, test_size=test_size, random_state=42\n",
    "        )\n",
    "        split_method = \"simple (optimizado para velocidad)\"\n",
    "        \n",
    "        print(f\"   Split: {X_train.shape[0]} train, {X_test.shape[0]} test ({split_method})\")\n",
    "        \n",
    "        # SOLUCIÓN: Optimizar hiperparámetros para mayor velocidad\n",
    "        n_samples, n_features = X_train.shape\n",
    "        # Reducir profundidad y número de árboles para entrenamientos más rápidos\n",
    "        max_depth = min(4, max(3, int(np.log2(n_features/2))))  # Profundidad reducida\n",
    "        n_estimators = min(60, max(30, int(30 + 5 * np.log(n_samples))))  # Menos árboles\n",
    "        learning_rate = min(0.3, max(0.08, 0.2))  # Learning rate más alto para convergencia rápida\n",
    "        subsample = 0.7  # Usar menos datos por árbol\n",
    "        colsample = 0.7  # Usar menos columnas por árbol\n",
    "        \n",
    "        # Configurar modelo XGBoost con paralelismo más eficiente\n",
    "        model = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample,\n",
    "            tree_method=tree_method,\n",
    "            n_jobs=1,  # Un hilo por modelo para maximizar paralelismo entre modelos\n",
    "            enable_categorical=False,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        # Entrenar modelo con mensaje de progreso\n",
    "        print(f\"   Entrenamiento ultra-rápido: {n_estimators} estimators, depth={max_depth}, lr={learning_rate:.3f}\")\n",
    "        fit_start = time.time()\n",
    "        \n",
    "        # Entrenamiento simplificado para mayor velocidad\n",
    "        model.fit(\n",
    "            X_train, y_train.ravel(),\n",
    "            eval_set=[(X_test, y_test.ravel())],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        fit_time = time.time() - fit_start\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test.ravel(), y_pred))\n",
    "        \n",
    "        # Extraer pesos de importancia para CEEMDAN vs TFV-EMD\n",
    "        importance = model.feature_importances_\n",
    "        cells_per_feature = cells_in_level\n",
    "        \n",
    "        # Promedio de importancia para cada fuente\n",
    "        ceemdan_importance = np.mean(importance[:cells_per_feature])\n",
    "        tvfemd_importance = np.mean(importance[cells_per_feature:])\n",
    "        \n",
    "        # Normalizar para que sumen 1\n",
    "        total_importance = ceemdan_importance + tvfemd_importance\n",
    "        ceemdan_weight = ceemdan_importance / total_importance\n",
    "        tvfemd_weight = tvfemd_importance / total_importance\n",
    "        \n",
    "        comp_time = time.time() - comp_start\n",
    "        \n",
    "        print(f\"✅ {level_name}, comp{component_idx}: RMSE={rmse:.4f}, tiempo={comp_time:.1f}s, \"\n",
    "              f\"pesos=[CEEMDAN={ceemdan_weight:.2f}, TFV-EMD={tvfemd_weight:.2f}]\")\n",
    "        \n",
    "        weights = (ceemdan_weight, tvfemd_weight)\n",
    "        \n",
    "        # Guardar modelo para uso futuro con información adicional\n",
    "        info = {\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'training_date': datetime.datetime.now().strftime(timestamp_format),\n",
    "            'hyper_params': {\n",
    "                'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'learning_rate': learning_rate,\n",
    "                'subsample': subsample,\n",
    "                'colsample_bytree': colsample\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        save_model(model, 'fusion', level_name, component_idx, info)\n",
    "        \n",
    "        # Devolver resultados\n",
    "        return {\n",
    "            'level': level_name,\n",
    "            'component': component_idx,\n",
    "            'model': model,\n",
    "            'weights': weights,\n",
    "            'rmse': rmse,\n",
    "            'fit_time': fit_time,\n",
    "            'total_time': comp_time,\n",
    "            'loaded': False\n",
    "        }\n",
    "    \n",
    "    # Procesar niveles y componentes usando paralelismo adaptativo\n",
    "    all_tasks = []\n",
    "    for level_name, mask in masks.items():\n",
    "        # Crear tareas para todos los componentes\n",
    "        for component_idx in range(3):\n",
    "            all_tasks.append((level_name, mask, component_idx))\n",
    "    \n",
    "    # SOLUCIÓN: FORZAR paralelismo siempre\n",
    "    all_results = []\n",
    "    \n",
    "    # Mostrar mensaje claro sobre el modo paralelo\n",
    "    print(f\"\\n⚡ Activando procesamiento paralelo forzado con {optimal_workers} workers para acelerar el entrenamiento\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=optimal_workers) as executor:\n",
    "        # Crear lista de futuros\n",
    "        futures = []\n",
    "        for level_name, mask, component_idx in all_tasks:\n",
    "            futures.append(executor.submit(\n",
    "                process_component, level_name, mask, component_idx\n",
    "            ))\n",
    "        \n",
    "        # Mostrar progreso mientras se completan las tareas\n",
    "        completed = 0\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            completed += 1\n",
    "            progress = completed / len(futures)\n",
    "            print(f\"⏳ Progreso global: {completed}/{len(futures)} componentes ({progress:.1%})\")\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error en componente: {str(e)}\")\n",
    "    \n",
    "    # Organizar resultados por nivel y componente\n",
    "    for result in all_results:\n",
    "        level = result['level']\n",
    "        component = result['component']\n",
    "        \n",
    "        # Guardar modelo y pesos\n",
    "        fusion_models[level][component] = result['model']\n",
    "        fusion_weights[level][component] = result['weights']\n",
    "        \n",
    "        # Registrar métricas para trazabilidad\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"rmse\", result['rmse'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"ceemdan_weight\", result['weights'][0])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"tvfemd_weight\", result['weights'][1])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"train_time\", result['fit_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"total_time\", result['total_time'])\n",
    "        tracker.log_metric(f\"{level}_comp{component}\", \"loaded\", result.get('loaded', False))\n",
    "    \n",
    "    # Resumen final\n",
    "    print(\"\\n🏁 Optimización de fusión completada:\")\n",
    "    for level_name, components in fusion_models.items():\n",
    "        valid_components = sum(1 for model in components if model is not None)\n",
    "        print(f\"  - {level_name}: {valid_components}/3 componentes entrenados\")\n",
    "    \n",
    "    tracker.add_checkpoint(\"Optimización de fusión completada\", \n",
    "                          {\"num_models\": sum(len(models) for models in fusion_models.values())})\n",
    "    \n",
    "    return fusion_models, fusion_weights\n",
    "\n",
    "# Funciones para evaluación de modelos\n",
    "@trace(\"Evaluación global de modelos\")\n",
    "def calculate_global_metrics(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Calcula métricas globales para todos los modelos.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Diccionario de predicciones por modelo\n",
    "        ground_truth: Valores reales de precipitación\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con métricas para cada modelo\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    metrics_list = []\n",
    "    \n",
    "    logger.info(\"Calculando métricas globales para todos los modelos...\")\n",
    "    \n",
    "    for model_name, preds in predictions.items():\n",
    "        # Aplanar arrays para cálculo de métricas globales\n",
    "        y_true = ground_truth.reshape(-1)\n",
    "        y_pred = preds.reshape(-1)\n",
    "        \n",
    "        # Filtrar NaN si existen\n",
    "        mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "        if np.sum(mask) < len(mask):\n",
    "            logger.warning(f\"Modelo {model_name}: {len(mask) - np.sum(mask)} valores NaN detectados y excluidos\")\n",
    "            y_true = y_true[mask]\n",
    "            y_pred = y_pred[mask]\n",
    "        \n",
    "        # Calcular métricas\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        # MAPE con manejo de divisiones por cero\n",
    "        mask_nonzero = y_true != 0\n",
    "        if np.sum(mask_nonzero) > 0:\n",
    "            mape = 100 * np.mean(np.abs((y_true[mask_nonzero] - y_pred[mask_nonzero]) / y_true[mask_nonzero]))\n",
    "        else:\n",
    "            mape = np.nan\n",
    "        \n",
    "        metrics_list.append({\n",
    "            'Model': model_name,\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'MAPE': mape,\n",
    "            'R²': r2\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Modelo {model_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, MAPE={mape:.2f}%, R²={r2:.4f}\")\n",
    "    \n",
    "    # Crear DataFrame con métricas\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df\n",
    "\n",
    "@trace(\"Evaluación por niveles de elevación\")\n",
    "def calculate_metrics_by_elevation(predictions, ground_truth, elevation_masks):\n",
    "    \"\"\"\n",
    "    Calcula métricas separadas por nivel de elevación para todos los modelos.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Diccionario de predicciones por modelo\n",
    "        ground_truth: Valores reales de precipitación\n",
    "        elevation_masks: Diccionario de máscaras por nivel de elevación\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con métricas para cada modelo y nivel de elevación\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    metrics_list = []\n",
    "    \n",
    "    logger.info(\"Calculando métricas por nivel de elevación...\")\n",
    "    \n",
    "    # Para cada nivel de elevación\n",
    "    for level_name, mask in elevation_masks.items():\n",
    "        # Máscara a índices\n",
    "        level_indices = np.where(mask)[0]\n",
    "        \n",
    "        # Para cada modelo\n",
    "        for model_name, preds in predictions.items():\n",
    "            # Preparar datos para este nivel\n",
    "            y_true_level = []\n",
    "            y_pred_level = []\n",
    "            \n",
    "            # Recopilar predicciones para todos los timesteps y horizontes\n",
    "            for t in range(ground_truth.shape[0]):\n",
    "                for h in range(ground_truth.shape[1]):\n",
    "                    y_true_level.append(ground_truth[t, h, level_indices])\n",
    "                    y_pred_level.append(preds[t, h, level_indices])\n",
    "            \n",
    "            # Convertir a arrays y aplanar\n",
    "            y_true_level = np.concatenate(y_true_level)\n",
    "            y_pred_level = np.concatenate(y_pred_level)\n",
    "            \n",
    "            # Filtrar NaN si existen\n",
    "            mask_valid = ~np.isnan(y_true_level) & ~np.isnan(y_pred_level)\n",
    "            if np.sum(mask_valid) < len(mask_valid):\n",
    "                logger.warning(f\"Modelo {model_name}, nivel {level_name}: {len(mask_valid) - np.sum(mask_valid)} valores NaN detectados y excluidos\")\n",
    "                y_true_level = y_true_level[mask_valid]\n",
    "                y_pred_level = y_pred_level[mask_valid]\n",
    "            \n",
    "            # Calcular métricas para este nivel\n",
    "            mae = mean_absolute_error(y_true_level, y_pred_level)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true_level, y_pred_level))\n",
    "            r2 = r2_score(y_true_level, y_pred_level)\n",
    "            \n",
    "            # MAPE con manejo de divisiones por cero\n",
    "            mask_nonzero = y_true_level != 0\n",
    "            if np.sum(mask_nonzero) > 0:\n",
    "                mape = 100 * np.mean(np.abs((y_true_level[mask_nonzero] - y_pred_level[mask_nonzero]) / y_true_level[mask_nonzero]))\n",
    "            else:\n",
    "                mape = np.nan\n",
    "            \n",
    "            metrics_list.append({\n",
    "                'Model': model_name,\n",
    "                'Elevation Level': level_name,\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'R²': r2\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"Modelo {model_name}, nivel {level_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, MAPE={mape:.2f}%, R²={r2:.4f}\")\n",
    "    \n",
    "    # Crear DataFrame con métricas\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df\n",
    "\n",
    "@trace(\"Evaluación por percentiles\")\n",
    "def calculate_metrics_by_percentiles(predictions, ground_truth, percentiles):\n",
    "    \"\"\"\n",
    "    Calcula métricas separadas por rangos de percentiles para todos los modelos.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Diccionario de predicciones por modelo\n",
    "        ground_truth: Valores reales de precipitación\n",
    "        percentiles: Lista de percentiles para definir los rangos\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con métricas para cada modelo y rango de percentil\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    metrics_list = []\n",
    "    \n",
    "    logger.info(f\"Calculando métricas por percentiles: {percentiles}\")\n",
    "    \n",
    "    # Calcular umbrales de percentiles en los datos reales\n",
    "    y_true_flat = ground_truth.reshape(-1)\n",
    "    y_true_nonzero = y_true_flat[y_true_flat > 0]  # Solo valores positivos\n",
    "    \n",
    "    if len(y_true_nonzero) == 0:\n",
    "        logger.warning(\"No hay valores positivos para calcular percentiles. Omitiendo cálculo por percentiles.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calcular umbrales\n",
    "    thresholds = [np.percentile(y_true_nonzero, p) for p in percentiles]\n",
    "    logger.info(f\"Umbrales de percentiles: {thresholds}\")\n",
    "    \n",
    "    # Para cada rango de percentiles\n",
    "    for i in range(len(percentiles)-1):\n",
    "        lower_pct = percentiles[i]\n",
    "        upper_pct = percentiles[i+1]\n",
    "        lower_val = thresholds[i]\n",
    "        upper_val = thresholds[i+1]\n",
    "        \n",
    "        range_name = f\"P{lower_pct}-P{upper_pct}\"\n",
    "        logger.info(f\"Calculando métricas para rango {range_name}: {lower_val:.4f} - {upper_val:.4f}\")\n",
    "        \n",
    "        # Para cada modelo\n",
    "        for model_name, preds in predictions.items():\n",
    "            # Aplanar arrays\n",
    "            y_true = ground_truth.reshape(-1)\n",
    "            y_pred = preds.reshape(-1)\n",
    "            \n",
    "            # Filtrar por rango de percentiles\n",
    "            if i == len(percentiles)-2:  # Último rango, incluir el valor superior\n",
    "                mask_range = (y_true >= lower_val) & (y_true <= upper_val)\n",
    "            else:\n",
    "                mask_range = (y_true >= lower_val) & (y_true < upper_val)\n",
    "            \n",
    "            # Si no hay datos en este rango, continuar\n",
    "            if np.sum(mask_range) == 0:\n",
    "                logger.warning(f\"No hay datos en rango {range_name} para modelo {model_name}\")\n",
    "                continue\n",
    "                \n",
    "            # Filtrar datos para este rango\n",
    "            y_true_range = y_true[mask_range]\n",
    "            y_pred_range = y_pred[mask_range]\n",
    "            \n",
    "            # Filtrar NaN si existen\n",
    "            mask_valid = ~np.isnan(y_true_range) & ~np.isnan(y_pred_range)\n",
    "            if np.sum(mask_valid) < len(mask_valid):\n",
    "                logger.warning(f\"Modelo {model_name}, rango {range_name}: {len(mask_valid) - np.sum(mask_valid)} valores NaN detectados y excluidos\")\n",
    "                y_true_range = y_true_range[mask_valid]\n",
    "                y_pred_range = y_pred_range[mask_valid]\n",
    "            \n",
    "            # Calcular métricas para este rango\n",
    "            mae = mean_absolute_error(y_true_range, y_pred_range)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true_range, y_pred_range))\n",
    "            r2 = r2_score(y_true_range, y_pred_range)\n",
    "            \n",
    "            # MAPE con manejo de divisiones por cero\n",
    "            mask_nonzero = y_true_range != 0\n",
    "            if np.sum(mask_nonzero) > 0:\n",
    "                mape = 100 * np.mean(np.abs((y_true_range[mask_nonzero] - y_pred_range[mask_nonzero]) / y_true_range[mask_nonzero]))\n",
    "            else:\n",
    "                mape = np.nan\n",
    "            \n",
    "            metrics_list.append({\n",
    "                'Model': model_name,\n",
    "                'Percentile Range': range_name,\n",
    "                'Value Range': f\"{lower_val:.4f} - {upper_val:.4f}\",\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'MAPE': mape,\n",
    "                'R²': r2,\n",
    "                'Samples': np.sum(mask_range)\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"Modelo {model_name}, rango {range_name}: MAE={mae:.4f}, RMSE={rmse:.4f}, MAPE={mape:.2f}%, R²={r2:.4f}\")\n",
    "    \n",
    "    # Crear DataFrame con métricas\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df\n",
    "\n",
    "@trace(\"Visualización de mapas de predicción\")\n",
    "def plot_all_model_maps(predictions, ground_truth, lat, lon, example_idx=0, horizon_idx=0):\n",
    "    \"\"\"\n",
    "    Genera mapas de predicciones para todos los modelos en un horizonte específico.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Diccionario de predicciones por modelo\n",
    "        ground_truth: Valores reales de precipitación\n",
    "        lat: Latitudes para el mapa\n",
    "        lon: Longitudes para el mapa\n",
    "        example_idx: Índice del ejemplo a visualizar\n",
    "        horizon_idx: Índice del horizonte a visualizar\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.feature as cfeature\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    \n",
    "    # Configurar visualización\n",
    "    n_models = len(predictions) + 1  # +1 para ground truth\n",
    "    n_cols = min(3, n_models)\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Crear figura\n",
    "    fig = plt.figure(figsize=(n_cols * 5, n_rows * 4))\n",
    "    \n",
    "    # Encontrar límites de colorbar consistentes para todos los mapas\n",
    "    vmin = ground_truth[example_idx, horizon_idx].min()\n",
    "    vmax = ground_truth[example_idx, horizon_idx].max()\n",
    "    \n",
    "    # Crear mapa para ground truth primero\n",
    "    ax = plt.subplot(n_rows, n_cols, 1, projection=ccrs.PlateCarree())\n",
    "    \n",
    "    # Reshape de datos para mapeo\n",
    "    ny, nx = len(lat), len(lon)\n",
    "    truth_map = ground_truth[example_idx, horizon_idx].reshape(ny, nx)\n",
    "    \n",
    "    # Configurar colormap para precipitación\n",
    "    cmap = plt.cm.YlGnBu\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    # Añadir características del mapa\n",
    "    ax.coastlines(resolution='10m')\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.RIVERS, linestyle='-', alpha=0.5)\n",
    "    \n",
    "    # Plotear datos\n",
    "    im = ax.pcolormesh(lon, lat, truth_map, cmap=cmap, norm=norm, transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # Añadir colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, axes_class=plt.Axes)\n",
    "    plt.colorbar(im, cax=cax, orientation=\"vertical\", label=\"Precipitation\")\n",
    "    \n",
    "    # Título y configuración\n",
    "    ax.set_title(f\"Ground Truth (H+{horizon_idx+1})\")\n",
    "    ax.gridlines(draw_labels=True, alpha=0.3)\n",
    "    \n",
    "    # Crear mapas para cada modelo\n",
    "    for i, (model_name, preds) in enumerate(predictions.items(), 2):\n",
    "        ax = plt.subplot(n_rows, n_cols, i, projection=ccrs.PlateCarree())\n",
    "        \n",
    "        # Reshape de datos para mapeo\n",
    "        pred_map = preds[example_idx, horizon_idx].reshape(ny, nx)\n",
    "        \n",
    "        # Añadir características del mapa\n",
    "        ax.coastlines(resolution='10m')\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        ax.add_feature(cfeature.RIVERS, linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Plotear datos\n",
    "        im = ax.pcolormesh(lon, lat, pred_map, cmap=cmap, norm=norm, transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # Añadir colorbar\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05, axes_class=plt.Axes)\n",
    "        plt.colorbar(im, cax=cax, orientation=\"vertical\", label=\"Precipitation\")\n",
    "        \n",
    "        # Título y configuración\n",
    "        ax.set_title(f\"{model_name} (H+{horizon_idx+1})\")\n",
    "        ax.gridlines(draw_labels=True, alpha=0.3)\n",
    "    \n",
    "    # Ajustar diseño y guardar\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{BASE}/models/output/prediction_maps_horizon{horizon_idx+1}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(f\"Mapas de predicción guardados para horizonte {horizon_idx+1}\")\n",
    "\n",
    "def visualize_process_tracker_results():\n",
    "    \"\"\"Visualiza los resultados del tracker de proceso\"\"\"\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Crear directorio de salida si no existe\n",
    "    vis_dir = Path(f\"{BASE}/models/output/visualizations\")\n",
    "    vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Visualizar tiempo por sección\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    section_names = list(tracker.section_times.keys())\n",
    "    section_times = list(tracker.section_times.values())\n",
    "    \n",
    "    # Ordenar por tiempo\n",
    "    indices = np.argsort(section_times)\n",
    "    section_names = [section_names[i] for i in indices]\n",
    "    section_times = [section_times[i] for i in indices]\n",
    "    \n",
    "    sns.barplot(x=section_times, y=section_names)\n",
    "    plt.title('Tiempo de ejecución por sección')\n",
    "    plt.xlabel('Tiempo (segundos)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{vis_dir}/section_times.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Visualizar uso de recursos a lo largo del tiempo\n",
    "    if tracker.resources:\n",
    "        times = [(r['timestamp'] - tracker.start_time) for r in tracker.resources]\n",
    "        mem_pcts = [r['memory_percent'] for r in tracker.resources]\n",
    "        cpu_pcts = [r['cpu_percent'] for r in tracker.resources]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(times, mem_pcts, label='Memoria (%)')\n",
    "        plt.plot(times, cpu_pcts, label='CPU (%)')\n",
    "        plt.axhline(y=90, color='r', linestyle='--', alpha=0.7, label='Límite crítico (90%)')\n",
    "        \n",
    "        # Añadir marcas de checkpoints\n",
    "        for cp in tracker.checkpoints:\n",
    "            plt.axvline(x=cp['elapsed_total'], color='g', alpha=0.5, linestyle='-.')\n",
    "        \n",
    "        plt.title('Uso de recursos durante la ejecución')\n",
    "        plt.xlabel('Tiempo (segundos)')\n",
    "        plt.ylabel('Uso (%)')\n",
    "        plt.ylim(0, 100)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f\"{vis_dir}/resource_usage.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    logger.info(f\"Visualizaciones del proceso guardadas en {vis_dir}\")\n",
    "\n",
    "def display_log_summary():\n",
    "    \"\"\"Muestra un resumen del archivo de log\"\"\"\n",
    "    log_file = LOG_DIR / log_filename\n",
    "    if not log_file.exists():\n",
    "        logger.warning(f\"No se encontró el archivo de log: {log_file}\")\n",
    "        return\n",
    "    \n",
    "    # Leer últimas líneas\n",
    "    try:\n",
    "        with open(log_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        # Mostrar stats básicos\n",
    "        total_lines = len(lines)\n",
    "        errors = sum(1 for line in lines if \" ERROR \" in line)\n",
    "        warnings = sum(1 for line in lines if \" WARNING \" in line)\n",
    "        infos = sum(1 for line in lines if \" INFO \" in line)\n",
    "        \n",
    "        print(f\"\\n📋 Resumen del log ({log_file.name}):\")\n",
    "        print(f\"  Total líneas: {total_lines}\")\n",
    "        print(f\"  Información: {infos}\")\n",
    "        print(f\"  Advertencias: {warnings}\")\n",
    "        print(f\"  Errores: {errors}\")\n",
    "        \n",
    "        # Mostrar últimos errores\n",
    "        if errors > 0:\n",
    "            print(\"\\n⚠️ Últimos errores:\")\n",
    "            error_lines = [line.strip() for line in lines if \" ERROR \" in line]\n",
    "            for line in error_lines[-min(5, len(error_lines)):]:\n",
    "                print(f\"  {line}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error procesando archivo de log: {str(e)}\")\n",
    "        \n",
    "@trace(\"Generación de fusión optimizada\")\n",
    "def generate_optimized_fusion(ceemdan_data, tvfemd_data, fusion_weights, elevation_masks):\n",
    "    \"\"\"\n",
    "    Genera features de fusión optimizadas basadas en los pesos de importancia aprendidos\n",
    "    \n",
    "    Args:\n",
    "        ceemdan_data: Array de características CEEMDAN (T, ny, nx, 3)\n",
    "        tvfemd_data: Array de características TFV-EMD (T, ny, nx, 3)\n",
    "        fusion_weights: Diccionario de pesos por nivel y componente\n",
    "        elevation_masks: Diccionario de máscaras por nivel de elevación\n",
    "        \n",
    "    Returns:\n",
    "        Array de fusión optimizada (T, ny, nx, 3)\n",
    "    \"\"\"\n",
    "    T, ny, nx, n_components = ceemdan_data.shape\n",
    "    logger.info(f\"Generando fusión optimizada: shape={T}×{ny}×{nx}×{n_components}\")\n",
    "    \n",
    "    # Inicializar array para fusión optimizada\n",
    "    fusion_optimized = np.zeros_like(ceemdan_data)\n",
    "    \n",
    "    # Para cada nivel de elevación\n",
    "    for level_name, mask in elevation_masks.items():\n",
    "        # Convertir máscara 1D a 2D para aplicarla a los datos\n",
    "        level_mask_2d = mask.reshape(ny, nx)\n",
    "        \n",
    "        # Para cada componente\n",
    "        for comp_idx in range(n_components):\n",
    "            # Obtener pesos para este nivel y componente\n",
    "            if level_name in fusion_weights and fusion_weights[level_name][comp_idx] is not None:\n",
    "                ceemdan_weight, tvfemd_weight = fusion_weights[level_name][comp_idx]\n",
    "            else:\n",
    "                logger.warning(f\"No hay pesos para {level_name}, componente {comp_idx}. Usando pesos uniformes.\")\n",
    "                ceemdan_weight = tvfemd_weight = 0.5\n",
    "            \n",
    "            # Aplicar fusión ponderada para este nivel y componente\n",
    "            for t in range(T):\n",
    "                # Extraer solo las celdas para este nivel\n",
    "                fusion_optimized[t, level_mask_2d, comp_idx] = (\n",
    "                    ceemdan_weight * ceemdan_data[t, level_mask_2d, comp_idx] +\n",
    "                    tvfemd_weight * tvfemd_data[t, level_mask_2d, comp_idx]\n",
    "                )\n",
    "    \n",
    "    logger.info(f\"Fusión optimizada generada correctamente con shape {fusion_optimized.shape}\")\n",
    "    return fusion_optimized"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
