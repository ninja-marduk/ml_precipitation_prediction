{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/hybrid_models_ST-HybridWaveStack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7b96f3ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7b96f3ea",
        "outputId": "2a35e011-4fab-4fe2-a366-fbb70fa49f7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-19 17:37:19,690 INFO âš™ CPU cores: 10, RAM libre: 3.0 GB\n",
            "2025-05-19 17:37:19,691 INFO ğŸ“‚ Cargando datasetsâ€¦\n",
            "2025-05-19 17:37:19,725 WARNING REF_DATE no hallado; usando Ãºltimo mes: 2025-02\n",
            "2025-05-19 17:37:19,726 INFO â–¶ Procesando CEEMDAN_high\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â–¶ï¸ Base path: /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 1.0134 - val_loss: 0.8062\n",
            "Epoch 2/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.7888 - val_loss: 0.7200\n",
            "Epoch 3/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - loss: 0.6007 - val_loss: 0.7267\n",
            "Epoch 4/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.5893 - val_loss: 0.7551\n",
            "Epoch 5/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.4737 - val_loss: 0.7128\n",
            "Epoch 6/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.4442 - val_loss: 0.6976\n",
            "Epoch 7/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.4166 - val_loss: 0.7586\n",
            "Epoch 8/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.4003 - val_loss: 0.6515\n",
            "Epoch 9/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - loss: 0.3680 - val_loss: 0.8289\n",
            "Epoch 10/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.3805 - val_loss: 0.7004\n",
            "Epoch 11/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.3930 - val_loss: 0.8003\n",
            "Epoch 12/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.3979 - val_loss: 0.6154\n",
            "Epoch 13/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.3388 - val_loss: 0.5631\n",
            "Epoch 14/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - loss: 0.3675 - val_loss: 0.7258\n",
            "Epoch 15/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - loss: 0.3475 - val_loss: 0.6760\n",
            "Epoch 16/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.3462 - val_loss: 0.6988\n",
            "Epoch 17/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.3640 - val_loss: 0.6482\n",
            "Epoch 18/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.3635 - val_loss: 0.7509\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-19 17:37:41,056 WARNING You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "2025-05-19 17:37:41,372 ERROR â€¼ Error en CEEMDAN_high, continÃºoâ€¦\n",
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/83/c6n8lktn4qx_fwp7ksllkkhn0dhtn2/T/ipykernel_42911/781569034.py\", line 272, in <module>\n",
            "    pm = scY.inverse_transform(preds_s[h,0]).reshape(ny,nx)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/preprocessing/_data.py\", line 1106, in inverse_transform\n",
            "    X = check_array(\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1093, in check_array\n",
            "    raise ValueError(msg)\n",
            "ValueError: Expected 2D array, got 1D array instead:\n",
            "array=[-0.09601123 -0.13580438 -0.1794446  ...  0.7315675   0.69340205\n",
            "  0.64309657].\n",
            "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
            "2025-05-19 17:37:41,373 INFO â–¶ Procesando CEEMDAN_medium\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - loss: 0.9550 - val_loss: 0.9645\n",
            "Epoch 2/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.8952 - val_loss: 1.0057\n",
            "Epoch 3/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - loss: 0.7863 - val_loss: 1.1768\n",
            "Epoch 4/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - loss: 0.6825 - val_loss: 0.8198\n",
            "Epoch 5/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.6333 - val_loss: 1.1406\n",
            "Epoch 6/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.5573 - val_loss: 0.9944\n",
            "Epoch 7/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - loss: 0.5086 - val_loss: 1.0982\n",
            "Epoch 8/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - loss: 0.4573 - val_loss: 1.4394\n",
            "Epoch 9/100\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - loss: 0.3940 - val_loss: 1.1079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-19 17:37:54,450 WARNING You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "2025-05-19 17:37:54,776 ERROR â€¼ Error en CEEMDAN_medium, continÃºoâ€¦\n",
            "Traceback (most recent call last):\n",
            "  File \"/var/folders/83/c6n8lktn4qx_fwp7ksllkkhn0dhtn2/T/ipykernel_42911/781569034.py\", line 272, in <module>\n",
            "    pm = scY.inverse_transform(preds_s[h,0]).reshape(ny,nx)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/preprocessing/_data.py\", line 1106, in inverse_transform\n",
            "    X = check_array(\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/Users/riperez/Conda/anaconda3/envs/precipitation_prediction/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 1093, in check_array\n",
            "    raise ValueError(msg)\n",
            "ValueError: Expected 2D array, got 1D array instead:\n",
            "array=[0.05129286 0.09772117 0.08344763 ... 0.47967255 0.4196765  0.44851854].\n",
            "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
            "2025-05-19 17:37:54,778 INFO â–¶ Procesando CEEMDAN_low\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mEl kernel se bloqueÃ³ al ejecutar cÃ³digo en la celda actual o en una celda anterior. \n",
            "\u001b[1;31mRevise el cÃ³digo de las celdas para identificar una posible causa del error. \n",
            "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquÃ­</a> para obtener mÃ¡s informaciÃ³n. \n",
            "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener mÃ¡s detalles."
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Entrenamiento Multiâ€rama con GRU encoderâ€“decoder y Transformer para low,\n",
        "validaciÃ³n y forecast parametrizables, metaâ€modelo XGBoost,\n",
        "paralelizaciÃ³n, trazabilidad y lÃ­mites del departamento de BoyacÃ¡.\n",
        "\"\"\"\n",
        "\n",
        "# 0) SupresiÃ³n de warnings irrelevantes\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "from cartopy.io import DownloadWarning\n",
        "warnings.filterwarnings(\"ignore\", category=DownloadWarning)\n",
        "\n",
        "# 1) ParÃ¡metros configurables\n",
        "INPUT_WINDOW   = 60          # nÃºmero de meses en la ventana de entrada\n",
        "OUTPUT_HORIZON = 3           # meses de validaciÃ³n y forecast\n",
        "REF_DATE       = \"2025-03\"   # fecha de referencia (yyyy-mm)\n",
        "\n",
        "# 2) Detectar entorno (Local / Colab)\n",
        "import sys\n",
        "from pathlib import Path\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    BASE_PATH = Path(\"/content/drive/MyDrive/ml_precipitation_prediction\")\n",
        "    !pip install -q xarray netCDF4 optuna seaborn cartopy xgboost ace_tools_open\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p/\".git\").exists():\n",
        "            BASE_PATH = p\n",
        "            break\n",
        "print(f\"â–¶ï¸ Base path: {BASE_PATH}\")\n",
        "\n",
        "# 3) Rutas y logger\n",
        "import logging\n",
        "MODEL_DIR   = BASE_PATH/\"models\"/\"output\"/\"trained_models\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FEATURES_NC = BASE_PATH/\"models\"/\"output\"/\"features_fusion_branches.nc\"\n",
        "FULL_NC     = BASE_PATH/\"data\"/\"output\"/\"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
        "SHP_USER    = Path(\"/mnt/data/MGN_Departamento.shp\")\n",
        "BOYACA_SHP  = SHP_USER if SHP_USER.exists() else BASE_PATH/\"data\"/\"input\"/\"shapes\"/\"MGN_Departamento.shp\"\n",
        "RESULTS_CSV = MODEL_DIR/f\"metrics_w{OUTPUT_HORIZON}_ref{REF_DATE}.csv\"\n",
        "IMAGE_DIR   = MODEL_DIR/\"images\"\n",
        "IMAGE_DIR.mkdir(exist_ok=True)\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 4) Imports principales\n",
        "import numpy            as np\n",
        "import pandas           as pd\n",
        "import xarray           as xr\n",
        "import geopandas        as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio.v2       as imageio\n",
        "import cartopy.crs      as ccrs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import psutil\n",
        "from joblib import cpu_count\n",
        "import tensorflow       as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Add, Dense, Flatten, RepeatVector, Input, TimeDistributed, GRU\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 5) Recursos hardware\n",
        "CORES     = cpu_count()\n",
        "AVAIL_RAM = psutil.virtual_memory().available / (1024**3)\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "USE_GPU = bool(gpus)\n",
        "if USE_GPU:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    logger.info(f\"ğŸ–¥ GPU disponible: {gpus[0].name}\")\n",
        "else:\n",
        "    tf.config.threading.set_inter_op_parallelism_threads(CORES)\n",
        "    tf.config.threading.set_intra_op_parallelism_threads(CORES)\n",
        "    logger.info(f\"âš™ CPU cores: {CORES}, RAM libre: {AVAIL_RAM:.1f} GB\")\n",
        "\n",
        "# 6) Modelos y utilitarios\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
        "    mae  = np.mean(np.abs(y_true - y_pred))\n",
        "    mape = np.mean(np.abs((y_true - y_pred)/(y_true + 1e-5)))*100\n",
        "    r2   = 1 - np.sum((y_true-y_pred)**2)/np.sum((y_true-np.mean(y_true))**2)\n",
        "    return rmse, mae, mape, r2\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, Y, batch_size=32):\n",
        "        self.X, self.Y = X.astype(np.float32), Y.astype(np.float32)\n",
        "        self.batch_size = batch_size\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X)/self.batch_size))\n",
        "    def __getitem__(self, idx):\n",
        "        sl = slice(idx*self.batch_size, (idx+1)*self.batch_size)\n",
        "        return self.X[sl], self.Y[sl]\n",
        "\n",
        "def build_gru_ed(input_shape, horizon, n_cells,\n",
        "                 latent=128, dropout=0.2):\n",
        "    inp = Input(shape=input_shape)\n",
        "    x   = GRU(latent, dropout=dropout)(inp)\n",
        "    x   = RepeatVector(horizon)(x)\n",
        "    x   = GRU(latent, dropout=dropout, return_sequences=True)(x)\n",
        "    out = TimeDistributed(Dense(n_cells))(x)\n",
        "    m   = Model(inp, out)\n",
        "    m.compile(\"adam\",\"mse\")\n",
        "    return m\n",
        "\n",
        "def build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                         head_size=64, num_heads=4, ff_dim=256, dropout=0.1):\n",
        "    inp = Input(shape=input_shape)                            # (window, n_feats)\n",
        "    attn = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inp, inp)\n",
        "    x    = Add()([inp, attn])\n",
        "    x    = LayerNormalization(epsilon=1e-6)(x)\n",
        "    ff   = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff   = Dense(input_shape[-1])(ff)\n",
        "    x    = Add()([x, ff])\n",
        "    x    = LayerNormalization(epsilon=1e-6)(x)\n",
        "    x    = Flatten()(x)\n",
        "    x    = Dense(horizon * n_cells)(x)\n",
        "    out  = layers.Reshape((horizon, n_cells))(x)\n",
        "    m    = Model(inp, out)\n",
        "    m.compile(\"adam\",\"mse\")\n",
        "    return m\n",
        "\n",
        "def build_gru_ed_low(input_shape, horizon, n_cells,\n",
        "                     latent=256, dropout=0.1, use_transformer=True):\n",
        "    if use_transformer:\n",
        "        return build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                                    head_size=64, num_heads=4, ff_dim=512, dropout=dropout)\n",
        "    else:\n",
        "        return build_gru_ed(input_shape, horizon, n_cells,\n",
        "                            latent=latent, dropout=dropout)\n",
        "\n",
        "# 7) Carga de datos y shapefile\n",
        "logger.info(\"ğŸ“‚ Cargando datasetsâ€¦\")\n",
        "ds_full = xr.open_dataset(FULL_NC)\n",
        "ds_feat = xr.open_dataset(FEATURES_NC)\n",
        "boyaca_gdf = gpd.read_file(BOYACA_SHP)\n",
        "if boyaca_gdf.crs is None:\n",
        "    boyaca_gdf.set_crs(epsg=4326, inplace=True)\n",
        "else:\n",
        "    boyaca_gdf = boyaca_gdf.to_crs(epsg=4326)\n",
        "\n",
        "times = ds_full.time.values.astype(\"datetime64[M]\")\n",
        "ref   = np.datetime64(REF_DATE,\"M\")\n",
        "if ref not in times:\n",
        "    ref = times[-1]\n",
        "    logger.warning(f\"REF_DATE no hallado; usando Ãºltimo mes: {ref}\")\n",
        "idx_ref = int(np.where(times==ref)[0][0])\n",
        "\n",
        "lat = ds_full.latitude.values\n",
        "lon = ds_full.longitude.values\n",
        "METHODS  = [\"CEEMDAN\",\"TVFEMD\",\"FUSION\"]\n",
        "BRANCHES = [\"high\",\"medium\",\"low\"]\n",
        "\n",
        "all_metrics  = []\n",
        "preds_store  = {}\n",
        "true_store   = {}\n",
        "histories    = {}   # guardar history por modelo\n",
        "\n",
        "# 8) Bucle principal\n",
        "for method in METHODS:\n",
        "    for branch in BRANCHES:\n",
        "        name = f\"{method}_{branch}\"\n",
        "        if name not in ds_feat.data_vars:\n",
        "            logger.warning(f\"âš  {name} no existe, saltando.\")\n",
        "            continue\n",
        "        logger.info(f\"â–¶ Procesando {name}\")\n",
        "        try:\n",
        "            # extraer y aplanar espacialmente\n",
        "            Xarr = ds_feat[name].values         # (T,ny,nx)\n",
        "            yarr = ds_full[\"total_precipitation\"].values\n",
        "            T, ny, nx = Xarr.shape\n",
        "            n_cells   = ny*nx\n",
        "\n",
        "            Xfull = Xarr.reshape(T, n_cells)\n",
        "            yfull = yarr.reshape(T, n_cells)\n",
        "\n",
        "            # ventanas deslizantes\n",
        "            Nw = T - INPUT_WINDOW - OUTPUT_HORIZON + 1\n",
        "            if Nw<=0:\n",
        "                logger.warning(\"âŒ Ventanas insuficientes.\")\n",
        "                continue\n",
        "\n",
        "            # construir Xs, ys\n",
        "            Xs = np.stack([Xfull[i:i+INPUT_WINDOW] for i in range(Nw)], axis=0)      # (Nw, window, n_cells)\n",
        "            ys = np.stack([yfull[i+INPUT_WINDOW:i+INPUT_WINDOW+OUTPUT_HORIZON]\n",
        "                           for i in range(Nw)], axis=0)                             # (Nw, horizon, n_cells)\n",
        "\n",
        "            # para low-branch: agregar sin/cos estacionales\n",
        "            if branch==\"low\":\n",
        "                months = pd.to_datetime(times).month.values\n",
        "                sin_feat = np.sin(2*np.pi*months/12)\n",
        "                cos_feat = np.cos(2*np.pi*months/12)\n",
        "                Ssin = np.stack([sin_feat[i:i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                Scos = np.stack([cos_feat[i:i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                # tile\n",
        "                sin_tile = np.repeat(Ssin[:,:,None], n_cells, axis=2)\n",
        "                cos_tile = np.repeat(Scos[:,:,None], n_cells, axis=2)\n",
        "                Xs = np.concatenate([Xs, sin_tile, cos_tile], axis=2)  # nuevo n_feats\n",
        "                n_feats = Xs.shape[2]\n",
        "            else:\n",
        "                n_feats = n_cells\n",
        "\n",
        "            # escalado global\n",
        "            scX = StandardScaler().fit(Xs.reshape(-1,n_feats))\n",
        "            scY = StandardScaler().fit(ys.reshape(-1,n_cells))\n",
        "            Xs_s = scX.transform(Xs.reshape(-1,n_feats)).reshape(Xs.shape)\n",
        "            ys_s = scY.transform(ys.reshape(-1,n_cells)).reshape(ys.shape)\n",
        "\n",
        "            # determinar Ã­ndices para validaciÃ³n centrados en REF_DATE\n",
        "            k_ref = idx_ref - INPUT_WINDOW + 1\n",
        "            k_ref = max(0, min(k_ref, Nw-1))\n",
        "            i0    = k_ref - (OUTPUT_HORIZON-1)\n",
        "            i0    = max(0, min(i0, Nw-OUTPUT_HORIZON))\n",
        "\n",
        "            # split train / validaciÃ³n\n",
        "            X_tr = Xs_s[:i0]\n",
        "            y_tr = ys_s[:i0]\n",
        "            X_va = Xs_s[i0:i0+OUTPUT_HORIZON]\n",
        "            y_va = ys_s[i0:i0+OUTPUT_HORIZON]\n",
        "\n",
        "            # cargar o entrenar modelo\n",
        "            if branch==\"low\":\n",
        "                model_dir = MODEL_DIR/f\"{name}_w{OUTPUT_HORIZON}_ref{ref}\"\n",
        "                saved_model_dir = model_dir/\"saved_model\"\n",
        "                if saved_model_dir.exists():\n",
        "                    model = tf.keras.models.load_model(saved_model_dir)\n",
        "                    logger.info(f\"â© Cargado low-branch SavedModel: {saved_model_dir}\")\n",
        "                else:\n",
        "                    model = build_gru_ed_low((INPUT_WINDOW,n_feats),\n",
        "                                             OUTPUT_HORIZON, n_cells,\n",
        "                                             latent=256, dropout=0.1,\n",
        "                                             use_transformer=True)\n",
        "                    hist = model.fit(\n",
        "                        DataGenerator(X_tr,y_tr),\n",
        "                        validation_data=DataGenerator(X_va,y_va),\n",
        "                        epochs=100,\n",
        "                        callbacks=[callbacks.EarlyStopping(\"val_loss\",patience=7,restore_best_weights=True)],\n",
        "                        verbose=1\n",
        "                    )\n",
        "                    # guardar\n",
        "                    model_dir.mkdir(parents=True, exist_ok=True)\n",
        "                    model.save(saved_model_dir)\n",
        "                    histories[name] = hist.history\n",
        "            else:\n",
        "                model_path = MODEL_DIR/f\"{name}_w{OUTPUT_HORIZON}_ref{ref}.h5\"\n",
        "                if model_path.exists():\n",
        "                    model = tf.keras.models.load_model(model_path)\n",
        "                    logger.info(f\"â© Cargado modelo: {model_path}\")\n",
        "                else:\n",
        "                    model = build_gru_ed((INPUT_WINDOW,n_feats), OUTPUT_HORIZON, n_cells) \\\n",
        "                            if branch==\"high\" else build_gru_ed((INPUT_WINDOW,n_feats), OUTPUT_HORIZON, n_cells)\n",
        "                    hist = model.fit(\n",
        "                        DataGenerator(X_tr,y_tr),\n",
        "                        validation_data=DataGenerator(X_va,y_va),\n",
        "                        epochs=100,\n",
        "                        callbacks=[callbacks.EarlyStopping(\"val_loss\",patience=5,restore_best_weights=True)],\n",
        "                        verbose=1\n",
        "                    )\n",
        "                    model.save(model_path)\n",
        "                    histories[name] = hist.history\n",
        "\n",
        "            # â€” ValidaciÃ³n: H=1â€¦H=OUTPUT_HORIZON\n",
        "            preds_s = model.predict(X_va, verbose=0)   # (H, H, n_cells) or (N_va, H, n_cells)\n",
        "            # asumimos X_va.shape[0]==OUTPUT_HORIZON\n",
        "            preds_s = preds_s.reshape(OUTPUT_HORIZON, OUTPUT_HORIZON, n_cells)\n",
        "            # invertimos escala\n",
        "            for h in range(OUTPUT_HORIZON):\n",
        "                date_val = str(times[i0+h+INPUT_WINDOW-1])\n",
        "                pm = scY.inverse_transform(preds_s[h,0]).reshape(ny,nx)\n",
        "                tm = scY.inverse_transform(y_va[h,0]).reshape(ny,nx)\n",
        "                rmse, mae, mape, r2 = evaluate_metrics(tm.ravel(), pm.ravel())\n",
        "                all_metrics.append({\n",
        "                    \"model\":name, \"branch\":branch,\n",
        "                    \"horizon\":h+1, \"type\":\"validation\",\n",
        "                    \"date\":date_val,\n",
        "                    \"RMSE\":rmse,\"MAE\":mae,\"MAPE\":mape,\"R2\":r2\n",
        "                })\n",
        "                preds_store [(name,date_val)] = pm\n",
        "                true_store  [(name,date_val)] = tm\n",
        "\n",
        "            # â€” Forecast tras REF_DATE\n",
        "            X_fc = Xs_s[k_ref:k_ref+1]\n",
        "            fc_s = model.predict(X_fc, verbose=0)[0]    # (H, n_cells)\n",
        "            FC   = scY.inverse_transform(fc_s)\n",
        "            for h in range(OUTPUT_HORIZON):\n",
        "                date_fc = str(times[idx_ref] + np.timedelta64(h+1,'M'))\n",
        "                all_metrics.append({\n",
        "                    \"model\":name, \"branch\":branch,\n",
        "                    \"horizon\":h+1, \"type\":\"forecast\",\n",
        "                    \"date\":date_fc,\n",
        "                    \"RMSE\":np.nan,\"MAE\":np.nan,\"MAPE\":np.nan,\"R2\":np.nan\n",
        "                })\n",
        "                preds_store[(name,date_fc)] = FC[h].reshape(ny,nx)\n",
        "\n",
        "        except Exception:\n",
        "            logger.exception(f\"â€¼ Error en {name}, continÃºoâ€¦\")\n",
        "            continue\n",
        "\n",
        "# 9) Guardar mÃ©tricas y mostrar tabla\n",
        "dfm = pd.DataFrame(all_metrics)\n",
        "dfm.to_csv(RESULTS_CSV, index=False)\n",
        "import ace_tools_open as tools\n",
        "tools.display_dataframe_to_user(name=f\"Metrics_w{OUTPUT_HORIZON}_ref{ref}\", dataframe=dfm)\n",
        "\n",
        "# 10) Curvas de entrenamiento\n",
        "for name,hist in histories.items():\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(hist[\"loss\"], label=\"train\")\n",
        "    plt.plot(hist[\"val_loss\"], label=\"val\")\n",
        "    plt.title(f\"Loss curve: {name}\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE\")\n",
        "    plt.legend(); plt.show()\n",
        "\n",
        "# 11) Mapas 3Ã—3 validaciÃ³n H=1\n",
        "dates_va = sorted({d for (_,d) in preds_store if\n",
        "                   any(r for r in all_metrics if r[\"date\"]==d and r[\"type\"]==\"validation\")})[:OUTPUT_HORIZON]\n",
        "for date_val in dates_va:\n",
        "    # gather global vmin/vmax\n",
        "    arrs = [preds_store[(f\"{m}_{b}\",date_val)].ravel()\n",
        "            for m in METHODS for b in BRANCHES\n",
        "            if (f\"{m}_{b}\",date_val) in preds_store]\n",
        "    vmin = np.min(arrs); vmax = np.max(arrs)\n",
        "    fig, axs = plt.subplots(3,3, figsize=(12,12),\n",
        "                             subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
        "    fig.suptitle(f\"ValidaciÃ³n H=1, {date_val}\", fontsize=16)\n",
        "    for i, b in enumerate(BRANCHES):\n",
        "        for j, m in enumerate(METHODS):\n",
        "            ax = axs[i,j]\n",
        "            ax.add_geometries(boyaca_gdf.geometry, ccrs.PlateCarree(),\n",
        "                              edgecolor=\"black\", facecolor=\"none\", linewidth=1)\n",
        "            key = (f\"{m}_{b}\", date_val)\n",
        "            if key in preds_store:\n",
        "                pcm = ax.pcolormesh(lon, lat, preds_store[key],\n",
        "                                    vmin=vmin, vmax=vmax,\n",
        "                                    transform=ccrs.PlateCarree(), cmap=\"Blues\")\n",
        "            ax.set_title(f\"{m}_{b}\")\n",
        "    cb = fig.colorbar(pcm, ax=axs, orientation=\"horizontal\", fraction=0.05, pad=0.04,\n",
        "                      label=\"PrecipitaciÃ³n (mm)\")\n",
        "    fig.savefig(IMAGE_DIR/f\"val_H1_{date_val}.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    # MAPE\n",
        "    arrs_mape = []\n",
        "    for (m,b) in [(m,b) for m in METHODS for b in BRANCHES]:\n",
        "        key = (f\"{m}_{b}\",date_val)\n",
        "        if key in preds_store and key in true_store:\n",
        "            mape_map = np.clip(np.abs((true_store[key]-preds_store[key])/(true_store[key]+1e-5))*100,0,200)\n",
        "            arrs_mape.append(mape_map.ravel())\n",
        "    vmin2, vmax2 = 0, np.max(arrs_mape)\n",
        "    fig, axs = plt.subplots(3,3, figsize=(12,12),\n",
        "                             subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
        "    fig.suptitle(f\"MAPE H=1, {date_val}\", fontsize=16)\n",
        "    for i, b in enumerate(BRANCHES):\n",
        "        for j, m in enumerate(METHODS):\n",
        "            ax = axs[i,j]\n",
        "            ax.add_geometries(boyaca_gdf.geometry, ccrs.PlateCarree(),\n",
        "                              edgecolor=\"black\", facecolor=\"none\", linewidth=1)\n",
        "            key = (f\"{m}_{b}\", date_val)\n",
        "            if key in preds_store and key in true_store:\n",
        "                mape_map = np.clip(np.abs((true_store[key]-preds_store[key])/(true_store[key]+1e-5))*100,0,200)\n",
        "                pcm2 = ax.pcolormesh(lon, lat, mape_map,\n",
        "                                     vmin=vmin2, vmax=vmax2,\n",
        "                                     transform=ccrs.PlateCarree(), cmap=\"Reds\")\n",
        "            ax.set_title(f\"{m}_{b}\")\n",
        "    cb2 = fig.colorbar(pcm2, ax=axs, orientation=\"horizontal\", fraction=0.05, pad=0.04,\n",
        "                       label=\"MAPE (%)\")\n",
        "    fig.savefig(IMAGE_DIR/f\"mape_H1_{date_val}.png\", dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "# 12) Metaâ€modelo XGBoost (low, h=3)\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_meta, y_meta = [], []\n",
        "for date in dates_va:\n",
        "    keys = [(f\"{m}_low\", date) for m in METHODS]\n",
        "    if all(k in preds_store for k in keys):\n",
        "        arrs = [preds_store[k].ravel() for k in keys]\n",
        "        X_meta.append(np.vstack(arrs).T)\n",
        "        y_meta.append(true_store[keys[0]].ravel())\n",
        "X_meta = np.concatenate(X_meta, axis=0)\n",
        "y_meta = np.concatenate(y_meta, axis=0)\n",
        "\n",
        "Xtr, Xte, ytr, yte = train_test_split(X_meta, y_meta, test_size=0.2, random_state=42)\n",
        "xgb = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=4, n_jobs=-1)\n",
        "xgb.fit(Xtr, ytr)\n",
        "yhat = xgb.predict(Xte)\n",
        "rmse_meta = np.sqrt(mean_squared_error(yte,yhat))\n",
        "print(f\"Metaâ€modelo XGB (low, h=3) RMSE: {rmse_meta:.3f}\")\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(yte, yhat, alpha=0.3, s=2)\n",
        "lims = [min(yte.min(),yhat.min()), max(yte.max(),yhat.max())]\n",
        "plt.plot(lims, lims, 'k--')\n",
        "plt.xlabel(\"True\"); plt.ylabel(\"Predicted\")\n",
        "plt.title(\"Metaâ€modelo XGB (low, h=3)\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "precipitation_prediction",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
