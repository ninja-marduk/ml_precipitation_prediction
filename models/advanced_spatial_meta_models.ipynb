{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Spatial Meta-Models: Stacking & Cross-Attention Fusion\n",
        "\n",
        "This notebook implements two meta-model strategies for advanced precipitation prediction:\n",
        "\n",
        "## Prerequisites\n",
        "This notebook requires pre-trained base models from `advanced_spatial_models.ipynb`:\n",
        "- ConvLSTM_Att models (3 experiments)\n",
        "- ConvGRU_Res models (3 experiments)  \n",
        "- Hybrid_Trans models (3 experiments)\n",
        "\n",
        "## üéØ Strategy 1: Stacking (Base Experiment)\n",
        "- **Approach**: Ensemble stacking of spatial models\n",
        "- **Difficulty**: ‚≠ê‚≠ê‚≠ê (High)\n",
        "- **Originality**: ‚≠ê‚≠ê‚≠ê‚≠ê (Very High)\n",
        "- **Citability**: ‚≠ê‚≠ê‚≠ê‚≠ê (Very High)\n",
        "- **Description**: Easy to implement, highly citable if it improves spatial/temporal robustness\n",
        "\n",
        "## üöÄ Strategy 2: Cross-Attention Fusion GRU ‚Üî LSTM-Att (Experimental)\n",
        "- **Approach**: Dual-attention decoder with cross-modal fusion\n",
        "- **Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê (Very High)\n",
        "- **Originality**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Breakthrough)\n",
        "- **Citability**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Breakthrough potential)\n",
        "- **Description**: Never reported in hydrology. Inspired by Vision-Language Transformers (ViLT, Perceiver IO)\n",
        "\n",
        "## üìä Development Methodology\n",
        "- Load pre-trained base models (no training duplication)\n",
        "- English language for all implementations\n",
        "- Consistent metrics: RMSE, MAE, MAPE, R¬≤\n",
        "- Same evaluation approach as base models\n",
        "- Comprehensive visualization and model exports\n",
        "- Output path: `output/Advanced_Spatial/meta_models/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and Imports for Meta-Models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "import logging\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# üîß FIXED: Add scipy import for Colab compatibility\n",
        "try:\n",
        "    from scipy.ndimage import gaussian_filter\n",
        "    SCIPY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    logger.warning(\"‚ö†Ô∏è scipy not available, installing...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scipy\"])\n",
        "    from scipy.ndimage import gaussian_filter\n",
        "    SCIPY_AVAILABLE = True\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logger.info(f\"üî• Using device: {device}\")\n",
        "\n",
        "# üîß FIXED: Synchronized paths with advanced_spatial_models.ipynb\n",
        "BASE_PATH = Path.cwd()\n",
        "while not (BASE_PATH / 'models').exists() and BASE_PATH.parent != BASE_PATH:\n",
        "    BASE_PATH = BASE_PATH.parent\n",
        "\n",
        "# Use 'advanced_spatial' (lowercase) to match advanced_spatial_models.ipynb\n",
        "ADVANCED_SPATIAL_ROOT = BASE_PATH / 'models' / 'output' / 'advanced_spatial'\n",
        "META_MODELS_ROOT = ADVANCED_SPATIAL_ROOT / 'meta_models'\n",
        "STACKING_OUTPUT = META_MODELS_ROOT / 'stacking'\n",
        "CROSS_ATTENTION_OUTPUT = META_MODELS_ROOT / 'cross_attention'\n",
        "\n",
        "# Create meta-model directories\n",
        "META_MODELS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "STACKING_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
        "CROSS_ATTENTION_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "logger.info(f\"üìÅ Project root: {BASE_PATH}\")\n",
        "logger.info(f\"üìÅ Advanced Spatial root: {ADVANCED_SPATIAL_ROOT}\")\n",
        "logger.info(f\"üìÅ Meta-models root: {META_MODELS_ROOT}\")\n",
        "\n",
        "# Visualization settings\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Pre-trained Base Models and Utility Functions\n",
        "\n",
        "def load_pretrained_base_models():\n",
        "    \"\"\"\n",
        "    Load pre-trained base models from advanced_spatial_models.ipynb output\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary containing loaded models and their metadata\n",
        "    \"\"\"\n",
        "    logger.info(\"üì¶ Loading pre-trained base models...\")\n",
        "    \n",
        "    # üîß FIXED: Define model structure matching advanced_spatial_models.ipynb exactly\n",
        "    experiments = ['ConvLSTM-ED', 'ConvLSTM-ED-KCE', 'ConvLSTM-ED-KCE-PAFC']\n",
        "    model_types = ['convlstm_att', 'convgru_res', 'hybrid_trans']\n",
        "    \n",
        "    logger.info(f\"üìÅ Looking for models in: {ADVANCED_SPATIAL_ROOT}\")\n",
        "    logger.info(f\"üìä Experiments: {experiments}\")\n",
        "    logger.info(f\"ü§ñ Model types: {model_types}\")\n",
        "    \n",
        "    loaded_models = {}\n",
        "    \n",
        "    for experiment in experiments:\n",
        "        for model_type in model_types:\n",
        "            model_path = ADVANCED_SPATIAL_ROOT / experiment / f\"{model_type}_best.keras\"\n",
        "            model_name = f\"{experiment}_{model_type}\"\n",
        "            \n",
        "            if model_path.exists():\n",
        "                try:\n",
        "                    logger.info(f\"   Loading {model_name} from {model_path}\")\n",
        "                    model = load_model(str(model_path), compile=False)\n",
        "                    loaded_models[model_name] = {\n",
        "                        'model': model,\n",
        "                        'experiment': experiment,\n",
        "                        'type': model_type,\n",
        "                        'path': model_path\n",
        "                    }\n",
        "                    logger.info(f\"   ‚úÖ Successfully loaded {model_name}\")\n",
        "                    \n",
        "                    # üîß ADDED: Memory management for Colab\n",
        "                    if is_colab:\n",
        "                        # Limit memory growth and cleanup\n",
        "                        import gc\n",
        "                        gc.collect()\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"   ‚ö†Ô∏è Failed to load {model_name}: {e}\")\n",
        "            else:\n",
        "                logger.warning(f\"   ‚ö†Ô∏è Model file not found: {model_path}\")\n",
        "    \n",
        "    logger.info(f\"‚úÖ Loaded {len(loaded_models)} base models\")\n",
        "    return loaded_models\n",
        "\n",
        "def evaluate_metrics_np(y_true, y_pred):\n",
        "    \"\"\"Calculate evaluation metrics for numpy arrays\"\"\"\n",
        "    # Remove NaN/Inf values\n",
        "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
        "    if mask.sum() == 0:\n",
        "        return np.nan, np.nan, np.nan, np.nan\n",
        "    \n",
        "    y_true, y_pred = y_true[mask], y_pred[mask]\n",
        "    \n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    \n",
        "    # MAPE calculation (avoid division by zero)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1e-8))) * 100\n",
        "    \n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    return rmse, mae, mape, r2\n",
        "\n",
        "def load_mock_data_for_testing():\n",
        "    \"\"\"\n",
        "    Create mock data for testing meta-models\n",
        "    This will be replaced with real predictions from loaded base models\n",
        "    \"\"\"\n",
        "    logger.info(\"üìä Loading mock data for meta-model testing...\")\n",
        "    \n",
        "    # Mock parameters (these will come from real base models)\n",
        "    n_samples = 100\n",
        "    horizon = 3\n",
        "    ny, nx = 64, 64\n",
        "    \n",
        "    # Generate mock base model predictions\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Create realistic precipitation-like data with spatial patterns\n",
        "    base_predictions = {}\n",
        "    experiments = ['ConvLSTM-ED', 'ConvLSTM-ED-KCE', 'ConvLSTM-ED-KCE-PAFC']\n",
        "    model_types = ['convlstm_att', 'convgru_res', 'hybrid_trans']\n",
        "    model_names = [f\"{exp}_{model_type}\" for exp in experiments for model_type in model_types]\n",
        "    \n",
        "    for model_name in model_names:\n",
        "        # Generate spatially coherent precipitation patterns\n",
        "        base_pred = np.random.exponential(scale=2.0, size=(n_samples, horizon, ny, nx))\n",
        "        base_pred = np.maximum(0, base_pred)  # Ensure non-negative\n",
        "        \n",
        "        # Add spatial smoothing for realism\n",
        "        from scipy.ndimage import gaussian_filter\n",
        "        for i in range(n_samples):\n",
        "            for h in range(horizon):\n",
        "                base_pred[i, h] = gaussian_filter(base_pred[i, h], sigma=1.5)\n",
        "        \n",
        "        base_predictions[model_name] = base_pred\n",
        "    \n",
        "    # Generate mock ground truth with some correlation to predictions\n",
        "    true_values = np.mean([pred for pred in base_predictions.values()], axis=0) + \\\n",
        "                  np.random.normal(0, 0.5, (n_samples, horizon, ny, nx))\n",
        "    true_values = np.maximum(0, true_values)  # Ensure non-negative\n",
        "    \n",
        "    logger.info(f\"‚úÖ Mock data created:\")\n",
        "    logger.info(f\"   Models: {len(model_names)}\")\n",
        "    logger.info(f\"   Samples: {n_samples}, Horizon: {horizon}\")\n",
        "    logger.info(f\"   Spatial dims: {ny}√ó{nx}\")\n",
        "    \n",
        "    return base_predictions, true_values, model_names\n",
        "\n",
        "def plot_training_history(history, title=\"Training History\", save_path=None):\n",
        "    \"\"\"Plot training and validation loss\"\"\"\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "    \n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    ax.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
        "    ax.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
        "    \n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel('Loss', fontsize=12)\n",
        "    ax.set_title(title, fontsize=14)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        logger.info(f\"üìà Training history saved to {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def save_metrics_to_csv(metrics_list, output_path):\n",
        "    \"\"\"Save metrics list to CSV file\"\"\"\n",
        "    df = pd.DataFrame(metrics_list)\n",
        "    df.to_csv(output_path, index=False)\n",
        "    logger.info(f\"üìä Metrics saved to {output_path}\")\n",
        "    return df\n",
        "\n",
        "# üîß FIXED: Load REAL Predictions from Advanced Spatial Models\n",
        "def load_real_predictions_from_manifests():\n",
        "    \"\"\"\n",
        "    Load REAL predictions from the exported manifests and prediction files\n",
        "    \n",
        "    Returns:\n",
        "        dict: Base model predictions\n",
        "        np.ndarray: Ground truth values  \n",
        "        list: Model names\n",
        "    \"\"\"\n",
        "    logger.info(\"üì¶ Loading REAL predictions from advanced_spatial_models.ipynb output...\")\n",
        "    \n",
        "    # Try to load from stacking manifest first\n",
        "    manifest_path = STACKING_OUTPUT / 'stacking_manifest.json'\n",
        "    predictions_dir = META_MODELS_ROOT / 'predictions'\n",
        "    \n",
        "    if not manifest_path.exists():\n",
        "        logger.warning(f\"‚ö†Ô∏è Manifest not found: {manifest_path}\")\n",
        "        logger.warning(\"üîÑ Falling back to mock data - Please run advanced_spatial_models.ipynb first!\")\n",
        "        return load_mock_data_for_testing()\n",
        "    \n",
        "    try:\n",
        "        # Load manifest\n",
        "        with open(manifest_path, 'r') as f:\n",
        "            manifest = json.load(f)\n",
        "        \n",
        "        logger.info(f\"‚úÖ Found manifest with {len(manifest['models'])} models\")\n",
        "        \n",
        "        # Load predictions for each model\n",
        "        base_predictions = {}\n",
        "        model_names = []\n",
        "        \n",
        "        for model_name, model_info in manifest['models'].items():\n",
        "            pred_file = Path(model_info['predictions_file'])\n",
        "            \n",
        "            if pred_file.exists():\n",
        "                try:\n",
        "                    predictions = np.load(pred_file)\n",
        "                    base_predictions[model_name] = predictions\n",
        "                    model_names.append(model_name)\n",
        "                    logger.info(f\"‚úÖ Loaded {model_name}: {predictions.shape}\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"‚ö†Ô∏è Failed to load {model_name}: {e}\")\n",
        "            else:\n",
        "                logger.warning(f\"‚ö†Ô∏è Prediction file not found: {pred_file}\")\n",
        "        \n",
        "        # Load ground truth\n",
        "        ground_truth_file = manifest.get('ground_truth_file')\n",
        "        if ground_truth_file and Path(ground_truth_file).exists():\n",
        "            true_values = np.load(ground_truth_file)\n",
        "            logger.info(f\"‚úÖ Loaded ground truth: {true_values.shape}\")\n",
        "        else:\n",
        "            logger.warning(\"‚ö†Ô∏è Ground truth not found, creating synthetic targets\")\n",
        "            # Create synthetic ground truth based on average predictions\n",
        "            if base_predictions:\n",
        "                first_pred = list(base_predictions.values())[0]\n",
        "                true_values = np.mean([pred for pred in base_predictions.values()], axis=0) + \\\n",
        "                            np.random.normal(0, 0.1, first_pred.shape)\n",
        "                true_values = np.maximum(0, true_values)\n",
        "            else:\n",
        "                return load_mock_data_for_testing()\n",
        "        \n",
        "        if not base_predictions:\n",
        "            logger.warning(\"‚ö†Ô∏è No predictions loaded, falling back to mock data\")\n",
        "            return load_mock_data_for_testing()\n",
        "        \n",
        "        logger.info(f\"üéØ Successfully loaded REAL predictions:\")\n",
        "        logger.info(f\"   Models: {len(model_names)}\")\n",
        "        logger.info(f\"   Samples: {true_values.shape[0]}\")\n",
        "        logger.info(f\"   Horizon: {true_values.shape[1]}\")\n",
        "        logger.info(f\"   Spatial dims: {true_values.shape[2]}√ó{true_values.shape[3]}\")\n",
        "        \n",
        "        return base_predictions, true_values, model_names\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error loading predictions: {e}\")\n",
        "        logger.warning(\"üîÑ Falling back to mock data\")\n",
        "        return load_mock_data_for_testing()\n",
        "\n",
        "def check_colab_compatibility():\n",
        "    \"\"\"Check if running in Google Colab and adjust paths accordingly\"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "        logger.info(\"üîó Running in Google Colab\")\n",
        "        \n",
        "        # Mount Google Drive if not already mounted\n",
        "        if not Path('/content/drive/MyDrive').exists():\n",
        "            logger.info(\"üìÅ Mounting Google Drive...\")\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "        \n",
        "        # üîß FIXED: Update paths for Colab with correct naming\n",
        "        global BASE_PATH, ADVANCED_SPATIAL_ROOT, META_MODELS_ROOT, STACKING_OUTPUT, CROSS_ATTENTION_OUTPUT\n",
        "        BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
        "        # Use 'advanced_spatial' (lowercase) to match advanced_spatial_models.ipynb\n",
        "        ADVANCED_SPATIAL_ROOT = BASE_PATH / 'models' / 'output' / 'advanced_spatial'\n",
        "        META_MODELS_ROOT = ADVANCED_SPATIAL_ROOT / 'meta_models'\n",
        "        STACKING_OUTPUT = META_MODELS_ROOT / 'stacking'\n",
        "        CROSS_ATTENTION_OUTPUT = META_MODELS_ROOT / 'cross_attention'\n",
        "        \n",
        "        logger.info(f\"üìÅ Updated paths for Colab:\")\n",
        "        logger.info(f\"   Base: {BASE_PATH}\")\n",
        "        logger.info(f\"   Advanced Spatial: {ADVANCED_SPATIAL_ROOT}\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except ImportError:\n",
        "        logger.info(\"üíª Running locally (not in Colab)\")\n",
        "        return False\n",
        "\n",
        "# Check Colab compatibility and adjust paths\n",
        "is_colab = check_colab_compatibility()\n",
        "\n",
        "# Load the pre-trained models (for fallback if needed)\n",
        "loaded_base_models = load_pretrained_base_models()\n",
        "\n",
        "# üöÄ Load REAL predictions instead of mock data\n",
        "base_predictions, true_values, model_names = load_real_predictions_from_manifests()\n",
        "\n",
        "# Extract specific models for cross-attention (GRU and LSTM)\n",
        "gru_models = [name for name in model_names if 'convgru_res' in name]\n",
        "lstm_models = [name for name in model_names if 'convlstm_att' in name]\n",
        "\n",
        "logger.info(f\"üéØ Models identified for Cross-Attention:\")\n",
        "logger.info(f\"   GRU models: {gru_models}\")\n",
        "logger.info(f\"   LSTM models: {lstm_models}\")\n",
        "\n",
        "# Prepare data splits\n",
        "n_samples = true_values.shape[0]\n",
        "train_size = int(0.8 * n_samples)\n",
        "train_indices = np.arange(train_size)\n",
        "val_indices = np.arange(train_size, n_samples)\n",
        "\n",
        "# Split base predictions\n",
        "train_base_predictions = {name: pred[train_indices] for name, pred in base_predictions.items()}\n",
        "val_base_predictions = {name: pred[val_indices] for name, pred in base_predictions.items()}\n",
        "train_targets = true_values[train_indices]\n",
        "val_targets = true_values[val_indices]\n",
        "\n",
        "logger.info(f\"üìä Data split completed:\")\n",
        "logger.info(f\"   Training samples: {len(train_indices)}\")\n",
        "logger.info(f\"   Validation samples: {len(val_indices)}\")\n",
        "logger.info(\"‚úÖ REAL data loading completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
