{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4e7925",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/main/models/base_models_STHyMOUNTAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3744f4",
   "metadata": {
    "id": "af3744f4"
   },
   "source": [
    "# 📘 Entrenamiento de Modelos Baseline para Predicción Espaciotemporal de Precipitación Mensual STHyMOUNTAIN\n",
    "\n",
    "Este notebook implementa modelos baseline para la predicción de precipitaciones usando datos espaciotemporales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994be36",
   "metadata": {},
   "source": [
    "## 🔍 Implementación de Modelos Avanzados y Técnicas de Validación\n",
    "\n",
    "Además de los modelos tabulares baseline, implementaremos:\n",
    "\n",
    "1. **Optimización avanzada con Optuna** para los modelos tabulares XGBoost y LightGBM\n",
    "2. **Validación robusta** mediante:\n",
    "   - Hold-Out Validation (ya implementada)\n",
    "   - Cross-Validation (k=5)\n",
    "   - Bootstrapping (100 muestras)\n",
    "3. **Modelos de Deep Learning** para capturar patrones espaciales y temporales:\n",
    "   - Redes CNN para patrones espaciales\n",
    "   - Redes ConvLSTM para patrones espaciotemporales\n",
    "\n",
    "El objetivo es proporcionar una evaluación completa de diferentes enfoques de modelado para la predicción de precipitación en regiones montañosas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Xjn1C7PKysIw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xjn1C7PKysIw",
    "outputId": "dd29e4f7-1612-4094-c1c6-06c9f06ccf11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06416284",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06416284",
    "outputId": "a8e4c864-34e9-41b2-d5c3-e6ccaaf3699e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ml_precipitation_prediction'...\n",
      "fatal: could not read Username for 'https://github.com': No such device or address\n",
      "[Errno 2] No such file or directory: 'ml_precipitation_prediction'\n",
      "/content\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (2025.1.2)\n",
      "Requirement already satisfied: netCDF4 in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.0.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from xarray) (24.2)\n",
      "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from xarray) (2.2.2)\n",
      "Requirement already satisfied: cftime in /usr/local/lib/python3.11/dist-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2025.1.31)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.14.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->xarray) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
      "Entorno configurado. Usando ruta base: /content/drive/MyDrive/ml_precipitation_prediction\n",
      "Directorio para salida de modelos creado: /content/drive/MyDrive/ml_precipitation_prediction/models/output\n"
     ]
    }
   ],
   "source": [
    "# Configuración del entorno (compatible con Colab y local)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detectar si estamos en Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Si estamos en Colab, clonar el repositorio\n",
    "    !git clone https://github.com/username/ml_precipitation_prediction.git\n",
    "    %cd ml_precipitation_prediction\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn\n",
    "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
    "else:\n",
    "    # Si estamos en local, usar la ruta actual\n",
    "    if '/models' in os.getcwd():\n",
    "        BASE_PATH = Path('..')\n",
    "    else:\n",
    "        BASE_PATH = Path('.')\n",
    "\n",
    "print(f\"Entorno configurado. Usando ruta base: {BASE_PATH}\")\n",
    "\n",
    "# Si BASE_PATH viene como string, lo convertimos\n",
    "BASE_PATH = Path(BASE_PATH)\n",
    "\n",
    "# Ahora puedes concatenar correctamente\n",
    "model_output_dir = BASE_PATH / 'models' / 'output'\n",
    "model_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directorio para salida de modelos creado: {model_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462e5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimización adaptativa de memoria RAM para Optuna (compatible con Colab y Local)\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "def get_available_memory():\n",
    "    \"\"\"Detecta la memoria RAM disponible en el sistema actual (Colab o local)\"\"\"\n",
    "    try:\n",
    "        # Obtener memoria disponible usando psutil\n",
    "        mem_info = psutil.virtual_memory()\n",
    "        available_mb = mem_info.available / (1024 * 1024)  # Convertir a MB\n",
    "        total_mb = mem_info.total / (1024 * 1024)  # Convertir a MB\n",
    "        \n",
    "        # Detectar si estamos en Colab\n",
    "        in_colab = 'google.colab' in sys.modules\n",
    "        if in_colab:\n",
    "            # En Colab, limitar el uso a un porcentaje conservador\n",
    "            print(\"Entorno detectado: Google Colab\")\n",
    "            max_usage_percent = 70  # Usar máximo 70% de la memoria disponible en Colab\n",
    "        else:\n",
    "            # En local, también ser conservador pero un poco menos\n",
    "            print(\"Entorno detectado: Local\")\n",
    "            max_usage_percent = 80  # Usar máximo 80% de la memoria disponible en local\n",
    "        \n",
    "        # Calcular memoria máxima a usar\n",
    "        max_memory_mb = available_mb * (max_usage_percent / 100)\n",
    "        \n",
    "        print(f\"Memoria total del sistema: {total_mb:.0f} MB\")\n",
    "        print(f\"Memoria disponible: {available_mb:.0f} MB\")\n",
    "        print(f\"Memoria máxima a utilizar: {max_memory_mb:.0f} MB ({max_usage_percent}% de la disponible)\")\n",
    "        \n",
    "        return max_memory_mb\n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener información de memoria: {e}\")\n",
    "        # Valor conservador por defecto\n",
    "        return 2000  # 2GB por defecto si no se puede detectar\n",
    "\n",
    "def calculate_optimal_batch_size(mem_per_sample_mb, max_memory_mb, min_batch=10):\n",
    "    \"\"\"Calcula el tamaño óptimo de lote basado en la memoria disponible\"\"\"\n",
    "    # Estimar cuánta memoria usa cada muestra (con margen de seguridad)\n",
    "    optimal_batch = int(max_memory_mb / (mem_per_sample_mb * 1.5))\n",
    "    return max(optimal_batch, min_batch)\n",
    "\n",
    "def configure_optuna_for_memory_efficiency(max_memory_mb, model_name):\n",
    "    \"\"\"Configura parámetros para Optuna basados en la memoria disponible\"\"\"\n",
    "    # Estimar número de trials y paralelismo basado en la memoria\n",
    "    if max_memory_mb < 1000:  # Menos de 1GB disponible\n",
    "        n_trials = 15  # Reducir número de trials\n",
    "        n_jobs = 1     # Sin paralelismo\n",
    "        use_sqlite = True  # Usar SQLite para ahorrar memoria\n",
    "    elif max_memory_mb < 4000:  # Menos de 4GB\n",
    "        n_trials = 25  # Número moderado de trials\n",
    "        n_jobs = min(2, os.cpu_count() or 2)  # Paralelismo limitado\n",
    "        use_sqlite = True  # Usar SQLite\n",
    "    else:  # 4GB o más\n",
    "        n_trials = 30  # Número normal de trials\n",
    "        n_jobs = min(3, os.cpu_count() or 2)  # Mejor paralelismo\n",
    "        use_sqlite = True  # Mantener SQLite para estabilidad\n",
    "    \n",
    "    # Definir configuración específica por modelo\n",
    "    model_config = {\n",
    "        'RandomForest': {\n",
    "            'n_estimators_max': min(500, int(50 + max_memory_mb / 100)),  # Adaptar a memoria\n",
    "            'max_depth_max': min(50, int(10 + max_memory_mb / 200))\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'tree_method': 'hist',  # Método eficiente en memoria\n",
    "            'max_depth_max': min(12, 3 + int(max_memory_mb / 1000))\n",
    "            'grow_policy': 'lossguide'  # Más eficiente en memoria\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'max_bin': min(255, 63 + int(max_memory_mb / 100)),  # Adaptar precisión\n",
    "            'num_leaves_max': min(150, 31 + int(max_memory_mb / 100))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Crear almacenamiento SQLite si es necesario\n",
    "    storage = None\n",
    "    if use_sqlite:\n",
    "        # Crear directorio para almacenamiento Optuna si no existe\n",
    "        os.makedirs(model_output_dir / 'optuna_storage', exist_ok=True)\n",
    "        storage = f\"sqlite:///{model_output_dir}/optuna_storage/optuna_{model_name}_{int(time.time())}.db\"\n",
    "    \n",
    "    # Devolver toda la configuración optimizada\n",
    "    return {\n",
    "        'n_trials': n_trials,\n",
    "        'n_jobs': n_jobs,\n",
    "        'storage': storage,\n",
    "        'model_config': model_config.get(model_name, {})\n",
    "    }\n",
    "\n",
    "def memory_efficient_objective_factory(model_name, X_train, y_train, max_memory_mb, model_config):\n",
    "    \"\"\"Crea una función objetivo para Optuna optimizada para memoria\"\"\"\n",
    "    # Configurar espacio de búsqueda adaptado a la memoria disponible\n",
    "    def objective(trial):\n",
    "        # Limpiar memoria antes de cada trial\n",
    "        gc.collect()\n",
    "        \n",
    "        # Configuración de parámetros específicos para cada modelo, adaptados a la memoria\n",
    "        if model_name == 'RandomForest':\n",
    "            n_estimators_max = model_config.get('n_estimators_max', 500)\n",
    "            max_depth_max = model_config.get('max_depth_max', 50)\n",
    "            \n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, n_estimators_max),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, max_depth_max),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'random_state': 42,\n",
    "                # Parámetros para optimizar memoria\n",
    "                'verbose': 0,\n",
    "                'n_jobs': 1 if max_memory_mb < 2000 else 2  # Limitar paralelismo según memoria\n",
    "            }\n",
    "            model = RandomForestRegressor(**params)\n",
    "        \n",
    "        elif model_name == 'XGBoost':\n",
    "            max_depth_max = model_config.get('max_depth_max', 12)\n",
    "            tree_method = model_config.get('tree_method', 'hist')\n",
    "            grow_policy = model_config.get('grow_policy', 'lossguide')\n",
    "            \n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, max_depth_max),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "                'random_state': 42,\n",
    "                # Optimización de memoria\n",
    "                'tree_method': tree_method,\n",
    "                'grow_policy': grow_policy,\n",
    "                'verbosity': 0,\n",
    "                'nthread': 1 if max_memory_mb < 2000 else 2  # Adaptar según memoria\n",
    "            }\n",
    "            model = XGBRegressor(**params)\n",
    "        \n",
    "        elif model_name == 'LightGBM':\n",
    "            num_leaves_max = model_config.get('num_leaves_max', 150)\n",
    "            max_bin = model_config.get('max_bin', 63)\n",
    "            \n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, num_leaves_max),\n",
    "                'max_depth': trial.suggest_int('max_depth', -1, 15),\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "                'random_state': 42,\n",
    "                # Parámetros para optimizar memoria\n",
    "                'verbose': -1,\n",
    "                'max_bin': max_bin,\n",
    "                'num_threads': 1 if max_memory_mb < 2000 else 2\n",
    "            }\n",
    "            model = LGBMRegressor(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Modelo no soportado: {model_name}\")\n",
    "        \n",
    "        # Evaluación con validación cruzada adaptada a memoria disponible\n",
    "        try:\n",
    "            # Reducir el número de folds si hay poca memoria\n",
    "            n_folds = 3 if max_memory_mb < 2000 else 5\n",
    "            from sklearn.model_selection import KFold\n",
    "            \n",
    "            kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "            scores = []\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X_train):\n",
    "                X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "                y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                \n",
    "                # Entrenar modelo\n",
    "                model.fit(X_fold_train, y_fold_train)\n",
    "                \n",
    "                # Evaluar y liberar memoria inmediatamente\n",
    "                y_pred = model.predict(X_fold_val)\n",
    "                rmse = np.sqrt(mean_squared_error(y_fold_val, y_pred))\n",
    "                scores.append(rmse)\n",
    "                \n",
    "                # Limpieza agresiva de memoria\n",
    "                del X_fold_train, X_fold_val, y_fold_train, y_fold_val, y_pred\n",
    "                gc.collect()\n",
    "            \n",
    "            # Liberar modelo también\n",
    "            del model\n",
    "            gc.collect()\n",
    "            \n",
    "            return np.mean(scores)\n",
    "        except Exception as e:\n",
    "            print(f\"Error en evaluación: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    return objective\n",
    "\n",
    "def run_memory_efficient_optimization(model_name='XGBoost'):\n",
    "    \"\"\"Ejecuta optimización con Optuna adaptada a la memoria disponible\"\"\"\n",
    "    print(f\"\\n🧠 Iniciando optimización adaptativa de memoria para {model_name}...\")\n",
    "    \n",
    "    # 1. Detectar memoria disponible\n",
    "    max_memory_mb = get_available_memory()\n",
    "    \n",
    "    # 2. Configurar Optuna según memoria disponible\n",
    "    optuna_config = configure_optuna_for_memory_efficiency(max_memory_mb, model_name)\n",
    "    \n",
    "    # 3. Mostrar configuración\n",
    "    print(f\"\\nConfiguración para {model_name}:\")\n",
    "    print(f\"- Trials: {optuna_config['n_trials']}\")\n",
    "    print(f\"- Paralelismo: {optuna_config['n_jobs']} jobs\")\n",
    "    print(f\"- Almacenamiento: {'SQLite' if optuna_config['storage'] else 'En memoria'}\")\n",
    "    print(f\"- Configuración específica: {optuna_config['model_config']}\")\n",
    "    \n",
    "    # 4. Crear estudio Optuna\n",
    "    from optuna.pruners import MedianPruner\n",
    "    pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        pruner=pruner,\n",
    "        storage=optuna_config['storage'],\n",
    "        study_name=f\"{model_name}_memory_optimized\",\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    \n",
    "    # 5. Crear función objetivo eficiente en memoria\n",
    "    objective = memory_efficient_objective_factory(\n",
    "        model_name, \n",
    "        X_train_scaled, \n",
    "        y_train, \n",
    "        max_memory_mb,\n",
    "        optuna_config['model_config']\n",
    "    )\n",
    "    \n",
    "    # 6. Callback para liberar memoria periódicamente\n",
    "    def gc_callback(study, trial):\n",
    "        if trial.number % 3 == 0:  # Cada 3 trials\n",
    "            gc.collect()\n",
    "    \n",
    "    # 7. Visualización inicial\n",
    "    color_map = {\n",
    "        'RandomForest': '#e6f3ff',  # Azul claro\n",
    "        'XGBoost': '#fff0e6',      # Naranja claro\n",
    "        'LightGBM': '#e6fff2'      # Verde claro\n",
    "    }\n",
    "    display(HTML(f'<div style=\"background-color:{color_map.get(model_name, \"#f5f5f5\")}; padding:10px; border-radius:5px;\">' +\n",
    "                f'<h3>🔍 Optimización adaptativa de memoria - {model_name}</h3>' +\n",
    "                f'<div>Memoria máxima a utilizar: {max_memory_mb:.0f} MB</div>' +\n",
    "                f'<div>Trials planeados: {optuna_config[\"n_trials\"]}</div>' +\n",
    "                f'</div>'))\n",
    "    \n",
    "    # 8. Iniciar optimización\n",
    "    callback = OptimizationProgressCallback(\n",
    "        optuna_config['n_trials'], f\"{model_name} (RAM optimizada)\"\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        objective, \n",
    "        n_trials=optuna_config['n_trials'],\n",
    "        n_jobs=optuna_config['n_jobs'],\n",
    "        gc_after_trial=True,\n",
    "        callbacks=[callback, gc_callback]\n",
    "    )\n",
    "    \n",
    "    # 9. Obtener mejores parámetros\n",
    "    best_params = study.best_params\n",
    "    best_value = study.best_value\n",
    "    \n",
    "    # 10. Visualizar resultados\n",
    "    display(HTML(f'<div style=\"background-color:{color_map.get(model_name, \"#f5f5f5\")}; padding:10px; border-radius:5px;\">' +\n",
    "                f'<h3>✅ Optimización completada - {model_name}</h3>' +\n",
    "                f'<div><b>Mejor RMSE:</b> {best_value:.4f}</div>' +\n",
    "                f'<div><b>Mejores parámetros:</b> {str(best_params)}</div>' +\n",
    "                f'</div>'))\n",
    "    \n",
    "    # 11. Entrenar modelo final con los mejores parámetros\n",
    "    print(\"\\nEntrenando modelo final con los mejores parámetros...\")\n",
    "    \n",
    "    if model_name == 'RandomForest':\n",
    "        best_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "    elif model_name == 'XGBoost':\n",
    "        best_model = XGBRegressor(**best_params, random_state=42)\n",
    "    elif model_name == 'LightGBM':\n",
    "        best_model = LGBMRegressor(**best_params, random_state=42)\n",
    "    \n",
    "    # 12. Evaluar modelo final\n",
    "    better_model, metrics = entrenar_y_evaluar_modelo(\n",
    "        best_model, f'{model_name}_RAM_Opt', X_train_scaled, y_train, X_test_scaled, y_test\n",
    "    )\n",
    "    \n",
    "    # 13. Guardar resultados\n",
    "    resultados_base[f'{model_name}_RAM_Opt'] = metrics\n",
    "    modelo_file = guardar_modelo(better_model, f'{model_name}_RAM_Opt')\n",
    "    modelos_guardados[f'{model_name}_RAM_Opt'] = modelo_file\n",
    "    \n",
    "    return best_params, better_model, metrics\n",
    "\n",
    "# Ejemplo de uso (descomenta para ejecutar):\n",
    "\"\"\"\n",
    "# Para ejecutar la optimización adaptativa solo de un modelo:\n",
    "xgb_params, xgb_model, xgb_metrics = run_memory_efficient_optimization('XGBoost')\n",
    "\n",
    "# Para ejecutar todos los modelos con optimización de memoria RAM:\n",
    "print(\"\\n🚀 Iniciando optimización adaptativa para todos los modelos...\")\n",
    "\n",
    "# Optimizar RandomForest con control de memoria adaptativo\n",
    "rf_params, rf_model, rf_metrics = run_memory_efficient_optimization('RandomForest')\n",
    "gc.collect()  # Liberar memoria entre modelos\n",
    "\n",
    "# Optimizar XGBoost con control de memoria adaptativo\n",
    "xgb_params, xgb_model, xgb_metrics = run_memory_efficient_optimization('XGBoost')\n",
    "gc.collect()  # Liberar memoria entre modelos\n",
    "\n",
    "# Optimizar LightGBM con control de memoria adaptativo\n",
    "lgbm_params, lgbm_model, lgbm_metrics = run_memory_efficient_optimization('LightGBM')\n",
    "gc.collect()  # Liberar memoria final\n",
    "\"\"\"\n",
    "\n",
    "# Ejecutar solo esta celda para iniciar la optimización adaptativa de memoria para XGBoost\n",
    "print(\"\\n🔍 Ejecutando optimización adaptativa de memoria RAM para XGBoost...\")\n",
    "xgb_params, xgb_model, xgb_metrics = run_memory_efficient_optimization('XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313434be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones adicionales para Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, save_model, load_model\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Conv2D, Conv3D, ConvLSTM2D, BatchNormalization, \n",
    "                                   MaxPooling2D, Flatten, Input, concatenate, Reshape, TimeDistributed)\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(\"TensorFlow versión:\", tf.__version__)\n",
    "\n",
    "# Configurar GPU si está disponible\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(f\"GPU disponible: {physical_devices}\")\n",
    "    # Permitir crecimiento de memoria según sea necesario\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "else:\n",
    "    print(\"No se detectó GPU. Usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fb555",
   "metadata": {
    "id": "e47fb555"
   },
   "outputs": [],
   "source": [
    "# 1. Importaciones necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import optuna\n",
    "import pickle\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importaciones para barras de progreso y mejora de visualización\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import time\n",
    "\n",
    "# Configurar visualización más atractiva\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26215d90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "26215d90",
    "outputId": "ccb06926-e151-453f-a306-999f15566bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando archivo en: /content/drive/MyDrive/ml_precipitation_prediction/data/output/complete_dataset_with_features.nc\n",
      "Intentando cargar el archivo: /content/drive/MyDrive/ml_precipitation_prediction/data/output/complete_dataset_with_features.nc\n",
      "Archivo cargado exitosamente con xarray\n",
      "\n",
      "Información del dataset:\n",
      "xarray.Dataset {\n",
      "dimensions:\n",
      "\ttime = 530 ;\n",
      "\tlatitude = 62 ;\n",
      "\tlongitude = 66 ;\n",
      "\n",
      "variables:\n",
      "\tdatetime64[ns] time(time) ;\n",
      "\tfloat32 latitude(latitude) ;\n",
      "\tfloat32 longitude(longitude) ;\n",
      "\tfloat32 total_precipitation(time, latitude, longitude) ;\n",
      "\tfloat32 max_daily_precipitation(time, latitude, longitude) ;\n",
      "\tfloat32 min_daily_precipitation(time, latitude, longitude) ;\n",
      "\tfloat32 daily_precipitation_std(time, latitude, longitude) ;\n",
      "\tfloat32 month_sin(time, latitude, longitude) ;\n",
      "\tfloat32 month_cos(time, latitude, longitude) ;\n",
      "\tfloat32 doy_sin(time, latitude, longitude) ;\n",
      "\tfloat32 doy_cos(time, latitude, longitude) ;\n",
      "\tfloat64 elevation(latitude, longitude) ;\n",
      "\tfloat32 slope(latitude, longitude) ;\n",
      "\tfloat32 aspect(latitude, longitude) ;\n",
      "\n",
      "// global attributes:\n",
      "\t:description = ST-HyMOUNTAIN-Net ready dataset with CHIRPS monthly precipitation and DEM variables ;\n",
      "\t:source = CHIRPS v2.0 & DEM Boyacá ;\n",
      "\t:created_at = 2025-04-27 19:02:24 ;\n",
      "}None\n",
      "\n",
      "Variables disponibles:\n",
      "- total_precipitation: (530, 62, 66)\n",
      "- max_daily_precipitation: (530, 62, 66)\n",
      "- min_daily_precipitation: (530, 62, 66)\n",
      "- daily_precipitation_std: (530, 62, 66)\n",
      "- month_sin: (530, 62, 66)\n",
      "- month_cos: (530, 62, 66)\n",
      "- doy_sin: (530, 62, 66)\n",
      "- doy_cos: (530, 62, 66)\n",
      "- elevation: (62, 66)\n",
      "- slope: (62, 66)\n",
      "- aspect: (62, 66)\n",
      "Dataset cargado con éxito. Dimensiones: (2168760, 14)\n",
      "\n",
      "Primeras filas del DataFrame:\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"    print(\\\"No se pudo cargar el dataset\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"time\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1981-01-01 00:00:00\",\n        \"max\": \"1981-01-01 00:00:00\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"1981-01-01 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latitude\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4.3249969482421875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"longitude\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -74.92500305175781\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_precipitation\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          40.750823974609375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max_daily_precipitation\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          21.819194793701172\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_daily_precipitation\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"daily_precipitation_std\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5.019045352935791\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month_sin\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month_cos\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.8660253882408142\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doy_sin\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.01720157451927662\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"doy_cos\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9998520612716675\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"elevation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 123.34058798530309,\n        \"min\": 248.7760453427361,\n        \"max\": 519.7501066909579,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          519.7501066909579\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"slope\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          89.86701965332031\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"aspect\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          73.48167419433594\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-15081ce3-b18d-4749-b97a-17789f6d32ca\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>total_precipitation</th>\n",
       "      <th>max_daily_precipitation</th>\n",
       "      <th>min_daily_precipitation</th>\n",
       "      <th>daily_precipitation_std</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>doy_sin</th>\n",
       "      <th>doy_cos</th>\n",
       "      <th>elevation</th>\n",
       "      <th>slope</th>\n",
       "      <th>aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.975006</td>\n",
       "      <td>47.381050</td>\n",
       "      <td>24.706928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.825776</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>493.784552</td>\n",
       "      <td>89.539551</td>\n",
       "      <td>102.044502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.925003</td>\n",
       "      <td>40.750824</td>\n",
       "      <td>21.819195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.019045</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>519.750107</td>\n",
       "      <td>89.867020</td>\n",
       "      <td>73.481674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.875008</td>\n",
       "      <td>46.338623</td>\n",
       "      <td>26.092327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.740223</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>248.776045</td>\n",
       "      <td>89.722221</td>\n",
       "      <td>65.916817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.825005</td>\n",
       "      <td>48.779938</td>\n",
       "      <td>29.421450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.611738</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>351.415728</td>\n",
       "      <td>86.986130</td>\n",
       "      <td>140.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>4.324997</td>\n",
       "      <td>-74.775002</td>\n",
       "      <td>38.932945</td>\n",
       "      <td>18.483061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.733574</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>278.261922</td>\n",
       "      <td>88.273293</td>\n",
       "      <td>18.439939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15081ce3-b18d-4749-b97a-17789f6d32ca')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-15081ce3-b18d-4749-b97a-17789f6d32ca button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-15081ce3-b18d-4749-b97a-17789f6d32ca');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-68be0e13-4bc8-4d50-8f9a-2810d0d15626\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-68be0e13-4bc8-4d50-8f9a-2810d0d15626')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-68be0e13-4bc8-4d50-8f9a-2810d0d15626 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "        time  latitude  longitude  total_precipitation  \\\n",
       "0 1981-01-01  4.324997 -74.975006            47.381050   \n",
       "1 1981-01-01  4.324997 -74.925003            40.750824   \n",
       "2 1981-01-01  4.324997 -74.875008            46.338623   \n",
       "3 1981-01-01  4.324997 -74.825005            48.779938   \n",
       "4 1981-01-01  4.324997 -74.775002            38.932945   \n",
       "\n",
       "   max_daily_precipitation  min_daily_precipitation  daily_precipitation_std  \\\n",
       "0                24.706928                      0.0                 5.825776   \n",
       "1                21.819195                      0.0                 5.019045   \n",
       "2                26.092327                      0.0                 5.740223   \n",
       "3                29.421450                      0.0                 5.611738   \n",
       "4                18.483061                      0.0                 3.733574   \n",
       "\n",
       "   month_sin  month_cos   doy_sin   doy_cos   elevation      slope      aspect  \n",
       "0        0.5   0.866025  0.017202  0.999852  493.784552  89.539551  102.044502  \n",
       "1        0.5   0.866025  0.017202  0.999852  519.750107  89.867020   73.481674  \n",
       "2        0.5   0.866025  0.017202  0.999852  248.776045  89.722221   65.916817  \n",
       "3        0.5   0.866025  0.017202  0.999852  351.415728  86.986130  140.916000  \n",
       "4        0.5   0.866025  0.017202  0.999852  278.261922  88.273293   18.439939  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Cargar el dataset NetCDF\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Carga un archivo NetCDF y lo convierte a pandas DataFrame\"\"\"\n",
    "    try:\n",
    "        # Cargar el archivo NetCDF con xarray\n",
    "        print(f\"Intentando cargar el archivo: {file_path}\")\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        print(\"Archivo cargado exitosamente con xarray\")\n",
    "\n",
    "        # Mostrar información del dataset cargado\n",
    "        print(\"\\nInformación del dataset:\")\n",
    "        print(ds.info())\n",
    "        print(\"\\nVariables disponibles:\")\n",
    "        for var_name in ds.data_vars:\n",
    "            print(f\"- {var_name}: {ds[var_name].shape}\")\n",
    "\n",
    "        # Convertir a DataFrame\n",
    "        df = ds.to_dataframe().reset_index()\n",
    "        return df, ds\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el archivo NetCDF: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Ruta al dataset\n",
    "data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features.nc'\n",
    "print(f\"Buscando archivo en: {data_file}\")\n",
    "\n",
    "# Cargar el dataset\n",
    "df, ds_original = load_dataset(data_file)\n",
    "\n",
    "# Verificar si se cargó correctamente\n",
    "if df is not None:\n",
    "    print(f\"Dataset cargado con éxito. Dimensiones: {df.shape}\")\n",
    "    print(\"\\nPrimeras filas del DataFrame:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"No se pudo cargar el dataset. Verificar la ruta y el formato del archivo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f0aebbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f0aebbc",
    "outputId": "48e82987-ff47-41e8-9f40-57e53cf90cf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna objetivo identificada: total_precipitation\n",
      "Filas antes de eliminar NaN: 2168760\n",
      "Filas después de eliminar NaN: 2168760\n",
      "\n",
      "Features seleccionadas (12):\n",
      "['latitude', 'longitude', 'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos', 'elevation', 'slope', 'aspect']\n",
      "\n",
      "Variable objetivo: total_precipitation\n"
     ]
    }
   ],
   "source": [
    "# 3. Preparación de los datos\n",
    "if df is not None:\n",
    "    # Identificar la columna objetivo (precipitación)\n",
    "    target_column = 'total_precipitation'  # Ajustar si tiene otro nombre en tu dataset\n",
    "\n",
    "    # Ver si existe 'precip_target' o usar 'total_precipitation'\n",
    "    if 'total_precipitation' in df.columns:\n",
    "        target_column = 'total_precipitation'\n",
    "\n",
    "    print(f\"Columna objetivo identificada: {target_column}\")\n",
    "\n",
    "    # Separar variables predictoras y variable objetivo\n",
    "    feature_cols = [col for col in df.columns if col != target_column and not pd.isna(df[col]).all()]\n",
    "\n",
    "    # Eliminar columnas no numéricas para los modelos (como fechas o coordenadas si no se usan como features)\n",
    "    non_feature_cols = ['time', 'spatial_ref']\n",
    "    feature_cols = [col for col in feature_cols if col not in non_feature_cols]\n",
    "\n",
    "    # Eliminar filas con valores NaN\n",
    "    print(f\"Filas antes de eliminar NaN: {df.shape[0]}\")\n",
    "    df_clean = df.dropna(subset=[target_column] + feature_cols)\n",
    "    print(f\"Filas después de eliminar NaN: {df_clean.shape[0]}\")\n",
    "\n",
    "    # Separar features y target\n",
    "    X = df_clean[feature_cols]\n",
    "    y = df_clean[target_column]\n",
    "\n",
    "    print(f\"\\nFeatures seleccionadas ({len(feature_cols)}):\\n{feature_cols}\")\n",
    "    print(f\"\\nVariable objetivo: {target_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da222af5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da222af5",
    "outputId": "579fbd57-a790-42dd-807a-e61fc1daacbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del conjunto de entrenamiento: (1735008, 12)\n",
      "Dimensiones del conjunto de prueba: (433752, 12)\n",
      "Escalador guardado en models/output/scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# 4. División del conjunto de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dimensiones del conjunto de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Dimensiones del conjunto de prueba: {X_test.shape}\")\n",
    "\n",
    "# 5. Estandarización de variables predictoras\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Guardar el scaler para uso futuro\n",
    "with open(model_output_dir / 'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Escalador guardado en models/output/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba053b",
   "metadata": {
    "id": "c5ba053b"
   },
   "outputs": [],
   "source": [
    "# 6. Funciones de evaluación y entrenamiento\n",
    "def evaluar_modelo(y_true, y_pred):\n",
    "    \"\"\"Evalúa el rendimiento de un modelo usando múltiples métricas\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "def entrenar_y_evaluar_modelo(modelo, nombre, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Entrena un modelo y evalúa su rendimiento con visualización del progreso\"\"\"\n",
    "    # Crear widget para mostrar información del proceso\n",
    "    display(HTML(f'<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\">' +\n",
    "                 f'<h3>🔄 Entrenando modelo: {nombre}</h3>' +\n",
    "                 f'<div id=\"status_{nombre}\">Estado: Iniciando entrenamiento...</div>' +\n",
    "                 f'</div>'))\n",
    "    \n",
    "    # Tiempo de inicio\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Entrenar el modelo con seguimiento visual según el tipo\n",
    "    if hasattr(modelo, 'fit_generator') or nombre in ['XGBoost', 'XGBoost_Optuna', 'LightGBM', 'LightGBM_Optuna']:\n",
    "        # Para modelos que soportan entrenamiento por lotes como XGBoost, LightGBM\n",
    "        print(f\"Entrenando {nombre} con visualización de progreso...\")\n",
    "        if hasattr(modelo, 'n_estimators'):\n",
    "            n_estimators = modelo.n_estimators\n",
    "            for i in tqdm(range(n_estimators), desc=f\"Entrenando {nombre}\"):\n",
    "                if i == 0:\n",
    "                    # Primera iteración, ajuste inicial\n",
    "                    if nombre.startswith('LightGBM'):\n",
    "                        # LightGBM tiene parámetro verbose\n",
    "                        temp_modelo = type(modelo)(n_estimators=1, **{k:v for k,v in modelo.get_params().items() \n",
    "                                                                 if k != 'n_estimators' and k != 'verbose'}, verbose=-1)\n",
    "                    else:\n",
    "                        temp_modelo = type(modelo)(n_estimators=1, **{k:v for k,v in modelo.get_params().items() \n",
    "                                                                if k != 'n_estimators'})\n",
    "                    temp_modelo.fit(X_train, y_train)\n",
    "                elif i == n_estimators - 1:\n",
    "                    # Última iteración, ajuste completo\n",
    "                    modelo.fit(X_train, y_train)\n",
    "                \n",
    "                # Actualizar progreso visual\n",
    "                if i % max(1, n_estimators // 10) == 0:\n",
    "                    clear_output(wait=True)\n",
    "                    display(HTML(f'<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\">' +\n",
    "                                f'<h3>🔄 Entrenando modelo: {nombre}</h3>' +\n",
    "                                f'<div id=\"status_{nombre}\">Estado: Progreso {i+1}/{n_estimators} estimadores ({((i+1)/n_estimators*100):.1f}%)</div>' +\n",
    "                                f'</div>'))\n",
    "                    time.sleep(0.1)  # Pequeña pausa para actualización visual\n",
    "        else:\n",
    "            # Si no tiene n_estimators, entrenamiento directo\n",
    "            modelo.fit(X_train, y_train)\n",
    "    else:\n",
    "        # Para modelos estándar como RandomForest\n",
    "        modelo.fit(X_train, y_train)\n",
    "    \n",
    "    # Tiempo de entrenamiento\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Visualizar tiempo de entrenamiento\n",
    "    display(HTML(f'<div style=\"background-color:#e6ffe6; padding:10px; border-radius:5px;\">' +\n",
    "                f'<h3>✅ Entrenamiento completado: {nombre}</h3>' +\n",
    "                f'<div>Tiempo de entrenamiento: {training_time:.2f} segundos</div>' +\n",
    "                f'</div>'))\n",
    "    \n",
    "    print(f\"Evaluando rendimiento de {nombre}...\")\n",
    "    predicciones = modelo.predict(X_test)\n",
    "    rmse, mae, r2 = evaluar_modelo(y_test, predicciones)\n",
    "    \n",
    "    # Visualizar métricas con estilo\n",
    "    display(HTML(f'<div style=\"background-color:#f5f5dc; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
    "                f'<h3>📊 Métricas para {nombre}</h3>' +\n",
    "                f'<table style=\"width:100%; text-align:left;\">' +\n",
    "                f'<tr><th>Métrica</th><th>Valor</th></tr>' +\n",
    "                f'<tr><td>RMSE</td><td>{rmse:.4f}</td></tr>' +\n",
    "                f'<tr><td>MAE</td><td>{mae:.4f}</td></tr>' +\n",
    "                f'<tr><td>R²</td><td>{r2:.4f}</td></tr>' +\n",
    "                f'</table></div>'))\n",
    "    \n",
    "    return modelo, (rmse, mae, r2)\n",
    "\n",
    "def guardar_modelo(modelo, nombre):\n",
    "    \"\"\"Guarda un modelo entrenado en disco\"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{nombre}_{timestamp}.pkl\"\n",
    "    with open(model_output_dir / filename, 'wb') as f:\n",
    "        pickle.dump(modelo, f)\n",
    "    \n",
    "    # Visualizar confirmación de guardado\n",
    "    display(HTML(f'<div style=\"background-color:#e6ffee; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
    "                f'<h3>💾 Modelo guardado</h3>' +\n",
    "                f'<div>Modelo <b>{nombre}</b> guardado como: {filename}</div>' +\n",
    "                f'</div>'))\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Entrenamiento de modelos base sin optimización\n",
    "\n",
    "# Inicializar diccionarios para almacenar resultados y modelos\n",
    "resultados_base = {}  # Para almacenar métricas (RMSE, MAE, R2)\n",
    "modelos_base = {}     # Para almacenar instancias de modelos\n",
    "modelos_guardados = {} # Para almacenar nombres de archivos guardados\n",
    "\n",
    "print(\"\\n🔍 Entrenando modelos baseline sin optimización de hiperparámetros...\")\n",
    "\n",
    "# 1. Modelo RandomForest básico\n",
    "print(\"\\nEntrenando RandomForest base...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model, rf_metrics = entrenar_y_evaluar_modelo(\n",
    "    rf_model, 'RandomForest', X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "resultados_base['RandomForest'] = rf_metrics\n",
    "modelos_base['RandomForest'] = rf_model\n",
    "modelo_file = guardar_modelo(rf_model, 'RandomForest')\n",
    "modelos_guardados['RandomForest'] = modelo_file\n",
    "\n",
    "# Visualizar importancia de características para RandomForest\n",
    "plt.figure(figsize=(12, 6))\n",
    "feat_importances = rf_model.feature_importances_\n",
    "indices = np.argsort(feat_importances)[::-1]\n",
    "plt.bar(range(len(indices)), feat_importances[indices], color='skyblue')\n",
    "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
    "plt.title('Importancia de Características - RandomForest')\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'randomforest_feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Modelo XGBoost básico\n",
    "print(\"\\nEntrenando XGBoost base...\")\n",
    "xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb_model, xgb_metrics = entrenar_y_evaluar_modelo(\n",
    "    xgb_model, 'XGBoost', X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "resultados_base['XGBoost'] = xgb_metrics\n",
    "modelos_base['XGBoost'] = xgb_model\n",
    "modelo_file = guardar_modelo(xgb_model, 'XGBoost')\n",
    "modelos_guardados['XGBoost'] = modelo_file\n",
    "\n",
    "# Visualizar importancia de características para XGBoost\n",
    "plt.figure(figsize=(12, 6))\n",
    "feat_importances = xgb_model.feature_importances_\n",
    "indices = np.argsort(feat_importances)[::-1]\n",
    "plt.bar(range(len(indices)), feat_importances[indices], color='coral')\n",
    "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
    "plt.title('Importancia de Características - XGBoost')\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'xgboost_feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. Modelo LightGBM básico\n",
    "print(\"\\nEntrenando LightGBM base...\")\n",
    "lgbm_model = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "lgbm_model, lgbm_metrics = entrenar_y_evaluar_modelo(\n",
    "    lgbm_model, 'LightGBM', X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "resultados_base['LightGBM'] = lgbm_metrics\n",
    "modelos_base['LightGBM'] = lgbm_model\n",
    "modelo_file = guardar_modelo(lgbm_model, 'LightGBM')\n",
    "modelos_guardados['LightGBM'] = modelo_file\n",
    "\n",
    "# Visualizar importancia de características para LightGBM\n",
    "plt.figure(figsize=(12, 6))\n",
    "feat_importances = lgbm_model.feature_importances_\n",
    "indices = np.argsort(feat_importances)[::-1]\n",
    "plt.bar(range(len(indices)), feat_importances[indices], color='lightgreen')\n",
    "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
    "plt.title('Importancia de Características - LightGBM')\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'lightgbm_feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# Comparar resultados de modelos base\n",
    "print(\"\\n🔍 Comparación de modelos base sin optimización:\")\n",
    "temp_df = pd.DataFrame(resultados_base, index=['RMSE', 'MAE', 'R2']).T\n",
    "print(\"\\nOrdenados por RMSE (menor es mejor):\")\n",
    "display(temp_df.sort_values('RMSE'))\n",
    "\n",
    "# Visualizar comparación de RMSE\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=temp_df.index, y=temp_df['RMSE'])\n",
    "plt.title('Comparación de RMSE - Modelos Base')\n",
    "plt.ylabel('RMSE (menor es mejor)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'baseline_rmse_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d90151",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c1d90151",
    "outputId": "3564d17f-5103-4615-e806-bbaa1457ef7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 01:29:40,112] A new study created in memory with name: no-name-f2097522-b917-45e3-a523-4d5b98a66044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando optimización de hiperparámetros para RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-28 01:58:10,879] Trial 0 finished with value: 36.8066452846521 and parameters: {'n_estimators': 343, 'max_depth': 34, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 0 with value: 36.8066452846521.\n",
      "[I 2025-04-28 02:18:38,335] Trial 1 finished with value: 41.63606143990411 and parameters: {'n_estimators': 304, 'max_depth': 12, 'min_samples_split': 4, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 0 with value: 36.8066452846521.\n",
      "[W 2025-04-28 02:18:39,457] Trial 2 failed with parameters: {'n_estimators': 371, 'max_depth': 49, 'min_samples_split': 7, 'min_samples_leaf': 10, 'max_features': 'auto'} because of the following error: ValueError('\\nAll the 5 fits failed.\\nIt is very likely that your model is misconfigured.\\nYou can try to debug the error by setting error_score=\\'raise\\'.\\n\\nBelow are more details about the failures:\\n--------------------------------------------------------------------------------\\n3 fits failed with the following error:\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\\n    estimator.fit(X_train, y_train, **fit_params)\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\\n    estimator._validate_params()\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\\n    validate_parameter_constraints(\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\\n    raise InvalidParameterError(\\nsklearn.utils._param_validation.InvalidParameterError: The \\'max_features\\' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {\\'log2\\', \\'sqrt\\'} or None. Got \\'auto\\' instead.\\n\\n--------------------------------------------------------------------------------\\n2 fits failed with the following error:\\nTraceback (most recent call last):\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\\n    estimator.fit(X_train, y_train, **fit_params)\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\\n    estimator._validate_params()\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\\n    validate_parameter_constraints(\\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\\n    raise InvalidParameterError(\\nsklearn.utils._param_validation.InvalidParameterError: The \\'max_features\\' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {\\'sqrt\\', \\'log2\\'} or None. Got \\'auto\\' instead.\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"<ipython-input-25-63084c155e09>\", line 13, in objective_rf\n",
      "    score = cross_val_score(model, X_train_scaled, y_train,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 684, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 431, in cross_validate\n",
      "    _warn_or_raise_about_fit_failures(results, error_score)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 517, in _warn_or_raise_about_fit_failures\n",
      "    raise ValueError(all_fits_failed_message)\n",
      "ValueError: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "[W 2025-04-28 02:18:39,463] Trial 2 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n    estimator._validate_params()\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n    estimator._validate_params()\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-63084c155e09>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iniciando optimización de hiperparámetros para RandomForest...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mstudy_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mstudy_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nMejores hiperparámetros encontrados:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-63084c155e09>\u001b[0m in \u001b[0;36mobjective_rf\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     11\u001b[0m     }\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     score = cross_val_score(model, X_train_scaled, y_train, \n\u001b[0m\u001b[1;32m     14\u001b[0m                           scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1)\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Devolvemos negativo porque optimizamos minimizando\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    685\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;31m# For callable scoring, the return type is only know after calling. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n3 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n    estimator._validate_params()\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n\n--------------------------------------------------------------------------------\n2 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n    estimator._validate_params()\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n"
     ]
    }
   ],
   "source": [
    "# 8. Optimización de hiperparámetros con Optuna (Refactorizado para los tres modelos)\n",
    "\n",
    "# Clase para mostrar el progreso de la optimización con Optuna\n",
    "class OptimizationProgressCallback:\n",
    "    def __init__(self, n_trials, desc=\"Optimización\"):\n",
    "        self.pbar = tqdm(total=n_trials, desc=desc)\n",
    "        self.best_value = float('inf')\n",
    "        self.n_trials = n_trials\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def __call__(self, study, trial):\n",
    "        # Actualizar la mejor métrica si hay mejora\n",
    "        if study.best_value < self.best_value:\n",
    "            self.best_value = study.best_value\n",
    "            best_params_str = \", \".join([f\"{k}={v}\" for k, v in study.best_params.items()])\n",
    "            elapsed = time.time() - self.start_time\n",
    "            minutes, seconds = divmod(elapsed, 60)\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f'<div style=\"background-color:#e6f3ff; padding:10px; border-radius:5px;\">' +\n",
    "                        f'<h3>🔍 Optimización de hiperparámetros en progreso</h3>' +\n",
    "                        f'<div><b>Mejor RMSE:</b> {self.best_value:.4f}</div>' +\n",
    "                        f'<div><b>Mejores parámetros:</b> {best_params_str}</div>' +\n",
    "                        f'<div><b>Prueba actual:</b> {trial.number+1}/{self.n_trials}</div>' +\n",
    "                        f'<div><b>Tiempo transcurrido:</b> {int(minutes)}m {int(seconds)}s</div>' +\n",
    "                        f'<div><b>Progreso:</b></div>' +\n",
    "                        f'</div>'))\n",
    "        \n",
    "        # Actualizar la barra de progreso\n",
    "        self.pbar.update(1)\n",
    "        self.pbar.set_postfix({\"mejor_rmse\": f\"{study.best_value:.4f}\"})\n",
    "        \n",
    "        # Si es la última iteración, cerramos la barra\n",
    "        if trial.number + 1 == self.n_trials:\n",
    "            self.pbar.close()\n",
    "\n",
    "def optimize_model_hyperparams(model_name, n_trials=30):\n",
    "    \"\"\"Función para optimizar hiperparámetros de diferentes modelos usando Optuna\"\"\"\n",
    "    # Definir espacio de búsqueda según el modelo\n",
    "    def objective(trial):\n",
    "        # Configuración de parámetros específicos para cada modelo\n",
    "        if model_name == 'RandomForest':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            model = RandomForestRegressor(**params)\n",
    "        \n",
    "        elif model_name == 'XGBoost':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            model = XGBRegressor(**params)\n",
    "        \n",
    "        elif model_name == 'LightGBM':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "                'max_depth': trial.suggest_int('max_depth', -1, 15),  # -1 significa sin restricción\n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n",
    "                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            model = LGBMRegressor(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Modelo no soportado: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            score = cross_val_score(model, X_train_scaled, y_train,\n",
    "                              scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1,\n",
    "                              error_score='raise')\n",
    "            return -np.mean(score)  # Devolvemos negativo porque optimizamos minimizando\n",
    "        except Exception as e:\n",
    "            print(f\"Error en prueba de hiperparámetros para {model_name}: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    # Crear y configurar estudio de Optuna\n",
    "    print(f\"\\nIniciando optimización de hiperparámetros para {model_name}...\")\n",
    "    \n",
    "    # Definir colores para cada modelo\n",
    "    color_map = {\n",
    "        'RandomForest': '#e6f3ff',  # Azul claro\n",
    "        'XGBoost': '#fff0e6',      # Naranja claro\n",
    "        'LightGBM': '#e6fff2'      # Verde claro\n",
    "    }\n",
    "    \n",
    "    # Visualización inicial\n",
    "    display(HTML(f'<div style=\"background-color:{color_map[model_name]}; padding:10px; border-radius:5px;\">' +\n",
    "                f'<h3>🔍 Iniciando optimización de hiperparámetros - {model_name}</h3>' +\n",
    "                f'<div>Pruebas totales: {n_trials}</div>' +\n",
    "                f'<div>Métrica objetivo: RMSE (menor es mejor)</div>' +\n",
    "                f'</div>'))\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    callback = OptimizationProgressCallback(n_trials, f\"{model_name} Optimization\")\n",
    "    exceptions = (ValueError,) if model_name == 'RandomForest' else None\n",
    "    study.optimize(objective, n_trials=n_trials, catch=exceptions, callbacks=[callback])\n",
    "    \n",
    "    # Visualizar resultados finales de optimización\n",
    "    best_rmse = study.best_value\n",
    "    best_params = study.best_params\n",
    "    best_params_str = \", \".join([f\"{k}={v}\" for k, v in best_params.items()])\n",
    "    \n",
    "    display(HTML(f'<div style=\"background-color:{color_map[model_name]}; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
    "                f'<h3>✅ Optimización completada - {model_name}</h3>' +\n",
    "                f'<div><b>Mejor RMSE:</b> {best_rmse:.4f}</div>' +\n",
    "                f'<div><b>Mejores parámetros:</b> {best_params_str}</div>' +\n",
    "                f'</div>'))\n",
    "    \n",
    "    # Crear y entrenar el mejor modelo con los parámetros optimizados\n",
    "    if model_name == 'RandomForest':\n",
    "        best_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "    elif model_name == 'XGBoost':\n",
    "        best_model = XGBRegressor(**best_params, random_state=42)\n",
    "    elif model_name == 'LightGBM':\n",
    "        best_model = LGBMRegressor(**best_params, random_state=42)\n",
    "    \n",
    "    # Entrenar y evaluar el modelo optimizado\n",
    "    mejor_modelo, metricas = entrenar_y_evaluar_modelo(\n",
    "        best_model, f'{model_name}_Optuna', X_train_scaled, y_train, X_test_scaled, y_test\n",
    "    )\n",
    "    \n",
    "    # Guardar los resultados y el modelo\n",
    "    resultados_base[f'{model_name}_Optuna'] = metricas\n",
    "    modelo_file = guardar_modelo(mejor_modelo, f'{model_name}_Optuna')\n",
    "    modelos_guardados[f'{model_name}_Optuna'] = modelo_file\n",
    "    \n",
    "    # Visualizar importancia de características\n",
    "    feature_importances = mejor_modelo.feature_importances_\n",
    "    indices = np.argsort(feature_importances)[::-1]\n",
    "    \n",
    "    # Definir colores para gráficas por modelo\n",
    "    plot_color_map = {\n",
    "        'RandomForest': 'skyblue',\n",
    "        'XGBoost': 'coral',\n",
    "        'LightGBM': 'lightgreen'\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(indices)), feature_importances[indices], color=plot_color_map[model_name])\n",
    "    plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
    "    plt.title(f'Importancia de Características - {model_name} Optimizado')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(model_output_dir / f'{model_name.lower()}_feature_importance.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return best_params, mejor_modelo, metricas\n",
    "\n",
    "# Optimizar cada uno de los modelos\n",
    "n_trials_per_model = 30  # Número de pruebas para cada modelo\n",
    "\n",
    "# Optimizar RandomForest\n",
    "print(\"Iniciando optimización de hiperparámetros para RandomForest...\")\n",
    "rf_best_params, mejor_rf, rf_metrics = optimize_model_hyperparams('RandomForest', n_trials_per_model)\n",
    "\n",
    "# Optimizar XGBoost\n",
    "print(\"Iniciando optimización de hiperparámetros para XGBoost...\")\n",
    "xgb_best_params, mejor_xgb, xgb_metrics = optimize_model_hyperparams('XGBoost', n_trials_per_model)\n",
    "\n",
    "# Optimizar LightGBM\n",
    "print(\"Iniciando optimización de hiperparámetros para LightGBM...\")\n",
    "lgbm_best_params, mejor_lgbm, lgbm_metrics = optimize_model_hyperparams('LightGBM', n_trials_per_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e1a844",
   "metadata": {
    "id": "a8e1a844"
   },
   "outputs": [],
   "source": [
    "# 9. Visualización de resultados\n",
    "resultados_df = pd.DataFrame(resultados_base, index=['RMSE', 'MAE', 'R2']).T\n",
    "\n",
    "# Visualizar resultados en tabla\n",
    "display(resultados_df)\n",
    "\n",
    "# Gráficas de comparación\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=resultados_df.index, y=resultados_df['RMSE'])\n",
    "plt.title('Comparación de RMSE entre modelos')\n",
    "plt.ylabel('RMSE (menor es mejor)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'rmse_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# Gráfica de R²\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=resultados_df.index, y=resultados_df['R2'])\n",
    "plt.title('Comparación de R² entre modelos')\n",
    "plt.ylabel('R² (mayor es mejor)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'r2_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b378c2",
   "metadata": {
    "id": "95b378c2"
   },
   "outputs": [],
   "source": [
    "# 10. Exportar resultados finales\n",
    "# Crear un dataset de xarray con resultados\n",
    "def create_results_dataset(resultados_df):\n",
    "    \"\"\"Crea un dataset de xarray con los resultados para exportar como NetCDF\"\"\"\n",
    "    modelo_names = list(resultados_df.index)\n",
    "    metrics = ['RMSE', 'MAE', 'R2']\n",
    "\n",
    "    # Crear arrays para cada métrica\n",
    "    rmse_values = resultados_df['RMSE'].values\n",
    "    mae_values = resultados_df['MAE'].values\n",
    "    r2_values = resultados_df['R2'].values\n",
    "\n",
    "    # Crear dataset\n",
    "    ds = xr.Dataset(\n",
    "        data_vars={\n",
    "            'RMSE': (['model'], rmse_values),\n",
    "            'MAE': (['model'], mae_values),\n",
    "            'R2': (['model'], r2_values)\n",
    "        },\n",
    "        coords={\n",
    "            'model': modelo_names,\n",
    "        },\n",
    "        attrs={\n",
    "            'description': 'Resultados de modelos de predicción de precipitación STHyMOUNTAIN',\n",
    "            'created': datetime.datetime.now().isoformat(),\n",
    "            'features_used': ', '.join(feature_cols)\n",
    "        }\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "# Crear dataset de resultados\n",
    "results_ds = create_results_dataset(resultados_df)\n",
    "\n",
    "# Guardar resultados como NetCDF\n",
    "results_file = model_output_dir / 'model_results.nc'\n",
    "results_ds.to_netcdf(results_file)\n",
    "print(f\"Resultados guardados como NetCDF en: {results_file}\")\n",
    "\n",
    "# También guardar como CSV para fácil visualización\n",
    "csv_file = model_output_dir / 'model_results.csv'\n",
    "resultados_df.to_csv(csv_file)\n",
    "print(f\"Resultados guardados como CSV en: {csv_file}\")\n",
    "\n",
    "# Crear un diccionario con información del modelo para uso futuro\n",
    "model_info = {\n",
    "    'date_trained': datetime.datetime.now().isoformat(),\n",
    "    'feature_columns': feature_cols,\n",
    "    'target_column': target_column,\n",
    "    'models_saved': modelos_guardados,\n",
    "    'results': resultados_df.to_dict(),\n",
    "    'best_model': resultados_df['RMSE'].idxmin(),\n",
    "    'scaler': 'scaler.pkl'\n",
    "}\n",
    "\n",
    "# Guardar información del modelo\n",
    "with open(model_output_dir / 'model_info.pkl', 'wb') as f:\n",
    "    pickle.dump(model_info, f)\n",
    "print(f\"Información del modelo guardada en: {model_output_dir / 'model_info.pkl'}\")\n",
    "\n",
    "print(\"\\n🔥 Entrenamiento completado con éxito! El mejor modelo es:\", resultados_df['RMSE'].idxmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Preparación de datos para modelos espaciales y espaciotemporales\n",
    "\n",
    "def reorganize_data_for_dl(df_clean, ds_original, target_column, feature_cols, time_steps=3):\n",
    "    \"\"\"\n",
    "    Reorganiza los datos del DataFrame en formato espacial o espaciotemporal para modelos CNN y ConvLSTM\n",
    "    \n",
    "    Args:\n",
    "        df_clean: DataFrame con datos limpios\n",
    "        ds_original: Dataset xarray original\n",
    "        target_column: Nombre de la columna objetivo\n",
    "        feature_cols: Lista de columnas de características\n",
    "        time_steps: Número de pasos temporales para secuencias (ConvLSTM)\n",
    "    \n",
    "    Returns:\n",
    "        Datos para CNN y ConvLSTM en formato adecuado\n",
    "    \"\"\"\n",
    "    print(\"Preparando datos para modelos espaciales y espaciotemporales...\")\n",
    "    \n",
    "    # 1. Extraer información espacial del dataset original\n",
    "    lats = ds_original.lat.values\n",
    "    lons = ds_original.lon.values\n",
    "    times = np.sort(df_clean['time'].unique())\n",
    "    \n",
    "    n_times = len(times)\n",
    "    n_lats = len(lats)\n",
    "    n_lons = len(lons)\n",
    "    n_features = len(feature_cols)\n",
    "    \n",
    "    print(f\"Dimensiones: tiempos={n_times}, lats={n_lats}, lons={n_lons}, features={n_features}\")\n",
    "    \n",
    "    # Para CNN necesitamos: [samples, lat, lon, features]\n",
    "    # Para ConvLSTM necesitamos: [samples, time_steps, lat, lon, features]\n",
    "    \n",
    "    # 2. Crear arrays vacíos para almacenar datos reestructurados\n",
    "    # Para CNN (formato espacial)\n",
    "    X_spatial = np.zeros((n_times, n_lats, n_lons, n_features))\n",
    "    y_spatial = np.zeros((n_times, n_lats, n_lons, 1))\n",
    "    \n",
    "    # 3. Llenar arrays para CNN\n",
    "    for i, t in enumerate(times):\n",
    "        # Filtrar datos para este tiempo\n",
    "        df_time = df_clean[df_clean['time'] == t]\n",
    "        \n",
    "        # Para cada lat/lon, extraer features y target\n",
    "        for lat_idx, lat in enumerate(lats):\n",
    "            for lon_idx, lon in enumerate(lons):\n",
    "                # Obtener el registro para estas coordenadas\n",
    "                df_point = df_time[(df_time['lat'] == lat) & (df_time['lon'] == lon)]\n",
    "                \n",
    "                if not df_point.empty:\n",
    "                    # Extraer características\n",
    "                    X_spatial[i, lat_idx, lon_idx, :] = df_point[feature_cols].values[0]\n",
    "                    # Extraer target\n",
    "                    y_spatial[i, lat_idx, lon_idx, 0] = df_point[target_column].values[0]\n",
    "                \n",
    "    print(f\"Datos espaciales para CNN preparados. Forma X: {X_spatial.shape}, y: {y_spatial.shape}\")\n",
    "    \n",
    "    # 4. Crear secuencias para ConvLSTM\n",
    "    # Necesitamos [samples, time_steps, lat, lon, features]\n",
    "    X_spatiotemporal = []\n",
    "    y_spatiotemporal = []\n",
    "    \n",
    "    # Solo procesamos si hay suficientes pasos de tiempo\n",
    "    if n_times > time_steps:\n",
    "        for i in range(n_times - time_steps):\n",
    "            # Secuencia de entrada: time_steps pasos consecutivos\n",
    "            X_seq = X_spatial[i:i+time_steps, :, :, :]\n",
    "            # Target: el valor del siguiente paso de tiempo\n",
    "            y_seq = y_spatial[i+time_steps, :, :, :]\n",
    "            \n",
    "            X_spatiotemporal.append(X_seq)\n",
    "            y_spatiotemporal.append(y_seq)\n",
    "        \n",
    "        # Convertir a arrays numpy\n",
    "        X_spatiotemporal = np.array(X_spatiotemporal)\n",
    "        y_spatiotemporal = np.array(y_spatiotemporal)\n",
    "        \n",
    "        print(f\"Datos espaciotemporales para ConvLSTM preparados. Forma X: {X_spatiotemporal.shape}, y: {y_spatiotemporal.shape}\")\n",
    "        \n",
    "        # División en entrenamiento/prueba para CNN\n",
    "        X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
    "            X_spatial, y_spatial, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # División en entrenamiento/prueba para ConvLSTM\n",
    "        X_train_convlstm, X_test_convlstm, y_train_convlstm, y_test_convlstm = train_test_split(\n",
    "            X_spatiotemporal, y_spatiotemporal, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Normalizar datos, solo las características\n",
    "        # Para CNN\n",
    "        for i in range(n_features):\n",
    "            # Calcular media y desviación estándar en datos de entrenamiento\n",
    "            feature_mean = np.mean(X_train_cnn[:, :, :, i])\n",
    "            feature_std = np.std(X_train_cnn[:, :, :, i])\n",
    "            \n",
    "            # Normalizar train y test\n",
    "            X_train_cnn[:, :, :, i] = (X_train_cnn[:, :, :, i] - feature_mean) / (feature_std + 1e-8)\n",
    "            X_test_cnn[:, :, :, i] = (X_test_cnn[:, :, :, i] - feature_mean) / (feature_std + 1e-8)\n",
    "        \n",
    "        # Para ConvLSTM (normalizar cada característica en cada paso de tiempo)\n",
    "        for i in range(n_features):\n",
    "            for t in range(time_steps):\n",
    "                # Calcular media y desviación estándar en datos de entrenamiento\n",
    "                feature_mean = np.mean(X_train_convlstm[:, t, :, :, i])\n",
    "                feature_std = np.std(X_train_convlstm[:, t, :, :, i])\n",
    "                \n",
    "                # Normalizar train y test\n",
    "                X_train_convlstm[:, t, :, :, i] = (X_train_convlstm[:, t, :, :, i] - feature_mean) / (feature_std + 1e-8)\n",
    "                X_test_convlstm[:, t, :, :, i] = (X_test_convlstm[:, t, :, :, i] - feature_mean) / (feature_std + 1e-8)\n",
    "        \n",
    "        return (\n",
    "            (X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn),\n",
    "            (X_train_convlstm, X_test_convlstm, y_train_convlstm, y_test_convlstm),\n",
    "            (lats, lons, times)\n",
    "        )\n",
    "    else:\n",
    "        print(\"No hay suficientes datos temporales para crear secuencias ConvLSTM.\")\n",
    "        # División en entrenamiento/prueba solo para CNN\n",
    "        X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
    "            X_spatial, y_spatial, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Normalizar datos CNN\n",
    "        for i in range(n_features):\n",
    "            feature_mean = np.mean(X_train_cnn[:, :, :, i])\n",
    "            feature_std = np.std(X_train_cnn[:, :, :, i])\n",
    "            \n",
    "            X_train_cnn[:, :, :, i] = (X_train_cnn[:, :, :, i] - feature_mean) / (feature_std + 1e-8)\n",
    "            X_test_cnn[:, :, :, i] = (X_test_cnn[:, :, :, i] - feature_mean) / (feature_std + 1e-8)\n",
    "        \n",
    "        return (\n",
    "            (X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn),\n",
    "            None,\n",
    "            (lats, lons, times)\n",
    "        )\n",
    "\n",
    "# Preparar datos para CNN y ConvLSTM\n",
    "try:\n",
    "    time_steps = 3  # Número de pasos temporales para secuencias\n",
    "    spatial_data, spatiotemporal_data, spatial_coords = reorganize_data_for_dl(\n",
    "        df_clean, ds_original, target_column, feature_cols, time_steps)\n",
    "    \n",
    "    # Desempaquetar datos espaciales (CNN)\n",
    "    X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = spatial_data\n",
    "    \n",
    "    # Verificar si hay datos espaciotemporales (ConvLSTM)\n",
    "    if spatiotemporal_data is not None:\n",
    "        X_train_convlstm, X_test_convlstm, y_train_convlstm, y_test_convlstm = spatiotemporal_data\n",
    "        print(\"Datos para CNN y ConvLSTM preparados exitosamente.\")\n",
    "    else:\n",
    "        print(\"Solo se prepararon datos para CNN. No hay suficientes datos temporales para ConvLSTM.\")\n",
    "    \n",
    "    # Recuperar coordenadas espaciales\n",
    "    lats, lons, times = spatial_coords\n",
    "except Exception as e:\n",
    "    print(f\"Error al preparar datos para modelos espaciales: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Implementación de modelo CNN para patrones espaciales\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    \"\"\"\n",
    "    Crea una arquitectura CNN para capturar patrones espaciales.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Dimensiones de los datos de entrada (lat, lon, features)\n",
    "    \n",
    "    Returns:\n",
    "        Modelo compilado de Keras\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Primera capa convolucional\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Segunda capa convolucional\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        # Tercera capa convolucional\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Flatten para conectar a capas densas\n",
    "        Flatten(),\n",
    "        \n",
    "        # Capas densas\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),  # Regularización para prevenir sobreajuste\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Capa de salida: Reshape para obtener predicciones espaciales\n",
    "        Dense(np.prod(input_shape[:2])),\n",
    "        Reshape(input_shape[:2] + (1,))  # Reshape a [lat, lon, 1]\n",
    "    ])\n",
    "    \n",
    "    # Compilar modelo con optimizador Adam\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',  # Error cuadrático medio para regresión\n",
    "        metrics=['mae']  # Error absoluto medio como métrica adicional\n",
    "    )\n",
    "    \n",
    "    print(f\"Modelo CNN creado con input_shape={input_shape}\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "try:\n",
    "    # Obtener dimensiones de entrada a partir de los datos de entrenamiento\n",
    "    input_shape_cnn = X_train_cnn.shape[1:]  # [lat, lon, features]\n",
    "    print(f\"Forma de datos de entrada CNN: {input_shape_cnn}\")\n",
    "    \n",
    "    # Crear y entrenar modelo CNN\n",
    "    model_cnn = create_cnn_model(input_shape_cnn)\n",
    "    \n",
    "    # Callbacks para entrenamiento\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ModelCheckpoint(filepath=str(model_output_dir / 'cnn_model.h5'), \n",
    "                       save_best_only=True, monitor='val_loss')\n",
    "    ]\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    print(\"\\nEntrenando modelo CNN...\")\n",
    "    history_cnn = model_cnn.fit(\n",
    "        X_train_cnn, \n",
    "        y_train_cnn,\n",
    "        batch_size=16,\n",
    "        epochs=50,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluar modelo en conjunto de prueba\n",
    "    print(\"\\nEvaluando modelo CNN en conjunto de prueba...\")\n",
    "    y_pred_cnn = model_cnn.predict(X_test_cnn)\n",
    "    \n",
    "    # Aplanar para evaluación\n",
    "    y_test_flat = y_test_cnn.reshape(-1)\n",
    "    y_pred_flat = y_pred_cnn.reshape(-1)\n",
    "    \n",
    "    # Eliminar valores NaN si existen\n",
    "    mask = ~np.isnan(y_test_flat) & ~np.isnan(y_pred_flat)\n",
    "    y_test_valid = y_test_flat[mask]\n",
    "    y_pred_valid = y_pred_flat[mask]\n",
    "    \n",
    "    # Calcular métricas\n",
    "    rmse_cnn = np.sqrt(mean_squared_error(y_test_valid, y_pred_valid))\n",
    "    mae_cnn = mean_absolute_error(y_test_valid, y_pred_valid)\n",
    "    r2_cnn = r2_score(y_test_valid, y_pred_valid)\n",
    "    \n",
    "    print(f\"CNN - RMSE: {rmse_cnn:.4f}, MAE: {mae_cnn:.4f}, R²: {r2_cnn:.4f}\")\n",
    "    \n",
    "    # Añadir resultados al DataFrame de resultados\n",
    "    resultados_base['CNN'] = (rmse_cnn, mae_cnn, r2_cnn)\n",
    "    \n",
    "    # Guardar modelo en formato H5\n",
    "    model_cnn_file = model_output_dir / 'cnn_model.h5'\n",
    "    model_cnn.save(model_cnn_file)\n",
    "    print(f\"Modelo CNN guardado en: {model_cnn_file}\")\n",
    "    \n",
    "    # Visualizar historia de entrenamiento\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Gráfica de pérdida\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_cnn.history['loss'], label='Train')\n",
    "    plt.plot(history_cnn.history['val_loss'], label='Validation')\n",
    "    plt.title('Pérdida CNN')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Gráfica de MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_cnn.history['mae'], label='Train')\n",
    "    plt.plot(history_cnn.history['val_mae'], label='Validation')\n",
    "    plt.title('MAE CNN')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(model_output_dir / 'cnn_training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Crear visualización espacial del error\n",
    "    # Elegir un ejemplo aleatorio del conjunto de prueba para visualización\n",
    "    sample_idx = np.random.randint(0, len(X_test_cnn))\n",
    "    sample_true = y_test_cnn[sample_idx, :, :, 0]\n",
    "    sample_pred = y_pred_cnn[sample_idx, :, :, 0]\n",
    "    sample_error = np.abs(sample_true - sample_pred)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Mapa de precipitación real\n",
    "    im0 = axes[0].imshow(sample_true, cmap='Blues')\n",
    "    axes[0].set_title('Precipitación Real')\n",
    "    plt.colorbar(im0, ax=axes[0], label='mm')\n",
    "    \n",
    "    # Mapa de precipitación predicha\n",
    "    im1 = axes[1].imshow(sample_pred, cmap='Blues')\n",
    "    axes[1].set_title('Precipitación Predicha (CNN)')\n",
    "    plt.colorbar(im1, ax=axes[1], label='mm')\n",
    "    \n",
    "    # Mapa de error\n",
    "    im2 = axes[2].imshow(sample_error, cmap='Reds')\n",
    "    axes[2].set_title('Error Absoluto')\n",
    "    plt.colorbar(im2, ax=axes[2], label='mm')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(model_output_dir / 'cnn_spatial_error.png')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error al implementar modelo CNN: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86974d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Implementación de modelo ConvLSTM para patrones espaciotemporales\n",
    "\n",
    "def create_convlstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Crea una arquitectura ConvLSTM para capturar patrones espaciotemporales.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Dimensiones de los datos de entrada (time_steps, lat, lon, features)\n",
    "    \n",
    "    Returns:\n",
    "        Modelo compilado de Keras\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Primera capa ConvLSTM\n",
    "        ConvLSTM2D(filters=32, kernel_size=(3, 3), padding='same',\n",
    "                   return_sequences=True, activation='tanh',\n",
    "                   recurrent_activation='hard_sigmoid',\n",
    "                   input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Segunda capa ConvLSTM\n",
    "        ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same',\n",
    "                   return_sequences=False, activation='tanh',\n",
    "                   recurrent_activation='hard_sigmoid'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Capas convolucionales para procesamiento final\n",
    "        Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Capa de salida para predicción\n",
    "        Conv2D(filters=1, kernel_size=(3, 3), padding='same', activation='linear')\n",
    "    ])\n",
    "    \n",
    "    # Compilar modelo con optimizador Adam\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',  # Error cuadrático medio para regresión\n",
    "        metrics=['mae']  # Error absoluto medio como métrica adicional\n",
    "    )\n",
    "    \n",
    "    print(f\"Modelo ConvLSTM creado con input_shape={input_shape}\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Creamos una clase para visualizar el progreso de entrenamiento con tqdm\n",
    "class TqdmCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, epochs, metrics=['loss', 'val_loss']):\n",
    "        self.epochs = epochs\n",
    "        self.metrics = metrics\n",
    "        self.tqdm_progress = None\n",
    "        self.epoch_count = 0\n",
    "        self.training_start = None\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.training_start = time.time()\n",
    "        display(HTML(f'<div style=\"background-color:#e6f2ff; padding:10px; border-radius:5px;\">' +\n",
    "                     f'<h3>🧠 Iniciando entrenamiento de ConvLSTM</h3>' +\n",
    "                     f'<div>Total épocas: {self.epochs}</div>' +\n",
    "                     f'<div>Métricas monitorizadas: {\", \".join(self.metrics)}</div>' +\n",
    "                     f'</div>'))\n",
    "        self.tqdm_progress = tqdm(total=self.epochs, desc=\"Entrenando ConvLSTM\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch_count += 1\n",
    "        \n",
    "        # Recoger métricas para mostrar\n",
    "        metrics_str = \", \".join([f\"{m}: {logs.get(m, 0):.4f}\" for m in self.metrics if m in logs])\n",
    "        self.tqdm_progress.set_postfix_str(metrics_str)\n",
    "        self.tqdm_progress.update(1)\n",
    "        \n",
    "        # Cada 5 épocas (o en la última), mostramos un resumen más completo\n",
    "        if epoch % 5 == 0 or epoch == self.epochs - 1:\n",
    "            elapsed = time.time() - self.training_start\n",
    "            minutes, seconds = divmod(elapsed, 60)\n",
    "            hours, minutes = divmod(minutes, 60)\n",
    "            \n",
    "            # Crear un resumen de progreso bonito\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f'<div style=\"background-color:#e6f2ff; padding:10px; border-radius:5px;\">' +\n",
    "                         f'<h3>🧠 Entrenando ConvLSTM - Progreso</h3>' +\n",
    "                         f'<div><b>Época:</b> {epoch+1}/{self.epochs} ({((epoch+1)/self.epochs*100):.1f}%)</div>' +\n",
    "                         f'<div><b>Tiempo transcurrido:</b> {int(hours)}h {int(minutes)}m {int(seconds)}s</div>' +\n",
    "                         f'<div><b>Métricas actuales:</b> {metrics_str}</div>' +\n",
    "                         f'<div><b>Mejor val_loss:</b> {min([logs.get(\"val_loss\", float(\"inf\"))] + [logs.get(\"val_loss\", float(\"inf\")) for logs in self.model.history.history.get(\"val_loss\", []) or []]):.4f}</div>' +\n",
    "                         f'</div>'))\n",
    "            self.tqdm_progress = tqdm(total=self.epochs, initial=self.epoch_count, desc=\"Entrenando ConvLSTM\")\n",
    "            \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.tqdm_progress.close()\n",
    "        elapsed = time.time() - self.training_start\n",
    "        minutes, seconds = divmod(elapsed, 60)\n",
    "        hours, minutes = divmod(minutes, 60)\n",
    "        \n",
    "        display(HTML(f'<div style=\"background-color:#e6ffe6; padding:10px; border-radius:5px;\">' +\n",
    "                     f'<h3>✅ Entrenamiento de ConvLSTM completado</h3>' +\n",
    "                     f'<div><b>Tiempo total:</b> {int(hours)}h {int(minutes)}m {int(seconds)}s</div>' +\n",
    "                     f'</div>'))\n",
    "\n",
    "try:\n",
    "    # Verificar si tenemos datos espaciotemporales para ConvLSTM\n",
    "    if 'spatiotemporal_data' in locals() and spatiotemporal_data is not None:\n",
    "        # Crear y entrenar modelo ConvLSTM\n",
    "        input_shape_convlstm = X_train_convlstm.shape[1:]  # [time_steps, lat, lon, features]\n",
    "        print(f\"Forma de datos de entrada ConvLSTM: {input_shape_convlstm}\")\n",
    "        \n",
    "        # Crear modelo\n",
    "        model_convlstm = create_convlstm_model(input_shape_convlstm)\n",
    "        \n",
    "        # Callbacks para entrenamiento\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            filepath=str(model_output_dir / 'convlstm_model.h5'),\n",
    "            save_best_only=True,\n",
    "            monitor='val_loss'\n",
    "        )\n",
    "        tqdm_callback = TqdmCallback(epochs=50, metrics=['loss', 'val_loss', 'mae', 'val_mae'])\n",
    "        \n",
    "        # Entrenar modelo con barra de progreso personalizada\n",
    "        print(\"\\nEntrenando modelo ConvLSTM...\")\n",
    "        history_convlstm = model_convlstm.fit(\n",
    "            X_train_convlstm,\n",
    "            y_train_convlstm,\n",
    "            batch_size=16,\n",
    "            epochs=50,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stopping, model_checkpoint, tqdm_callback],\n",
    "            verbose=0  # No mostrar la barra de progreso estándar, usamos nuestra personalizada\n",
    "        )\n",
    "        \n",
    "        # Evaluar modelo en conjunto de prueba\n",
    "        print(\"\\nEvaluando modelo ConvLSTM en conjunto de prueba...\")\n",
    "        y_pred_convlstm = model_convlstm.predict(X_test_convlstm)\n",
    "        \n",
    "        # Aplanar para evaluación\n",
    "        y_test_flat_convlstm = y_test_convlstm.reshape(-1)\n",
    "        y_pred_flat_convlstm = y_pred_convlstm.reshape(-1)\n",
    "        \n",
    "        # Eliminar valores NaN si existen\n",
    "        mask_convlstm = ~np.isnan(y_test_flat_convlstm) & ~np.isnan(y_pred_flat_convlstm)\n",
    "        y_test_valid_convlstm = y_test_flat_convlstm[mask_convlstm]\n",
    "        y_pred_valid_convlstm = y_pred_flat_convlstm[mask_convlstm]\n",
    "        \n",
    "        # Calcular métricas\n",
    "        rmse_convlstm = np.sqrt(mean_squared_error(y_test_valid_convlstm, y_pred_valid_convlstm))\n",
    "        mae_convlstm = mean_absolute_error(y_test_valid_convlstm, y_pred_valid_convlstm)\n",
    "        r2_convlstm = r2_score(y_test_valid_convlstm, y_pred_valid_convlstm)\n",
    "        \n",
    "        print(f\"ConvLSTM - RMSE: {rmse_convlstm:.4f}, MAE: {mae_convlstm:.4f}, R²: {r2_convlstm:.4f}\")\n",
    "        \n",
    "        # Visualizar resultados con estilo\n",
    "        display(HTML(f'<div style=\"background-color:#f5f5dc; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
    "                    f'<h3>📊 Métricas para ConvLSTM</h3>' +\n",
    "                    f'<table style=\"width:100%; text-align:left;\">' +\n",
    "                    f'<tr><th>Métrica</th><th>Valor</th></tr>' +\n",
    "                    f'<tr><td>RMSE</td><td>{rmse_convlstm:.4f}</td></tr>' +\n",
    "                    f'<tr><td>MAE</td><td>{mae_convlstm:.4f}</td></tr>' +\n",
    "                    f'<tr><td>R²</td><td>{r2_convlstm:.4f}</td></tr>' +\n",
    "                    f'</table></div>'))\n",
    "        \n",
    "        # Añadir resultados al DataFrame de resultados\n",
    "        resultados_base['ConvLSTM'] = (rmse_convlstm, mae_convlstm, r2_convlstm)\n",
    "        \n",
    "        # Guardar modelo en formato H5\n",
    "        model_convlstm_file = model_output_dir / 'convlstm_model.h5'\n",
    "        model_convlstm.save(model_convlstm_file)\n",
    "        print(f\"Modelo ConvLSTM guardado en: {model_convlstm_file}\")\n",
    "        \n",
    "        # Visualizar historia de entrenamiento\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Gráfica de pérdida\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history_convlstm.history['loss'], label='Train', linewidth=2)\n",
    "        plt.plot(history_convlstm.history['val_loss'], label='Validation', linewidth=2, linestyle='--')\n",
    "        plt.title('Pérdida ConvLSTM', fontsize=14)\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        \n",
    "        # Gráfica de MAE\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history_convlstm.history['mae'], label='Train', linewidth=2)\n",
    "        plt.plot(history_convlstm.history['val_mae'], label='Validation', linewidth=2, linestyle='--')\n",
    "        plt.title('MAE ConvLSTM', fontsize=14)\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('MAE', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(model_output_dir / 'convlstm_training_history.png')\n",
    "        plt.show()\n",
    "        \n",
    "        # Visualizar predicciones vs valores reales\n",
    "        # Elegir un ejemplo aleatorio del conjunto de prueba para visualización\n",
    "        sample_idx_convlstm = np.random.randint(0, len(X_test_convlstm))\n",
    "        sample_true_convlstm = y_test_convlstm[sample_idx_convlstm, :, :, 0]\n",
    "        sample_pred_convlstm = y_pred_convlstm[sample_idx_convlstm, :, :, 0]\n",
    "        sample_error_convlstm = np.abs(sample_true_convlstm - sample_pred_convlstm)\n",
    "        \n",
    "        # Crear gráficas con mejor aspecto visual\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # Mapa de precipitación real\n",
    "        im0 = axes[0].imshow(sample_true_convlstm, cmap='Blues', interpolation='nearest')\n",
    "        axes[0].set_title('Precipitación Real', fontsize=14)\n",
    "        plt.colorbar(im0, ax=axes[0], label='mm')\n",
    "        axes[0].grid(False)\n",
    "        \n",
    "        # Mapa de precipitación predicha\n",
    "        im1 = axes[1].imshow(sample_pred_convlstm, cmap='Blues', interpolation='nearest')\n",
    "        axes[1].set_title('Predicción ConvLSTM', fontsize=14)\n",
    "        plt.colorbar(im1, ax=axes[1], label='mm')\n",
    "        axes[1].grid(False)\n",
    "        \n",
    "        # Mapa de error\n",
    "        im2 = axes[2].imshow(sample_error_convlstm, cmap='Reds', interpolation='nearest')\n",
    "        axes[2].set_title('Error Absoluto', fontsize=14)\n",
    "        plt.colorbar(im2, ax=axes[2], label='mm')\n",
    "        axes[2].grid(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(model_output_dir / 'convlstm_predictions.png')\n",
    "        plt.show()\n",
    "        \n",
    "        # Resumen final de resultados\n",
    "        print(\"\\n🌟 Modelos entrenados y evaluados:\")\n",
    "        display(HTML(f'<div style=\"background-color:#f0f8ff; padding:15px; border-radius:8px; margin-top:20px;\">' +\n",
    "                    f'<h2>🏆 Resumen de Resultados</h2>' +\n",
    "                    f'<p>Se han entrenado tanto modelos de machine learning (RandomForest, XGBoost, LightGBM) ' +\n",
    "                    f'como modelos de deep learning (CNN, ConvLSTM).</p>' +\n",
    "                    f'<h3>Ranking según RMSE (menor es mejor):</h3>' +\n",
    "                    f'</div>'))\n",
    "        \n",
    "        # Ranking de modelos\n",
    "        resultados_df_final = pd.DataFrame(resultados_base, index=['RMSE', 'MAE', 'R2']).T\n",
    "        resultados_ordenados = resultados_df_final.sort_values('RMSE')\n",
    "        display(resultados_ordenados)\n",
    "        \n",
    "        # Gráfica de resumen\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.barplot(x=resultados_ordenados.index, y=resultados_ordenados['RMSE'], palette='viridis')\n",
    "        plt.title('RMSE por Modelo (menor es mejor)', fontsize=14)\n",
    "        plt.ylabel('RMSE (mm)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.grid(True, axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.barplot(x=resultados_ordenados.index, y=resultados_ordenados['R2'], palette='plasma')\n",
    "        plt.title('R² por Modelo (mayor es mejor)', fontsize=14)\n",
    "        plt.ylabel('Coeficiente R²', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "        plt.grid(True, axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(model_output_dir / 'modelos_ranking_final.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No se pudieron preparar los datos espaciotemporales para el modelo ConvLSTM.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al implementar modelo ConvLSTM: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed0729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Validación cruzada (Cross-Validation) para evaluar modelos\n",
    "\n",
    "def evaluate_model_with_cv(modelo, nombre, X, y, n_splits=5):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo usando validación cruzada (k-fold)\n",
    "    \n",
    "    Args:\n",
    "        modelo: Modelo a evaluar\n",
    "        nombre: Nombre del modelo\n",
    "        X: Características\n",
    "        y: Variable objetivo\n",
    "        n_splits: Número de divisiones (folds)\n",
    "        \n",
    "    Returns:\n",
    "        Promedio de métricas y lista de métricas por fold\n",
    "    \"\"\"\n",
    "    print(f\"\\nRealizando validación cruzada para {nombre} con {n_splits} folds...\")\n",
    "    \n",
    "    # Crear instancia de KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Listas para almacenar métricas de cada fold\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    r2_scores = []\n",
    "    \n",
    "    # Iterar sobre cada fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"Fold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        # Dividir datos para este fold\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        modelo.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Predecir\n",
    "        y_pred_fold = modelo.predict(X_val_fold)\n",
    "        \n",
    "        # Calcular métricas\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred_fold))\n",
    "        mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "        r2 = r2_score(y_val_fold, y_pred_fold)\n",
    "        \n",
    "        # Almacenar métricas\n",
    "        rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        r2_scores.append(r2)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "    \n",
    "    # Calcular promedios\n",
    "    mean_rmse = np.mean(rmse_scores)\n",
    "    mean_mae = np.mean(mae_scores)\n",
    "    mean_r2 = np.mean(r2_scores)\n",
    "    \n",
    "    # Calcular desviaciones estándar\n",
    "    std_rmse = np.std(rmse_scores)\n",
    "    std_mae = np.std(mae_scores)\n",
    "    std_r2 = np.std(r2_scores)\n",
    "    \n",
    "    print(f\"\\nResumen de validación cruzada para {nombre}:\")\n",
    "    print(f\"RMSE: {mean_rmse:.4f} ± {std_rmse:.4f}\")\n",
    "    print(f\"MAE: {mean_mae:.4f} ± {std_mae:.4f}\")\n",
    "    print(f\"R²: {mean_r2:.4f} ± {std_r2:.4f}\")\n",
    "    \n",
    "    # Crear diccionario con resultados\n",
    "    cv_results = {\n",
    "        'mean_rmse': mean_rmse,\n",
    "        'mean_mae': mean_mae,\n",
    "        'mean_r2': mean_r2,\n",
    "        'std_rmse': std_rmse,\n",
    "        'std_mae': std_mae,\n",
    "        'std_r2': std_r2,\n",
    "        'rmse_scores': rmse_scores,\n",
    "        'mae_scores': mae_scores,\n",
    "        'r2_scores': r2_scores\n",
    "    }\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "# Aplicar validación cruzada a los modelos tabulares\n",
    "print(\"\\n🔄 Ejecutando validación cruzada para modelos tabulares...\")\n",
    "cv_results = {}\n",
    "\n",
    "# Lista de modelos a evaluar con validación cruzada\n",
    "models_for_cv = {\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Solo tomamos el mejor modelo de cada categoría para CV\n",
    "if 'RandomForest_Optuna' in resultados_base:\n",
    "    models_for_cv['RandomForest_Optuna'] = RandomForestRegressor(**rf_best_params, random_state=42)\n",
    "if 'XGBoost_Optuna' in resultados_base:\n",
    "    models_for_cv['XGBoost_Optuna'] = XGBRegressor(**xgb_best_params, random_state=42)\n",
    "if 'LightGBM_Optuna' in resultados_base:\n",
    "    models_for_cv['LightGBM_Optuna'] = LGBMRegressor(**lgbm_best_params, random_state=42)\n",
    "\n",
    "# Ejecutar validación cruzada para cada modelo\n",
    "for nombre, modelo in models_for_cv.items():\n",
    "    cv_results[nombre] = evaluate_model_with_cv(modelo, nombre, X_train_scaled, y_train, n_splits=5)\n",
    "\n",
    "# Visualizar resultados de validación cruzada\n",
    "plt.figure(figsize=(12, 6))\n",
    "modelo_names = list(cv_results.keys())\n",
    "mean_rmses = [cv_results[model]['mean_rmse'] for model in modelo_names]\n",
    "std_rmses = [cv_results[model]['std_rmse'] for model in modelo_names]\n",
    "\n",
    "# Crear barplot con barras de error\n",
    "bars = plt.bar(modelo_names, mean_rmses, yerr=std_rmses, capsize=5, color='skyblue', alpha=0.7)\n",
    "plt.title('Comparación de RMSE con Validación Cruzada (5-fold)')\n",
    "plt.ylabel('RMSE (menor es mejor)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for bar, rmse, std in zip(bars, mean_rmses, std_rmses):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + std + 0.05,\n",
    "             f'{rmse:.3f}±{std:.3f}', ha='center', va='bottom', rotation=0, size=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(model_output_dir / 'cross_validation_rmse.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c517aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Bootstrapping para evaluar intervalos de confianza\n",
    "\n",
    "def bootstrap_evaluate_model(model, X, y, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Evalúa el modelo usando bootstrapping para obtener intervalos de confianza\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado a evaluar\n",
    "        X: Características\n",
    "        y: Variable objetivo\n",
    "        n_iterations: Número de iteraciones de bootstrap\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con métricas e intervalos de confianza\n",
    "    \"\"\"\n",
    "    print(f\"Realizando bootstrapping con {n_iterations} iteraciones...\")\n",
    "    \n",
    "    # Convertir a numpy arrays si no lo son ya\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = X.values\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = y.values\n",
    "        \n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Listas para almacenar métricas de cada iteración\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    r2_scores = []\n",
    "    \n",
    "    # Realizar iteraciones de bootstrap\n",
    "    for i in range(n_iterations):\n",
    "        if i % 20 == 0:  # Solo imprimir cada 20 iteraciones para no saturar la salida\n",
    "            print(f\"Iteración bootstrap {i+1}/{n_iterations}\")\n",
    "            \n",
    "        # Muestreo con reemplazo\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        \n",
    "        # Crear muestra bootstrap\n",
    "        X_boot = X[indices, :]\n",
    "        y_boot = y[indices]\n",
    "        \n",
    "        # Predecir\n",
    "        y_pred = model.predict(X_boot)\n",
    "        \n",
    "        # Calcular métricas\n",
    "        rmse = np.sqrt(mean_squared_error(y_boot, y_pred))\n",
    "        mae = mean_absolute_error(y_boot, y_pred)\n",
    "        r2 = r2_score(y_boot, y_pred)\n",
    "        \n",
    "        # Almacenar métricas\n",
    "        rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        r2_scores.append(r2)\n",
    "    \n",
    "    # Calcular estadísticas de bootstrapping\n",
    "    # Mediana como estimación central y percentiles 2.5 y 97.5 para intervalo de confianza del 95%\n",
    "    rmse_median = np.median(rmse_scores)\n",
    "    rmse_lower = np.percentile(rmse_scores, 2.5)\n",
    "    rmse_upper = np.percentile(rmse_scores, 97.5)\n",
    "    \n",
    "    mae_median = np.median(mae_scores)\n",
    "    mae_lower = np.percentile(mae_scores, 2.5)\n",
    "    mae_upper = np.percentile(mae_scores, 97.5)\n",
    "    \n",
    "    r2_median = np.median(r2_scores)\n",
    "    r2_lower = np.percentile(r2_scores, 2.5)\n",
    "    r2_upper = np.percentile(r2_scores, 97.5)\n",
    "    \n",
    "    print(\"\\nResultados de bootstrapping:\")\n",
    "    print(f\"RMSE: {rmse_median:.4f} (IC 95%: [{rmse_lower:.4f}, {rmse_upper:.4f}])\")\n",
    "    print(f\"MAE: {mae_median:.4f} (IC 95%: [{mae_lower:.4f}, {mae_upper:.4f}])\")\n",
    "    print(f\"R²: {r2_median:.4f} (IC 95%: [{r2_lower:.4f}, {r2_upper:.4f}])\")\n",
    "    \n",
    "    # Crear diccionario con resultados\n",
    "    bootstrap_results = {\n",
    "        'rmse_median': rmse_median,\n",
    "        'rmse_ci': (rmse_lower, rmse_upper),\n",
    "        'mae_median': mae_median,\n",
    "        'mae_ci': (mae_lower, mae_upper),\n",
    "        'r2_median': r2_median,\n",
    "        'r2_ci': (r2_lower, r2_upper),\n",
    "        'rmse_scores': rmse_scores,\n",
    "        'mae_scores': mae_scores,\n",
    "        'r2_scores': r2_scores\n",
    "    }\n",
    "    \n",
    "    return bootstrap_results\n",
    "\n",
    "# Realizar bootstrapping para el mejor modelo basado en RMSE\n",
    "print(\"\\n🔄 Ejecutando evaluación por bootstrapping para el mejor modelo...\")\n",
    "\n",
    "# Encontrar el mejor modelo basado en RMSE\n",
    "best_model_name = min(resultados_base.items(), key=lambda x: x[1][0])[0]\n",
    "print(f\"Mejor modelo según RMSE: {best_model_name}\")\n",
    "\n",
    "# Conseguir instancia del mejor modelo\n",
    "if 'XGBoost_Optuna' in best_model_name:\n",
    "    best_model = mejor_xgb\n",
    "elif 'LightGBM_Optuna' in best_model_name:\n",
    "    best_model = mejor_lgbm\n",
    "elif 'RandomForest_Optuna' in best_model_name:\n",
    "    best_model = mejor_rf\n",
    "elif 'CNN' in best_model_name:\n",
    "    print(\"No se puede aplicar bootstrapping directamente al modelo CNN, solo a modelos tabulares.\")\n",
    "    best_model = None\n",
    "elif 'ConvLSTM' in best_model_name:\n",
    "    print(\"No se puede aplicar bootstrapping directamente al modelo ConvLSTM, solo a modelos tabulares.\")\n",
    "    best_model = None\n",
    "else:\n",
    "    # Obtener el modelo original de modelos_base\n",
    "    model_class = best_model_name.split('_')[0]\n",
    "    if model_class == 'RandomForest':\n",
    "        best_model = modelos_base['RandomForest']\n",
    "    elif model_class == 'XGBoost':\n",
    "        best_model = modelos_base['XGBoost']\n",
    "    elif model_class == 'LightGBM':\n",
    "        best_model = modelos_base['LightGBM']\n",
    "        \n",
    "# Aplicar bootstrapping al mejor modelo si es tabular\n",
    "if best_model is not None:\n",
    "    bootstrap_results = bootstrap_evaluate_model(best_model, X_test_scaled, y_test.values, n_iterations=100)\n",
    "    \n",
    "    # Visualizar distribución de RMSE por bootstrapping\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Histograma de RMSE\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(bootstrap_results['rmse_scores'], bins=20, alpha=0.7, color='skyblue')\n",
    "    plt.axvline(bootstrap_results['rmse_median'], color='red', linestyle='--', label=f\"Mediana: {bootstrap_results['rmse_median']:.3f}\")\n",
    "    plt.axvline(bootstrap_results['rmse_ci'][0], color='green', linestyle=':', label=f\"IC 95%: [{bootstrap_results['rmse_ci'][0]:.3f}, {bootstrap_results['rmse_ci'][1]:.3f}]\")\n",
    "    plt.axvline(bootstrap_results['rmse_ci'][1], color='green', linestyle=':')\n",
    "    plt.title(f'Distribución de RMSE - {best_model_name}')\n",
    "    plt.xlabel('RMSE')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Histograma de MAE\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(bootstrap_results['mae_scores'], bins=20, alpha=0.7, color='lightgreen')\n",
    "    plt.axvline(bootstrap_results['mae_median'], color='red', linestyle='--', label=f\"Mediana: {bootstrap_results['mae_median']:.3f}\")\n",
    "    plt.axvline(bootstrap_results['mae_ci'][0], color='green', linestyle=':', label=f\"IC 95%: [{bootstrap_results['mae_ci'][0]:.3f}, {bootstrap_results['mae_ci'][1]:.3f}]\")\n",
    "    plt.axvline(bootstrap_results['mae_ci'][1], color='green', linestyle=':')\n",
    "    plt.title(f'Distribución de MAE - {best_model_name}')\n",
    "    plt.xlabel('MAE')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Histograma de R²\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist(bootstrap_results['r2_scores'], bins=20, alpha=0.7, color='salmon')\n",
    "    plt.axvline(bootstrap_results['r2_median'], color='red', linestyle='--', label=f\"Mediana: {bootstrap_results['r2_median']:.3f}\")\n",
    "    plt.axvline(bootstrap_results['r2_ci'][0], color='green', linestyle=':', label=f\"IC 95%: [{bootstrap_results['r2_ci'][0]:.3f}, {bootstrap_results['r2_ci'][1]:.3f}]\")\n",
    "    plt.axvline(bootstrap_results['r2_ci'][1], color='green', linestyle=':')\n",
    "    plt.title(f'Distribución de R² - {best_model_name}')\n",
    "    plt.xlabel('R²')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(model_output_dir / 'bootstrap_distributions.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
