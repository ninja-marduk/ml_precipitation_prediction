#!/usr/bin/env python3
"""
Script para entrenar modelos espaciales avanzados de predicci√≥n de precipitaci√≥n
Basado en el an√°lisis de resultados previos con mejoras significativas
"""

import os
import sys
import gc
import warnings
from pathlib import Path
from datetime import datetime
import json

import numpy as np
import pandas as pd
import xarray as xr
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers, regularizers
from tensorflow.keras import backend as K
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import geopandas as gpd
import cartopy.crs as ccrs

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-whitegrid')

# ============================= CONFIGURACI√ìN =============================

# Paths
BASE_PATH = Path.cwd()
for p in [BASE_PATH, *BASE_PATH.parents]:
    if (p / '.git').exists():
        BASE_PATH = p
        break

DATA_FILE = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc'
OUT_ROOT = BASE_PATH / 'models' / 'output' / 'Advanced_Spatial'
OUT_ROOT.mkdir(parents=True, exist_ok=True)

# Hiperpar√°metros optimizados
CONFIG = {
    'INPUT_WINDOW': 60,
    'HORIZON': 3,
    'EPOCHS': 100,
    'BATCH_SIZE': 16,  # Aumentado de 4 a 16
    'LEARNING_RATE': 5e-4,  # Reducido de 1e-3
    'PATIENCE': 10,
    'DROPOUT': 0.2,
    'L2_REG': 1e-5,
    'WARMUP_EPOCHS': 5
}

# Feature sets
EXPERIMENTS = {
    'BASIC': ['year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',
              'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',
              'elevation', 'slope', 'aspect'],
    'KCE': ['year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',
            'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',
            'elevation', 'slope', 'aspect', 'elev_high', 'elev_med', 'elev_low'],
    'PAFC': ['year', 'month', 'month_sin', 'month_cos', 'doy_sin', 'doy_cos',
             'max_daily_precipitation', 'min_daily_precipitation', 'daily_precipitation_std',
             'elevation', 'slope', 'aspect', 'elev_high', 'elev_med', 'elev_low',
             'total_precipitation_lag1', 'total_precipitation_lag2', 'total_precipitation_lag12']
}

# ============================= CAPAS PERSONALIZADAS =============================

class SpatialAttention(layers.Layer):
    """Atenci√≥n espacial para resaltar regiones importantes"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
    def build(self, input_shape):
        self.conv = layers.Conv2D(1, (7, 7), padding='same', activation='sigmoid')
        super().build(input_shape)
        
    def call(self, inputs):
        avg_pool = K.mean(inputs, axis=-1, keepdims=True)
        max_pool = K.max(inputs, axis=-1, keepdims=True)
        concat = layers.Concatenate(axis=-1)([avg_pool, max_pool])
        attention = self.conv(concat)
        return layers.Multiply()([inputs, attention])


class ChannelAttention(layers.Layer):
    """Atenci√≥n de canal para ponderar features importantes"""
    
    def __init__(self, reduction_ratio=8, **kwargs):
        super().__init__(**kwargs)
        self.reduction_ratio = reduction_ratio
        
    def build(self, input_shape):
        channels = input_shape[-1]
        self.fc1 = layers.Dense(channels // self.reduction_ratio, activation='relu')
        self.fc2 = layers.Dense(channels, activation='sigmoid')
        self.gap = layers.GlobalAveragePooling2D()
        self.gmp = layers.GlobalMaxPooling2D()
        super().build(input_shape)
        
    def call(self, inputs):
        avg_pool = self.gap(inputs)
        max_pool = self.gmp(inputs)
        
        avg_out = self.fc2(self.fc1(avg_pool))
        max_out = self.fc2(self.fc1(max_pool))
        
        attention = avg_out + max_out
        attention = K.expand_dims(K.expand_dims(attention, 1), 1)
        
        return layers.Multiply()([inputs, attention])


class CBAM(layers.Layer):
    """Convolutional Block Attention Module"""
    
    def __init__(self, reduction_ratio=8, **kwargs):
        super().__init__(**kwargs)
        self.channel_attention = ChannelAttention(reduction_ratio)
        self.spatial_attention = SpatialAttention()
        
    def call(self, inputs):
        x = self.channel_attention(inputs)
        x = self.spatial_attention(x)
        return x


# ============================= ARQUITECTURAS MEJORADAS =============================

def build_convlstm_attention(input_shape, n_features, lat, lon):
    """ConvLSTM mejorado con mecanismo de atenci√≥n"""
    
    inp = layers.Input(shape=input_shape)
    
    # Primera capa con m√°s filtros y regularizaci√≥n
    x = layers.ConvLSTM2D(
        64, (3, 3), 
        padding='same', 
        return_sequences=True,
        kernel_regularizer=regularizers.l1_l2(l1=0, l2=CONFIG['L2_REG'])
    )(inp)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(CONFIG['DROPOUT'])(x)
    
    # Segunda capa
    x = layers.ConvLSTM2D(
        32, (3, 3), 
        padding='same', 
        return_sequences=True,
        kernel_regularizer=regularizers.l1_l2(l1=0, l2=CONFIG['L2_REG'])
    )(x)
    x = layers.BatchNormalization()(x)
    
    # Aplicar atenci√≥n temporal
    x = layers.TimeDistributed(CBAM())(x)
    
    # Capa final
    x = layers.ConvLSTM2D(
        16, (3, 3), 
        padding='same', 
        return_sequences=False,
        kernel_regularizer=regularizers.l1_l2(l1=0, l2=CONFIG['L2_REG'])
    )(x)
    x = layers.BatchNormalization()(x)
    
    # Cabeza de salida multi-escala
    out = advanced_spatial_head(x, lat, lon)
    
    return models.Model(inp, out, name='ConvLSTM_Attention')


def build_convgru_residual(input_shape, n_features, lat, lon):
    """ConvGRU con conexiones residuales y BatchNorm"""
    
    inp = layers.Input(shape=input_shape)
    
    # Proyecci√≥n inicial
    x = layers.TimeDistributed(
        layers.Conv2D(32, (1, 1), padding='same')
    )(inp)
    
    # Bloque ConvGRU 1
    gru1 = ConvGRU2D(64, (3, 3), return_sequences=True)(x)
    gru1 = layers.BatchNormalization()(gru1)
    gru1 = layers.Dropout(CONFIG['DROPOUT'])(gru1)
    
    # Bloque ConvGRU 2 con skip connection
    gru2 = ConvGRU2D(32, (3, 3), return_sequences=False)(gru1)
    gru2 = layers.BatchNormalization()(gru2)
    
    # Skip connection desde input
    skip = layers.TimeDistributed(
        layers.Conv2D(32, (1, 1), padding='same')
    )(inp)
    skip = layers.Lambda(lambda x: x[:, -1])(skip)  # Tomar √∫ltimo timestep
    
    # Combinar con residual
    x = layers.Add()([gru2, skip])
    x = layers.Activation('relu')(x)
    
    # Cabeza de salida
    out = advanced_spatial_head(x, lat, lon)
    
    return models.Model(inp, out, name='ConvGRU_Residual')


def build_hybrid_transformer(input_shape, n_features, lat, lon):
    """Modelo h√≠brido CNN + Transformer para capturar patrones complejos"""
    
    inp = layers.Input(shape=input_shape)
    
    # Encoder convolucional
    x = layers.TimeDistributed(
        layers.Conv2D(64, (3, 3), padding='same', activation='relu')
    )(inp)
    x = layers.TimeDistributed(layers.BatchNormalization())(x)
    x = layers.TimeDistributed(
        layers.Conv2D(32, (3, 3), padding='same', activation='relu')
    )(x)
    x = layers.TimeDistributed(layers.BatchNormalization())(x)
    
    # Reducir dimensionalidad espacial
    x = layers.TimeDistributed(layers.MaxPooling2D((2, 2)))(x)
    x = layers.TimeDistributed(layers.Flatten())(x)
    
    # Self-attention temporal
    x = layers.MultiHeadAttention(
        num_heads=4, 
        key_dim=32,
        dropout=CONFIG['DROPOUT']
    )(x, x)
    x = layers.LayerNormalization()(x)
    
    # Agregaci√≥n temporal con LSTM
    x = layers.LSTM(128, return_sequences=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(CONFIG['DROPOUT'])(x)
    
    # Decodificador espacial
    x = layers.Dense(lat * lon * 16)(x)
    x = layers.Reshape((lat, lon, 16))(x)
    
    # Cabeza de salida
    out = advanced_spatial_head(x, lat, lon)
    
    return models.Model(inp, out, name='Hybrid_Transformer')


def advanced_spatial_head(x, lat, lon):
    """Cabeza de proyecci√≥n mejorada con procesamiento multi-escala"""
    
    # Aplicar CBAM
    x = CBAM()(x)
    
    # Procesamiento multi-escala
    conv1 = layers.Conv2D(CONFIG['HORIZON'], (1, 1), padding='same')(x)
    conv3 = layers.Conv2D(CONFIG['HORIZON'], (3, 3), padding='same')(x)
    conv5 = layers.Conv2D(CONFIG['HORIZON'], (5, 5), padding='same')(x)
    
    # Combinar escalas
    x = layers.Add()([conv1, conv3, conv5])
    x = layers.BatchNormalization()(x)
    x = layers.Activation('linear')(x)
    
    # Reshape a formato de salida
    x = layers.Lambda(
        lambda t: tf.transpose(t, [0, 3, 1, 2]),
        output_shape=(CONFIG['HORIZON'], lat, lon)
    )(x)
    x = layers.Lambda(
        lambda t: tf.expand_dims(t, -1),
        output_shape=(CONFIG['HORIZON'], lat, lon, 1)
    )(x)
    
    return x


# ============================= UTILIDADES =============================

def cosine_decay_with_warmup(epoch):
    """Learning rate schedule con warmup y cosine decay"""
    lr_base = CONFIG['LEARNING_RATE']
    total_epochs = CONFIG['EPOCHS']
    warmup_epochs = CONFIG['WARMUP_EPOCHS']
    
    if epoch < warmup_epochs:
        return lr_base * (epoch + 1) / warmup_epochs
    else:
        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)
        return lr_base * 0.5 * (1 + np.cos(np.pi * progress))


def create_callbacks(model_name, exp_name, model_path):
    """Crear callbacks optimizados para el entrenamiento"""
    
    return [
        callbacks.EarlyStopping(
            monitor='val_loss',
            patience=CONFIG['PATIENCE'],
            restore_best_weights=True,
            verbose=1,
            min_delta=1e-4
        ),
        callbacks.ModelCheckpoint(
            model_path,
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        ),
        callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=CONFIG['PATIENCE'] // 2,
            min_lr=1e-7,
            verbose=1
        ),
        callbacks.LearningRateScheduler(cosine_decay_with_warmup, verbose=0),
        callbacks.CSVLogger(
            model_path.parent / f"{model_name}_training.csv",
            separator=',',
            append=False
        )
    ]


def windowed_arrays(X, y, input_window, horizon):
    """Crear ventanas deslizantes para series temporales"""
    seq_X, seq_y = [], []
    T = len(X)
    
    for start in range(T - input_window - horizon + 1):
        end_w = start + input_window
        end_y = end_w + horizon
        Xw, yw = X[start:end_w], y[end_w:end_y]
        
        if not (np.isnan(Xw).any() or np.isnan(yw).any()):
            seq_X.append(Xw)
            seq_y.append(yw)
    
    return np.asarray(seq_X, dtype=np.float32), np.asarray(seq_y, dtype=np.float32)


# ============================= MAIN =============================

def main():
    """Funci√≥n principal para entrenar modelos avanzados"""
    
    print("="*70)
    print("üöÄ ENTRENAMIENTO DE MODELOS ESPACIALES AVANZADOS")
    print("="*70)
    
    # Configurar GPU
    for gpu in tf.config.list_physical_devices('GPU'):
        tf.config.experimental.set_memory_growth(gpu, True)
    
    # Cargar datos
    print("\nüìÇ Cargando dataset...")
    ds = xr.open_dataset(DATA_FILE)
    lat, lon = len(ds.latitude), len(ds.longitude)
    print(f"   Dimensiones: time={len(ds.time)}, lat={lat}, lon={lon}")
    
    # Definir modelos
    MODELS = {
        'ConvLSTM_Att': build_convlstm_attention,
        'ConvGRU_Res': build_convgru_residual,
        'Hybrid_Trans': build_hybrid_transformer
    }
    
    # Resultados
    all_results = []
    
    # Entrenar para cada experimento
    for exp_name, features in EXPERIMENTS.items():
        print(f"\n{'='*60}")
        print(f"üî¨ EXPERIMENTO: {exp_name} ({len(features)} features)")
        print(f"{'='*60}")
        
        # Preparar datos
        Xarr = ds[features].to_array().transpose('time', 'latitude', 'longitude', 'variable').values
        yarr = ds['total_precipitation'].values[..., None]
        
        X, y = windowed_arrays(Xarr, yarr, CONFIG['INPUT_WINDOW'], CONFIG['HORIZON'])
        
        # Split 80/10/10
        n_train = int(0.8 * len(X))
        n_val = int(0.9 * len(X))
        
        # Normalizaci√≥n
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        
        X_flat = X.reshape(-1, len(features))
        y_flat = y.reshape(-1, 1)
        
        scaler_X.fit(X_flat[:n_train])
        scaler_y.fit(y_flat[:n_train])
        
        X_scaled = scaler_X.transform(X_flat).reshape(X.shape)
        y_scaled = scaler_y.transform(y_flat).reshape(y.shape)
        
        # Splits
        X_train = X_scaled[:n_train]
        y_train = y_scaled[:n_train]
        X_val = X_scaled[n_train:n_val]
        y_val = y_scaled[n_train:n_val]
        X_test = X_scaled[n_val:]
        y_test = y_scaled[n_val:]
        
        print(f"   Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
        
        # Crear directorio para experimento
        exp_dir = OUT_ROOT / exp_name
        exp_dir.mkdir(exist_ok=True)
        
        # Entrenar cada modelo
        for model_name, model_builder in MODELS.items():
            print(f"\n{'‚îÄ'*50}")
            print(f"Modelo: {model_name}")
            print(f"{'‚îÄ'*50}")
            
            try:
                # Construir modelo
                if 'ConvGRU' in model_name:
                    # Importar ConvGRU2D del notebook original
                    from tensorflow.keras.layers import ConvLSTM2D as ConvGRU2D
                
                model = model_builder(
                    input_shape=(CONFIG['INPUT_WINDOW'], lat, lon, len(features)),
                    n_features=len(features),
                    lat=lat,
                    lon=lon
                )
                
                # Compilar
                model.compile(
                    optimizer=optimizers.AdamW(
                        learning_rate=CONFIG['LEARNING_RATE'],
                        weight_decay=CONFIG['L2_REG']
                    ),
                    loss='mse',
                    metrics=['mae']
                )
                
                print(f"   Par√°metros: {model.count_params():,}")
                
                # Callbacks
                model_path = exp_dir / f"{model_name}_best.keras"
                callbacks_list = create_callbacks(model_name, exp_name, model_path)
                
                # Entrenar
                print(f"\n   üèÉ Entrenando...")
                history = model.fit(
                    X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=CONFIG['EPOCHS'],
                    batch_size=CONFIG['BATCH_SIZE'],
                    callbacks=callbacks_list,
                    verbose=1
                )
                
                # Evaluar en test
                print(f"\n   üìä Evaluando...")
                test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)
                
                # Predicciones para m√©tricas
                y_pred_scaled = model.predict(X_test[:10], verbose=0)
                y_pred = scaler_y.inverse_transform(
                    y_pred_scaled.reshape(-1, 1)
                ).reshape(-1, CONFIG['HORIZON'], lat, lon)
                y_true = scaler_y.inverse_transform(
                    y_test[:10].reshape(-1, 1)
                ).reshape(-1, CONFIG['HORIZON'], lat, lon)
                
                # Calcular m√©tricas por horizonte
                for h in range(CONFIG['HORIZON']):
                    rmse = np.sqrt(mean_squared_error(
                        y_true[:, h].ravel(), 
                        y_pred[:, h].ravel()
                    ))
                    mae = mean_absolute_error(
                        y_true[:, h].ravel(), 
                        y_pred[:, h].ravel()
                    )
                    r2 = r2_score(
                        y_true[:, h].ravel(), 
                        y_pred[:, h].ravel()
                    )
                    
                    result = {
                        'Experiment': exp_name,
                        'Model': model_name,
                        'Horizon': h + 1,
                        'RMSE': rmse,
                        'MAE': mae,
                        'R2': r2,
                        'Test_Loss': test_loss,
                        'Parameters': model.count_params(),
                        'Best_Epoch': len(history.history['loss'])
                    }
                    
                    all_results.append(result)
                    print(f"      H={h+1}: RMSE={rmse:.4f}, MAE={mae:.4f}, R¬≤={r2:.4f}")
                
                # Guardar historia
                history_path = exp_dir / f"{model_name}_history.json"
                with open(history_path, 'w') as f:
                    json.dump(history.history, f, indent=2)
                
                # Limpiar memoria
                K.clear_session()
                gc.collect()
                
            except Exception as e:
                print(f"   ‚ùå Error: {str(e)}")
                continue
    
    # Guardar resultados
    results_df = pd.DataFrame(all_results)
    results_df.to_csv(OUT_ROOT / 'advanced_results.csv', index=False)
    
    print("\n" + "="*70)
    print("‚úÖ ENTRENAMIENTO COMPLETADO")
    print(f"üìä Resultados guardados en: {OUT_ROOT / 'advanced_results.csv'}")
    print("="*70)
    
    # Mostrar resumen
    print("\nüìà RESUMEN DE MEJORES MODELOS:")
    print("‚îÄ"*50)
    
    for exp in EXPERIMENTS.keys():
        exp_data = results_df[results_df['Experiment'] == exp]
        if not exp_data.empty:
            best_idx = exp_data.groupby('Model')['RMSE'].mean().idxmin()
            best_data = exp_data[exp_data['Model'] == best_idx]
            avg_rmse = best_data['RMSE'].mean()
            avg_r2 = best_data['R2'].mean()
            
            print(f"\n{exp}:")
            print(f"  ‚Ä¢ Mejor modelo: {best_idx}")
            print(f"  ‚Ä¢ RMSE promedio: {avg_rmse:.4f}")
            print(f"  ‚Ä¢ R¬≤ promedio: {avg_r2:.4f}")


if __name__ == "__main__":
    main() 