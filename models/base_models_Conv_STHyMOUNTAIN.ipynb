{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4e7925",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/main/models/base_models_STHyMOUNTAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3744f4",
   "metadata": {
    "id": "af3744f4"
   },
   "source": [
    "# 📘 Entrenamiento de Modelos Baseline para Predicción Espaciotemporal de Precipitación Mensual STHyMOUNTAIN\n",
    "\n",
    "Este notebook implementa modelos baseline para la predicción de precipitaciones usando datos espaciotemporales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994be36",
   "metadata": {},
   "source": [
    "## 🔍 Implementación de Modelos Avanzados y Técnicas de Validación\n",
    "\n",
    "Además de los modelos tabulares baseline, implementaremos:\n",
    "\n",
    "1. **Modelos de Deep Learning** para capturar patrones espaciales y temporales:\n",
    "   - Redes CNN para patrones espaciales\n",
    "   - Redes ConvLSTM para patrones temporales\n",
    "   - Redes ConvGRU para patrones temporales\n",
    "\n",
    "El objetivo es proporcionar una evaluación completa de diferentes enfoques de modelado para la predicción de precipitación en regiones montañosas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06416284",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06416284",
    "outputId": "a8e4c864-34e9-41b2-d5c3-e6ccaaf3699e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH = /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction\n",
      "Dataset → time=518, lat=61, lon=65\n",
      "\n",
      "=== Experimento BASIC (12 feats) ===\n",
      "→ ConvLSTM\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "# ───────────────────────── IMPORTS ─────────────────────────\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import sys, os, gc, warnings, shutil\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, ConvLSTM2D, SimpleRNN, BatchNormalization, MaxPooling2D,\n",
    "    Dropout, Flatten, Dense, Reshape, TimeDistributed, Add, LayerNormalization\n",
    ")\n",
    "# ── ConvGRU2D: intento nativo → fallback custom si no existe ──\n",
    "try:\n",
    "    from tensorflow.keras.layers import ConvGRU2D  # TF ≥ 2.8\n",
    "except ImportError:  # ⬅️ define capa manualmente (TF < 2.8)\n",
    "    from tensorflow.python.keras.layers.recurrent import RNN\n",
    "    from tensorflow.python.keras.layers.convolutional_recurrent import ConvRNN2D\n",
    "    from tensorflow.python.keras.utils import conv_utils\n",
    "    from tensorflow.python.keras import backend as K, activations, initializers, regularizers, constraints\n",
    "    from tensorflow.python.ops import array_ops\n",
    "\n",
    "    class _ConvGRU2DCell(tf.keras.layers.Layer):\n",
    "        def __init__(self, filters, kernel_size, activation=\"tanh\", recurrent_activation=\"sigmoid\", **kw):\n",
    "            super().__init__(**kw)\n",
    "            self.filters = filters\n",
    "            self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, \"kernel_size\")\n",
    "            self.activation = activations.get(activation)\n",
    "            self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        def build(self, input_shape):\n",
    "            channel_axis = -1\n",
    "            input_dim = int(input_shape[channel_axis])\n",
    "            kernel_shape = self.kernel_size + (input_dim, self.filters * 3)\n",
    "            rec_shape    = self.kernel_size + (self.filters, self.filters * 3)\n",
    "            self.kernel = self.add_weight(\"kernel\", shape=kernel_shape, initializer=\"glorot_uniform\")\n",
    "            self.recurrent_kernel = self.add_weight(\"recurrent_kernel\", shape=rec_shape, initializer=\"orthogonal\")\n",
    "            self.bias = self.add_weight(\"bias\", shape=(self.filters * 3,), initializer=\"zeros\")\n",
    "            self.built = True\n",
    "        def call(self, inputs, states):\n",
    "            h_prev = states[0]\n",
    "            k_z, k_r, k_h = array_ops.split(self.kernel, 3, axis=3)\n",
    "            rk_z, rk_r, rk_h = array_ops.split(self.recurrent_kernel, 3, axis=3)\n",
    "            b_z, b_r, b_h = array_ops.split(self.bias, 3)\n",
    "            x_z = K.conv2d(inputs, k_z, padding=\"same\") + b_z\n",
    "            x_r = K.conv2d(inputs, k_r, padding=\"same\") + b_r\n",
    "            x_h = K.conv2d(inputs, k_h, padding=\"same\") + b_h\n",
    "            h_z = K.conv2d(h_prev, rk_z, padding=\"same\")\n",
    "            h_r = K.conv2d(h_prev, rk_r, padding=\"same\")\n",
    "            h_h = K.conv2d(h_prev, rk_h, padding=\"same\")\n",
    "            z = self.recurrent_activation(x_z + h_z)\n",
    "            r = self.recurrent_activation(x_r + h_r)\n",
    "            hh = self.activation(x_h + r * h_h)\n",
    "            h = (1 - z) * h_prev + z * hh\n",
    "            return h, [h]\n",
    "        @property\n",
    "        def state_size(self):\n",
    "            return (self.filters, None, None)\n",
    "    class ConvGRU2D(ConvRNN2D):\n",
    "        def __init__(self, filters, kernel_size, **kw):\n",
    "            cell = _ConvGRU2DCell(filters, kernel_size, **kw)\n",
    "            super().__init__(cell, **kw)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid'); sns.set_context('notebook')\n",
    "\n",
    "# ───────────────────────── ENTORNO ─────────────────────────\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive, output\n",
    "    drive.mount('/content/drive')\n",
    "    if not (Path('/content')/'ml_precipitation_prediction').exists():\n",
    "        !git clone -q https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
    "    %cd /content/ml_precipitation_prediction || true\n",
    "    !pip install -q -r requirements.txt xarray netCDF4 seaborn scikit-learn cartopy geopandas imageio\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    for p in [BASE_PATH,*BASE_PATH.parents]:\n",
    "        if (p/'.git').exists(): BASE_PATH=p; break\n",
    "print('BASE_PATH =', BASE_PATH)\n",
    "\n",
    "import matplotlib.pyplot as plt, seaborn as sns, geopandas as gpd, imageio.v2 as imageio\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# ───────────────────────── PATHS & CONST ─────────────────────────\n",
    "DATA_FILE = BASE_PATH/'data'/'output'/(\n",
    "    'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc')\n",
    "OUT_ROOT  = BASE_PATH/'models'/'output'/'Spatial_CONVRNN'\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SHAPE_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
    "DEPT_GDF   = gpd.read_file(SHAPE_DIR/'MGN_Departamento.shp')\n",
    "\n",
    "INPUT_WINDOW = 60; HORIZON = 3; EPOCHS = 50; BATCH = 4; LR = 1e-3\n",
    "PATIENCE = 6\n",
    "\n",
    "# ───────────────────────── FEATURE SETS ─────────────────────────\n",
    "BASE_FEATS = ['year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
    "              'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
    "              'elevation','slope','aspect']\n",
    "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
    "KCE_FEATS = BASE_FEATS + ELEV_CLUSTER\n",
    "PAFC_FEATS= KCE_FEATS + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'BASIC': BASE_FEATS,\n",
    "    'KCE'  : KCE_FEATS,\n",
    "    'PAFC' : PAFC_FEATS\n",
    "}\n",
    "\n",
    "# ───────────────────────── DATASET ─────────────────────────\n",
    "ds = xr.open_dataset(DATA_FILE)\n",
    "lat, lon = len(ds.latitude), len(ds.longitude)\n",
    "print(f\"Dataset → time={len(ds.time)}, lat={lat}, lon={lon}\")\n",
    "\n",
    "# ───────────────────────── HELPERS ─────────────────────────\n",
    "\n",
    "def windowed_arrays(X:np.ndarray, y:np.ndarray):\n",
    "    \"\"\"Crea ventanas deslizantes 5‑D para X y 4‑D para y.\"\"\"\n",
    "    seq_X, seq_y = [], []\n",
    "    T = len(X)\n",
    "    for start in range(T-INPUT_WINDOW-HORIZON+1):\n",
    "        end_w = start+INPUT_WINDOW; end_y=end_w+HORIZON\n",
    "        Xw = X[start:end_w]; yw = y[end_w:end_y]\n",
    "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
    "            continue\n",
    "        seq_X.append(Xw); seq_y.append(yw)\n",
    "    return np.array(seq_X,dtype=np.float32), np.array(seq_y,dtype=np.float32)\n",
    "\n",
    "\n",
    "def quick_plot(ax,data,cmap,title,vmin=None,vmax=None):\n",
    "    mesh = ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',\n",
    "                         vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
    "    ax.coastlines(); ax.add_geometries(DEPT_GDF.geometry,ccrs.PlateCarree(),edgecolor='black',facecolor='none',linewidth=1)\n",
    "    ax.gridlines(draw_labels=True,top_labels=False,right_labels=False)\n",
    "    ax.set_title(title,fontsize=9); return mesh\n",
    "\n",
    "# ───────────────────────── MODEL FACTORIES ─────────────────────────\n",
    "\n",
    "def build_conv_lstm(n_feats:int):\n",
    "    inp = Input(shape=(INPUT_WINDOW,lat,lon,n_feats))\n",
    "    x   = ConvLSTM2D(64,(3,3),padding='same',return_sequences=True)(inp)\n",
    "    x   = ConvLSTM2D(32,(3,3),padding='same')(x)\n",
    "    out = Dense(HORIZON*lat*lon)(Flatten()(x))\n",
    "    out = Reshape((HORIZON,lat,lon,1))(out)\n",
    "    m   = Model(inp,out,'ConvLSTM'); m.compile('adam','mse'); return m\n",
    "\n",
    "\n",
    "def build_conv_gru(n_feats:int):\n",
    "    inp = Input(shape=(INPUT_WINDOW,lat,lon,n_feats))\n",
    "    x = ConvGRU2D(64,(3,3),padding='same',return_sequences=True)(inp)\n",
    "    x = ConvGRU2D(32,(3,3),padding='same')(x)\n",
    "    out = Dense(HORIZON*lat*lon)(Flatten()(x))\n",
    "    out = Reshape((HORIZON,lat,lon,1))(out)\n",
    "    m = Model(inp,out,'ConvGRU'); m.compile('adam','mse'); return m\n",
    "\n",
    "\n",
    "def build_conv_rnn(n_feats:int):\n",
    "    inp = Input(shape=(INPUT_WINDOW,lat,lon,n_feats))\n",
    "    x   = Flatten()(inp)\n",
    "    x   = SimpleRNN(128,activation='tanh')(x)\n",
    "    out = Dense(HORIZON*lat*lon)(x)\n",
    "    out = Reshape((HORIZON,lat,lon,1))(out)\n",
    "    m = Model(inp,out,'ConvRNN'); m.compile('adam','mse'); return m\n",
    "\n",
    "MODELS = {\n",
    "    'ConvLSTM': build_conv_lstm,\n",
    "    'ConvGRU' : build_conv_gru,\n",
    "    'ConvRNN' : build_conv_rnn\n",
    "}\n",
    "\n",
    "# ───────────────────────── TRAIN + EVAL LOOP ─────────────────────────\n",
    "results=[]\n",
    "for exp, feat_list in EXPERIMENTS.items():\n",
    "    print(f\"\\n=== Experimento {exp} ({len(feat_list)} feats) ===\")\n",
    "    Xarr = ds[feat_list].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
    "    yarr = ds['total_precipitation'].values.astype(np.float32)[...,None]\n",
    "    X, y = windowed_arrays(Xarr, yarr)\n",
    "    split=int(0.8*len(X))\n",
    "\n",
    "    sx = StandardScaler().fit(X[:split].reshape(-1,len(feat_list)))\n",
    "    sy = StandardScaler().fit(y[:split].reshape(-1,1))\n",
    "    X_sc = sx.transform(X.reshape(-1,len(feat_list))).reshape(X.shape)\n",
    "    y_sc = sy.transform(y.reshape(-1,1)).reshape(y.shape)\n",
    "    X_tr, X_va = X_sc[:split], X_sc[split:]\n",
    "    y_tr, y_va = y_sc[:split], y_sc[split:]\n",
    "\n",
    "    OUT_EXP = OUT_ROOT/exp; OUT_EXP.mkdir(exist_ok=True)\n",
    "\n",
    "    for mdl_name,builder in MODELS.items():\n",
    "        print(f\"→ {mdl_name}\")\n",
    "        model_path = OUT_EXP/f\"{mdl_name.lower()}_best.keras\"\n",
    "        if model_path.exists():\n",
    "            print(\"⏩ already trained – loading\"); model=tf.keras.models.load_model(model_path,compile=False)\n",
    "        else:\n",
    "            model = builder(n_feats=len(feat_list))\n",
    "            cb=[EarlyStopping('val_loss',patience=PATIENCE,restore_best_weights=True),\n",
    "                ModelCheckpoint(model_path,save_best_only=True)]\n",
    "            model.fit(X_tr,y_tr,validation_data=(X_va,y_va),epochs=EPOCHS,batch_size=BATCH,callbacks=cb,verbose=0)\n",
    "        # ─ Pred última ventana validación ─\n",
    "        y_hat_sc = model.predict(X_va[-1:],verbose=0)\n",
    "        y_hat = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
    "        y_true= sy.inverse_transform(y_va[-1:].reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
    "\n",
    "        # ─ Mapas & GIF ─\n",
    "        vmin,vmax=0,max(y_true.max(),y_hat.max()); frames=[]\n",
    "        dates=pd.date_range(ds.time.values[-HORIZON],periods=HORIZON,freq='MS')\n",
    "        for h in range(HORIZON):\n",
    "            err=np.clip(np.abs((y_true[h]-y_hat[h])/(y_true[h]+1e-5))*100,0,100)\n",
    "            fig,axs=plt.subplots(1,3,figsize=(12,4),subplot_kw={'projection':ccrs.PlateCarree()})\n",
    "            quick_plot(axs[0],y_true[h],'Blues',f\"Real h={h+1}\",vmin,vmax)\n",
    "            quick_plot(axs[1],y_hat[h],'Blues',f\"{mdl_name} h={h+1}\",vmin,vmax)\n",
    "            quick_plot(axs[2],err,'Reds',f\"MAPE% h={h+1}\",0,100)\n",
    "            fig.suptitle(f\"{mdl_name} – {exp} – {dates[h].strftime('%Y-%m')}\")\n",
    "            png=OUT_EXP/f\"{mdl_name}_{h+1}.png\"; fig.savefig(png,bbox_inches='tight'); plt.close(fig)\n",
    "            frames.append(imageio.imread(png))\n",
    "        imageio.mimsave(OUT_EXP/f\"{mdl_name}.gif\",frames,fps=0.5)\n",
    "\n",
    "        # ─ Métricas ─\n",
    "        for h in range(HORIZON):\n",
    "            results.append({\n",
    "                'Experiment':exp,'Model':mdl_name,'H':h+1,\n",
    "                'RMSE':np.sqrt(mean_squared_error(y_true[h].ravel(),y_hat[h].ravel())),\n",
    "                'MAE': mean_absolute_error(y_true[h].ravel(),y_hat[h].ravel()),\n",
    "                'R2' : r2_score(y_true[h].ravel(),y_hat[h].ravel())\n",
    "            })\n",
    "        tf.keras.backend.clear_session(); gc.collect()\n",
    "\n",
    "# ───────────────────────── CSV FINAL ─────────────────────────\n",
    "res_df=pd.DataFrame(results)\n",
    "res_df.to_csv(OUT_ROOT/'metrics_spatial.csv',index=False)\n",
    "print(\"\\n📑 Metrics saved →\", OUT_ROOT/'metrics_spatial.csv')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
