{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce9b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────── IMPORTS ─────────────────────────\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import sys, os, gc, warnings\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, ConvLSTM2D, SimpleRNN, Flatten, Dense, Reshape,\n",
    "    Lambda, Permute, Layer, TimeDistributed\n",
    ")\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# ── ConvGRU2D: Implementación robusta ───────────────────────────\n",
    "class ConvGRU2DCell(Layer):\n",
    "    \"\"\"Celda ConvGRU2D robusta y completa\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, kernel_size, padding='same', activation='tanh', \n",
    "                 recurrent_activation='sigmoid', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.padding = padding\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.recurrent_activation = tf.keras.activations.get(recurrent_activation)\n",
    "        self.state_size = (filters,)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        \n",
    "        # Kernel para input (z, r, h)\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(*self.kernel_size, input_dim, self.filters * 3),\n",
    "            initializer='glorot_uniform',\n",
    "            name='kernel'\n",
    "        )\n",
    "        \n",
    "        # Kernel recurrente (z, r, h)\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(*self.kernel_size, self.filters, self.filters * 3),\n",
    "            initializer='orthogonal',\n",
    "            name='recurrent_kernel'\n",
    "        )\n",
    "        \n",
    "        # Bias\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.filters * 3,),\n",
    "            initializer='zeros',\n",
    "            name='bias'\n",
    "        )\n",
    "        \n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, states):\n",
    "        h_tm1 = states[0]  # Estado anterior\n",
    "        \n",
    "        # Convoluciones para input\n",
    "        x_conv = K.conv2d(inputs, self.kernel, padding=self.padding)\n",
    "        x_z, x_r, x_h = tf.split(x_conv, 3, axis=-1)\n",
    "        \n",
    "        # Convoluciones para estado recurrente\n",
    "        h_conv = K.conv2d(h_tm1, self.recurrent_kernel, padding=self.padding)\n",
    "        h_z, h_r, h_h = tf.split(h_conv, 3, axis=-1)\n",
    "        \n",
    "        # Bias\n",
    "        b_z, b_r, b_h = tf.split(self.bias, 3)\n",
    "        \n",
    "        # Gates\n",
    "        z = self.recurrent_activation(x_z + h_z + b_z)  # Update gate\n",
    "        r = self.recurrent_activation(x_r + h_r + b_r)  # Reset gate\n",
    "        \n",
    "        # Candidate hidden state\n",
    "        h_candidate = self.activation(x_h + r * h_h + b_h)\n",
    "        \n",
    "        # New hidden state\n",
    "        h = (1 - z) * h_tm1 + z * h_candidate\n",
    "        \n",
    "        return h, [h]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'padding': self.padding,\n",
    "            'activation': tf.keras.activations.serialize(self.activation),\n",
    "            'recurrent_activation': tf.keras.activations.serialize(self.recurrent_activation)\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class ConvGRU2D(Layer):\n",
    "    \"\"\"ConvGRU2D completo con soporte para return_sequences\"\"\"\n",
    "    \n",
    "    def __init__(self, filters, kernel_size, padding='same', activation='tanh',\n",
    "                 recurrent_activation='sigmoid', return_sequences=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.return_sequences = return_sequences\n",
    "        self.cell = ConvGRU2DCell(\n",
    "            filters, kernel_size, padding, activation, recurrent_activation\n",
    "        )\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.cell.build(input_shape[2:])  # Sin batch y time\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, time, height, width, channels)\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        time_steps = tf.shape(inputs)[1]\n",
    "        height = tf.shape(inputs)[2]\n",
    "        width = tf.shape(inputs)[3]\n",
    "        \n",
    "        # Estado inicial\n",
    "        initial_state = tf.zeros((batch_size, height, width, self.filters))\n",
    "        \n",
    "        # Procesar secuencia\n",
    "        outputs = []\n",
    "        state = initial_state\n",
    "        \n",
    "        for t in range(inputs.shape[1]):\n",
    "            output, [state] = self.cell(inputs[:, t], [state])\n",
    "            outputs.append(output)\n",
    "        \n",
    "        outputs = tf.stack(outputs, axis=1)\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return outputs\n",
    "        else:\n",
    "            return outputs[:, -1]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'padding': self.padding,\n",
    "            'activation': self.activation,\n",
    "            'recurrent_activation': self.recurrent_activation,\n",
    "            'return_sequences': self.return_sequences\n",
    "        })\n",
    "        return config\n",
    "\n",
    "print(\"✅ ConvGRU2D implementado de forma robusta\")\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt, seaborn as sns, geopandas as gpd, imageio.v2 as imageio\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid'); sns.set_context('notebook')\n",
    "\n",
    "# ───────────────────────── ENTORNO / GPU ─────────────────────────\n",
    "## ╭─────────────────────────── Rutas ──────────────────────────╮\n",
    "# ▶️ Path configuration\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/ml_precipitation_prediction')\n",
    "    # Instalar dependencias necesarias\n",
    "    %pip install -r requirements.txt\n",
    "    %pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn ace_tools_open cartopy geopandas\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
    "        if (p / '.git').exists():\n",
    "            BASE_PATH = p; break\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# limitar crecimiento de memoria‑GPU (evita OOM)\n",
    "for g in tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "# ───────────────────────── PATHS & CONST ─────────────────────────\n",
    "DATA_FILE = BASE_PATH/'data'/'output'/(\n",
    "    'complete_dataset_with_features_with_clusters_elevation_windows_imfs_with_onehot_elevation_clean.nc')\n",
    "OUT_ROOT  = BASE_PATH/'models'/'output'/'Spatial_CONVRNN'\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SHAPE_DIR = BASE_PATH/'data'/'input'/'shapes'\n",
    "DEPT_GDF   = gpd.read_file(SHAPE_DIR/'MGN_Departamento.shp')\n",
    "\n",
    "INPUT_WINDOW = 60\n",
    "HORIZON = 3 \n",
    "EPOCHS = 50\n",
    "BATCH = 4\n",
    "LR = 1e-3\n",
    "PATIENCE = 6\n",
    "\n",
    "# ───────────────────────── FEATURE SETS ─────────────────────────\n",
    "BASE_FEATS = ['year','month','month_sin','month_cos','doy_sin','doy_cos',\n",
    "              'max_daily_precipitation','min_daily_precipitation','daily_precipitation_std',\n",
    "              'elevation','slope','aspect']\n",
    "ELEV_CLUSTER = ['elev_high','elev_med','elev_low']\n",
    "KCE_FEATS = BASE_FEATS + ELEV_CLUSTER\n",
    "PAFC_FEATS= KCE_FEATS + ['total_precipitation_lag1','total_precipitation_lag2','total_precipitation_lag12']\n",
    "EXPERIMENTS = {'BASIC':BASE_FEATS,'KCE':KCE_FEATS,'PAFC':PAFC_FEATS}\n",
    "\n",
    "# ───────────────────────── DATASET ─────────────────────────\n",
    "ds = xr.open_dataset(DATA_FILE)\n",
    "lat, lon = len(ds.latitude), len(ds.longitude)\n",
    "print(f\"Dataset → time={len(ds.time)}, lat={lat}, lon={lon}\")\n",
    "\n",
    "# ───────────────────────── HELPERS ─────────────────────────\n",
    "\n",
    "def windowed_arrays(X:np.ndarray, y:np.ndarray):\n",
    "    seq_X, seq_y = [], []\n",
    "    T = len(X)\n",
    "    for start in range(T-INPUT_WINDOW-HORIZON+1):\n",
    "        end_w = start+INPUT_WINDOW; end_y=end_w+HORIZON\n",
    "        Xw, yw = X[start:end_w], y[end_w:end_y]\n",
    "        if np.isnan(Xw).any() or np.isnan(yw).any():\n",
    "            continue\n",
    "        seq_X.append(Xw); seq_y.append(yw)\n",
    "    return np.asarray(seq_X,dtype=np.float32), np.asarray(seq_y,dtype=np.float32)\n",
    "\n",
    "def quick_plot(ax,data,cmap,title,vmin=None,vmax=None):\n",
    "    mesh = ax.pcolormesh(ds.longitude,ds.latitude,data,cmap=cmap,shading='nearest',\n",
    "                         vmin=vmin,vmax=vmax,transform=ccrs.PlateCarree())\n",
    "    ax.coastlines(); ax.add_geometries(DEPT_GDF.geometry,ccrs.PlateCarree(),\n",
    "                                       edgecolor='black',facecolor='none',linewidth=1)\n",
    "    # etiquetas desactivadas para evitar bug en cartopy\n",
    "    ax.gridlines(draw_labels=False, linewidth=.5, linestyle='--', alpha=.4)\n",
    "    ax.set_title(title,fontsize=9)\n",
    "    return mesh\n",
    "\n",
    "# ───────────────────────── LIGHTWEIGHT HEAD ─────────────────────────\n",
    "\n",
    "def _spatial_head(x):\n",
    "    \"\"\"Proyección 1×1 → (B, H,lat,lon,1) con *shape hints* para que\n",
    "    Keras pueda reconstruir la capa `Lambda` al volver a cargar el modelo.\n",
    "    \"\"\"\n",
    "    #   1) Conv 1×1 que genera H mapas (uno por horizonte)\n",
    "    x = Conv2D(\n",
    "        HORIZON,\n",
    "        (1, 1),\n",
    "        padding=\"same\",\n",
    "        activation=\"linear\",\n",
    "        name=\"head_conv1x1\",\n",
    "    )(x)  # ==> (B, lat, lon, H)\n",
    "\n",
    "    #   2) Transponemos a (B, H, lat, lon)\n",
    "    x = Lambda(\n",
    "        lambda t: tf.transpose(t, [0, 3, 1, 2]),\n",
    "        output_shape=(HORIZON, lat, lon),\n",
    "        name=\"head_transpose\",\n",
    "    )(x)\n",
    "\n",
    "    #   3) Añadimos eje canales: (B, H, lat, lon, 1)\n",
    "    x = Lambda(\n",
    "        lambda t: tf.expand_dims(t, -1),\n",
    "        output_shape=(HORIZON, lat, lon, 1),\n",
    "        name=\"head_expand_dim\",\n",
    "    )(x)\n",
    "    return x\n",
    "\n",
    "# ───────────────────────── MODEL FACTORIES ─────────────────────────\n",
    "\n",
    "def build_conv_lstm(n_feats:int):\n",
    "    inp = Input(shape=(INPUT_WINDOW,lat,lon,n_feats))\n",
    "    x   = ConvLSTM2D(32,(3,3),padding='same',return_sequences=True)(inp)\n",
    "    x   = ConvLSTM2D(16,(3,3),padding='same',return_sequences=False)(x)\n",
    "    out = _spatial_head(x)\n",
    "    return Model(inp, out, name='ConvLSTM')\n",
    "\n",
    "def build_conv_gru(n_feats: int):\n",
    "    \"\"\"\n",
    "    Construye modelo ConvGRU usando nuestra implementación robusta\n",
    "    \"\"\"\n",
    "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
    "    \n",
    "    # Usar nuestra implementación de ConvGRU2D\n",
    "    x = ConvGRU2D(32, (3, 3), padding=\"same\", return_sequences=True)(inp)\n",
    "    x = ConvGRU2D(16, (3, 3), padding=\"same\", return_sequences=False)(x)\n",
    "    \n",
    "    out = _spatial_head(x)\n",
    "    return Model(inp, out, name=\"ConvGRU\")\n",
    "\n",
    "def build_conv_rnn(n_feats:int):\n",
    "    \"\"\"\n",
    "    Modelo ConvRNN corregido: procesa secuencias temporales de imágenes\n",
    "    \"\"\"\n",
    "    inp = Input(shape=(INPUT_WINDOW, lat, lon, n_feats))\n",
    "    \n",
    "    # Opción 1: Usar TimeDistributed para procesar cada frame\n",
    "    # Aplicar convolución a cada timestep\n",
    "    x = TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu'))(inp)\n",
    "    x = TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'))(x)\n",
    "    \n",
    "    # Aplanar cada frame para pasarlo por RNN\n",
    "    x = TimeDistributed(Flatten())(x)  # (batch, time, features)\n",
    "    \n",
    "    # RNN sobre la secuencia temporal\n",
    "    x = SimpleRNN(128, activation='tanh', return_sequences=False)(x)\n",
    "    \n",
    "    # Proyectar a la salida deseada\n",
    "    x = Dense(HORIZON * lat * lon)(x)\n",
    "    out = Reshape((HORIZON, lat, lon, 1))(x)\n",
    "    \n",
    "    return Model(inp, out, name='ConvRNN')\n",
    "\n",
    "MODELS = {'ConvLSTM': build_conv_lstm, 'ConvGRU': build_conv_gru, 'ConvRNN': build_conv_rnn}\n",
    "\n",
    "# ───────────────────────── TRAIN + EVAL LOOP ─────────────────────────\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, CSVLogger, Callback\n",
    "import json\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Callback personalizado para visualización en tiempo real\n",
    "class TrainingMonitor(Callback):\n",
    "    \"\"\"Callback para monitorear el entrenamiento en tiempo real\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, experiment_name):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.experiment_name = experiment_name\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.lrs = []\n",
    "        self.epochs = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Guardar métricas\n",
    "        self.epochs.append(epoch + 1)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.lrs.append(self.model.optimizer.learning_rate.numpy())\n",
    "        \n",
    "        # Limpiar output anterior\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Crear visualización\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot de losses\n",
    "        ax1.plot(self.epochs, self.losses, 'b-', label='Train Loss', linewidth=2)\n",
    "        ax1.plot(self.epochs, self.val_losses, 'r-', label='Val Loss', linewidth=2)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title(f'{self.model_name} - {self.experiment_name} - Training Progress')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot de learning rate\n",
    "        ax2.plot(self.epochs, self.lrs, 'g-', label='Learning Rate', linewidth=2)\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Learning Rate')\n",
    "        ax2.set_title('Learning Rate Schedule')\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "        plt.close()\n",
    "        \n",
    "        # Mostrar métricas actuales\n",
    "        print(f\"\\n📊 Época {epoch + 1}/{self.params['epochs']}\")\n",
    "        print(f\"   • Loss: {logs.get('loss'):.6f}\")\n",
    "        print(f\"   • Val Loss: {logs.get('val_loss'):.6f}\")\n",
    "        print(f\"   • MAE: {logs.get('mae'):.6f}\")\n",
    "        print(f\"   • Val MAE: {logs.get('val_mae'):.6f}\")\n",
    "        print(f\"   • Learning Rate: {self.lrs[-1]:.2e}\")\n",
    "        \n",
    "        # Mostrar mejora\n",
    "        if len(self.val_losses) > 1:\n",
    "            improvement = (self.val_losses[-2] - self.val_losses[-1]) / self.val_losses[-2] * 100\n",
    "            print(f\"   • Mejora: {improvement:.2f}%\")\n",
    "\n",
    "# Diccionario para almacenar historiales de entrenamiento\n",
    "all_histories = {}\n",
    "results = []\n",
    "\n",
    "# Función para guardar hiperparámetros\n",
    "def save_hyperparameters(exp_path, model_name, hyperparams):\n",
    "    \"\"\"Guarda los hiperparámetros en un archivo JSON\"\"\"\n",
    "    hp_file = exp_path / f\"{model_name}_hyperparameters.json\"\n",
    "    with open(hp_file, 'w') as f:\n",
    "        json.dump(hyperparams, f, indent=4)\n",
    "    print(f\"   💾 Hiperparámetros guardados en: {hp_file.name}\")\n",
    "\n",
    "# Función para plotear curvas de aprendizaje\n",
    "def plot_learning_curves(history, exp_path, model_name, show=True):\n",
    "    \"\"\"Genera y guarda las curvas de aprendizaje\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss (MSE)')\n",
    "    axes[0].set_title(f'{model_name} - Loss Evolution')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning Rate\n",
    "    if 'lr' in history.history:\n",
    "        axes[1].plot(history.history['lr'], label='Learning Rate', linewidth=2, color='green')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Learning Rate')\n",
    "        axes[1].set_title(f'{model_name} - Learning Rate Schedule')\n",
    "        axes[1].set_yscale('log')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar\n",
    "    curves_path = exp_path / f\"{model_name}_learning_curves.png\"\n",
    "    plt.savefig(curves_path, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    return curves_path\n",
    "\n",
    "# Función para mostrar resumen de entrenamiento\n",
    "def print_training_summary(history, model_name, exp_name):\n",
    "    \"\"\"Imprime un resumen del entrenamiento\"\"\"\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    best_val_loss = min(history.history['val_loss'])\n",
    "    best_epoch = history.history['val_loss'].index(best_val_loss) + 1\n",
    "    \n",
    "    print(f\"\\n   📊 Resumen de entrenamiento {model_name} - {exp_name}:\")\n",
    "    print(f\"      • Épocas totales: {len(history.history['loss'])}\")\n",
    "    print(f\"      • Loss final (train): {final_loss:.6f}\")\n",
    "    print(f\"      • Loss final (val): {final_val_loss:.6f}\")\n",
    "    print(f\"      • Mejor loss (val): {best_val_loss:.6f} en época {best_epoch}\")\n",
    "    if 'lr' in history.history:\n",
    "        final_lr = history.history['lr'][-1]\n",
    "        print(f\"      • Learning rate final: {final_lr:.2e}\")\n",
    "\n",
    "for exp, feat_list in EXPERIMENTS.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"🔬 EXPERIMENTO: {exp} ({len(feat_list)} features)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Preparar datos\n",
    "    Xarr = ds[feat_list].to_array().transpose('time','latitude','longitude','variable').values.astype(np.float32)\n",
    "    yarr = ds['total_precipitation'].values.astype(np.float32)[...,None]\n",
    "    X, y = windowed_arrays(Xarr, yarr)\n",
    "    split = int(0.8*len(X))\n",
    "\n",
    "    sx = StandardScaler().fit(X[:split].reshape(-1,len(feat_list)))\n",
    "    sy = StandardScaler().fit(y[:split].reshape(-1,1))\n",
    "    X_sc = sx.transform(X.reshape(-1,len(feat_list))).reshape(X.shape)\n",
    "    y_sc = sy.transform(y.reshape(-1,1)).reshape(y.shape)\n",
    "    X_tr, X_va = X_sc[:split], X_sc[split:]\n",
    "    y_tr, y_va = y_sc[:split], y_sc[split:]\n",
    "\n",
    "    OUT_EXP = OUT_ROOT/exp\n",
    "    OUT_EXP.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Crear subdirectorio para métricas de entrenamiento\n",
    "    METRICS_DIR = OUT_EXP / 'training_metrics'\n",
    "    METRICS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    for mdl_name, builder in MODELS.items():\n",
    "        print(f\"\\n{'─'*50}\")\n",
    "        print(f\"🤖 Modelo: {mdl_name}\")\n",
    "        print(f\"{'─'*50}\")\n",
    "        \n",
    "        model_path = OUT_EXP/f\"{mdl_name.lower()}_best.keras\"\n",
    "        if model_path.exists():\n",
    "            model_path.unlink()\n",
    "        \n",
    "        try:\n",
    "            # Construir modelo\n",
    "            model = builder(n_feats=len(feat_list))\n",
    "            \n",
    "            # Definir optimizador con configuración explícita\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "            model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "            \n",
    "            # Hiperparámetros\n",
    "            hyperparams = {\n",
    "                'experiment': exp,\n",
    "                'model': mdl_name,\n",
    "                'features': feat_list,\n",
    "                'n_features': len(feat_list),\n",
    "                'input_window': INPUT_WINDOW,\n",
    "                'horizon': HORIZON,\n",
    "                'batch_size': BATCH,\n",
    "                'initial_lr': LR,\n",
    "                'epochs': EPOCHS,\n",
    "                'patience': PATIENCE,\n",
    "                'train_samples': len(X_tr),\n",
    "                'val_samples': len(X_va),\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'model_params': model.count_params()\n",
    "            }\n",
    "            \n",
    "            # Guardar hiperparámetros\n",
    "            save_hyperparameters(METRICS_DIR, mdl_name, hyperparams)\n",
    "            \n",
    "            # Callbacks mejorados\n",
    "            csv_logger = CSVLogger(\n",
    "                METRICS_DIR / f\"{mdl_name}_training_log.csv\",\n",
    "                separator=',',\n",
    "                append=False\n",
    "            )\n",
    "            \n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=PATIENCE//2,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            early_stop = EarlyStopping(\n",
    "                'val_loss',\n",
    "                patience=PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            checkpoint = ModelCheckpoint(\n",
    "                model_path,\n",
    "                save_best_only=True,\n",
    "                monitor='val_loss',\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Agregar monitor de entrenamiento\n",
    "            training_monitor = TrainingMonitor(mdl_name, exp)\n",
    "            \n",
    "            callbacks = [early_stop, checkpoint, reduce_lr, csv_logger, training_monitor]\n",
    "            \n",
    "            # Entrenar con verbose=0 para usar nuestro monitor personalizado\n",
    "            print(f\"\\n🏃 Iniciando entrenamiento...\")\n",
    "            print(f\"   📊 Visualización en tiempo real activada\")\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_tr, y_tr,\n",
    "                validation_data=(X_va, y_va),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0  # Usar 0 para que solo se muestre nuestro monitor\n",
    "            )\n",
    "            \n",
    "            # Guardar historial\n",
    "            all_histories[f\"{exp}_{mdl_name}\"] = history\n",
    "            \n",
    "            # Mostrar resumen de entrenamiento\n",
    "            print_training_summary(history, mdl_name, exp)\n",
    "            \n",
    "            # Plotear y guardar curvas de aprendizaje\n",
    "            plot_learning_curves(history, METRICS_DIR, mdl_name, show=True)\n",
    "            \n",
    "            # Guardar historial como JSON\n",
    "            history_dict = {\n",
    "                'loss': [float(x) for x in history.history['loss']],\n",
    "                'val_loss': [float(x) for x in history.history['val_loss']],\n",
    "                'mae': [float(x) for x in history.history.get('mae', [])],\n",
    "                'val_mae': [float(x) for x in history.history.get('val_mae', [])],\n",
    "                'lr': [float(x) for x in history.history.get('lr', [])]\n",
    "            }\n",
    "            \n",
    "            with open(METRICS_DIR / f\"{mdl_name}_history.json\", 'w') as f:\n",
    "                json.dump(history_dict, f, indent=4)\n",
    "\n",
    "            # ─ Predicciones y visualización ─\n",
    "            print(f\"\\n🎯 Generando predicciones...\")\n",
    "            y_hat_sc = model.predict(X_va[-1:], verbose=0)\n",
    "            y_hat = sy.inverse_transform(y_hat_sc.reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
    "            y_true = sy.inverse_transform(y_va[-1:].reshape(-1,1)).reshape(HORIZON,lat,lon)\n",
    "\n",
    "            # ─ Mapas & GIF ─\n",
    "            vmin, vmax = 0, max(y_true.max(), y_hat.max())\n",
    "            frames = []\n",
    "            dates = pd.date_range(ds.time.values[-HORIZON], periods=HORIZON, freq='MS')\n",
    "            \n",
    "            for h in range(HORIZON):\n",
    "                err = np.clip(np.abs((y_true[h]-y_hat[h])/(y_true[h]+1e-5))*100, 0, 100)\n",
    "                fig, axs = plt.subplots(1, 3, figsize=(12, 4), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "                quick_plot(axs[0], y_true[h], 'Blues', f\"Real h={h+1}\", vmin, vmax)\n",
    "                quick_plot(axs[1], y_hat[h], 'Blues', f\"{mdl_name} h={h+1}\", vmin, vmax)\n",
    "                quick_plot(axs[2], err, 'Reds', f\"MAPE% h={h+1}\", 0, 100)\n",
    "                fig.suptitle(f\"{mdl_name} – {exp} – {dates[h].strftime('%Y-%m')}\")\n",
    "                png = OUT_EXP/f\"{mdl_name}_{h+1}.png\"\n",
    "                fig.savefig(png, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "                frames.append(imageio.imread(png))\n",
    "            \n",
    "            imageio.mimsave(OUT_EXP/f\"{mdl_name}.gif\", frames, fps=0.5)\n",
    "\n",
    "            # ─ Métricas de evaluación ─\n",
    "            for h in range(HORIZON):\n",
    "                rmse = np.sqrt(mean_squared_error(y_true[h].ravel(), y_hat[h].ravel()))\n",
    "                mae = mean_absolute_error(y_true[h].ravel(), y_hat[h].ravel())\n",
    "                r2 = r2_score(y_true[h].ravel(), y_hat[h].ravel())\n",
    "                \n",
    "                results.append({\n",
    "                    'Experiment': exp,\n",
    "                    'Model': mdl_name,\n",
    "                    'H': h+1,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAE': mae,\n",
    "                    'R2': r2\n",
    "                })\n",
    "                \n",
    "                print(f\"   📈 H={h+1}: RMSE={rmse:.4f}, MAE={mae:.4f}, R²={r2:.4f}\")\n",
    "            \n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Error en {mdl_name}: {str(e)}\")\n",
    "            print(f\"  → Saltando {mdl_name} para {exp}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "# ───────────────────────── CSV FINAL ─────────────────────────\n",
    "res_df=pd.DataFrame(results)\n",
    "res_df.to_csv(OUT_ROOT/'metrics_spatial.csv',index=False)\n",
    "print(\"\\n📑 Metrics saved →\", OUT_ROOT/'metrics_spatial.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────── VISUALIZACIÓN COMPARATIVA ─────────────────────────\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 GENERANDO VISUALIZACIONES COMPARATIVAS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear directorio para comparaciones\n",
    "COMP_DIR = OUT_ROOT / 'comparisons'\n",
    "COMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. Comparación de métricas entre modelos\n",
    "if res_df is not None and len(res_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # RMSE por modelo y experimento\n",
    "    pivot_rmse = res_df.pivot_table(values='RMSE', index='Model', columns='Experiment', aggfunc='mean')\n",
    "    pivot_rmse.plot(kind='bar', ax=axes[0,0])\n",
    "    axes[0,0].set_title('RMSE Promedio por Modelo y Experimento')\n",
    "    axes[0,0].set_ylabel('RMSE')\n",
    "    axes[0,0].legend(title='Experimento')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # MAE por modelo y experimento\n",
    "    pivot_mae = res_df.pivot_table(values='MAE', index='Model', columns='Experiment', aggfunc='mean')\n",
    "    pivot_mae.plot(kind='bar', ax=axes[0,1])\n",
    "    axes[0,1].set_title('MAE Promedio por Modelo y Experimento')\n",
    "    axes[0,1].set_ylabel('MAE')\n",
    "    axes[0,1].legend(title='Experimento')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # R² por modelo y experimento\n",
    "    pivot_r2 = res_df.pivot_table(values='R2', index='Model', columns='Experiment', aggfunc='mean')\n",
    "    pivot_r2.plot(kind='bar', ax=axes[1,0])\n",
    "    axes[1,0].set_title('R² Promedio por Modelo y Experimento')\n",
    "    axes[1,0].set_ylabel('R²')\n",
    "    axes[1,0].legend(title='Experimento')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Métricas por horizonte\n",
    "    for metric in ['RMSE', 'MAE', 'R2']:\n",
    "        res_df.groupby(['H', 'Model'])[metric].mean().unstack().plot(ax=axes[1,1], marker='o')\n",
    "    axes[1,1].set_title('Evolución de Métricas por Horizonte')\n",
    "    axes[1,1].set_xlabel('Horizonte (meses)')\n",
    "    axes[1,1].set_ylabel('Valor de la métrica')\n",
    "    axes[1,1].legend(title='Modelo', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(COMP_DIR / 'metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 2. Tabla resumen de mejores modelos\n",
    "print(\"\\n📋 TABLA RESUMEN - MEJORES MODELOS POR EXPERIMENTO:\")\n",
    "print(\"─\" * 60)\n",
    "\n",
    "best_models = res_df.groupby('Experiment').apply(\n",
    "    lambda x: x.loc[x['RMSE'].idxmin()]\n",
    ")[['Model', 'RMSE', 'MAE', 'R2']]\n",
    "\n",
    "print(best_models.to_string())\n",
    "\n",
    "# 3. Comparación de curvas de aprendizaje\n",
    "if all_histories:\n",
    "    n_experiments = len(all_histories)\n",
    "    fig, axes = plt.subplots(\n",
    "        (n_experiments + 2) // 3, 3, \n",
    "        figsize=(15, 5 * ((n_experiments + 2) // 3))\n",
    "    )\n",
    "    axes = axes.flatten() if n_experiments > 3 else [axes]\n",
    "    \n",
    "    for idx, (key, history) in enumerate(all_histories.items()):\n",
    "        if idx < len(axes):\n",
    "            ax = axes[idx]\n",
    "            ax.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "            ax.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel('Loss')\n",
    "            ax.set_title(f'{key}')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ocultar ejes no utilizados\n",
    "    for idx in range(len(all_histories), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(COMP_DIR / 'all_learning_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 4. Resumen de hiperparámetros y tiempos de entrenamiento\n",
    "print(\"\\n⏱️ RESUMEN DE ENTRENAMIENTO:\")\n",
    "print(\"─\" * 80)\n",
    "\n",
    "for exp in EXPERIMENTS.keys():\n",
    "    metrics_dir = OUT_ROOT / exp / 'training_metrics'\n",
    "    if metrics_dir.exists():\n",
    "        print(f\"\\n🔬 Experimento: {exp}\")\n",
    "        for model in MODELS.keys():\n",
    "            hp_file = metrics_dir / f\"{model}_hyperparameters.json\"\n",
    "            history_file = metrics_dir / f\"{model}_history.json\"\n",
    "            \n",
    "            if hp_file.exists() and history_file.exists():\n",
    "                with open(hp_file, 'r') as f:\n",
    "                    hp = json.load(f)\n",
    "                with open(history_file, 'r') as f:\n",
    "                    hist = json.load(f)\n",
    "                \n",
    "                print(f\"\\n   • {model}:\")\n",
    "                print(f\"     - Parámetros del modelo: {hp['model_params']:,}\")\n",
    "                print(f\"     - Épocas entrenadas: {len(hist['loss'])}\")\n",
    "                print(f\"     - Mejor loss validación: {min(hist['val_loss']):.6f}\")\n",
    "                print(f\"     - Learning rate final: {hist['lr'][-1] if 'lr' in hist else 'N/A'}\")\n",
    "\n",
    "# 5. Generar GIF comparativo de predicciones\n",
    "print(\"\\n🎬 Generando GIF comparativo...\")\n",
    "\n",
    "for exp in EXPERIMENTS.keys():\n",
    "    exp_dir = OUT_ROOT / exp\n",
    "    if exp_dir.exists():\n",
    "        # Buscar todos los GIFs generados\n",
    "        gif_files = list(exp_dir.glob(\"*.gif\"))\n",
    "        if gif_files:\n",
    "            print(f\"\\n   📁 {exp}: {len(gif_files)} GIFs encontrados\")\n",
    "            for gif in gif_files:\n",
    "                print(f\"      • {gif.name}\")\n",
    "\n",
    "print(\"\\n✅ Visualizaciones comparativas completadas!\")\n",
    "print(f\"📂 Resultados guardados en: {COMP_DIR}\")\n",
    "\n",
    "# 6. Mostrar imágenes de predicción más recientes\n",
    "print(\"\\n🖼️ PREDICCIONES MÁS RECIENTES:\")\n",
    "for exp in EXPERIMENTS.keys():\n",
    "    exp_dir = OUT_ROOT / exp\n",
    "    if exp_dir.exists():\n",
    "        print(f\"\\n{exp}:\")\n",
    "        # Mostrar primera imagen de cada modelo\n",
    "        for model in MODELS.keys():\n",
    "            img_path = exp_dir / f\"{model}_1.png\"\n",
    "            if img_path.exists():\n",
    "                from IPython.display import Image, display\n",
    "                print(f\"  {model}:\")\n",
    "                display(Image(str(img_path), width=800))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
