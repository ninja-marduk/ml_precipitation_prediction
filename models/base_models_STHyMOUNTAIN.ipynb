{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f4e7925",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/main/models/base_models_STHyMOUNTAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af3744f4",
      "metadata": {
        "id": "af3744f4"
      },
      "source": [
        "# üìò Entrenamiento de Modelos Baseline para Predicci√≥n Espaciotemporal de Precipitaci√≥n Mensual STHyMOUNTAIN\n",
        "\n",
        "Este notebook implementa modelos baseline para la predicci√≥n de precipitaciones usando datos espaciotemporales."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a994be36",
      "metadata": {},
      "source": [
        "## üîç Implementaci√≥n de Modelos Avanzados y T√©cnicas de Validaci√≥n\n",
        "\n",
        "Adem√°s de los modelos tabulares baseline, implementaremos:\n",
        "\n",
        "1. **Optimizaci√≥n avanzada con Optuna** para los modelos tabulares XGBoost y LightGBM\n",
        "2. **Validaci√≥n robusta** mediante:\n",
        "   - Hold-Out Validation (ya implementada)\n",
        "   - Cross-Validation (k=5)\n",
        "   - Bootstrapping (100 muestras)\n",
        "3. **Modelos de Deep Learning** para capturar patrones espaciales y temporales:\n",
        "   - Redes CNN para patrones espaciales\n",
        "   - Redes ConvLSTM para patrones espaciotemporales\n",
        "\n",
        "El objetivo es proporcionar una evaluaci√≥n completa de diferentes enfoques de modelado para la predicci√≥n de precipitaci√≥n en regiones monta√±osas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "06416284",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06416284",
        "outputId": "a8e4c864-34e9-41b2-d5c3-e6ccaaf3699e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entorno configurado. Usando ruta base: ..\n",
            "Directorio para salida de modelos creado: ../models/output\n"
          ]
        }
      ],
      "source": [
        "# Configuraci√≥n del entorno (compatible con Colab y local)\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detectar si estamos en Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')   \n",
        "    # Si estamos en Colab, clonar el repositorio\n",
        "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
        "    %cd ml_precipitation_prediction\n",
        "    # Instalar dependencias necesarias\n",
        "    !pip install -r requirements.txt\n",
        "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn\n",
        "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
        "else:\n",
        "    # Si estamos en local, usar la ruta actual\n",
        "    if '/models' in os.getcwd():\n",
        "        BASE_PATH = Path('..')\n",
        "    else:\n",
        "        BASE_PATH = Path('.')\n",
        "\n",
        "print(f\"Entorno configurado. Usando ruta base: {BASE_PATH}\")\n",
        "\n",
        "# Si BASE_PATH viene como string, lo convertimos\n",
        "BASE_PATH = Path(BASE_PATH)\n",
        "\n",
        "# Ahora puedes concatenar correctamente\n",
        "model_output_dir = BASE_PATH / 'models' / 'output'\n",
        "model_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Directorio para salida de modelos creado: {model_output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "e47fb555",
      "metadata": {
        "id": "e47fb555"
      },
      "outputs": [],
      "source": [
        "# 1. Importaciones necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import optuna\n",
        "import pickle\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Importaciones para barras de progreso y mejora de visualizaci√≥n\n",
        "from tqdm.notebook import tqdm, trange\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import time\n",
        "\n",
        "# Configurar visualizaci√≥n m√°s atractiva\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"notebook\", font_scale=1.2)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "313434be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaciones adicionales para Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, save_model, load_model\n",
        "from tensorflow.keras.layers import (Dense, Dropout, Conv2D, Conv3D, ConvLSTM2D, BatchNormalization, \n",
        "                                   MaxPooling2D, Flatten, Input, concatenate, Reshape, TimeDistributed)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(\"TensorFlow versi√≥n:\", tf.__version__)\n",
        "\n",
        "# Configurar GPU si est√° disponible\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    print(f\"GPU disponible: {physical_devices}\")\n",
        "    # Permitir crecimiento de memoria seg√∫n sea necesario\n",
        "    for device in physical_devices:\n",
        "        tf.config.experimental.set_memory_growth(device, True)\n",
        "else:\n",
        "    print(\"No se detect√≥ GPU. Usando CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26215d90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "26215d90",
        "outputId": "ccb06926-e151-453f-a306-999f15566bd9"
      },
      "outputs": [],
      "source": [
        "# 2. Cargar el dataset NetCDF\n",
        "def load_dataset(file_path):\n",
        "    \"\"\"Carga un archivo NetCDF y lo convierte a pandas DataFrame\"\"\"\n",
        "    try:\n",
        "        # Cargar el archivo NetCDF con xarray\n",
        "        print(f\"Intentando cargar el archivo: {file_path}\")\n",
        "        ds = xr.open_dataset(file_path)\n",
        "        print(\"Archivo cargado exitosamente con xarray\")\n",
        "\n",
        "        # Mostrar informaci√≥n del dataset cargado\n",
        "        print(\"\\nInformaci√≥n del dataset:\")\n",
        "        print(ds.info())\n",
        "        print(\"\\nVariables disponibles:\")\n",
        "        for var_name in ds.data_vars:\n",
        "            print(f\"- {var_name}: {ds[var_name].shape}\")\n",
        "\n",
        "        # Convertir a DataFrame\n",
        "        df = ds.to_dataframe().reset_index()\n",
        "        return df, ds\n",
        "    except Exception as e:\n",
        "        print(f\"Error al cargar el archivo NetCDF: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Ruta al dataset\n",
        "data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features.nc'\n",
        "print(f\"Buscando archivo en: {data_file}\")\n",
        "\n",
        "# Cargar el dataset\n",
        "df, ds_original = load_dataset(data_file)\n",
        "\n",
        "# Verificar si se carg√≥ correctamente\n",
        "if df is not None:\n",
        "    print(f\"Dataset cargado con √©xito. Dimensiones: {df.shape}\")\n",
        "    print(\"\\nPrimeras filas del DataFrame:\")\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"No se pudo cargar el dataset. Verificar la ruta y el formato del archivo.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f0aebbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f0aebbc",
        "outputId": "48e82987-ff47-41e8-9f40-57e53cf90cf7"
      },
      "outputs": [],
      "source": [
        "# 3. Preparaci√≥n de los datos\n",
        "if df is not None:\n",
        "    # Identificar la columna objetivo (precipitaci√≥n)\n",
        "    target_column = 'total_precipitation'  # Ajustar si tiene otro nombre en tu dataset\n",
        "\n",
        "    # Ver si existe 'precip_target' o usar 'total_precipitation'\n",
        "    if 'total_precipitation' in df.columns:\n",
        "        target_column = 'total_precipitation'\n",
        "\n",
        "    print(f\"Columna objetivo identificada: {target_column}\")\n",
        "\n",
        "    # Separar variables predictoras y variable objetivo\n",
        "    feature_cols = [col for col in df.columns if col != target_column and not pd.isna(df[col]).all()]\n",
        "\n",
        "    # Eliminar columnas no num√©ricas para los modelos (como fechas o coordenadas si no se usan como features)\n",
        "    non_feature_cols = ['time', 'spatial_ref']\n",
        "    feature_cols = [col for col in feature_cols if col not in non_feature_cols]\n",
        "\n",
        "    # Eliminar filas con valores NaN\n",
        "    print(f\"Filas antes de eliminar NaN: {df.shape[0]}\")\n",
        "    df_clean = df.dropna(subset=[target_column] + feature_cols)\n",
        "    print(f\"Filas despu√©s de eliminar NaN: {df_clean.shape[0]}\")\n",
        "\n",
        "    # Separar features y target\n",
        "    X = df_clean[feature_cols]\n",
        "    y = df_clean[target_column]\n",
        "\n",
        "    print(f\"\\nFeatures seleccionadas ({len(feature_cols)}):\\n{feature_cols}\")\n",
        "    print(f\"\\nVariable objetivo: {target_column}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "da222af5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da222af5",
        "outputId": "579fbd57-a790-42dd-807a-e61fc1daacbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensiones del conjunto de entrenamiento: (1735008, 12)\n",
            "Dimensiones del conjunto de prueba: (433752, 12)\n",
            "Escalador guardado en models/output/scaler.pkl\n"
          ]
        }
      ],
      "source": [
        "# 4. Divisi√≥n del conjunto de datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Dimensiones del conjunto de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Dimensiones del conjunto de prueba: {X_test.shape}\")\n",
        "\n",
        "# 5. Estandarizaci√≥n de variables predictoras\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Guardar el scaler para uso futuro\n",
        "with open(model_output_dir / 'scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "print(\"Escalador guardado en models/output/scaler.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c5ba053b",
      "metadata": {
        "id": "c5ba053b"
      },
      "outputs": [],
      "source": [
        "# 6. Funciones de evaluaci√≥n y entrenamiento\n",
        "def evaluar_modelo(y_true, y_pred):\n",
        "    \"\"\"Eval√∫a el rendimiento de un modelo usando m√∫ltiples m√©tricas\"\"\"\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return rmse, mae, r2\n",
        "\n",
        "def entrenar_y_evaluar_modelo(modelo, nombre, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Entrena un modelo y eval√∫a su rendimiento con visualizaci√≥n del progreso\"\"\"\n",
        "    # Crear widget para mostrar informaci√≥n del proceso\n",
        "    display(HTML(f'<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\">' +\n",
        "                 f'<h3>üîÑ Entrenando modelo: {nombre}</h3>' +\n",
        "                 f'<div id=\"status_{nombre}\">Estado: Iniciando entrenamiento...</div>' +\n",
        "                 f'</div>'))\n",
        "    \n",
        "    # Tiempo de inicio\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Entrenar el modelo con seguimiento visual seg√∫n el tipo\n",
        "    if hasattr(modelo, 'fit_generator') or nombre in ['XGBoost', 'XGBoost_Optuna', 'LightGBM', 'LightGBM_Optuna']:\n",
        "        # Para modelos que soportan entrenamiento por lotes como XGBoost, LightGBM\n",
        "        print(f\"Entrenando {nombre} con visualizaci√≥n de progreso...\")\n",
        "        if hasattr(modelo, 'n_estimators'):\n",
        "            n_estimators = modelo.n_estimators\n",
        "            for i in tqdm(range(n_estimators), desc=f\"Entrenando {nombre}\"):\n",
        "                if i == 0:\n",
        "                    # Primera iteraci√≥n, ajuste inicial\n",
        "                    if nombre.startswith('LightGBM'):\n",
        "                        # LightGBM tiene par√°metro verbose\n",
        "                        temp_modelo = type(modelo)(n_estimators=1, **{k:v for k,v in modelo.get_params().items() \n",
        "                                                                 if k != 'n_estimators' and k != 'verbose'}, verbose=-1)\n",
        "                    else:\n",
        "                        temp_modelo = type(modelo)(n_estimators=1, **{k:v for k,v in modelo.get_params().items() \n",
        "                                                                if k != 'n_estimators'})\n",
        "                    temp_modelo.fit(X_train, y_train)\n",
        "                elif i == n_estimators - 1:\n",
        "                    # √öltima iteraci√≥n, ajuste completo\n",
        "                    modelo.fit(X_train, y_train)\n",
        "                \n",
        "                # Actualizar progreso visual\n",
        "                if i % max(1, n_estimators // 10) == 0:\n",
        "                    clear_output(wait=True)\n",
        "                    display(HTML(f'<div style=\"background-color:#f0f8ff; padding:10px; border-radius:5px;\">' +\n",
        "                                f'<h3>üîÑ Entrenando modelo: {nombre}</h3>' +\n",
        "                                f'<div id=\"status_{nombre}\">Estado: Progreso {i+1}/{n_estimators} estimadores ({((i+1)/n_estimators*100):.1f}%)</div>' +\n",
        "                                f'</div>'))\n",
        "                    time.sleep(0.1)  # Peque√±a pausa para actualizaci√≥n visual\n",
        "        else:\n",
        "            # Si no tiene n_estimators, entrenamiento directo\n",
        "            modelo.fit(X_train, y_train)\n",
        "    else:\n",
        "        # Para modelos est√°ndar como RandomForest\n",
        "        modelo.fit(X_train, y_train)\n",
        "    \n",
        "    # Tiempo de entrenamiento\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Visualizar tiempo de entrenamiento\n",
        "    display(HTML(f'<div style=\"background-color:#e6ffe6; padding:10px; border-radius:5px;\">' +\n",
        "                f'<h3>‚úÖ Entrenamiento completado: {nombre}</h3>' +\n",
        "                f'<div>Tiempo de entrenamiento: {training_time:.2f} segundos</div>' +\n",
        "                f'</div>'))\n",
        "    \n",
        "    print(f\"Evaluando rendimiento de {nombre}...\")\n",
        "    predicciones = modelo.predict(X_test)\n",
        "    rmse, mae, r2 = evaluar_modelo(y_test, predicciones)\n",
        "    \n",
        "    # Visualizar m√©tricas con estilo\n",
        "    display(HTML(f'<div style=\"background-color:#f5f5dc; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
        "                f'<h3>üìä M√©tricas para {nombre}</h3>' +\n",
        "                f'<table style=\"width:100%; text-align:left;\">' +\n",
        "                f'<tr><th>M√©trica</th><th>Valor</th></tr>' +\n",
        "                f'<tr><td>RMSE</td><td>{rmse:.4f}</td></tr>' +\n",
        "                f'<tr><td>MAE</td><td>{mae:.4f}</td></tr>' +\n",
        "                f'<tr><td>R¬≤</td><td>{r2:.4f}</td></tr>' +\n",
        "                f'</table></div>'))\n",
        "    \n",
        "    return modelo, (rmse, mae, r2)\n",
        "\n",
        "def guardar_modelo(modelo, nombre):\n",
        "    \"\"\"Guarda un modelo entrenado en disco\"\"\"\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"{nombre}_{timestamp}.pkl\"\n",
        "    with open(model_output_dir / filename, 'wb') as f:\n",
        "        pickle.dump(modelo, f)\n",
        "    \n",
        "    # Visualizar confirmaci√≥n de guardado\n",
        "    display(HTML(f'<div style=\"background-color:#e6ffee; padding:10px; border-radius:5px; margin-top:10px;\">' +\n",
        "                f'<h3>üíæ Modelo guardado</h3>' +\n",
        "                f'<div>Modelo <b>{nombre}</b> guardado como: {filename}</div>' +\n",
        "                f'</div>'))\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e59f9865",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Entrenamiento de modelos base sin optimizaci√≥n\n",
        "\n",
        "# Inicializar diccionarios para almacenar resultados y modelos\n",
        "resultados_base = {}  # Para almacenar m√©tricas (RMSE, MAE, R2)\n",
        "modelos_base = {}     # Para almacenar instancias de modelos\n",
        "modelos_guardados = {} # Para almacenar nombres de archivos guardados\n",
        "\n",
        "print(\"\\nüîç Entrenando modelos baseline sin optimizaci√≥n de hiperpar√°metros...\")\n",
        "\n",
        "# 1. Modelo RandomForest b√°sico\n",
        "print(\"\\nEntrenando RandomForest base...\")\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model, rf_metrics = entrenar_y_evaluar_modelo(\n",
        "    rf_model, 'RandomForest', X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "resultados_base['RandomForest'] = rf_metrics\n",
        "modelos_base['RandomForest'] = rf_model\n",
        "modelo_file = guardar_modelo(rf_model, 'RandomForest')\n",
        "modelos_guardados['RandomForest'] = modelo_file\n",
        "\n",
        "# Visualizar importancia de caracter√≠sticas para RandomForest\n",
        "plt.figure(figsize=(12, 6))\n",
        "feat_importances = rf_model.feature_importances_\n",
        "indices = np.argsort(feat_importances)[::-1]\n",
        "plt.bar(range(len(indices)), feat_importances[indices], color='skyblue')\n",
        "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
        "plt.title('Importancia de Caracter√≠sticas - RandomForest')\n",
        "plt.tight_layout()\n",
        "plt.savefig(model_output_dir / 'randomforest_feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# 2. Modelo XGBoost b√°sico\n",
        "print(\"\\nEntrenando XGBoost base...\")\n",
        "xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
        "xgb_model, xgb_metrics = entrenar_y_evaluar_modelo(\n",
        "    xgb_model, 'XGBoost', X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "resultados_base['XGBoost'] = xgb_metrics\n",
        "modelos_base['XGBoost'] = xgb_model\n",
        "modelo_file = guardar_modelo(xgb_model, 'XGBoost')\n",
        "modelos_guardados['XGBoost'] = modelo_file\n",
        "\n",
        "# Visualizar importancia de caracter√≠sticas para XGBoost\n",
        "plt.figure(figsize=(12, 6))\n",
        "feat_importances = xgb_model.feature_importances_\n",
        "indices = np.argsort(feat_importances)[::-1]\n",
        "plt.bar(range(len(indices)), feat_importances[indices], color='coral')\n",
        "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
        "plt.title('Importancia de Caracter√≠sticas - XGBoost')\n",
        "plt.tight_layout()\n",
        "plt.savefig(model_output_dir / 'xgboost_feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# 3. Modelo LightGBM b√°sico\n",
        "print(\"\\nEntrenando LightGBM base...\")\n",
        "lgbm_model = LGBMRegressor(n_estimators=100, random_state=42)\n",
        "lgbm_model, lgbm_metrics = entrenar_y_evaluar_modelo(\n",
        "    lgbm_model, 'LightGBM', X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "resultados_base['LightGBM'] = lgbm_metrics\n",
        "modelos_base['LightGBM'] = lgbm_model\n",
        "modelo_file = guardar_modelo(lgbm_model, 'LightGBM')\n",
        "modelos_guardados['LightGBM'] = modelo_file\n",
        "\n",
        "# Visualizar importancia de caracter√≠sticas para LightGBM\n",
        "plt.figure(figsize=(12, 6))\n",
        "feat_importances = lgbm_model.feature_importances_\n",
        "indices = np.argsort(feat_importances)[::-1]\n",
        "plt.bar(range(len(indices)), feat_importances[indices], color='lightgreen')\n",
        "plt.xticks(range(len(indices)), [feature_cols[i] for i in indices], rotation=90)\n",
        "plt.title('Importancia de Caracter√≠sticas - LightGBM')\n",
        "plt.tight_layout()\n",
        "plt.savefig(model_output_dir / 'lightgbm_feature_importance.png')\n",
        "plt.show()\n",
        "\n",
        "# Comparar resultados de modelos base\n",
        "print(\"\\nüîç Comparaci√≥n de modelos base sin optimizaci√≥n:\")\n",
        "temp_df = pd.DataFrame(resultados_base, index=['RMSE', 'MAE', 'R2']).T\n",
        "print(\"\\nOrdenados por RMSE (menor es mejor):\")\n",
        "display(temp_df.sort_values('RMSE'))\n",
        "\n",
        "# Visualizar comparaci√≥n de RMSE\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=temp_df.index, y=temp_df['RMSE'])\n",
        "plt.title('Comparaci√≥n de RMSE - Modelos Base')\n",
        "plt.ylabel('RMSE (menor es mejor)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(model_output_dir / 'baseline_rmse_comparison.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "462e5648",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizaci√≥n adaptativa de memoria RAM para Optuna (compatible con Colab y Local)\n",
        "import gc\n",
        "import psutil\n",
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "import optuna\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from IPython.display import HTML\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def get_available_memory():\n",
        "    \"\"\"Detecta la memoria RAM disponible en el sistema actual (Colab o local)\"\"\"\n",
        "    try:\n",
        "        # Obtener memoria disponible usando psutil\n",
        "        mem_info = psutil.virtual_memory()\n",
        "        available_mb = mem_info.available / (1024 * 1024)  # Convertir a MB\n",
        "        total_mb = mem_info.total / (1024 * 1024)  # Convertir a MB\n",
        "        \n",
        "        # Detectar si estamos en Colab\n",
        "        in_colab = 'google.colab' in sys.modules\n",
        "        if in_colab:\n",
        "            # En Colab, limitar el uso a un porcentaje conservador\n",
        "            print(\"Entorno detectado: Google Colab\")\n",
        "            max_usage_percent = 70  # Usar m√°ximo 70% de la memoria disponible en Colab\n",
        "        else:\n",
        "            # En local, tambi√©n ser conservador pero un poco menos\n",
        "            print(\"Entorno detectado: Local\")\n",
        "            max_usage_percent = 80  # Usar m√°ximo 80% de la memoria disponible en local\n",
        "        \n",
        "        # Calcular memoria m√°xima a usar\n",
        "        max_memory_mb = available_mb * (max_usage_percent / 100)\n",
        "        \n",
        "        print(f\"Memoria total del sistema: {total_mb:.0f} MB\")\n",
        "        print(f\"Memoria disponible: {available_mb:.0f} MB\")\n",
        "        print(f\"Memoria m√°xima a utilizar: {max_memory_mb:.0f} MB ({max_usage_percent}% de la disponible)\")\n",
        "        \n",
        "        return max_memory_mb\n",
        "    except Exception as e:\n",
        "        print(f\"Error al obtener informaci√≥n de memoria: {e}\")\n",
        "        # Valor conservador por defecto\n",
        "        return 2000  # 2GB por defecto si no se puede detectar\n",
        "\n",
        "def calculate_optimal_batch_size(mem_per_sample_mb, max_memory_mb, min_batch=10):\n",
        "    \"\"\"Calcula el tama√±o √≥ptimo de lote basado en la memoria disponible\"\"\"\n",
        "    # Estimar cu√°nta memoria usa cada muestra (con margen de seguridad)\n",
        "    optimal_batch = int(max_memory_mb / (mem_per_sample_mb * 1.5))\n",
        "    return max(optimal_batch, min_batch)\n",
        "\n",
        "def configure_optuna_for_memory_efficiency(max_memory_mb, model_name):\n",
        "    \"\"\"Configura par√°metros para Optuna basados en la memoria disponible\"\"\"\n",
        "    # Estimar n√∫mero de trials y paralelismo basado en la memoria\n",
        "    if max_memory_mb < 1000:  # Menos de 1GB disponible\n",
        "        n_trials = 15  # Reducir n√∫mero de trials\n",
        "        n_jobs = 1     # Sin paralelismo\n",
        "        use_sqlite = True  # Usar SQLite para ahorrar memoria\n",
        "    elif max_memory_mb < 4000:  # Menos de 4GB\n",
        "        n_trials = 25  # N√∫mero moderado de trials\n",
        "        n_jobs = min(2, os.cpu_count() or 2)  # Paralelismo limitado\n",
        "        use_sqlite = True  # Usar SQLite\n",
        "    else:  # 4GB o m√°s\n",
        "        n_trials = 30  # N√∫mero normal de trials\n",
        "        n_jobs = min(3, os.cpu_count() or 2)  # Mejor paralelismo\n",
        "        use_sqlite = True  # Mantener SQLite para estabilidad\n",
        "    \n",
        "    # Definir configuraci√≥n espec√≠fica por modelo\n",
        "    model_config = {\n",
        "        'RandomForest': {\n",
        "            'n_estimators_max': min(500, int(50 + max_memory_mb / 100)),  # Adaptar a memoria\n",
        "            'max_depth_max': min(50, int(10 + max_memory_mb / 200))\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'tree_method': 'hist',  # M√©todo eficiente en memoria\n",
        "            'max_depth_max': min(12, 3 + int(max_memory_mb / 1000)),\n",
        "            'grow_policy': 'lossguide'  # M√°s eficiente en memoria\n",
        "        },\n",
        "        'LightGBM': {\n",
        "            'max_bin': min(255, 63 + int(max_memory_mb / 100)),  # Adaptar precisi√≥n\n",
        "            'num_leaves_max': min(150, 31 + int(max_memory_mb / 100))\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Crear almacenamiento SQLite si es necesario\n",
        "    storage = None\n",
        "    if use_sqlite:\n",
        "        # Crear directorio para almacenamiento Optuna si no existe\n",
        "        os.makedirs(model_output_dir / 'optuna_storage', exist_ok=True)\n",
        "        storage = f\"sqlite:///{model_output_dir}/optuna_storage/optuna_{model_name}_{int(time.time())}.db\"\n",
        "    \n",
        "    # Devolver toda la configuraci√≥n optimizada\n",
        "    return {\n",
        "        'n_trials': n_trials,\n",
        "        'n_jobs': n_jobs,\n",
        "        'storage': storage,\n",
        "        'model_config': model_config.get(model_name, {})\n",
        "    }\n",
        "\n",
        "def memory_efficient_objective_factory(model_name, X_train, y_train, max_memory_mb, model_config):\n",
        "    \"\"\"Crea una funci√≥n objetivo para Optuna optimizada para memoria\"\"\"\n",
        "    # Configurar espacio de b√∫squeda adaptado a la memoria disponible\n",
        "    def objective(trial):\n",
        "        # Limpiar memoria antes de cada trial\n",
        "        gc.collect()\n",
        "        \n",
        "        # Configuraci√≥n de par√°metros espec√≠ficos para cada modelo, adaptados a la memoria\n",
        "        if model_name == 'RandomForest':\n",
        "            n_estimators_max = model_config.get('n_estimators_max', 500)\n",
        "            max_depth_max = model_config.get('max_depth_max', 50)\n",
        "            \n",
        "            params = {\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 100, n_estimators_max),\n",
        "                'max_depth': trial.suggest_int('max_depth', 5, max_depth_max),\n",
        "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
        "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
        "                'random_state': 42,\n",
        "                # Par√°metros para optimizar memoria\n",
        "                'verbose': 0,\n",
        "                'n_jobs': 1 if max_memory_mb < 2000 else 2  # Limitar paralelismo seg√∫n memoria\n",
        "            }\n",
        "            model = RandomForestRegressor(**params)\n",
        "        \n",
        "        elif model_name == 'XGBoost':\n",
        "            max_depth_max = model_config.get('max_depth_max', 12)\n",
        "            tree_method = model_config.get('tree_method', 'hist')\n",
        "            grow_policy = model_config.get('grow_policy', 'lossguide')\n",
        "            \n",
        "            params = {\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, max_depth_max),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
        "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
        "                'random_state': 42,\n",
        "                # Optimizaci√≥n de memoria\n",
        "                'tree_method': tree_method,\n",
        "                'grow_policy': grow_policy,\n",
        "                'verbosity': 0,\n",
        "                'nthread': 1 if max_memory_mb < 2000 else 2  # Adaptar seg√∫n memoria\n",
        "            }\n",
        "            model = XGBRegressor(**params)\n",
        "        \n",
        "        elif model_name == 'LightGBM':\n",
        "            num_leaves_max = model_config.get('num_leaves_max', 150)\n",
        "            max_bin = model_config.get('max_bin', 63)\n",
        "            \n",
        "            params = {\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "                'num_leaves': trial.suggest_int('num_leaves', 20, num_leaves_max),\n",
        "                'max_depth': trial.suggest_int('max_depth', -1, 15),\n",
        "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100),\n",
        "                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
        "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
        "                'random_state': 42,\n",
        "                # Par√°metros para optimizar memoria\n",
        "                'verbose': -1,\n",
        "                'max_bin': max_bin,\n",
        "                'num_threads': 1 if max_memory_mb < 2000 else 2\n",
        "            }\n",
        "            model = LGBMRegressor(**params)\n",
        "        else:\n",
        "            raise ValueError(f\"Modelo no soportado: {model_name}\")\n",
        "        \n",
        "        # Evaluaci√≥n con validaci√≥n cruzada adaptada a memoria disponible\n",
        "        try:\n",
        "            # Reducir el n√∫mero de folds si hay poca memoria\n",
        "            n_folds = 3 if max_memory_mb < 2000 else 5\n",
        "            from sklearn.model_selection import KFold\n",
        "            \n",
        "            kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "            scores = []\n",
        "            \n",
        "            for train_idx, val_idx in kf.split(X_train):\n",
        "                X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
        "                y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "                \n",
        "                # Entrenar modelo\n",
        "                model.fit(X_fold_train, y_fold_train)\n",
        "                \n",
        "                # Evaluar y liberar memoria inmediatamente\n",
        "                y_pred = model.predict(X_fold_val)\n",
        "                rmse = np.sqrt(mean_squared_error(y_fold_val, y_pred))\n",
        "                scores.append(rmse)\n",
        "                \n",
        "                # Limpieza agresiva de memoria\n",
        "                del X_fold_train, X_fold_val, y_fold_train, y_fold_val, y_pred\n",
        "                gc.collect()\n",
        "            \n",
        "            # Liberar modelo tambi√©n\n",
        "            del model\n",
        "            gc.collect()\n",
        "            \n",
        "            return np.mean(scores)\n",
        "        except Exception as e:\n",
        "            print(f\"Error en evaluaci√≥n: {e}\")\n",
        "            return float('inf')\n",
        "    \n",
        "    return objective\n",
        "\n",
        "def run_memory_efficient_optimization(model_name='XGBoost', X_train_scaled=None, y_train=None, X_test_scaled=None, y_test=None):\n",
        "    \"\"\"Ejecuta optimizaci√≥n con Optuna adaptada a la memoria disponible\"\"\"\n",
        "    print(f\"\\nüß† Iniciando optimizaci√≥n adaptativa de memoria para {model_name}...\")\n",
        "    \n",
        "    # Verificar si se proporcionaron los datos\n",
        "    if X_train_scaled is None or y_train is None or X_test_scaled is None or y_test is None:\n",
        "        raise ValueError(\"Es necesario proporcionar los conjuntos de datos X_train_scaled, y_train, X_test_scaled y y_test\")\n",
        "    \n",
        "    # 1. Detectar memoria disponible\n",
        "    max_memory_mb = get_available_memory()\n",
        "    \n",
        "    # 2. Configurar Optuna seg√∫n memoria disponible\n",
        "    optuna_config = configure_optuna_for_memory_efficiency(max_memory_mb, model_name)\n",
        "    \n",
        "    # 3. Mostrar configuraci√≥n\n",
        "    print(f\"\\nConfiguraci√≥n para {model_name}:\")\n",
        "    print(f\"- Trials: {optuna_config['n_trials']}\")\n",
        "    print(f\"- Paralelismo: {optuna_config['n_jobs']} jobs\")\n",
        "    print(f\"- Almacenamiento: {'SQLite' if optuna_config['storage'] else 'En memoria'}\")\n",
        "    print(f\"- Configuraci√≥n espec√≠fica: {optuna_config['model_config']}\")\n",
        "    \n",
        "    # 4. Crear estudio Optuna\n",
        "    from optuna.pruners import MedianPruner\n",
        "    pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
        "    \n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        pruner=pruner,\n",
        "        storage=optuna_config['storage'],\n",
        "        study_name=f\"{model_name}_memory_optimized\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    \n",
        "    # 5. Crear funci√≥n objetivo eficiente en memoria\n",
        "    objective = memory_efficient_objective_factory(\n",
        "        model_name, \n",
        "        X_train_scaled, \n",
        "        y_train, \n",
        "        max_memory_mb,\n",
        "        optuna_config['model_config']\n",
        "    )\n",
        "    \n",
        "    # 6. Callback para liberar memoria peri√≥dicamente\n",
        "    def gc_callback(study, trial):\n",
        "        if trial.number % 3 == 0:  # Cada 3 trials\n",
        "            gc.collect()\n",
        "    \n",
        "    # 7. Visualizaci√≥n inicial\n",
        "    color_map = {\n",
        "        'RandomForest': '#e6f3ff',  # Azul claro\n",
        "        'XGBoost': '#fff0e6',      # Naranja claro\n",
        "        'LightGBM': '#e6fff2'      # Verde claro\n",
        "    }\n",
        "    display(HTML(f'<div style=\"background-color:{color_map.get(model_name, \"#f5f5f5\")}; padding:10px; border-radius:5px;\">' +\n",
        "                f'<h3>üîç Optimizaci√≥n adaptativa de memoria - {model_name}</h3>' +\n",
        "                f'<div>Memoria m√°xima a utilizar: {max_memory_mb:.0f} MB</div>' +\n",
        "                f'<div>Trials planeados: {optuna_config[\"n_trials\"]}</div>' +\n",
        "                f'</div>'))\n",
        "    \n",
        "    # 8. Iniciar optimizaci√≥n\n",
        "    callback = OptimizationProgressCallback(\n",
        "        optuna_config['n_trials'], f\"{model_name} (RAM optimizada)\"\n",
        "    )\n",
        "    \n",
        "    study.optimize(\n",
        "        objective, \n",
        "        n_trials=optuna_config['n_trials'],\n",
        "        n_jobs=optuna_config['n_jobs'],\n",
        "        gc_after_trial=True,\n",
        "        callbacks=[callback, gc_callback]\n",
        "    )\n",
        "    \n",
        "    # 9. Obtener mejores par√°metros\n",
        "    best_params = study.best_params\n",
        "    best_value = study.best_value\n",
        "    \n",
        "    # 10. Visualizar resultados\n",
        "    display(HTML(f'<div style=\"background-color:{color_map.get(model_name, \"#f5f5f5\")}; padding:10px; border-radius:5px;\">' +\n",
        "                f'<h3>‚úÖ Optimizaci√≥n completada - {model_name}</h3>' +\n",
        "                f'<div><b>Mejor RMSE:</b> {best_value:.4f}</div>' +\n",
        "                f'<div><b>Mejores par√°metros:</b> {str(best_params)}</div>' +\n",
        "                f'</div>'))\n",
        "    \n",
        "    # 11. Entrenar modelo final con los mejores par√°metros\n",
        "    print(\"\\nEntrenando modelo final con los mejores par√°metros...\")\n",
        "    \n",
        "    if model_name == 'RandomForest':\n",
        "        best_model = RandomForestRegressor(**best_params, random_state=42)\n",
        "    elif model_name == 'XGBoost':\n",
        "        best_model = XGBRegressor(**best_params, random_state=42)\n",
        "    elif model_name == 'LightGBM':\n",
        "        best_model = LGBMRegressor(**best_params, random_state=42)\n",
        "    \n",
        "    # 12. Evaluar modelo final\n",
        "    better_model, metrics = entrenar_y_evaluar_modelo(\n",
        "        best_model, f'{model_name}_RAM_Opt', X_train_scaled, y_train, X_test_scaled, y_test\n",
        "    )\n",
        "    \n",
        "    # 13. Guardar resultados\n",
        "    resultados_base[f'{model_name}_RAM_Opt'] = metrics\n",
        "    modelo_file = guardar_modelo(better_model, f'{model_name}_RAM_Opt')\n",
        "    modelos_guardados[f'{model_name}_RAM_Opt'] = modelo_file\n",
        "    \n",
        "    return best_params, better_model, metrics\n",
        "\n",
        "# Ejecutar solo esta celda para iniciar la optimizaci√≥n adaptativa de memoria para XGBoost\n",
        "print(\"\\nüîç Ejecutando optimizaci√≥n adaptativa de memoria RAM para XGBoost...\")\n",
        "xgb_params, xgb_model, xgb_metrics = run_memory_efficient_optimization('XGBoost', X_train_scaled, y_train, X_test_scaled, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "precipitation_prediction",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
