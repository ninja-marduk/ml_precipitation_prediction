{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4e7925",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/main/models/base_models_STHyMOUNTAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3744f4",
   "metadata": {
    "id": "af3744f4"
   },
   "source": [
    "# 📘 Entrenamiento de Modelos Baseline para Predicción Espaciotemporal de Precipitación Mensual STHyMOUNTAIN\n",
    "\n",
    "Este notebook implementa modelos baseline para la predicción de precipitaciones usando datos espaciotemporales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a994be36",
   "metadata": {},
   "source": [
    "## 🔍 Implementación de Modelos Avanzados y Técnicas de Validación\n",
    "\n",
    "Además de los modelos tabulares baseline, implementaremos:\n",
    "\n",
    "1. **Optimización avanzada con Optuna** para los modelos tabulares XGBoost y LightGBM\n",
    "2. **Validación robusta** mediante:\n",
    "   - Hold-Out Validation (ya implementada)\n",
    "   - Cross-Validation (k=5)\n",
    "   - Bootstrapping (100 muestras)\n",
    "3. **Modelos de Deep Learning** para capturar patrones espaciales y temporales:\n",
    "   - Redes CNN para patrones espaciales\n",
    "   - Redes ConvLSTM para patrones espaciotemporales\n",
    "\n",
    "El objetivo es proporcionar una evaluación completa de diferentes enfoques de modelado para la predicción de precipitación en regiones montañosas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06416284",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06416284",
    "outputId": "a8e4c864-34e9-41b2-d5c3-e6ccaaf3699e"
   },
   "outputs": [],
   "source": [
    "# Configuración del entorno (compatible con Colab y local)\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Detectar si estamos en Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')   \n",
    "    # Si estamos en Colab, clonar el repositorio\n",
    "    !git clone https://github.com/ninja-marduk/ml_precipitation_prediction.git\n",
    "    %cd ml_precipitation_prediction\n",
    "    # Instalar dependencias necesarias\n",
    "    !pip install -r requirements.txt\n",
    "    !pip install xarray netCDF4 optuna matplotlib seaborn lightgbm xgboost scikit-learn\n",
    "    BASE_PATH = '/content/drive/MyDrive/ml_precipitation_prediction'\n",
    "else:\n",
    "    # Si estamos en local, usar la ruta actual\n",
    "    if '/models' in os.getcwd():\n",
    "        BASE_PATH = Path('..')\n",
    "    else:\n",
    "        BASE_PATH = Path('.')\n",
    "\n",
    "print(f\"Entorno configurado. Usando ruta base: {BASE_PATH}\")\n",
    "\n",
    "# Si BASE_PATH viene como string, lo convertimos\n",
    "BASE_PATH = Path(BASE_PATH)\n",
    "\n",
    "# Ahora puedes concatenar correctamente\n",
    "model_output_dir = BASE_PATH / 'models' / 'output'\n",
    "model_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directorio para salida de modelos creado: {model_output_dir}\")\n",
    "\n",
    "# Implementación de resiliencia para interacción con Google Drive y restauración de datos\n",
    "def backup_dataframe(df, backup_path):\n",
    "    \"\"\"Guarda un DataFrame como respaldo en formato Parquet.\"\"\"\n",
    "    try:\n",
    "        df.to_parquet(backup_path, index=False)\n",
    "        print(f\"Respaldo del DataFrame guardado en: {backup_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar respaldo del DataFrame: {e}\")\n",
    "\n",
    "def restore_dataframe(backup_path):\n",
    "    \"\"\"Restaura un DataFrame desde un archivo de respaldo en formato Parquet.\"\"\"\n",
    "    try:\n",
    "        if backup_path.exists():\n",
    "            df_restored = pd.read_parquet(backup_path)\n",
    "            print(f\"DataFrame restaurado desde: {backup_path}\")\n",
    "            return df_restored\n",
    "        else:\n",
    "            print(f\"No se encontró el archivo de respaldo en: {backup_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al restaurar el DataFrame: {e}\")\n",
    "        return None\n",
    "\n",
    "# Ruta para respaldo temporal del DataFrame\n",
    "temp_dir = BASE_PATH / 'data' / 'output' / 'temp'\n",
    "temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "temp_file_path = temp_dir / 'dataframe_backup.parquet'\n",
    "\n",
    "# Respaldo inicial del DataFrame principal\n",
    "if 'df' in locals() and df is not None:\n",
    "    backup_dataframe(df, temp_file_path)\n",
    "\n",
    "# Modificar interacción con Google Drive para reintentos\n",
    "max_retries = 3\n",
    "retry_delay = 5  # segundos\n",
    "\n",
    "def mount_google_drive():\n",
    "    \"\"\"Intenta montar Google Drive con reintentos.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"Google Drive montado exitosamente.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error al montar Google Drive (intento {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "    print(\"No se pudo montar Google Drive después de varios intentos.\")\n",
    "    return False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not mount_google_drive():\n",
    "        print(\"Usando datos en memoria o restaurando desde respaldo local.\")\n",
    "        df = restore_dataframe(temp_file_path)\n",
    "\n",
    "# Restaurar modelos guardados en caso de fallo\n",
    "model_files = {\n",
    "    'RandomForest': model_output_dir / 'RandomForest.pkl',\n",
    "    'XGBoost': model_output_dir / 'XGBoost.pkl',\n",
    "    'LightGBM': model_output_dir / 'LightGBM.pkl'\n",
    "}\n",
    "\n",
    "def load_saved_model(model_name, model_path):\n",
    "    \"\"\"Carga un modelo guardado desde disco.\"\"\"\n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            print(f\"Modelo {model_name} cargado desde: {model_path}\")\n",
    "            return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Inicializar `modelos_base` como un diccionario vacío\n",
    "modelos_base = {}\n",
    "\n",
    "# Intentar cargar modelos guardados\n",
    "for model_name, model_path in model_files.items():\n",
    "    if model_name not in modelos_base:\n",
    "        modelos_base[model_name] = load_saved_model(model_name, model_path)\n",
    "\n",
    "# Implementación de resiliencia para modelos CNN y ConvLSTM\n",
    "\n",
    "# Respaldo y restauración de modelos CNN y ConvLSTM\n",
    "cnn_model_path = model_output_dir / 'cnn_model.h5'\n",
    "convlstm_model_path = model_output_dir / 'convlstm_model.h5'\n",
    "\n",
    "def backup_model(model, model_path):\n",
    "    \"\"\"Guarda un modelo de Keras como respaldo.\"\"\"\n",
    "    try:\n",
    "        model.save(model_path)\n",
    "        print(f\"Modelo respaldado en: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al guardar respaldo del modelo: {e}\")\n",
    "\n",
    "def restore_model(model_path):\n",
    "    \"\"\"Restaura un modelo de Keras desde un archivo de respaldo.\"\"\"\n",
    "    try:\n",
    "        if model_path.exists():\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "            print(f\"Modelo restaurado desde: {model_path}\")\n",
    "            return model\n",
    "        else:\n",
    "            print(f\"No se encontró el archivo de respaldo en: {model_path}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al restaurar el modelo: {e}\")\n",
    "        return None\n",
    "\n",
    "# Respaldo inicial de modelos si existen\n",
    "if 'cnn_model' in locals() and cnn_model is not None:\n",
    "    backup_model(cnn_model, cnn_model_path)\n",
    "if 'convlstm_model' in locals() and convlstm_model is not None:\n",
    "    backup_model(convlstm_model, convlstm_model_path)\n",
    "\n",
    "# Restaurar modelos en caso de fallo\n",
    "if 'cnn_model' not in locals() or cnn_model is None:\n",
    "    cnn_model = restore_model(cnn_model_path)\n",
    "if 'convlstm_model' not in locals() or convlstm_model is None:\n",
    "    convlstm_model = restore_model(convlstm_model_path)\n",
    "\n",
    "# Modificar interacción con Google Drive para reintentos\n",
    "max_retries = 3\n",
    "retry_delay = 5  # segundos\n",
    "\n",
    "def mount_google_drive():\n",
    "    \"\"\"Intenta montar Google Drive con reintentos.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"Google Drive montado exitosamente.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error al montar Google Drive (intento {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "    print(\"No se pudo montar Google Drive después de varios intentos.\")\n",
    "    return False\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not mount_google_drive():\n",
    "        print(\"Usando datos en memoria o restaurando desde respaldo local para modelos CNN y ConvLSTM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fb555",
   "metadata": {
    "id": "e47fb555"
   },
   "outputs": [],
   "source": [
    "# 1. Importaciones necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import optuna\n",
    "import pickle\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importaciones para barras de progreso y mejora de visualización\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import time\n",
    "\n",
    "# Configurar visualización más atractiva\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145dccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ST-HybridWaveStack: Deep Stacking CNN, LSTM, GRU, BLSTM + ELM con clusterización de elevación y datos CHIRPS reales\n",
    "\n",
    "# ==============================================\n",
    "# 1. Configuraciones generales y carga de datos\n",
    "# ==============================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, LSTM, GRU, Bidirectional, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# ==============================================\n",
    "# 2. Cargar dataset real con y sin cluster_elevation\n",
    "# ==============================================\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Carga un archivo NetCDF y lo convierte a pandas DataFrame\"\"\"\n",
    "    try:\n",
    "        # Cargar el archivo NetCDF con xarray\n",
    "        print(f\"Intentando cargar el archivo: {file_path}\")\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        print(\"Archivo cargado exitosamente con xarray\")\n",
    "\n",
    "        # Mostrar información del dataset cargado\n",
    "        print(\"\\nInformación del dataset:\")\n",
    "        print(ds.info())\n",
    "        print(\"\\nVariables disponibles:\")\n",
    "        for var_name in ds.data_vars:\n",
    "            print(f\"- {var_name}: {ds[var_name].shape}\")\n",
    "\n",
    "        # Convertir a DataFrame\n",
    "        df = ds.to_dataframe().reset_index()\n",
    "        return df, ds\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el archivo NetCDF: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Ruta al dataset\n",
    "data_file_path = BASE_PATH / 'data' / 'output'\n",
    "data_file = BASE_PATH / 'data' / 'output' / 'complete_dataset_with_features_with_clusters_elevation.nc'\n",
    "print(f\"Buscando archivo en: {data_file}\")\n",
    "\n",
    "# Cargar el dataset\n",
    "df, ds_with_clusters = load_dataset(data_file)\n",
    "\n",
    "# Verificar si se cargó correctamente\n",
    "if df is not None:\n",
    "    print(f\"Dataset cargado con éxito. Dimensiones: {df.shape}\")\n",
    "    print(\"\\nPrimeras filas del DataFrame:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"No se pudo cargar el dataset. Verificar la ruta y el formato del archivo.\")\n",
    "\n",
    "# Variables a usar\n",
    "vars_to_use = ['total_precipitation', 'month_sin', 'month_cos', 'elevation', 'slope', 'aspect', 'cluster_elevation']\n",
    "\n",
    "# Extraer dimensiones\n",
    "data_array = ds_with_clusters[vars_to_use].to_array().transpose('time', 'latitude', 'longitude', 'variable')\n",
    "data_np = data_array.values\n",
    "\n",
    "# Preprocesar la variable categórica 'cluster_elevation'\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Identificar el índice de la variable categórica\n",
    "cluster_elevation_index = vars_to_use.index('cluster_elevation')\n",
    "\n",
    "# Aplicar LabelEncoder para convertir 'low', 'medium', 'high', etc., a valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "# Extract the 3D slice for 'cluster_elevation'\n",
    "categorical_feature_slice = data_np[..., cluster_elevation_index]\n",
    "original_shape = categorical_feature_slice.shape\n",
    "\n",
    "# Flatten the slice for LabelEncoder, then reshape back\n",
    "# Ensure the input to fit_transform is 1D\n",
    "flattened_slice = categorical_feature_slice.ravel()\n",
    "encoded_values = label_encoder.fit_transform(flattened_slice)\n",
    "\n",
    "# Assign the reshaped encoded values back to data_np\n",
    "data_np[..., cluster_elevation_index] = encoded_values.reshape(original_shape)\n",
    "\n",
    "# Convertir a tipo float para evitar problemas de dtype\n",
    "data_np = data_np.astype(float)\n",
    "\n",
    "# Variable objetivo\n",
    "target = ds_with_clusters['total_precipitation'].values  # [time, lat, lon]\n",
    "\n",
    "# Reestructurar para convertir a muestras individuales\n",
    "samples, lat, lon, n_features = data_np.shape\n",
    "X = data_np.reshape(samples, lat * lon, n_features)\n",
    "y = target.reshape(samples, lat * lon)\n",
    "\n",
    "# Quitar NaNs (si los hay)\n",
    "mask = ~np.isnan(y)\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "# ==============================================\n",
    "# 3. Crear ventanas para predicción a 12 meses\n",
    "# ==============================================\n",
    "def create_sequences(X, y, window=12):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - window):\n",
    "        X_seq.append(X[i:i+window])\n",
    "        y_seq.append(y[i+window])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X, y, window=12)\n",
    "\n",
    "# Dividir train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "# ==============================================\n",
    "# 4. Clase ELM\n",
    "# ==============================================\n",
    "class ELMRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_hidden=1000, activation=np.tanh):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.activation = activation\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        input_size = X.shape[1]\n",
    "        self.input_weights = np.random.normal(size=(input_size, self.n_hidden))\n",
    "        self.biases = np.random.normal(size=(self.n_hidden,))\n",
    "        H = self.activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        self.output_weights = np.dot(np.linalg.pinv(H), y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        H = self.activation(np.dot(X, self.input_weights) + self.biases)\n",
    "        return np.dot(H, self.output_weights)\n",
    "\n",
    "# ==============================================\n",
    "# 5. Modelos base\n",
    "# ==============================================\n",
    "def build_model(model_type, input_shape, output_neurons):\n",
    "    model = Sequential()\n",
    "    if model_type == 'LSTM':\n",
    "        model.add(LSTM(64, input_shape=input_shape))\n",
    "    elif model_type == 'GRU':\n",
    "        model.add(GRU(64, input_shape=input_shape))\n",
    "    elif model_type == 'BLSTM':\n",
    "        model.add(Bidirectional(LSTM(64), input_shape=input_shape))\n",
    "    elif model_type == 'CNN':\n",
    "        # input_shape for Reshape is (timesteps, features_per_timestep)\n",
    "        # Reshape to (timesteps, features_per_timestep, 1) to be (height, width, channels) for Conv2D\n",
    "        model.add(Reshape((*input_shape, 1), input_shape=input_shape))\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu', padding='same')) # Added padding\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Flatten())\n",
    "    model.add(Dense(output_neurons)) # Use output_neurons\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# ==============================================\n",
    "# 6. Entrenamiento de modelos base\n",
    "# ==============================================\n",
    "# Ensure preds_train and preds_test are initialized (they are from notebook state, but good practice)\n",
    "if 'preds_train' not in globals():\n",
    "    preds_train = []\n",
    "if 'preds_test' not in globals():\n",
    "    preds_test = []\n",
    "\n",
    "# Reshape X_train and X_test for the models\n",
    "# Original X_train shape: (num_sequences, window, lat*lon, n_features)\n",
    "# Reshape to: (num_sequences, window, lat*lon*n_features)\n",
    "X_train_feed = X_train.reshape((X_train.shape[0], X_train.shape[1], -1))\n",
    "X_test_feed = X_test.reshape((X_test.shape[0], X_test.shape[1], -1))\n",
    "\n",
    "# Define input shape for models: (timesteps, features_per_timestep)\n",
    "input_shape_for_build = (X_train_feed.shape[1], X_train_feed.shape[2])\n",
    "\n",
    "# Define output neurons for models\n",
    "# Since y_train is 1D (each sequence predicts a single value), output_neurons should be 1.\n",
    "# The original comment \"# y_train shape: (num_sequences, lat*lon)\" seems inconsistent with current y_train shape.\n",
    "output_neurons_for_build = 1\n",
    "\n",
    "# Early stopping callback (assuming 'es' is defined from notebook state)\n",
    "# If not, define it: es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "if 'es' not in globals():\n",
    "    print(\"Warning: EarlyStopping callback 'es' not found in global scope. Defining a default one.\")\n",
    "    es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "plot_dir = \"ST_HybridWaveStack/plots\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# model_names is defined in the notebook state\n",
    "for name in model_names:\n",
    "    print(f\"\\nTraining {name} model...\")\n",
    "    \n",
    "    model = build_model(name, input_shape_for_build, output_neurons_for_build)\n",
    "    \n",
    "    print(f\"Input shape for model: {input_shape_for_build}\")\n",
    "    print(f\"Output neurons for model: {output_neurons_for_build}\")\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(X_train_feed, y_train,\n",
    "                        epochs=50,  # Example: 50 epochs\n",
    "                        batch_size=32, # Example: batch size 32\n",
    "                        validation_split=0.2, # Example: 20% validation split\n",
    "                        callbacks=[es],\n",
    "                        verbose=1)\n",
    "    \n",
    "    # Make predictions\n",
    "    pred_train_model = model.predict(X_train_feed).flatten()\n",
    "    pred_test_model = model.predict(X_test_feed).flatten()\n",
    "\n",
    "    # Guardar curvas de aprendizaje\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.title(f'Learning Curve {name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{plot_dir}/learning_{name}.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Append predictions\n",
    "    preds_train.append(pred_train_model)\n",
    "    preds_test.append(pred_test_model)\n",
    "\n",
    "print(\"\\nBase models training complete.\")\n",
    "print(f\"Number of training prediction sets: {len(preds_train)}\")\n",
    "print(f\"Number of test prediction sets: {len(preds_test)}\")\n",
    "if preds_train:\n",
    "    print(f\"Shape of first training prediction set: {preds_train[0].shape}\")\n",
    "if preds_test:\n",
    "    print(f\"Shape of first test prediction set: {preds_test[0].shape}\")\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 7. Meta-modelo ELM\n",
    "# ==============================================\n",
    "# Flatten predictions from base models to ensure they are 2D\n",
    "preds_train_flat = [pred.flatten() for pred in preds_train]\n",
    "preds_test_flat = [pred.flatten() for pred in preds_test]\n",
    "\n",
    "# Stack flattened predictions to create input for ELMRegressor\n",
    "X_train_elm = np.column_stack(preds_train_flat)\n",
    "X_test_elm = np.column_stack(preds_test_flat)\n",
    "\n",
    "# Flatten target variables to match the input shape\n",
    "y_train_elm = y_train.flatten()\n",
    "y_test_elm = y_test.flatten()\n",
    "\n",
    "elm = ELMRegressor(n_hidden=500)\n",
    "elm.fit(X_train_elm, y_train_elm)\n",
    "y_pred_elm = elm.predict(X_test_elm)\n",
    "\n",
    "# ==============================================\n",
    "# 8. Evaluación\n",
    "# ==============================================\n",
    "def print_metrics(y_true, y_pred, label):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Predicho ELM')\n",
    "plt.title('Comparación ELM')\n",
    "plot_dir = \"ST_HybridWaveStack/plots\" # Define plot_dir if not already defined in this scope\n",
    "os.makedirs(plot_dir, exist_ok=True) # Ensure directory exists\n",
    "plt.savefig(f\"{plot_dir}/scatter_elm.png\")\n",
    "plt.close()\n",
    "\n",
    "# ==============================================\n",
    "# ==============================================\n",
    "# 9. Visualización resultados\n",
    "# ==============================================\n",
    "plt.figure()\n",
    "plt.scatter(y_test, y_pred_elm, alpha=0.7)\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Predicho ELM')\n",
    "plt.title('Comparación ELM')\n",
    "plt.savefig(\"ST_HybridWaveStack/plots/scatter_elm.png\")\n",
    "plt.close()\n",
    "\n",
    "# ==============================================\n",
    "# 10. Comentario sobre cluster_elevation\n",
    "# ==============================================\n",
    "# La variable 'cluster_elevation' se usa como feature categórica. Puede:\n",
    "# - Mejorar la capacidad de generalización del modelo en regiones montañosas\n",
    "# - Actuar como representación abstracta del terreno\n",
    "# Se debe incluir como variable adicional normalizada o one-hot si fuera necesario.\n",
    "\n",
    "print(\"Pipeline completo ejecutado. Resultados y gráficos guardados.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "precipitation_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
