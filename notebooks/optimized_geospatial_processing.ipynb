{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3de28d6",
   "metadata": {},
   "source": [
    "# Procesamiento Geoespacial Optimizado\n",
    "\n",
    "Este notebook demuestra técnicas optimizadas para procesar grandes conjuntos de datos geoespaciales sin que el kernel de Jupyter se bloquee."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968bd078",
   "metadata": {},
   "source": [
    "## 1. Configuración inicial y carga de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629337e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Asegurarse de que el directorio principal está en el path\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "# Importar utilidades propias\n",
    "from utils.memory_utils import (\n",
    "    print_memory_usage, limit_memory_usage, set_memory_limit, \n",
    "    load_data_with_chunks, process_by_chunks, mask_and_align_coordinates_safe\n",
    ")\n",
    "from utils.custom_logger import setup_logger\n",
    "\n",
    "# Configurar logging\n",
    "log_file = os.path.join(project_dir, 'logs', f'log-{datetime.now().strftime(\"%Y-%m-%d\")}.log')\n",
    "logger = setup_logger(log_file)\n",
    "\n",
    "# Configurar matplotlib para visualización\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf322723",
   "metadata": {},
   "source": [
    "## 2. Configurar límites de memoria para prevenir bloqueos\n",
    "\n",
    "Limitar la memoria que puede usar el kernel ayuda a prevenir que se bloquee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89162124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer límite al 80% de la memoria disponible\n",
    "set_memory_limit(80)\n",
    "\n",
    "# Ver uso actual de memoria\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ba61b",
   "metadata": {},
   "source": [
    "## 3. Cargar datos con chunkings\n",
    "\n",
    "En lugar de cargar todo el conjunto de datos en memoria, cargamos porciones manejables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a613ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas a los datasets (ajustar según tu configuración)\n",
    "CHIRPS_MONTHLY_PATH = \"/Users/riperez/Conda/anaconda3/doc/precipitation/output/boyaca_region_monthly_sum.nc\"\n",
    "DEM_PATH = \"/Users/riperez/Conda/anaconda3/doc/precipitation/output/dem_boyaca_90.nc\"\n",
    "\n",
    "# Cargar datasets con chunks\n",
    "dem_ds = load_data_with_chunks(DEM_PATH, {'latitude': 500, 'longitude': 500})\n",
    "chirps_ds = load_data_with_chunks(CHIRPS_MONTHLY_PATH, {'latitude': 500, 'longitude': 500, 'time': 10})\n",
    "\n",
    "# Información básica sobre los datasets\n",
    "print(\"\\nInformación del dataset DEM:\")\n",
    "print(dem_ds.info())\n",
    "\n",
    "print(\"\\nInformación del dataset CHIRPS:\")\n",
    "print(chirps_ds.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a04268c",
   "metadata": {},
   "source": [
    "## 4. Procesamiento por chunks\n",
    "\n",
    "En lugar de procesar todo el conjunto de datos a la vez, lo dividimos en trozos más pequeños y los procesamos uno por uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62610bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el dataset DEM en chunks más pequeños\n",
    "dem_chunks = process_by_chunks(dem_ds, chunk_size=400)\n",
    "\n",
    "# Ver cuántos chunks tenemos\n",
    "print(f\"Número de chunks DEM: {len(dem_chunks)}\")\n",
    "print(f\"Tamaño del primer chunk: {dem_chunks[0].dims}\")\n",
    "\n",
    "# Mostrar memoria después de chunking\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa1dc1",
   "metadata": {},
   "source": [
    "## 5. Alineación de coordenadas segura\n",
    "\n",
    "Esta es una versión optimizada de mask_and_align_coordinates que evita bloqueos del kernel al procesar por lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541348fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alinear coordenadas entre datasets DEM y CHIRPS\n",
    "dem_aligned, chirps_aligned = mask_and_align_coordinates_safe(dem_ds, chirps_ds, batch_size=300)\n",
    "\n",
    "# Ver información sobre los datasets alineados\n",
    "print(\"\\nDimensiones DEM alineado:\")\n",
    "print(dem_aligned.dims)\n",
    "\n",
    "print(\"\\nDimensiones CHIRPS alineado:\")\n",
    "print(chirps_aligned.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e1c6e",
   "metadata": {},
   "source": [
    "## 6. Análisis estadístico seguro con control de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats_safely(dataset, variable_name):\n",
    "    \"\"\"Calcula estadísticas básicas de manera segura para la memoria\"\"\"\n",
    "    # Ver uso de memoria antes de empezar\n",
    "    print_memory_usage()\n",
    "    \n",
    "    stats = {}\n",
    "    \n",
    "    # Intentar calcular estadísticas básicas\n",
    "    try:\n",
    "        stats['mean'] = float(dataset[variable_name].mean().values)\n",
    "        # Liberar memoria después de cada operación\n",
    "        gc.collect()\n",
    "        \n",
    "        stats['std'] = float(dataset[variable_name].std().values)\n",
    "        gc.collect()\n",
    "        \n",
    "        stats['min'] = float(dataset[variable_name].min().values)\n",
    "        gc.collect()\n",
    "        \n",
    "        stats['max'] = float(dataset[variable_name].max().values)\n",
    "        gc.collect()\n",
    "        \n",
    "        # Para percentiles, usamos numpy con .values para controlar mejor la memoria\n",
    "        # Convertimos a valores numpy, aplicamos función y liberamos memoria rápido\n",
    "        values = dataset[variable_name].values.flatten()\n",
    "        mask = ~np.isnan(values)\n",
    "        valid_values = values[mask]\n",
    "        stats['percentile_25'] = np.percentile(valid_values, 25)\n",
    "        stats['median'] = np.percentile(valid_values, 50)\n",
    "        stats['percentile_75'] = np.percentile(valid_values, 75)\n",
    "        \n",
    "        # Liberar memoria explícitamente\n",
    "        del values, mask, valid_values\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculando estadísticas para {variable_name}: {e}\")\n",
    "    \n",
    "    # Ver uso de memoria después de terminar\n",
    "    print_memory_usage()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Calcular estadísticas para DEM\n",
    "if 'DEM' in dem_aligned:\n",
    "    dem_stats = calculate_stats_safely(dem_aligned, 'DEM')\n",
    "    print(\"Estadísticas DEM:\")\n",
    "    for key, value in dem_stats.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Calcular estadísticas para precipitación\n",
    "if 'total_precipitation' in chirps_aligned:\n",
    "    precip_stats = calculate_stats_safely(chirps_aligned, 'total_precipitation')\n",
    "    print(\"\\nEstadísticas de Precipitación:\")\n",
    "    for key, value in precip_stats.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c775661",
   "metadata": {},
   "source": [
    "## 7. Visualización segura de datos\n",
    "\n",
    "Visualizar datasets grandes puede consumir mucha memoria. Aquí demostramos cómo hacerlo de forma segura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e68d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_safely(dataset, variable, time_index=0):\n",
    "    \"\"\"Visualiza datos geoespaciales de forma segura\"\"\"\n",
    "    try:\n",
    "        # Si tenemos dimensión de tiempo, seleccionar un slice\n",
    "        if 'time' in dataset.dims:\n",
    "            # Tomar solo un punto de tiempo para visualizar\n",
    "            data_to_plot = dataset[variable].isel(time=time_index)\n",
    "            title = f\"{variable} - Tiempo: {dataset.time.values[time_index]}\"\n",
    "        else:\n",
    "            data_to_plot = dataset[variable]\n",
    "            title = f\"{variable}\"\n",
    "        \n",
    "        # Reducir resolución para visualización si el dataset es muy grande\n",
    "        if data_to_plot.size > 1_000_000:  # 1 millón de puntos\n",
    "            logging.info(\"Dataset muy grande, reduciendo resolución para visualización\")\n",
    "            # Tomar 1 de cada N puntos\n",
    "            step = max(1, int(data_to_plot.size / 500_000))\n",
    "            if 'latitude' in data_to_plot.dims and 'longitude' in data_to_plot.dims:\n",
    "                data_to_plot = data_to_plot.isel(\n",
    "                    latitude=slice(None, None, step),\n",
    "                    longitude=slice(None, None, step)\n",
    "                )\n",
    "        \n",
    "        # Crear figura\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Graficar\n",
    "        data_to_plot.plot(cmap='viridis')\n",
    "        plt.title(title)\n",
    "        plt.colorbar(label=variable)\n",
    "        \n",
    "        # Mostrar\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Liberar memoria\n",
    "        plt.close()\n",
    "        del data_to_plot\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al visualizar {variable}: {e}\")\n",
    "\n",
    "# Visualizar DEM\n",
    "if 'DEM' in dem_aligned:\n",
    "    plot_safely(dem_aligned, 'DEM')\n",
    "\n",
    "# Visualizar precipitación para diferentes puntos de tiempo\n",
    "if 'total_precipitation' in chirps_aligned and 'time' in chirps_aligned.dims:\n",
    "    # Visualizar primero, medio y último punto temporal\n",
    "    time_points = [0, len(chirps_aligned.time)//2, -1]\n",
    "    for t in time_points:\n",
    "        plot_safely(chirps_aligned, 'total_precipitation', time_index=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc4373",
   "metadata": {},
   "source": [
    "## 8. Cálculo de correlaciones por bloques\n",
    "\n",
    "Calcular correlaciones entre datos de precipitación y elevación puede consumir mucha memoria cuando se hace todo a la vez. Aquí lo hacemos por bloques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1013c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_by_blocks(dem_data, precip_data, block_size=200):\n",
    "    \"\"\"Calcula correlación entre DEM y precipitación por bloques\"\"\"\n",
    "    # Verificar que tenemos las dimensiones espaciales correctas\n",
    "    dem_lat_name = 'latitude' if 'latitude' in dem_data.dims else 'lat'\n",
    "    dem_lon_name = 'longitude' if 'longitude' in dem_data.dims else 'lon'\n",
    "    \n",
    "    precip_lat_name = 'latitude' if 'latitude' in precip_data.dims else 'lat'\n",
    "    precip_lon_name = 'longitude' if 'longitude' in precip_data.dims else 'lon'\n",
    "    \n",
    "    # Colapsar la dimensión temporal para precipitación (promedio)\n",
    "    if 'time' in precip_data.dims:\n",
    "        precip_mean = precip_data['total_precipitation'].mean(dim='time')\n",
    "    else:\n",
    "        precip_mean = precip_data['total_precipitation']\n",
    "    \n",
    "    # Inicializar array para correlaciones\n",
    "    corr_data = np.zeros((len(precip_data[precip_lat_name]), len(precip_data[precip_lon_name])))\n",
    "    corr_data[:] = np.nan  # Inicializar con NaN\n",
    "    \n",
    "    # Obtener arrays numpy para cálculos más eficientes\n",
    "    dem_values = dem_data['DEM'].values\n",
    "    precip_values = precip_mean.values\n",
    "    \n",
    "    # Procesar por bloques para evitar problemas de memoria\n",
    "    for i in range(0, dem_values.shape[0], block_size):\n",
    "        i_end = min(i + block_size, dem_values.shape[0])\n",
    "        \n",
    "        for j in range(0, dem_values.shape[1], block_size):\n",
    "            j_end = min(j + block_size, dem_values.shape[1])\n",
    "            \n",
    "            # Extraer bloque\n",
    "            dem_block = dem_values[i:i_end, j:j_end]\n",
    "            precip_block = precip_values[i:i_end, j:j_end]\n",
    "            \n",
    "            # Calcular correlación para este bloque si hay suficientes datos válidos\n",
    "            mask = ~(np.isnan(dem_block) | np.isnan(precip_block))\n",
    "            if np.sum(mask) > 10:  # Asegurar que tenemos suficientes puntos\n",
    "                dem_flat = dem_block[mask]\n",
    "                precip_flat = precip_block[mask]\n",
    "                \n",
    "                # Calcular coeficiente de correlación\n",
    "                if len(dem_flat) > 0:\n",
    "                    corr = np.corrcoef(dem_flat, precip_flat)[0, 1]\n",
    "                    # Almacenar solo en el centro del bloque para simplificar\n",
    "                    center_i = i + (i_end - i) // 2\n",
    "                    center_j = j + (j_end - j) // 2\n",
    "                    if center_i < corr_data.shape[0] and center_j < corr_data.shape[1]:\n",
    "                        corr_data[center_i, center_j] = corr\n",
    "            \n",
    "            # Liberar memoria\n",
    "            if (j_end % (block_size * 5)) == 0:\n",
    "                gc.collect()\n",
    "                print_memory_usage()\n",
    "    \n",
    "    # Crear DataArray con las correlaciones\n",
    "    corr_da = xr.DataArray(\n",
    "        corr_data,\n",
    "        dims=[precip_lat_name, precip_lon_name],\n",
    "        coords={\n",
    "            precip_lat_name: precip_data[precip_lat_name],\n",
    "            precip_lon_name: precip_data[precip_lon_name]\n",
    "        },\n",
    "        name=\"dem_precip_correlation\"\n",
    "    )\n",
    "    \n",
    "    return corr_da\n",
    "\n",
    "# Calcular correlaciones si tenemos datos alineados\n",
    "if 'DEM' in dem_aligned and 'total_precipitation' in chirps_aligned:\n",
    "    print(\"Calculando correlación entre elevación y precipitación...\")\n",
    "    correlation = calculate_correlation_by_blocks(dem_aligned, chirps_aligned, block_size=150)\n",
    "    \n",
    "    # Visualizar correlación\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    correlation.plot(cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    plt.title('Correlación entre Elevación y Precipitación')\n",
    "    plt.colorbar(label='Coeficiente de Correlación')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac2947",
   "metadata": {},
   "source": [
    "## 9. Guardar resultados con control de memoria\n",
    "\n",
    "Finalmente, guardamos resultados pero asegurándonos de no sobrecargar la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9721871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_safely(dataset, filepath, chunks=None):\n",
    "    \"\"\"Guarda un dataset en archivo netCDF de forma segura\"\"\"\n",
    "    logging.info(f\"Guardando dataset en {filepath}...\")\n",
    "    \n",
    "    try:\n",
    "        # Si el dataset es muy grande, asegurar chunking para escritura\n",
    "        if chunks is not None and dataset.nbytes > 1e9:  # 1 GB\n",
    "            dataset = dataset.chunk(chunks)\n",
    "        \n",
    "        # Crear directorio si no existe\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n",
    "        \n",
    "        # Guardar archivo\n",
    "        dataset.to_netcdf(filepath)\n",
    "        logging.info(f\"Dataset guardado exitosamente en {filepath}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error guardando dataset: {e}\")\n",
    "        return False\n",
    "\n",
    "# Guardar resultados con control de chunks\n",
    "output_dir = os.path.join(project_dir, 'data', 'transformation', 'build')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Guardar dataset de correlación\n",
    "if 'correlation' in locals():\n",
    "    correlation_ds = xr.Dataset({'dem_precip_correlation': correlation})\n",
    "    save_dataset_safely(\n",
    "        correlation_ds, \n",
    "        os.path.join(output_dir, 'dem_precip_correlation_optimized.nc'),\n",
    "        chunks={'latitude': 200, 'longitude': 200}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa199e",
   "metadata": {},
   "source": [
    "## 10. Limpiar recursos y liberar memoria\n",
    "\n",
    "Es importante limpiar recursos al final del notebook para evitar bloqueos al ejecutar otras celdas o notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cerrar todos los datasets abiertos\n",
    "for var_name in ['dem_ds', 'chirps_ds', 'dem_aligned', 'chirps_aligned']:\n",
    "    if var_name in locals():\n",
    "        try:\n",
    "            locals()[var_name].close()\n",
    "            logging.info(f\"Dataset {var_name} cerrado exitosamente\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Cerrar figuras de matplotlib\n",
    "plt.close('all')\n",
    "\n",
    "# Forzar recolección de basura\n",
    "gc.collect()\n",
    "\n",
    "# Verificar uso final de memoria\n",
    "mem_usage = print_memory_usage()\n",
    "print(f\"\\nProcesamiento finalizado. Uso final de memoria: {mem_usage:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
