import os
import sys
import numpy as np
import pandas as pd
import xarray as xr
import logging
from datetime import datetime

# Agregar el directorio ra√≠z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Importar utilidades
from utils.feature_validator import (
    BASE_FEATURES, ELEVATION_CLUSTER_FEATURES, LAG_FEATURES, IMF_FEATURES,
    validate_features_for_model, MODEL_FEATURE_REQUIREMENTS
)
from utils.dataloader_utils import improved_build_dataloaders
from models.model_factory import create_model

# Configurar logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"logs/models_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def run_experiment(model_type, dataset_path, output_dir, config=None):
    """
    Ejecuta un experimento completo con un modelo espec√≠fico.
    
    Args:
        model_type: Tipo de modelo a utilizar
        dataset_path: Ruta al dataset
        output_dir: Directorio de salida
        config: Configuraci√≥n adicional
        
    Returns:
        Resultados del experimento
    """
    logger.info(f"üöÄ Iniciando experimento con modelo: {model_type}")
    logger.info(f"   Dataset: {dataset_path}")
    logger.info(f"   Salida: {output_dir}")
    
    # Cargar dataset
    logger.info("üìä Cargando dataset...")
    try:
        ds = xr.open_dataset(dataset_path)
        logger.info(f"   ‚úÖ Dataset cargado con √©xito. Dimensiones: {ds.dims}")
    except Exception as e:
        logger.error(f"   ‚ùå Error al cargar dataset: {e}")
        return None
    
    # Determinar caracter√≠sticas necesarias seg√∫n el modelo
    if model_type not in MODEL_FEATURE_REQUIREMENTS:
        logger.error(f"‚ùå Modelo desconocido: {model_type}")
        return None
    
    required_features = MODEL_FEATURE_REQUIREMENTS[model_type]['required']
    logger.info(f"üìã El modelo requiere {len(required_features)} caracter√≠sticas")
    
    # Construir dataloaders con la versi√≥n mejorada
    logger.info("üîÑ Construyendo dataloaders...")
    feature_data, feature_shapes, available_features = improved_build_dataloaders(
        ds, required_features, time_window=12, batch_size=32
    )
    
    # Verificar que todas las caracter√≠sticas requeridas est√©n disponibles
    is_valid, missing_features = validate_features_for_model(model_type, available_features)
    if not is_valid:
        logger.error(f"‚ùå No se puede continuar. Faltan caracter√≠sticas: {missing_features}")
        return None
    
    # Definir formas de entrada/salida seg√∫n las caracter√≠sticas disponibles
    n_features = len(available_features)
    time_window = 12  # Se puede configurar
    spatial_shape = feature_shapes[available_features[0]][1:]  # Tomar forma espacial de la primera caracter√≠stica
    
    input_shape = (time_window,) + spatial_shape + (n_features,)
    output_shape = spatial_shape + (1,)  # Forma de salida para predicci√≥n de precipitaci√≥n
    
    logger.info(f"üìê Forma de entrada: {input_shape}, Forma de salida: {output_shape}")
    
    # Crear modelo
    logger.info("üèóÔ∏è Creando modelo...")
    model = create_model(model_type, available_features, input_shape, output_shape, config)
    
    if model is None:
        logger.error("‚ùå No se pudo crear el modelo")
        return None
    
    logger.info("‚úÖ Modelo creado correctamente")
    
    # Aqu√≠ ir√≠a el entrenamiento y evaluaci√≥n del modelo
    # ...
    
    logger.info("‚úÖ Experimento completado con √©xito")
    return {
        "model_type": model_type,
        "features_used": available_features,
        "input_shape": input_shape,
        "output_shape": output_shape,
        "model": model
    }

def run_all_experiments():
    """
    Ejecuta experimentos con todos los modelos disponibles.
    """
    # Dataset sint√©tico para simulaci√≥n
    dataset_path = "data/output/synthetic_dataset.nc"
    output_dir = "models/output/experiments"
    
    # Crear dataset sint√©tico para pruebas
    create_synthetic_dataset(dataset_path)
    
    # Definir modelos a probar
    models_to_test = [
        'ConvGRU-ED',
        'ConvGRU-ED-KCE',
        'ConvGRU-ED-KCE-PAFC',
        'AE-FUSION-ConvGRU-ED-KCE-PAFC-MHA',
        'AE-FUSION-ConvGRU-ED-KCE-PAFC-MHA-TopoMask'
    ]
    
    results = {}
    
    for model_type in models_to_test:
        logger.info(f"\n{'='*80}\nüîç EJECUTANDO EXPERIMENTO CON MODELO: {model_type}\n{'='*80}")
        result = run_experiment(model_type, dataset_path, output_dir)
        results[model_type] = result
        
        # Imprimir resultado
        if result:
            logger.info(f"‚úÖ Experimento con {model_type} completado con √©xito")
        else:
            logger.error(f"‚ùå Experimento con {model_type} fall√≥")
    
    # Resumen final
    logger.info("\nüèÅ RESUMEN DE EXPERIMENTOS")
    logger.info(f"{'='*80}")
    for model_type, result in results.items():
        status = "‚úÖ √âXITO" if result else "‚ùå FALLO"
        logger.info(f"{status} - {model_type}")
    
    return results

def create_synthetic_dataset(output_path):
    """
    Crea un dataset sint√©tico para pruebas con todas las caracter√≠sticas necesarias.
    """
    logger.info(f"üî® Creando dataset sint√©tico en: {output_path}")
    
    # Crear directorios si no existen
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # Dimensiones
    time = pd.date_range('2000-01-01', periods=100, freq='M')
    lat = np.linspace(0, 10, 20)
    lon = np.linspace(0, 10, 20)
    
    # Variables
    data_vars = {}
    
    # 1. Variables base
    for feature in BASE_FEATURES:
        if feature in ['month_sin', 'month_cos', 'doy_sin', 'doy_cos']:
            # Variables c√≠clicas
            data_vars[feature] = (('time', 'lat', 'lon'), np.random.random((len(time), len(lat), len(lon))))
        else:
            # Variables normales
            data_vars[feature] = (('time', 'lat', 'lon'), np.random.random((len(time), len(lat), len(lon))))
    
    # 2. Variable categ√≥rica cluster_elevation (high, medium, low)
    cluster_data = np.random.choice(['high', 'medium', 'low'], size=(len(lat), len(lon)))
    data_vars['cluster_elevation'] = (('lat', 'lon'), cluster_data)
    
    # 3. Variables de lag
    for feature in LAG_FEATURES:
        data_vars[feature] = (('time', 'lat', 'lon'), np.random.random((len(time), len(lat), len(lon))))
    
    # 4. Variables IMF
    for feature in IMF_FEATURES:
        data_vars[feature] = (('time', 'lat', 'lon'), np.random.random((len(time), len(lat), len(lon))))
    
    # 5. Variable objetivo (precipitaci√≥n total)
    data_vars['total_precipitation'] = (('time', 'lat', 'lon'), np.random.random((len(time), len(lat), len(lon))))
    
    # Crear dataset
    ds = xr.Dataset(
        data_vars=data_vars,
        coords={
            'time': time,
            'lat': lat,
            'lon': lon
        }
    )
    
    # Guardar dataset
    try:
        ds.to_netcdf(output_path)
        logger.info(f"‚úÖ Dataset guardado en: {output_path}")
    except Exception as e:
        logger.error(f"‚ùå Error al guardar dataset: {e}")

if __name__ == "__main__":
    # Crear directorio de logs si no existe
    os.makedirs("logs", exist_ok=True)
    
    logger.info("üöÄ Iniciando script de ejecuci√≥n de modelos")
    results = run_all_experiments()
    logger.info("‚úÖ Script completado") 