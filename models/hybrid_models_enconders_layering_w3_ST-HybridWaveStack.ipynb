{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninja-marduk/ml_precipitation_prediction/blob/feature%2Fhybrid-models/models/hybrid_models_enconders_layering_w3_ST-HybridWaveStack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7b96f3ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7b96f3ea",
        "outputId": "975c1075-a5e6-4368-a96e-62b0c2abe62a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ñ∂Ô∏è Base path: /Users/riperez/Conda/anaconda3/envs/precipitation_prediction/github.com/ml_precipitation_prediction\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-23 10:44:04,015 INFO ‚öô CPU cores: 10, RAM libre: 3.7 GB\n",
            "2025-05-23 10:44:04,017 INFO üìÇ Cargando datasets‚Ä¶\n",
            "2025-05-23 10:44:05,630 INFO REF_DATE fuera de rango; usando √∫ltimo mes: 2025-02\n",
            "2025-05-23 10:44:05,631 INFO ‚ñ∂ Procesando CEEMDAN_high\n",
            "2025-05-23 10:44:06,866 INFO ‚è© Cargado modelo: CEEMDAN_high_w3_ref2025-02.keras\n",
            "2025-05-23 10:44:07,295 INFO ‚ñ∂ Procesando CEEMDAN_medium\n",
            "2025-05-23 10:44:08,259 INFO ‚è© Cargado modelo: CEEMDAN_medium_w3_ref2025-02.keras\n",
            "2025-05-23 10:44:08,715 INFO ‚ñ∂ Procesando CEEMDAN_low\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mEl kernel se bloque√≥ al ejecutar c√≥digo en la celda actual o en una celda anterior. \n",
            "\u001b[1;31mRevise el c√≥digo de las celdas para identificar una posible causa del error. \n",
            "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqu√≠</a> para obtener m√°s informaci√≥n. \n",
            "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener m√°s detalles."
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Entrenamiento Multi‚Äêrama con GRU encoder‚Äìdecoder y Transformer para low,\n",
        "validaci√≥n y forecast parametrizables, meta‚Äêmodelo XGBoost (stacking low H=1),\n",
        "paralelizaci√≥n, trazabilidad y l√≠mites del departamento de Boyac√°.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "# 0) Detectar entorno (Local / Colab)\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    BASE_PATH = Path(\"/content/drive/MyDrive/ml_precipitation_prediction\")\n",
        "    !pip install -q xarray netCDF4 optuna seaborn cartopy xgboost ace_tools_open\n",
        "else:\n",
        "    BASE_PATH = Path.cwd()\n",
        "    for p in [BASE_PATH, *BASE_PATH.parents]:\n",
        "        if (p/\".git\").exists():\n",
        "            BASE_PATH = p\n",
        "            break\n",
        "print(f\"‚ñ∂Ô∏è Base path: {BASE_PATH}\")\n",
        "\n",
        "# 1) Suprimir warnings irrelevantes\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "from cartopy.io import DownloadWarning\n",
        "warnings.filterwarnings(\"ignore\", category=DownloadWarning)\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel(\"ERROR\")\n",
        "\n",
        "# 2) Par√°metros configurables\n",
        "INPUT_WINDOW    = 60          # meses de entrada\n",
        "OUTPUT_HORIZON  = 3           # meses de validaci√≥n y forecast\n",
        "REF_DATE        = \"2025-03\"   # fecha de referencia yyyy-mm\n",
        "MAX_EPOCHS      = 300\n",
        "PATIENCE_ES     = 30\n",
        "LR_FACTOR       = 0.5\n",
        "LR_PATIENCE     = 10\n",
        "DROPOUT         = 0.1\n",
        "\n",
        "# 3) Rutas y logger\n",
        "MODEL_DIR    = BASE_PATH/\"models\"/\"output\"/\"trained_models\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FEATURES_NC  = BASE_PATH/\"models\"/\"output\"/\"features_fusion_branches.nc\"\n",
        "FULL_NC      = BASE_PATH/\"data\"/\"output\"/\"complete_dataset_with_features_with_clusters_elevation_with_windows.nc\"\n",
        "SHP_USER     = Path(\"/mnt/data/MGN_Departamento.shp\")\n",
        "BOYACA_SHP   = SHP_USER if SHP_USER.exists() else BASE_PATH/\"data\"/\"input\"/\"shapes\"/\"MGN_Departamento.shp\"\n",
        "RESULTS_CSV  = MODEL_DIR/f\"metrics_w{OUTPUT_HORIZON}_ref{REF_DATE}.csv\"\n",
        "IMAGE_DIR    = MODEL_DIR/\"images\"\n",
        "IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 4) Imports principales\n",
        "import numpy            as np\n",
        "import pandas           as pd\n",
        "import xarray           as xr\n",
        "import geopandas        as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs      as ccrs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import psutil\n",
        "from joblib import cpu_count\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, GRU, RepeatVector, TimeDistributed, Dense,\n",
        "    MultiHeadAttention, Add, LayerNormalization, Flatten\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "# 5) Recursos hardware\n",
        "CORES     = cpu_count()\n",
        "AVAIL_RAM = psutil.virtual_memory().available / (1024**3)\n",
        "gpus      = tf.config.list_physical_devices(\"GPU\")\n",
        "USE_GPU   = bool(gpus)\n",
        "if USE_GPU:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    logger.info(f\"üñ• GPU disponible: {gpus[0].name}\")\n",
        "else:\n",
        "    tf.config.threading.set_inter_op_parallelism_threads(CORES)\n",
        "    tf.config.threading.set_intra_op_parallelism_threads(CORES)\n",
        "    logger.info(f\"‚öô CPU cores: {CORES}, RAM libre: {AVAIL_RAM:.1f} GB\")\n",
        "\n",
        "# 6) Modelos y utilitarios\n",
        "def evaluate_metrics(y_true, y_pred):\n",
        "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
        "    mae  = np.mean(np.abs(y_true - y_pred))\n",
        "    mape = np.mean(np.abs((y_true - y_pred)/(y_true + 1e-5))) * 100\n",
        "    r2   = 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "    return rmse, mae, mape, r2\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, Y, batch_size=32, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.X, self.Y = X.astype(np.float32), Y.astype(np.float32)\n",
        "        self.batch_size = batch_size\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.X) / self.batch_size))\n",
        "    def __getitem__(self, idx):\n",
        "        sl = slice(idx*self.batch_size, (idx+1)*self.batch_size)\n",
        "        return self.X[sl], self.Y[sl]\n",
        "\n",
        "def build_gru_ed(input_shape, horizon, n_cells, latent=128, dropout=DROPOUT):\n",
        "    inp = Input(shape=input_shape)\n",
        "    x = GRU(latent, dropout=dropout)(inp)\n",
        "    x = RepeatVector(horizon)(x)\n",
        "    x = GRU(latent, dropout=dropout, return_sequences=True)(x)\n",
        "    out = TimeDistributed(Dense(n_cells))(x)\n",
        "    m = Model(inp, out)\n",
        "    m.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    return m\n",
        "\n",
        "def build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                         head_size=64, num_heads=4, ff_dim=256, dropout=0.1):\n",
        "    inp = Input(shape=input_shape)\n",
        "    attn = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inp, inp)\n",
        "    x = Add()([inp, attn])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    ff = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = Dense(input_shape[-1])(ff)\n",
        "    x = Add()([x, ff])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(horizon * n_cells)(x)\n",
        "    out = tf.reshape(x, (-1, horizon, n_cells))\n",
        "    m = Model(inp, out)\n",
        "    m.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "    return m\n",
        "\n",
        "def build_gru_ed_low(input_shape, horizon, n_cells,\n",
        "                     latent=256, dropout=0.1, use_transformer=True):\n",
        "    if use_transformer:\n",
        "        try:\n",
        "            return build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                                        head_size=64, num_heads=4,\n",
        "                                        ff_dim=512, dropout=dropout)\n",
        "        except tf.errors.ResourceExhaustedError:\n",
        "            logger.warning(\"OOM Transformer ‚Üí usando GRU‚ÄêED para low-branch\")\n",
        "    return build_gru_ed(input_shape, horizon, n_cells,\n",
        "                        latent=latent, dropout=dropout)\n",
        "\n",
        "def build_gru_ed_medium_high(input_shape, horizon, n_cells, latent=128, dropout=0.1, use_transformer=True):\n",
        "    \"\"\"\n",
        "    Crea un modelo GRU Encoder-Decoder o Transformer Encoder-Decoder para medium y high.\n",
        "    Si hay error de memoria con el Transformer, usa GRU-ED.\n",
        "    \"\"\"\n",
        "    from tensorflow.keras.layers import (\n",
        "        Input, GRU, RepeatVector, TimeDistributed, Dense,\n",
        "        MultiHeadAttention, Add, LayerNormalization, Flatten\n",
        "    )\n",
        "    from tensorflow.keras.models import Model\n",
        "    import tensorflow as tf\n",
        "\n",
        "    def build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                             head_size=64, num_heads=4, ff_dim=256, dropout=0.1):\n",
        "        inp = Input(shape=input_shape)\n",
        "        attn = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inp, inp)\n",
        "        x = Add()([inp, attn])\n",
        "        x = LayerNormalization(epsilon=1e-6)(x)\n",
        "        ff = Dense(ff_dim, activation=\"relu\")(x)\n",
        "        ff = Dense(input_shape[-1])(ff)\n",
        "        x = Add()([x, ff])\n",
        "        x = LayerNormalization(epsilon=1e-6)(x)\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(horizon * n_cells)(x)\n",
        "        out = tf.reshape(x, (-1, horizon, n_cells))\n",
        "        m = Model(inp, out)\n",
        "        m.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "        return m\n",
        "\n",
        "    def build_gru_ed(input_shape, horizon, n_cells, latent=128, dropout=0.1):\n",
        "        inp = Input(shape=input_shape)\n",
        "        x = GRU(latent, dropout=dropout)(inp)\n",
        "        x = RepeatVector(horizon)(x)\n",
        "        x = GRU(latent, dropout=dropout, return_sequences=True)(x)\n",
        "        out = TimeDistributed(Dense(n_cells))(x)\n",
        "        m = Model(inp, out)\n",
        "        m.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "        return m\n",
        "\n",
        "    try:\n",
        "        if use_transformer:\n",
        "            return build_transformer_ed(input_shape, horizon, n_cells,\n",
        "                                        head_size=64, num_heads=4,\n",
        "                                        ff_dim=512, dropout=dropout)\n",
        "        else:\n",
        "            return build_gru_ed(input_shape, horizon, n_cells,\n",
        "                                latent=latent, dropout=dropout)\n",
        "    except tf.errors.ResourceExhaustedError:\n",
        "        print(\"‚ö†Ô∏è OOM Transformer ‚Üí usando GRU‚ÄêED para medium/high\")\n",
        "        return build_gru_ed(input_shape, horizon, n_cells,\n",
        "                            latent=latent, dropout=dropout)\n",
        "\n",
        "# 7) Carga datos y shapefile\n",
        "logger.info(\"üìÇ Cargando datasets‚Ä¶\")\n",
        "ds_full = xr.open_dataset(FULL_NC)\n",
        "ds_feat = xr.open_dataset(FEATURES_NC)\n",
        "boyaca_gdf = gpd.read_file(BOYACA_SHP)\n",
        "if boyaca_gdf.crs is None:\n",
        "    boyaca_gdf.set_crs(epsg=4326, inplace=True)\n",
        "else:\n",
        "    boyaca_gdf = boyaca_gdf.to_crs(epsg=4326)\n",
        "\n",
        "times      = ds_full.time.values.astype(\"datetime64[M]\")\n",
        "user_ref   = np.datetime64(REF_DATE, \"M\")\n",
        "last_avail = times[-1]\n",
        "if user_ref > last_avail:\n",
        "    ref = last_avail\n",
        "    logger.info(f\"REF_DATE fuera de rango; usando √∫ltimo mes: {ref}\")\n",
        "else:\n",
        "    ref = user_ref\n",
        "\n",
        "# fechas expl√≠citas\n",
        "val_dates = [\n",
        "    str(ref),\n",
        "    str((ref - np.timedelta64(1,'M')).astype(\"datetime64[M]\")),\n",
        "    str((ref - np.timedelta64(2,'M')).astype(\"datetime64[M]\"))\n",
        "]\n",
        "fc_dates  = [\n",
        "    str((ref + np.timedelta64(i+1,'M')).astype(\"datetime64[M]\"))\n",
        "    for i in range(OUTPUT_HORIZON)\n",
        "]\n",
        "\n",
        "idx_ref = int(np.where(times == ref)[0][0])\n",
        "lat     = ds_full.latitude.values\n",
        "lon     = ds_full.longitude.values\n",
        "METHODS   = [\"CEEMDAN\",\"TVFEMD\",\"FUSION\"]\n",
        "BRANCHES  = [\"high\",\"medium\",\"low\"]\n",
        "\n",
        "all_metrics = []\n",
        "preds_store = {}\n",
        "true_store  = {}\n",
        "histories   = {}\n",
        "\n",
        "# callbacks\n",
        "es_cb = callbacks.EarlyStopping(\"val_loss\", patience=PATIENCE_ES, restore_best_weights=True)\n",
        "lr_cb = callbacks.ReduceLROnPlateau(\"val_loss\", factor=LR_FACTOR, patience=LR_PATIENCE, min_lr=1e-6)\n",
        "\n",
        "# 8) Bucle principal\n",
        "for method in METHODS:\n",
        "    for branch in BRANCHES:\n",
        "        name = f\"{method}_{branch}\"\n",
        "        if name not in ds_feat.data_vars:\n",
        "            logger.warning(f\"‚ö† {name} no existe, salto.\")\n",
        "            continue\n",
        "        logger.info(f\"‚ñ∂ Procesando {name}\")\n",
        "        try:\n",
        "            # extraer y aplanar\n",
        "            Xarr = ds_feat[name].values            # (T, ny, nx)\n",
        "            Yarr = ds_full[\"total_precipitation\"].values  # (T, ny, nx)\n",
        "            T, ny, nx = Xarr.shape\n",
        "            n_cells   = ny * nx\n",
        "\n",
        "            Xfull = Xarr.reshape(T, n_cells)\n",
        "            yfull = Yarr.reshape(T, n_cells)\n",
        "\n",
        "            # ventanas\n",
        "            Nw = T - INPUT_WINDOW - OUTPUT_HORIZON + 1\n",
        "            if Nw <= 0:\n",
        "                logger.warning(\"‚ùå Ventanas insuficientes.\")\n",
        "                continue\n",
        "\n",
        "            Xs = np.stack([Xfull[i : i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "            ys = np.stack([yfull[i+INPUT_WINDOW : i+INPUT_WINDOW+OUTPUT_HORIZON]\n",
        "                           for i in range(Nw)], axis=0)\n",
        "\n",
        "            # sin/cos para low\n",
        "            if branch == \"low\":\n",
        "                months = pd.to_datetime(ds_full.time.values).month.values\n",
        "                s = np.sin(2 * np.pi * months/12)\n",
        "                c = np.cos(2 * np.pi * months/12)\n",
        "                Ss = np.stack([s[i : i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                Cs = np.stack([c[i : i+INPUT_WINDOW] for i in range(Nw)], axis=0)\n",
        "                Ss = np.repeat(Ss[:,:,None], n_cells, axis=2)\n",
        "                Cs = np.repeat(Cs[:,:,None], n_cells, axis=2)\n",
        "                Xs = np.concatenate([Xs, Ss, Cs], axis=2)\n",
        "                n_feats = Xs.shape[2]\n",
        "            else:\n",
        "                n_feats = n_cells\n",
        "\n",
        "            # escalado\n",
        "            scX = StandardScaler().fit(Xs.reshape(-1, n_feats))\n",
        "            scY = StandardScaler().fit(ys.reshape(-1, n_cells))\n",
        "            Xs_s = scX.transform(Xs.reshape(-1, n_feats)).reshape(Xs.shape)\n",
        "            ys_s = scY.transform(ys.reshape(-1, n_cells)).reshape(ys.shape)\n",
        "\n",
        "            # partici√≥n centrada en REF_DATE\n",
        "            k_ref = np.clip(idx_ref - INPUT_WINDOW + 1, 0, Nw-1)\n",
        "            i0    = np.clip(k_ref - (OUTPUT_HORIZON-1), 0, Nw-OUTPUT_HORIZON)\n",
        "\n",
        "            X_tr, y_tr = Xs_s[:i0], ys_s[:i0]\n",
        "            X_va, y_va = Xs_s[i0 : i0+OUTPUT_HORIZON], ys_s[i0 : i0+OUTPUT_HORIZON]\n",
        "\n",
        "            # cargar/entrenar\n",
        "            model_path = MODEL_DIR/f\"{name}_w{OUTPUT_HORIZON}_ref{ref}.keras\"\n",
        "            if model_path.exists():\n",
        "                model = tf.keras.models.load_model(str(model_path), compile=False)\n",
        "                model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "                logger.info(f\"‚è© Cargado modelo: {model_path.name}\")\n",
        "            else:\n",
        "                if branch == \"low\":\n",
        "                    model = build_gru_ed_low((INPUT_WINDOW, n_feats), OUTPUT_HORIZON, n_cells)\n",
        "                else:\n",
        "                    model = build_gru_ed_medium_high((INPUT_WINDOW, n_feats), OUTPUT_HORIZON, n_cells)\n",
        "\n",
        "                hist = model.fit(\n",
        "                    DataGenerator(X_tr, y_tr),\n",
        "                    validation_data=DataGenerator(X_va, y_va),\n",
        "                    epochs=MAX_EPOCHS,\n",
        "                    callbacks=[es_cb, lr_cb],\n",
        "                    verbose=1\n",
        "                )\n",
        "                model.save(str(model_path))\n",
        "                histories[name] = hist.history\n",
        "\n",
        "            # validaci√≥n H=1..H\n",
        "            preds = model.predict(X_va, verbose=0).reshape(OUTPUT_HORIZON, OUTPUT_HORIZON, n_cells)\n",
        "            for h in range(OUTPUT_HORIZON):\n",
        "                date_val = val_dates[h]\n",
        "                pm_flat  = preds[h,0]\n",
        "                tm_flat  = y_va[h,0]\n",
        "                pm = scY.inverse_transform(pm_flat.reshape(1,-1))[0].reshape(ny,nx)\n",
        "                tm = scY.inverse_transform(tm_flat.reshape(1,-1))[0].reshape(ny,nx)\n",
        "                rmse, mae, mape, r2 = evaluate_metrics(tm.ravel(), pm.ravel())\n",
        "                all_metrics.append({\n",
        "                    \"model\": name, \"branch\": branch, \"horizon\": h+1,\n",
        "                    \"type\":\"validation\", \"date\": date_val,\n",
        "                    \"RMSE\": rmse, \"MAE\": mae, \"MAPE\": mape, \"R2\": r2\n",
        "                })\n",
        "                preds_store[(name,date_val)] = pm\n",
        "                true_store[(name,date_val)]  = tm\n",
        "\n",
        "            # forecast\n",
        "            X_fc = Xs_s[k_ref : k_ref+1]\n",
        "            fc_s = model.predict(X_fc, verbose=0)[0]\n",
        "            FC   = scY.inverse_transform(fc_s)\n",
        "            for h in range(OUTPUT_HORIZON):\n",
        "                date_fc = fc_dates[h]\n",
        "                all_metrics.append({\n",
        "                    \"model\": name, \"branch\": branch, \"horizon\": h+1,\n",
        "                    \"type\":\"forecast\", \"date\": date_fc,\n",
        "                    \"RMSE\": np.nan, \"MAE\": np.nan, \"MAPE\": np.nan, \"R2\": np.nan\n",
        "                })\n",
        "                preds_store[(name,date_fc)] = FC[h].reshape(ny,nx)\n",
        "\n",
        "        except Exception:\n",
        "            logger.exception(f\"‚Äº Error en {name}, continuo‚Ä¶\")\n",
        "            continue\n",
        "\n",
        "# 9) Guardar m√©tricas y mostrar tabla\n",
        "dfm = pd.DataFrame(all_metrics)\n",
        "dfm.to_csv(RESULTS_CSV, index=False)\n",
        "import ace_tools_open as tools\n",
        "tools.display_dataframe_to_user(name=f\"Metrics_w{OUTPUT_HORIZON}_ref{ref}\", dataframe=dfm)\n",
        "\n",
        "# 10) Curvas de entrenamiento\n",
        "for name, hist in histories.items():\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(hist[\"loss\"],  label=\"train\")\n",
        "    plt.plot(hist[\"val_loss\"],label=\"val\")\n",
        "    plt.title(f\"Loss curve: {name}\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE\")\n",
        "    plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "# 10bis) True vs Predicted por rama y horizonte\n",
        "for branch in BRANCHES:\n",
        "    for h in range(1, OUTPUT_HORIZON+1):\n",
        "        plt.figure(figsize=(5,5))\n",
        "        for method in METHODS:\n",
        "            key = f\"{method}_{branch}\"\n",
        "            date_val = val_dates[h-1]\n",
        "            if (key, date_val) in preds_store and (key, date_val) in true_store:\n",
        "                y_true = true_store[(key, date_val)].ravel()\n",
        "                y_pred = preds_store[(key, date_val)].ravel()\n",
        "                plt.scatter(y_true, y_pred, alpha=0.3, s=2, label=method)\n",
        "        lims = [0, max(plt.xlim()[1], plt.ylim()[1])]\n",
        "        plt.plot(lims, lims, 'k--')\n",
        "        plt.xlabel(\"True\"); plt.ylabel(\"Predicted\")\n",
        "        plt.title(f\"True vs Pred ‚Äî {branch}, H={h}\")\n",
        "        plt.legend(); plt.tight_layout(); plt.show()\n",
        "\n",
        "# 11) Mapas 3√ó3 validaci√≥n H=1\n",
        "xmin, ymin, xmax, ymax = boyaca_gdf.total_bounds\n",
        "for date_val in val_dates:\n",
        "    arrs = [preds_store[(f\"{m}_{b}\",date_val)].ravel()\n",
        "            for m in METHODS for b in BRANCHES\n",
        "            if (f\"{m}_{b}\",date_val) in preds_store]\n",
        "    if not arrs:\n",
        "        logger.warning(f\"No hay predicciones para {date_val}, salto plot.\")\n",
        "        continue\n",
        "    vmin, vmax = np.min(arrs), np.max(arrs)\n",
        "    fig, axs = plt.subplots(3,3, figsize=(12,12), subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
        "    fig.suptitle(f\"Validaci√≥n H=1 ‚Äî {date_val}\", fontsize=16)\n",
        "    for i, b in enumerate(BRANCHES):\n",
        "        for j, m in enumerate(METHODS):\n",
        "            ax = axs[i,j]\n",
        "            ax.set_extent([xmin, xmax, ymin, ymax], ccrs.PlateCarree())\n",
        "            ax.add_geometries(boyaca_gdf.geometry, ccrs.PlateCarree(),\n",
        "                              edgecolor=\"black\", facecolor=\"none\", linewidth=1)\n",
        "            key = (f\"{m}_{b}\", date_val)\n",
        "            if key in preds_store:\n",
        "                pcm = ax.pcolormesh(lon, lat, preds_store[key],\n",
        "                                    vmin=vmin, vmax=vmax,\n",
        "                                    transform=ccrs.PlateCarree(), cmap=\"Blues\")\n",
        "            ax.set_title(f\"{m}_{b}\")\n",
        "    fig.colorbar(pcm, ax=axs, orientation=\"horizontal\",\n",
        "                 fraction=0.05, pad=0.04, label=\"Precipitaci√≥n (mm)\")\n",
        "    fig.savefig(IMAGE_DIR/f\"val_H1_{date_val}.png\", dpi=150); plt.show()\n",
        "\n",
        "    arrs_mape = [\n",
        "        np.clip(np.abs((true_store[k] - preds_store[k])/(true_store[k]+1e-5))*100,0,200).ravel()\n",
        "        for k in preds_store if k[1]==date_val and k in true_store\n",
        "    ]\n",
        "    if not arrs_mape: continue\n",
        "    vmin2, vmax2 = 0, np.max(arrs_mape)\n",
        "    fig, axs = plt.subplots(3,3, figsize=(12,12), subplot_kw={\"projection\":ccrs.PlateCarree()})\n",
        "    fig.suptitle(f\"MAPE H=1 ‚Äî {date_val}\", fontsize=16)\n",
        "    for i, b in enumerate(BRANCHES):\n",
        "        for j, m in enumerate(METHODS):\n",
        "            ax = axs[i,j]\n",
        "            ax.set_extent([xmin, xmax, ymin, ymax], ccrs.PlateCarree())\n",
        "            ax.add_geometries(boyaca_gdf.geometry, ccrs.PlateCarree(),\n",
        "                              edgecolor=\"black\", facecolor=\"none\", linewidth=1)\n",
        "            key = (f\"{m}_{b}\", date_val)\n",
        "            if key in preds_store and key in true_store:\n",
        "                mmap = np.clip(np.abs((true_store[key] - preds_store[key])/(true_store[key]+1e-5))*100,0,200)\n",
        "                pcm2 = ax.pcolormesh(lon, lat, mmap,\n",
        "                                     vmin=vmin2, vmax=vmax2,\n",
        "                                     transform=ccrs.PlateCarree(), cmap=\"Reds\")\n",
        "            ax.set_title(f\"{m}_{b}\")\n",
        "    fig.colorbar(pcm2, ax=axs, orientation=\"horizontal\",\n",
        "                 fraction=0.05, pad=0.04, label=\"MAPE (%)\")\n",
        "    fig.savefig(IMAGE_DIR/f\"mape_H1_{date_val}.png\", dpi=150); plt.show()\n",
        "\n",
        "# 13) Meta‚Äêmodelo XGBoost (stacking low H=1)\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_meta, y_meta = [], []\n",
        "for date_val in val_dates:\n",
        "    keys = [(f\"{m}_low\", date_val) for m in METHODS]\n",
        "    if all(k in preds_store for k in keys) and keys[0] in true_store:\n",
        "        arrs = [preds_store[k].ravel() for k in keys]\n",
        "        X_meta.append(np.vstack(arrs).T)\n",
        "        y_meta.append(true_store[keys[0]].ravel())\n",
        "\n",
        "if X_meta:\n",
        "    Xm = np.concatenate(X_meta, axis=0)\n",
        "    ym = np.concatenate(y_meta, axis=0)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(Xm, ym, test_size=0.2, random_state=42)\n",
        "\n",
        "    xgb = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=4, n_jobs=-1, verbosity=0)\n",
        "    xgb.fit(Xtr, ytr)\n",
        "\n",
        "    # exportar modelo\n",
        "    xgb.save_model(str(MODEL_DIR/f\"xgb_low_H1_{ref}.json\"))\n",
        "\n",
        "    yhat = xgb.predict(Xte)\n",
        "    rmse_meta = np.sqrt(mean_squared_error(yte, yhat))\n",
        "    logger.info(f\"Meta‚Äêmodelo XGB (low, H=1) RMSE: {rmse_meta:.3f}\")\n",
        "\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.scatter(yte, yhat, alpha=0.3, s=2)\n",
        "    lims = [min(yte.min(), yhat.min()), max(yte.max(), yhat.max())]\n",
        "    plt.plot(lims, lims, 'k--')\n",
        "    plt.xlabel(\"True\"); plt.ylabel(\"Predicted\")\n",
        "    plt.title(\"Meta‚Äêmodelo XGB (low, H=1)\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "    print(f\"Modelo XGB exportado en: {MODEL_DIR/f'xgb_low_H1_{ref}.json'}\")\n",
        "\n",
        "    # 13bis) Tabla de m√©tricas Meta‚Äêmodelo por mes\n",
        "    meta_metrics = []\n",
        "    for i, date_val in enumerate(val_dates):\n",
        "        # construir X_meta_i, y_meta_i\n",
        "        X_mi, y_mi = [], []\n",
        "        for method in METHODS:\n",
        "            key = (f\"{method}_low\", date_val)\n",
        "            if key in preds_store and key in true_store:\n",
        "                X_mi.append(preds_store[key].ravel())\n",
        "        if not X_mi:\n",
        "            continue\n",
        "        X_mi = np.vstack(X_mi).T\n",
        "        y_mi = true_store[(f\"{METHODS[0]}_low\", date_val)].ravel()\n",
        "        yhat_i = xgb.predict(X_mi)\n",
        "        rm, ma, maP, r_ = evaluate_metrics(y_mi, yhat_i)\n",
        "        meta_metrics.append({\n",
        "            \"horizon\": i+1, \"date\": date_val,\n",
        "            \"RMSE\": rm, \"MAE\": ma, \"MAPE\": maP, \"R2\": r_\n",
        "        })\n",
        "    df_meta = pd.DataFrame(meta_metrics)\n",
        "    display(df_meta)\n",
        "    df_meta.to_csv(MODEL_DIR/f\"meta_metrics_w{OUTPUT_HORIZON}_ref{ref}.csv\", index=False)\n",
        "else: \n",
        "    logger.warning(\"‚ö† No hay muestras para entrenar el meta‚Äêmodelo XGB.\")\n",
        "\n",
        "logger.info(\"üèÅ Proceso completo.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "precipitation_prediction",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
